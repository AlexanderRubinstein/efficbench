{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d6b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e0e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchs = ['lb', 'mmlu', 'helm_lite', 'alpaca']\n",
    "number_item = 100\n",
    "method = 'anchor-irt'\n",
    "tinyBenchmarks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e445fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lb 0.01906167653388267 [0.01906168 0.01998235 0.02033831 0.0202809  0.02015149]\n",
      "mmlu 0.024398970156813124 [0.02728616 0.02829303 0.03106703 0.02439897 0.03167772]\n",
      "helm_lite 0.0242250825876071 [0.02748594 0.02689979 0.02783222 0.02589602 0.02422508]\n",
      "alpaca 0.01119042421102733 [0.0116694  0.01212778 0.01119042 0.01195137 0.01160708]\n"
     ]
    }
   ],
   "source": [
    "for bench in benchs:\n",
    "    with open(f'results/samples_{bench}_iterations-5.pickle', 'rb') as handle:\n",
    "        sample_data = pickle.load(handle)\n",
    "\n",
    "    scenarios = list(sample_data['scenarios_position'].keys())\n",
    "\n",
    "    with open(f'results/results_{bench}_split-iid_iterations-5.pickle', 'rb') as handle:\n",
    "        results = pickle.load(handle)\n",
    "        avg_error = np.mean([np.mean([results[it][number_item][method+\"_naive\"][scenario] for it in results.keys()],axis=0) for scenario in scenarios], axis=0)\n",
    "    \n",
    "    print(bench,np.min(avg_error),avg_error)\n",
    "    best_it = np.argmin(avg_error)\n",
    "    \n",
    "    optimal_lambdas = {}\n",
    "    for scenario in scenarios:\n",
    "        optimal_lambdas[scenario] = sample_data['opt_lambds'][method+\"_gpirt\"][scenario][number_item]\n",
    "    \n",
    "    tinyBenchmarks[bench] = {'seen_examples':sample_data['seen_items'][method][number_item][best_it],\n",
    "                             'examples_weights':sample_data['item_weights'][method][number_item][best_it],\n",
    "                             'irt_parameters':{\"A\":sample_data[\"A\"], \"B\":sample_data[\"B\"]},\n",
    "                             'scenarios_position':sample_data['scenarios_position'],\n",
    "                             'subscenarios_position':sample_data['subscenarios_position'],\n",
    "                             'optimal_lambdas':optimal_lambdas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d28287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tinyBenchmarks.pkl', 'wb') as f:\n",
    "    pickle.dump(tinyBenchmarks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6453b",
   "metadata": {},
   "source": [
    "Checking invidivual performances of different benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d7a3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['truthfulqa', 'gsm8k', 'winogrande', 'arc', 'hellaswag', 'mmlu'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinyBenchmarks['lb']['scenarios_position'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed9196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/felipemaiapolo/tinyBenchmarks\n",
      "  Cloning https://github.com/felipemaiapolo/tinyBenchmarks to /tmp/pip-req-build-42evdu31\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/felipemaiapolo/tinyBenchmarks /tmp/pip-req-build-42evdu31\n",
      "  Resolved https://github.com/felipemaiapolo/tinyBenchmarks to commit 7bd5ac75fee01d7ac0b55f9b8198abb443f6a316\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy (from tinyBenchmarks==1.0.0)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy (from tinyBenchmarks==1.0.0)\n",
      "  Using cached scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting requests (from tinyBenchmarks==1.0.0)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->tinyBenchmarks==1.0.0)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->tinyBenchmarks==1.0.0)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->tinyBenchmarks==1.0.0)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->tinyBenchmarks==1.0.0)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Building wheels for collected packages: tinyBenchmarks\n",
      "  Building wheel for tinyBenchmarks (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinyBenchmarks: filename=tinyBenchmarks-1.0.0-py3-none-any.whl size=5562 sha256=90396eaada1f62411dcf413851785cc480fc75198a6547d68f41236e5b18ad7c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ldzsk2yh/wheels/0f/0f/46/9477a640e48dd8de63caaba2183eebc868510e7654e2154a60\n",
      "Successfully built tinyBenchmarks\n",
      "Installing collected packages: urllib3, numpy, idna, charset-normalizer, certifi, scipy, requests, tinyBenchmarks\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "arviz 0.16.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "gradio-client 0.8.0 requires typing-extensions~=4.0, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.6 numpy-1.26.4 requests-2.31.0 scipy-1.12.0 tinyBenchmarks-1.0.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --ignore-installed git+https://github.com/felipemaiapolo/tinyBenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e19f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinyBenchmarks as tb\n",
    "import multiprocessing as mp\n",
    "from run_experiment import get_data\n",
    "from utils import prepare_and_split_data\n",
    "\n",
    "def array_to_markdown_table(avg, std, headers, row_names):\n",
    "    \"\"\"\n",
    "    Converts a NumPy array into a Markdown table.\n",
    "    \n",
    "    Parameters:\n",
    "        array (np.array): The NumPy array to be converted.\n",
    "        headers (list): A list of strings for the table headers.\n",
    "        row_names (list): A list of strings for the row names.\n",
    "    \n",
    "    Returns:\n",
    "        str: A string formatted as a Markdown table.\n",
    "    \"\"\"\n",
    "    # Start with the header\n",
    "    markdown_table = \"|| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    # Add the separator\n",
    "    markdown_table += \"|--|-\" + \"-|-\".join([\"\" for _ in headers]) + \"-|\\n\"\n",
    "    \n",
    "    # Add each row of the array to the table\n",
    "    i=0\n",
    "    for row_name, row_values in zip(row_names, avg):\n",
    "        row_str = [f\"{row_values[j]:.3f} ({std[i][j]:.3f})\" for j in range(len(row_values))]  # Format numbers to 3 decimal places\n",
    "        markdown_table += f\"| {row_name} | \" + \" | \".join(row_str) + \" |\\n\"\n",
    "        i+=1\n",
    "    return markdown_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eced1231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| truthfulqa | 0.013 (0.010) | 0.016 (0.013) | 0.011 (0.009) |\n",
      "| gsm8k | 0.022 (0.017) | 0.022 (0.017) | 0.020 (0.015) |\n",
      "| winogrande | 0.022 (0.017) | 0.011 (0.013) | 0.011 (0.011) |\n",
      "| arc | 0.022 (0.018) | 0.012 (0.010) | 0.010 (0.009) |\n",
      "| hellaswag | 0.013 (0.016) | 0.011 (0.020) | 0.011 (0.018) |\n",
      "| mmlu | 0.024 (0.018) | 0.017 (0.017) | 0.015 (0.015) |\n",
      "\n",
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| mmlu | 0.024 (0.017) | 0.016 (0.015) | 0.016 (0.015) |\n",
      "\n",
      "25 100\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| alpaca_v2 | 0.012 (0.015) | 0.020 (0.021) | 0.016 (0.016) |\n",
      "\n",
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| truthfulqa | 0.013 (0.010) | 0.010 (0.009) | 0.011 (0.009) |\n",
      "\n",
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| gsm8k | 0.022 (0.017) | 0.029 (0.022) | 0.020 (0.017) |\n",
      "\n",
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| winogrande | 0.022 (0.017) | 0.016 (0.014) | 0.015 (0.013) |\n",
      "\n",
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| arc | 0.022 (0.018) | 0.017 (0.014) | 0.017 (0.013) |\n",
      "\n",
      "99 395\n",
      "|| IRT | p-IRT | gp-IRT |\n",
      "|--|--|--|--|\n",
      "| hellaswag | 0.013 (0.016) | 0.015 (0.012) | 0.015 (0.012) |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bench in ['lb','mmlu','alpaca','truthfulqa', 'gsm8k', 'winogrande', 'arc', 'hellaswag']:\n",
    "    ###\n",
    "    number_of_examples = 100\n",
    "    lb_scenarios = ['truthfulqa', 'gsm8k', 'winogrande', 'arc', 'hellaswag']\n",
    "    benchs = ['lb', 'mmlu', 'helm_lite', 'alpaca']\n",
    "    if bench in lb_scenarios: bench_name = 'lb'\n",
    "    else: bench_name = bench\n",
    "\n",
    "    data, scenarios, set_of_rows = get_data(bench_name, split='iid')\n",
    "    test_rows = set_of_rows[0]\n",
    "    scores_train, scores_test, balance_weights, scenarios_position, subscenarios_position = prepare_and_split_data(scenarios, scenarios, data, test_rows)\n",
    "\n",
    "    seen_examples = tinyBenchmarks[bench_name]['seen_examples']\n",
    "    scenarios_position = tinyBenchmarks[bench_name]['scenarios_position']\n",
    "\n",
    "    if bench not in benchs:\n",
    "        scenarios = [bench]\n",
    "        ind_scenario = number_of_examples*([i for i,s in enumerate(scenarios_position.keys()) if s==bench][0])\n",
    "        seen_examples = seen_examples[ind_scenario:ind_scenario+number_of_examples]\n",
    "    else:\n",
    "        scenarios = list(scenarios_position.keys())\n",
    "\n",
    "    ###\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    accs = pool.starmap(tb.evaluate, [(scores_test[llm,seen_examples], bench) for llm in range(scores_test.shape[0])])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    estimates = np.array([[[acc[scenario]['irt'], acc[scenario]['pirt'], acc[scenario]['gpirt']] for scenario in scenarios] for acc in accs])\n",
    "    true_accs = np.array([(balance_weights*scores_test)[:,scenarios_position[scenario]].mean(axis=1) for scenario in scenarios]).T[:,:,None]\n",
    "    errors = np.abs(estimates-true_accs)\n",
    "\n",
    "    markdown_table = array_to_markdown_table(errors.mean(axis=0), errors.std(axis=0), [\"IRT\", \"p-IRT\", \"gp-IRT\"], scenarios)\n",
    "    print(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439354a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
