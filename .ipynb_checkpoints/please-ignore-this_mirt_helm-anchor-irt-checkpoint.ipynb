{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952aeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "from experiments import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491d564",
   "metadata": {},
   "source": [
    "Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad64dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {'cnn':['summarization_cnndm:temperature=0.3,device=cuda,'], \n",
    "             'xsum':['summarization_xsum:temperature=0.3,device=cuda,'], \n",
    "             'boolq:':['boolq:'],\n",
    "             'civil_comments':['civil_comments:demographic=LGBTQ,',\n",
    "                               'civil_comments:demographic=all,',\n",
    "                               'civil_comments:demographic=black,',\n",
    "                               'civil_comments:demographic=christian,',\n",
    "                               'civil_comments:demographic=female,',\n",
    "                               'civil_comments:demographic=male,',\n",
    "                               'civil_comments:demographic=muslim,',\n",
    "                               'civil_comments:demographic=other_religions,',\n",
    "                               'civil_comments:demographic=white,'],\n",
    "             'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':['commonsense:dataset=hellaswag,method=multiple_choice_separate_original,'],\n",
    "             'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':['commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,'],\n",
    "             'imdb:':['imdb:'],\n",
    "             'mmlu':['mmlu:subject=abstract_algebra,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=college_chemistry,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=computer_security,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=econometrics,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=us_foreign_policy,method=multiple_choice_joint,'],\n",
    "             'msmarco:track=regular,valid_topk=30,':['msmarco:track=regular,valid_topk=30,'],\n",
    "             #'msmarco:track=trec,valid_topk=30,':['msmarco:track=trec,valid_topk=30,'],\n",
    "             'narrative_qa:':['narrative_qa:'],\n",
    "             'natural_qa:mode=closedbook,':['natural_qa:mode=closedbook,'],\n",
    "             'natural_qa:mode=openbook_longans,':['natural_qa:mode=openbook_longans,'],\n",
    "             'quac:':['quac:'],\n",
    "             'raft':['raft:subset=ade_corpus_v2,',\n",
    "                     'raft:subset=banking_77,',\n",
    "                     'raft:subset=neurips_impact_statement_risks,',\n",
    "                     'raft:subset=one_stop_english,',\n",
    "                     'raft:subset=overruling,',\n",
    "                     'raft:subset=semiconductor_org_types,',\n",
    "                     'raft:subset=systematic_review_inclusion,',\n",
    "                     'raft:subset=tai_safety_research,',\n",
    "                     'raft:subset=terms_of_service,',\n",
    "                     'raft:subset=tweet_eval_hate,',\n",
    "                     'raft:subset=twitter_complaints,'],\n",
    "             'truthful_qa:task=mc_single,method=multiple_choice_joint,':['truthful_qa:task=mc_single,method=multiple_choice_joint,']}\n",
    "            \n",
    "scenarios_metrics = {'boolq:':'em', \n",
    "                     'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':'em',\n",
    "                     'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':'em',\n",
    "                     'imdb:':'em', \n",
    "                     'mmlu':'em', \n",
    "                     'msmarco:track=regular,valid_topk=30,':'RR@10', \n",
    "                     'msmarco:track=trec,valid_topk=30,':'NDCG@10', \n",
    "                     'narrative_qa:':'f1', \n",
    "                     'natural_qa:mode=closedbook,':'f1', \n",
    "                     'natural_qa:mode=openbook_longans,':'f1', \n",
    "                     'quac:':'f1', \n",
    "                     'raft':'em', \n",
    "                     'truthful_qa:task=mc_single,method=multiple_choice_joint,':'em'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14725d5d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e324e4",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b24d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/helm.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797477e3",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ca743e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [5, 6, 7, 8, 9, 10, 11],\n",
       " [4, 12, 13],\n",
       " [14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
       " [23, 24, 25, 26, 27]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "iterations = 10\n",
    "Ds = [5, 10, 15, 20, 25] #\n",
    "\n",
    "num_elements = 4\n",
    "set_of_rows = [[0,1,2,3], #ai21\n",
    "               [5,6,7,8,9,10,11], #cohere\n",
    "               [4,12,13], #anthropic+microsoft\n",
    "               [14,15,16,17,18,19,20,21,22], #openai\n",
    "               [23,24,25,26,27]] #together\n",
    "set_of_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c6fd5",
   "metadata": {},
   "source": [
    "### Predicting accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451897a",
   "metadata": {},
   "source": [
    "Full (one IRT model for all scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5503788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models [0, 1, 2, 3]\n",
      "\n",
      "i) choosing optimal D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [10:08<00:00, 121.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opt D= 5 errors= [0.044579911983804656, 0.0453168327810513, 0.047692893828327226, 0.049759814361327995, 0.05001942216565462] \n",
      "\n",
      "\n",
      "ii) choosing optimal lambdas\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:40<00:00, 12.38it/s]\n",
      "100%|██████████████████████████████████████████| 15/15 [30:29<00:00, 121.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}, 'anchor_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}, 'anchor-irt_gpirt': {'cnn': {10: 1.0, 25: 0.44, 50: 0.22, 75: 0.33, 100: 0.33}, 'xsum': {10: 0.89, 25: 0.22, 50: 0.22, 75: 0.33, 100: 0.33}, 'boolq:': {10: 0.33, 25: 0.33, 50: 0.11, 75: 0.0, 100: 0.0}, 'civil_comments': {10: None, 25: 0.11, 50: 0.11, 75: None, 100: None}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.0, 25: None, 50: 0.0, 75: 0.0, 100: None}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.0, 25: 0.0, 50: 0.0, 75: 0.0, 100: 0.11}, 'imdb:': {10: 0.0, 25: 0.0, 50: 0.0, 75: 0.0, 100: 0.0}, 'mmlu': {10: 0.44, 25: 0.33, 50: 0.22, 75: 0.33, 100: 0.33}, 'msmarco:track=regular,valid_topk=30,': {10: 0.67, 25: 0.78, 50: 0.78, 75: 0.67, 100: 0.67}, 'narrative_qa:': {10: 0.44, 25: 0.56, 50: None, 75: None, 100: 0.11}, 'natural_qa:mode=closedbook,': {10: 0.0, 25: 0.11, 50: 0.11, 75: 0.11, 100: 0.22}, 'natural_qa:mode=openbook_longans,': {10: 0.44, 25: 0.22, 50: 0.56, 75: 0.44, 100: 0.56}, 'quac:': {10: 0.11, 25: 0.44, 50: 0.44, 75: 0.33, 100: 0.44}, 'raft': {10: 0.22, 25: 0.11, 50: 0.33, 75: 0.56, 100: 0.56}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.67, 25: 0.56, 50: 0.44, 75: 0.44, 100: 0.67}}, 'disc_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}}\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:02<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "v) running anchor points with IRT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [46:51<00:00, 562.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models [5, 6, 7, 8, 9, 10, 11]\n",
      "\n",
      "i) choosing optimal D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [09:42<00:00, 116.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opt D= 5 errors= [0.05381872300941188, 0.05182277303935221, 0.0543264667069326, 0.055148286293135375, 0.059695620278795394] \n",
      "\n",
      "\n",
      "ii) choosing optimal lambdas\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:30<00:00, 16.17it/s]\n",
      "100%|██████████████████████████████████████████| 15/15 [27:15<00:00, 109.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}, 'anchor_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}, 'anchor-irt_gpirt': {'cnn': {10: 1.0, 25: 0.56, 50: 0.44, 75: 0.56, 100: 0.44}, 'xsum': {10: 0.78, 25: 0.33, 50: 0.33, 75: 0.33, 100: 0.33}, 'boolq:': {10: 0.22, 25: 0.33, 50: 0.33, 75: 0.0, 100: 0.0}, 'civil_comments': {10: None, 25: None, 50: 0.11, 75: None, 100: None}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: None, 25: 0.0, 50: 0.0, 75: 0.11, 100: 0.11}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.0, 25: 0.11, 50: 0.0, 75: None, 100: None}, 'imdb:': {10: 0.0, 25: 0.0, 50: 0.0, 75: 0.0, 100: 0.0}, 'mmlu': {10: 0.44, 25: 0.22, 50: 0.22, 75: 0.22, 100: 0.44}, 'msmarco:track=regular,valid_topk=30,': {10: 0.44, 25: 0.67, 50: 0.67, 75: 0.78, 100: 0.78}, 'narrative_qa:': {10: 0.56, 25: 0.67, 50: 1.0, 75: 0.67, 100: None}, 'natural_qa:mode=closedbook,': {10: 0.0, 25: 0.22, 50: 0.11, 75: 0.11, 100: 0.22}, 'natural_qa:mode=openbook_longans,': {10: 0.44, 25: 0.56, 50: 0.44, 75: 0.44, 100: 0.44}, 'quac:': {10: 0.44, 25: 0.56, 50: 0.56, 75: 0.44, 100: 0.44}, 'raft': {10: 0.56, 25: 0.56, 50: 0.44, 75: 0.56, 100: 0.0}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.67, 25: 0.56, 50: 0.44, 75: 0.22, 100: 0.0}}, 'disc_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}}\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:48<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "v) running anchor points with IRT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 5/5 [1:18:33<00:00, 942.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models [4, 12, 13]\n",
      "\n",
      "i) choosing optimal D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [09:50<00:00, 118.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opt D= 10 errors= [0.04014776870666264, 0.03758734187511835, 0.03701439713524953, 0.03676330704824828, 0.040117398393908904] \n",
      "\n",
      "\n",
      "ii) choosing optimal lambdas\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [01:18<00:00,  6.41it/s]\n",
      "100%|██████████████████████████████████████████| 15/15 [30:28<00:00, 121.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}, 'anchor_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}, 'anchor-irt_gpirt': {'cnn': {10: 1.0, 25: 0.56, 50: 0.22, 75: 0.33, 100: 0.33}, 'xsum': {10: 0.44, 25: 0.22, 50: 0.22, 75: 0.33, 100: 0.33}, 'boolq:': {10: 0.11, 25: 0.56, 50: 0.22, 75: 0.0, 100: 0.0}, 'civil_comments': {10: None, 25: None, 50: 0.11, 75: 0.11, 100: 0.11}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.11, 25: 0.11, 50: 0.0, 75: 0.0, 100: None}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.0, 25: 0.22, 50: 0.0, 75: None, 100: 0.11}, 'imdb:': {10: 0.0, 25: 0.0, 50: None, 75: None, 100: None}, 'mmlu': {10: 0.33, 25: 0.44, 50: 0.22, 75: 0.22, 100: None}, 'msmarco:track=regular,valid_topk=30,': {10: 0.33, 25: 0.78, 50: 0.44, 75: 0.44, 100: 0.56}, 'narrative_qa:': {10: 0.33, 25: 0.56, 50: 0.22, 75: 0.0, 100: 0.0}, 'natural_qa:mode=closedbook,': {10: None, 25: 0.11, 50: 0.22, 75: 0.11, 100: 0.22}, 'natural_qa:mode=openbook_longans,': {10: 0.44, 25: 0.56, 50: 0.56, 75: 0.56, 100: 0.56}, 'quac:': {10: 0.22, 25: 0.33, 50: 0.22, 75: 0.44, 100: 0.44}, 'raft': {10: 0.11, 25: 0.44, 50: 0.67, 75: 0.67, 100: 0.56}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.44, 25: 0.78, 50: 0.56, 75: 0.22, 100: 0.22}}, 'disc_gpirt': {'cnn': {}, 'xsum': {}, 'boolq:': {}, 'civil_comments': {}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {}, 'imdb:': {}, 'mmlu': {}, 'msmarco:track=regular,valid_topk=30,': {}, 'narrative_qa:': {}, 'natural_qa:mode=closedbook,': {}, 'natural_qa:mode=openbook_longans,': {}, 'quac:': {}, 'raft': {}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {}}}\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [02:08<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "v) running anchor points with IRT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [36:04<00:00, 432.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models [14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "\n",
      "i) choosing optimal D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [09:46<00:00, 117.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opt D= 5 errors= [0.045810498090451464, 0.04502040451789275, 0.046899226917154604, 0.04578801553785239, 0.04541521518840881] \n",
      "\n",
      "\n",
      "ii) choosing optimal lambdas\n",
      " - debiasing IRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:23<00:00, 21.33it/s]\n",
      " 47%|████████████████████                       | 7/15 [14:08<14:18, 107.37s/it]"
     ]
    }
   ],
   "source": [
    "scenario_name = 'full' #we are evaluating all scenarios at once\n",
    "chosen_scenarios = list(scenarios.keys())\n",
    "results_full, accs_full = evaluate_scenarios(data, scenario_name, chosen_scenarios, scenarios, set_of_rows, Ds, iterations, device, bench='irt_helm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['anchor-irt_naive', 'anchor-irt_cirt', 'anchor-irt_pirt', 'anchor-irt_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46321c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {'boolq:':['boolq:'],\n",
    "             'civil_comments':['civil_comments:demographic=LGBTQ,',\n",
    "                               'civil_comments:demographic=all,',\n",
    "                               'civil_comments:demographic=black,',\n",
    "                               'civil_comments:demographic=christian,',\n",
    "                               'civil_comments:demographic=female,',\n",
    "                               'civil_comments:demographic=male,',\n",
    "                               'civil_comments:demographic=muslim,',\n",
    "                               'civil_comments:demographic=other_religions,',\n",
    "                               'civil_comments:demographic=white,'],\n",
    "             'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':['commonsense:dataset=hellaswag,method=multiple_choice_separate_original,'],\n",
    "             'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':['commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,'],\n",
    "             'imdb:':['imdb:'],\n",
    "             'mmlu':['mmlu:subject=abstract_algebra,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=college_chemistry,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=computer_security,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=econometrics,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=us_foreign_policy,method=multiple_choice_joint,'],\n",
    "             #'msmarco:track=regular,valid_topk=30,':['msmarco:track=regular,valid_topk=30,'],\n",
    "             #'msmarco:track=trec,valid_topk=30,':['msmarco:track=trec,valid_topk=30,'],\n",
    "             #'narrative_qa:':['narrative_qa:'],\n",
    "             #'natural_qa:mode=closedbook,':['natural_qa:mode=closedbook,'],\n",
    "             #'natural_qa:mode=openbook_longans,':['natural_qa:mode=openbook_longans,'],\n",
    "             #'quac:':['quac:'],\n",
    "             'raft':['raft:subset=ade_corpus_v2,',\n",
    "                     'raft:subset=banking_77,',\n",
    "                     'raft:subset=neurips_impact_statement_risks,',\n",
    "                     'raft:subset=one_stop_english,',\n",
    "                     'raft:subset=overruling,',\n",
    "                     'raft:subset=semiconductor_org_types,',\n",
    "                     'raft:subset=systematic_review_inclusion,',\n",
    "                     'raft:subset=tai_safety_research,',\n",
    "                     'raft:subset=terms_of_service,',\n",
    "                     'raft:subset=tweet_eval_hate,',\n",
    "                     'raft:subset=twitter_complaints,'],\n",
    "             'truthful_qa:task=mc_single,method=multiple_choice_joint,':['truthful_qa:task=mc_single,method=multiple_choice_joint,']}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['anchor-irt_naive', 'anchor-irt_cirt', 'anchor-irt_pirt', 'anchor-irt_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe5c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
