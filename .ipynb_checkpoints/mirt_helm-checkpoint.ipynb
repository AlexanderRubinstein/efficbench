{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952aeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "from experiments import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491d564",
   "metadata": {},
   "source": [
    "Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad64dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {'cnn':['summarization_cnndm:temperature=0.3,device=cuda,'], \n",
    "             'xsum':['summarization_xsum:temperature=0.3,device=cuda,'], \n",
    "             'boolq:':['boolq:'],\n",
    "             'civil_comments':['civil_comments:demographic=LGBTQ,',\n",
    "                               'civil_comments:demographic=all,',\n",
    "                               'civil_comments:demographic=black,',\n",
    "                               'civil_comments:demographic=christian,',\n",
    "                               'civil_comments:demographic=female,',\n",
    "                               'civil_comments:demographic=male,',\n",
    "                               'civil_comments:demographic=muslim,',\n",
    "                               'civil_comments:demographic=other_religions,',\n",
    "                               'civil_comments:demographic=white,'],\n",
    "             'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':['commonsense:dataset=hellaswag,method=multiple_choice_separate_original,'],\n",
    "             'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':['commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,'],\n",
    "             'imdb:':['imdb:'],\n",
    "             'mmlu':['mmlu:subject=abstract_algebra,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=college_chemistry,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=computer_security,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=econometrics,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=us_foreign_policy,method=multiple_choice_joint,'],\n",
    "             'msmarco:track=regular,valid_topk=30,':['msmarco:track=regular,valid_topk=30,'],\n",
    "             #'msmarco:track=trec,valid_topk=30,':['msmarco:track=trec,valid_topk=30,'],\n",
    "             'narrative_qa:':['narrative_qa:'],\n",
    "             'natural_qa:mode=closedbook,':['natural_qa:mode=closedbook,'],\n",
    "             'natural_qa:mode=openbook_longans,':['natural_qa:mode=openbook_longans,'],\n",
    "             'quac:':['quac:'],\n",
    "             'raft':['raft:subset=ade_corpus_v2,',\n",
    "                     'raft:subset=banking_77,',\n",
    "                     'raft:subset=neurips_impact_statement_risks,',\n",
    "                     'raft:subset=one_stop_english,',\n",
    "                     'raft:subset=overruling,',\n",
    "                     'raft:subset=semiconductor_org_types,',\n",
    "                     'raft:subset=systematic_review_inclusion,',\n",
    "                     'raft:subset=tai_safety_research,',\n",
    "                     'raft:subset=terms_of_service,',\n",
    "                     'raft:subset=tweet_eval_hate,',\n",
    "                     'raft:subset=twitter_complaints,'],\n",
    "             'truthful_qa:task=mc_single,method=multiple_choice_joint,':['truthful_qa:task=mc_single,method=multiple_choice_joint,']}\n",
    "            \n",
    "scenarios_metrics = {'boolq:':'em', \n",
    "                     'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':'em',\n",
    "                     'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':'em',\n",
    "                     'imdb:':'em', \n",
    "                     'mmlu':'em', \n",
    "                     'msmarco:track=regular,valid_topk=30,':'RR@10', \n",
    "                     'msmarco:track=trec,valid_topk=30,':'NDCG@10', \n",
    "                     'narrative_qa:':'f1', \n",
    "                     'natural_qa:mode=closedbook,':'f1', \n",
    "                     'natural_qa:mode=openbook_longans,':'f1', \n",
    "                     'quac:':'f1', \n",
    "                     'raft':'em', \n",
    "                     'truthful_qa:task=mc_single,method=multiple_choice_joint,':'em'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14725d5d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e324e4",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b24d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/helm.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797477e3",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ca743e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [5, 6, 7, 8, 9, 10, 11],\n",
       " [4, 12, 13],\n",
       " [14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
       " [23, 24, 25, 26, 27]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "iterations = 10\n",
    "Ds = [5, 10, 15, 20, 25] #\n",
    "\n",
    "set_of_rows = [[0,1,2,3], #ai21\n",
    "               [5,6,7,8,9,10,11], #cohere\n",
    "               [4,12,13], #anthropic+microsoft\n",
    "               [14,15,16,17,18,19,20,21,22], #openai\n",
    "               [23,24,25,26,27]] #together\n",
    "set_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc3c677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai21_j1-grande',\n",
       " 'ai21_j1-grande-v2-beta',\n",
       " 'ai21_j1-jumbo',\n",
       " 'ai21_j1-large',\n",
       " 'anthropic_stanford-online-all-v4-s3',\n",
       " 'cohere_command-medium-beta',\n",
       " 'cohere_command-xlarge-beta',\n",
       " 'cohere_large-20220720',\n",
       " 'cohere_medium-20220720',\n",
       " 'cohere_medium-20221108',\n",
       " 'cohere_xlarge-20220609',\n",
       " 'cohere_xlarge-20221108',\n",
       " 'microsoft_TNLGv2_530B',\n",
       " 'microsoft_TNLGv2_7B',\n",
       " 'openai_ada',\n",
       " 'openai_babbage',\n",
       " 'openai_curie',\n",
       " 'openai_davinci',\n",
       " 'openai_text-ada-001',\n",
       " 'openai_text-babbage-001',\n",
       " 'openai_text-curie-001',\n",
       " 'openai_text-davinci-002',\n",
       " 'openai_text-davinci-003',\n",
       " 'together_bloom',\n",
       " 'together_gpt-j-6b',\n",
       " 'together_gpt-neox-20b',\n",
       " 'together_opt-175b',\n",
       " 'together_opt-66b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['models']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c6fd5",
   "metadata": {},
   "source": [
    "### Predicting accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451897a",
   "metadata": {},
   "source": [
    "Full (one IRT model for all scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5503788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models [0, 1, 2, 3]\n",
      "\n",
      "i) choosing optimal D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [14:13<00:00, 170.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opt D= 5 errors= [0.044579911983804656, 0.0453168327810513, 0.047692893828327226, 0.049759814361327995, 0.05001942216565462] \n",
      "\n",
      "\n",
      "ii) choosing optimal lambdas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 15/15 [00:00<00:00, 33411.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_gpirt': {'cnn': {10: 0.5505698352323718, 25: 0.7538521812501997, 50: 0.8596530418120308, 75: 0.9018433662154779, 100: 0.9245305683197678}, 'xsum': {10: 0.10611472364294021, 25: 0.22885886611138165, 50: 0.372473800568467, 75: 0.47099420083697296, 100: 0.542777283492321}, 'boolq:': {10: 0.09549278534748269, 25: 0.20882066321582507, 50: 0.3454948605201695, 75: 0.4419044351820251, 100: 0.5135580531115531}, 'civil_comments': {10: 0.03445619767330429, 25: 0.08190717924487449, 50: 0.15141258107195896, 75: 0.21113465042095697, 100: 0.2630031729043548}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.2028549402625461, 25: 0.38882480252358737, 50: 0.5599335521904084, 75: 0.6561891636343071, 100: 0.7178941069690664}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.038526343227734794, 25: 0.09105389750791747, 50: 0.16690998990222974, 75: 0.23108018885882844, 100: 0.2860717473439651}, 'imdb:': {10: 0.27383991583388934, 25: 0.4852702450095601, 50: 0.6534437037839355, 75: 0.73878752677371, 100: 0.7904033288687337}, 'mmlu': {10: 0.018112739446646212, 25: 0.044084122285576434, 50: 0.08444553718348492, 75: 0.12153669022831111, 100: 0.1557395632846742}, 'msmarco:track=regular,valid_topk=30,': {10: 0.3391428994945548, 25: 0.5619733444985507, 50: 0.7195684183445067, 75: 0.7937675847653787, 100: 0.8369174621586297}, 'narrative_qa:': {10: 0.20562450460491055, 25: 0.39288200885407465, 50: 0.5641281980191546, 75: 0.6600233932784126, 100: 0.7213324313615452}, 'natural_qa:mode=closedbook,': {10: 0.072935466853048, 25: 0.16435743748230605, 50: 0.2823144031057966, 75: 0.3710896308435248, 100: 0.4403201000035939}, 'natural_qa:mode=openbook_longans,': {10: 0.03831869603286036, 25: 0.09058981472118931, 50: 0.16612994821402896, 75: 0.23008307745018192, 100: 0.284925275212189}, 'quac:': {10: 0.039142907564896925, 25: 0.09243028382812037, 50: 0.1692195560603171, 75: 0.23402825535232974, 100: 0.28945727974393803}, 'raft': {10: 0.04042436904805463, 25: 0.09528327331334839, 50: 0.17398836563094106, 75: 0.2400956256917856, 100: 0.2964056045605423}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.2930988377938415, 25: 0.5089764746903719, 50: 0.6745982899366395, 75: 0.7566724608419091, 100: 0.8056837200785196}}, 'anchor_gpirt': {'cnn': {10: 0.7101516135838696, 25: 0.8596530418120308, 50: 0.9245305683197678, 75: 0.9483886867193242, 100: 0.9607855375630007}, 'xsum': {10: 0.19186929054421412, 25: 0.372473800568467, 50: 0.542777283492321, 75: 0.6403753333208038, 100: 0.7036366030275719}, 'boolq:': {10: 0.17433758875407482, 25: 0.3454948605201695, 50: 0.5135580531115531, 75: 0.6129455245433646, 100: 0.6786103143593165}, 'civil_comments': {10: 0.06661702593266502, 25: 0.15141258107195896, 50: 0.2630031729043548, 75: 0.34865594894435953, 100: 0.41647270338927583}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.3372891168710154, 25: 0.5599335521904084, 50: 0.7178941069690664, 75: 0.792408473672632, 100: 0.8357838868609534}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.07419425319148884, 25: 0.16690998990222974, 50: 0.2860717473439651, 75: 0.3754104581490054, 100: 0.44487680867691753}, 'imdb:': {10: 0.429944002272258, 25: 0.6534437037839355, 50: 0.7904033288687337, 75: 0.849773207361934, 100: 0.882933265509677}, 'mmlu': {10: 0.03558100934183508, 25: 0.08444553718348492, 50: 0.1557395632846742, 75: 0.21673243735533948, 100: 0.2695063286438926}, 'msmarco:track=regular,valid_topk=30,': {10: 0.5065074080183087, 25: 0.7195684183445067, 50: 0.8369174621586297, 75: 0.8850283520640183, 100: 0.9112194525878555}, 'narrative_qa:': {10: 0.34110870145642036, 25: 0.5641281980191546, 50: 0.7213324313615452, 75: 0.7951977013708458, 100: 0.8381093834280265}, 'natural_qa:mode=closedbook,': {10: 0.1359549928328307, 25: 0.2823144031057966, 50: 0.4403201000035939, 75: 0.541306158978421, 100: 0.611419780925775}, 'natural_qa:mode=openbook_longans,': {10: 0.07380912272747454, 25: 0.16612994821402896, 50: 0.284925275212189, 75: 0.3740935578548356, 100: 0.44348925296864017}, 'quac:': {10: 0.07533690944708173, 25: 0.1692195560603171, 50: 0.28945727974393803, 75: 0.37929156700794003, 100: 0.4489598597658369}, 'raft': {10: 0.0777074629365732, 25: 0.17398836563094106, 50: 0.2964056045605423, 75: 0.3872211476560101, 100: 0.45727294531562657}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.4533278187673542, 25: 0.6745982899366395, 50: 0.8056837200785196, 75: 0.861483831173927, 100: 0.8923863145241013}}, 'anchor-irt_gpirt': {'cnn': {10: 0.7101516135838696, 25: 0.8596530418120308, 50: 0.9245305683197678, 75: 0.9483886867193242, 100: 0.9607855375630007}, 'xsum': {10: 0.19186929054421412, 25: 0.372473800568467, 50: 0.542777283492321, 75: 0.6403753333208038, 100: 0.7036366030275719}, 'boolq:': {10: 0.17433758875407482, 25: 0.3454948605201695, 50: 0.5135580531115531, 75: 0.6129455245433646, 100: 0.6786103143593165}, 'civil_comments': {10: 0.06661702593266502, 25: 0.15141258107195896, 50: 0.2630031729043548, 75: 0.34865594894435953, 100: 0.41647270338927583}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.3372891168710154, 25: 0.5599335521904084, 50: 0.7178941069690664, 75: 0.792408473672632, 100: 0.8357838868609534}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.07419425319148884, 25: 0.16690998990222974, 50: 0.2860717473439651, 75: 0.3754104581490054, 100: 0.44487680867691753}, 'imdb:': {10: 0.429944002272258, 25: 0.6534437037839355, 50: 0.7904033288687337, 75: 0.849773207361934, 100: 0.882933265509677}, 'mmlu': {10: 0.03558100934183508, 25: 0.08444553718348492, 50: 0.1557395632846742, 75: 0.21673243735533948, 100: 0.2695063286438926}, 'msmarco:track=regular,valid_topk=30,': {10: 0.5065074080183087, 25: 0.7195684183445067, 50: 0.8369174621586297, 75: 0.8850283520640183, 100: 0.9112194525878555}, 'narrative_qa:': {10: 0.34110870145642036, 25: 0.5641281980191546, 50: 0.7213324313615452, 75: 0.7951977013708458, 100: 0.8381093834280265}, 'natural_qa:mode=closedbook,': {10: 0.1359549928328307, 25: 0.2823144031057966, 50: 0.4403201000035939, 75: 0.541306158978421, 100: 0.611419780925775}, 'natural_qa:mode=openbook_longans,': {10: 0.07380912272747454, 25: 0.16612994821402896, 50: 0.284925275212189, 75: 0.3740935578548356, 100: 0.44348925296864017}, 'quac:': {10: 0.07533690944708173, 25: 0.1692195560603171, 50: 0.28945727974393803, 75: 0.37929156700794003, 100: 0.4489598597658369}, 'raft': {10: 0.0777074629365732, 25: 0.17398836563094106, 50: 0.2964056045605423, 75: 0.3872211476560101, 100: 0.45727294531562657}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.4533278187673542, 25: 0.6745982899366395, 50: 0.8056837200785196, 75: 0.861483831173927, 100: 0.8923863145241013}}, 'disc_gpirt': {'cnn': {10: 0.7101516135838696, 25: 0.8596530418120308, 50: 0.9245305683197678, 75: 0.9483886867193242, 100: 0.9607855375630007}, 'xsum': {10: 0.19186929054421412, 25: 0.372473800568467, 50: 0.542777283492321, 75: 0.6403753333208038, 100: 0.7036366030275719}, 'boolq:': {10: 0.17433758875407482, 25: 0.3454948605201695, 50: 0.5135580531115531, 75: 0.6129455245433646, 100: 0.6786103143593165}, 'civil_comments': {10: 0.06661702593266502, 25: 0.15141258107195896, 50: 0.2630031729043548, 75: 0.34865594894435953, 100: 0.41647270338927583}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.3372891168710154, 25: 0.5599335521904084, 50: 0.7178941069690664, 75: 0.792408473672632, 100: 0.8357838868609534}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.07419425319148884, 25: 0.16690998990222974, 50: 0.2860717473439651, 75: 0.3754104581490054, 100: 0.44487680867691753}, 'imdb:': {10: 0.429944002272258, 25: 0.6534437037839355, 50: 0.7904033288687337, 75: 0.849773207361934, 100: 0.882933265509677}, 'mmlu': {10: 0.03558100934183508, 25: 0.08444553718348492, 50: 0.1557395632846742, 75: 0.21673243735533948, 100: 0.2695063286438926}, 'msmarco:track=regular,valid_topk=30,': {10: 0.5065074080183087, 25: 0.7195684183445067, 50: 0.8369174621586297, 75: 0.8850283520640183, 100: 0.9112194525878555}, 'narrative_qa:': {10: 0.34110870145642036, 25: 0.5641281980191546, 50: 0.7213324313615452, 75: 0.7951977013708458, 100: 0.8381093834280265}, 'natural_qa:mode=closedbook,': {10: 0.1359549928328307, 25: 0.2823144031057966, 50: 0.4403201000035939, 75: 0.541306158978421, 100: 0.611419780925775}, 'natural_qa:mode=openbook_longans,': {10: 0.07380912272747454, 25: 0.16612994821402896, 50: 0.284925275212189, 75: 0.3740935578548356, 100: 0.44348925296864017}, 'quac:': {10: 0.07533690944708173, 25: 0.1692195560603171, 50: 0.28945727974393803, 75: 0.37929156700794003, 100: 0.4489598597658369}, 'raft': {10: 0.0777074629365732, 25: 0.17398836563094106, 50: 0.2964056045605423, 75: 0.3872211476560101, 100: 0.45727294531562657}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.4533278187673542, 25: 0.6745982899366395, 50: 0.8056837200785196, 75: 0.861483831173927, 100: 0.8923863145241013}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iv) running random eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████           | 3/4 [26:25<08:48, 528.62s/it]"
     ]
    }
   ],
   "source": [
    "scenario_name = 'full' #we are evaluating all scenarios at once\n",
    "chosen_scenarios = list(scenarios.keys())\n",
    "sampling = {'random_sampling':True,'anchor_sampling':False,\n",
    "            'anchor-irt_sampling':True,'disc_sampling':False}\n",
    "results_full, accs_full = evaluate_scenarios(data, scenario_name, chosen_scenarios, scenarios, set_of_rows, Ds, iterations, device, bench='irt_helm', sampling = sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/results_full_helm.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_full, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('results/accs_full_helm.pickle', 'wb') as handle:\n",
    "    pickle.dump(accs_full, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f46a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['random_naive', 'random_cirt', 'random_pirt', 'random_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['anchor-irt_naive', 'anchor-irt_cirt', 'anchor-irt_pirt', 'anchor-irt_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f159f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feba3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96d7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281b813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78783241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59608c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3b3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b380b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results/results_full_helm.pickle', 'rb') as handle:\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sce in scenarios.keys():\n",
    "    y=np.stack([results[m][100]['random_pirt'][sce] for m in results.keys()]).mean(axis=1)\n",
    "    x=np.vstack([data['data'][s]['correctness'] for s in scenarios[sce]]).mean(axis=0)\n",
    "    plt.plot(x,y,'bo')\n",
    "    plt.xlabel('acc')\n",
    "    plt.xlabel('error')\n",
    "    plt.title(sce)\n",
    "    plt.savefig(f'plots/scenario-{sce}.png', bbox_inches='tight', dpi=300, transparent=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b0b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e345c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
