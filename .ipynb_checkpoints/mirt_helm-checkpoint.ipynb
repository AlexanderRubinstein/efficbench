{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952aeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "from experiments import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491d564",
   "metadata": {},
   "source": [
    "Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad64dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {'cnn':['summarization_cnndm:temperature=0.3,device=cuda,'], \n",
    "             'xsum':['summarization_xsum:temperature=0.3,device=cuda,'], \n",
    "             'boolq:':['boolq:'],\n",
    "             'civil_comments':['civil_comments:demographic=LGBTQ,',\n",
    "                               'civil_comments:demographic=all,',\n",
    "                               'civil_comments:demographic=black,',\n",
    "                               'civil_comments:demographic=christian,',\n",
    "                               'civil_comments:demographic=female,',\n",
    "                               'civil_comments:demographic=male,',\n",
    "                               'civil_comments:demographic=muslim,',\n",
    "                               'civil_comments:demographic=other_religions,',\n",
    "                               'civil_comments:demographic=white,'],\n",
    "             'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':['commonsense:dataset=hellaswag,method=multiple_choice_separate_original,'],\n",
    "             'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':['commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,'],\n",
    "             'imdb:':['imdb:'],\n",
    "             'mmlu':['mmlu:subject=abstract_algebra,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=college_chemistry,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=computer_security,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=econometrics,method=multiple_choice_joint,',\n",
    "                     'mmlu:subject=us_foreign_policy,method=multiple_choice_joint,'],\n",
    "             'msmarco:track=regular,valid_topk=30,':['msmarco:track=regular,valid_topk=30,'],\n",
    "             #'msmarco:track=trec,valid_topk=30,':['msmarco:track=trec,valid_topk=30,'],\n",
    "             'narrative_qa:':['narrative_qa:'],\n",
    "             'natural_qa:mode=closedbook,':['natural_qa:mode=closedbook,'],\n",
    "             'natural_qa:mode=openbook_longans,':['natural_qa:mode=openbook_longans,'],\n",
    "             'quac:':['quac:'],\n",
    "             'raft':['raft:subset=ade_corpus_v2,',\n",
    "                     'raft:subset=banking_77,',\n",
    "                     'raft:subset=neurips_impact_statement_risks,',\n",
    "                     'raft:subset=one_stop_english,',\n",
    "                     'raft:subset=overruling,',\n",
    "                     'raft:subset=semiconductor_org_types,',\n",
    "                     'raft:subset=systematic_review_inclusion,',\n",
    "                     'raft:subset=tai_safety_research,',\n",
    "                     'raft:subset=terms_of_service,',\n",
    "                     'raft:subset=tweet_eval_hate,',\n",
    "                     'raft:subset=twitter_complaints,'],\n",
    "             'truthful_qa:task=mc_single,method=multiple_choice_joint,':['truthful_qa:task=mc_single,method=multiple_choice_joint,']}\n",
    "            \n",
    "scenarios_metrics = {'boolq:':'em', \n",
    "                     'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,':'em',\n",
    "                     'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,':'em',\n",
    "                     'imdb:':'em', \n",
    "                     'mmlu':'em', \n",
    "                     'msmarco:track=regular,valid_topk=30,':'RR@10', \n",
    "                     'msmarco:track=trec,valid_topk=30,':'NDCG@10', \n",
    "                     'narrative_qa:':'f1', \n",
    "                     'natural_qa:mode=closedbook,':'f1', \n",
    "                     'natural_qa:mode=openbook_longans,':'f1', \n",
    "                     'quac:':'f1', \n",
    "                     'raft':'em', \n",
    "                     'truthful_qa:task=mc_single,method=multiple_choice_joint,':'em'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14725d5d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e324e4",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b24d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/helm.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797477e3",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ca743e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [5, 6, 7, 8, 9, 10, 11],\n",
       " [4, 12, 13],\n",
       " [14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
       " [23, 24, 25, 26, 27]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "iterations = 10\n",
    "Ds = [5, 10, 15, 20] #\n",
    "\n",
    "set_of_rows = [[0,1,2,3], #ai21\n",
    "               [5,6,7,8,9,10,11], #cohere\n",
    "               [4,12,13], #anthropic+microsoft\n",
    "               [14,15,16,17,18,19,20,21,22], #openai\n",
    "               [23,24,25,26,27]] #together\n",
    "set_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc3c677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai21_j1-grande',\n",
       " 'ai21_j1-grande-v2-beta',\n",
       " 'ai21_j1-jumbo',\n",
       " 'ai21_j1-large',\n",
       " 'anthropic_stanford-online-all-v4-s3',\n",
       " 'cohere_command-medium-beta',\n",
       " 'cohere_command-xlarge-beta',\n",
       " 'cohere_large-20220720',\n",
       " 'cohere_medium-20220720',\n",
       " 'cohere_medium-20221108',\n",
       " 'cohere_xlarge-20220609',\n",
       " 'cohere_xlarge-20221108',\n",
       " 'microsoft_TNLGv2_530B',\n",
       " 'microsoft_TNLGv2_7B',\n",
       " 'openai_ada',\n",
       " 'openai_babbage',\n",
       " 'openai_curie',\n",
       " 'openai_davinci',\n",
       " 'openai_text-ada-001',\n",
       " 'openai_text-babbage-001',\n",
       " 'openai_text-curie-001',\n",
       " 'openai_text-davinci-002',\n",
       " 'openai_text-davinci-003',\n",
       " 'together_bloom',\n",
       " 'together_gpt-j-6b',\n",
       " 'together_gpt-neox-20b',\n",
       " 'together_opt-175b',\n",
       " 'together_opt-66b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['models']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c6fd5",
   "metadata": {},
   "source": [
    "### Predicting accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451897a",
   "metadata": {},
   "source": [
    "Full (one IRT model for all scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5503788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating models [0, 1, 2, 3]\n",
      "\n",
      "i) choosing optimal D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 4/4 [12:18<00:00, 184.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- opt D= 5 errors= [0.044579911983804656, 0.0453168327810513, 0.047692893828327226, 0.049759814361327995] \n",
      "\n",
      "\n",
      "ii) choosing optimal lambdas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 15/15 [00:00<00:00, 10540.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_gpirt': {'cnn': {10: 0.6106500153682252, 25: 0.7710186221163557, 50: 0.8389766601558348, 75: 0.8755169245268296, 100: 0.9143715646295693}, 'xsum': {10: 0.09864735668411334, 25: 0.27348288919988994, 50: 0.38082260331151807, 75: 0.4623254879248749, 100: 0.5845693496878107}, 'boolq:': {10: 0.09335207393175736, 25: 0.2281357754643307, 50: 0.35042482226473454, 75: 0.5616991102464602, 100: 0.6345438206318922}, 'civil_comments': {10: 0.06043252750157128, 25: 0.05845086140679586, 50: 0.1786826212729647, 75: 0.19488823789360563, 100: 0.28262542556346976}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.31853410497191076, 25: 0.49227117762582373, 50: 0.6283794731110073, 75: 0.7121305377685969, 100: 0.7645432827468794}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.037441018432384975, 25: 0.11296227042386832, 50: 0.2064591320881373, 75: 0.2877500866062527, 100: 0.4378412715932999}, 'imdb:': {10: 0.3455834566196065, 25: 0.5156409811826264, 50: 0.787173321179137, 75: 0.864045701613148, 100: 0.8364202101366279}, 'mmlu': {10: 0.018073203025455533, 25: 0.05610278480163488, 50: 0.10067249546182988, 75: 0.19206832823566175, 100: 0.1618349766413339}, 'msmarco:track=regular,valid_topk=30,': {10: 0.35559902298593943, 25: 0.5939677875290111, 50: 0.7531737010246538, 75: 0.8372907906945496, 100: 0.8804993802154986}, 'narrative_qa:': {10: 0.21700428847491493, 25: 0.3357603251928366, 50: 0.6062969541494124, 75: 0.7249469396806961, 100: 0.768269765371957}, 'natural_qa:mode=closedbook,': {10: 0.07381099088535344, 25: 0.16397709026251978, 50: 0.2998061701771193, 75: 0.3801340697729903, 100: 0.42689444575235885}, 'natural_qa:mode=openbook_longans,': {10: 0.03780576467917032, 25: 0.1353796502807352, 50: 0.20665227519015333, 75: 0.2770974321195231, 100: 0.3539554421925138}, 'quac:': {10: 0.036469223714063165, 25: 0.10144727160257894, 50: 0.1947844320505403, 75: 0.2772059323517002, 100: 0.3640699761031464}, 'raft': {10: 0.058118279917931115, 25: 0.12835365243443222, 50: 0.281589665446879, 75: 0.29911033650082974, 100: 0.48086093390591744}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.27728625557435405, 25: 0.5443116339204008, 50: 0.6841586888768892, 75: 0.7925650889335321, 100: 0.8609150531718377}}, 'anchor_gpirt': {'cnn': {10: 0.8387760494717652, 25: 0.8817759658680794, 50: 0.9405704871605911, 75: 0.9476218940414034, 100: 0.9567224978014934}, 'xsum': {10: 0.40513662365139586, 25: 0.4873476144116455, 50: 0.6232609673937587, 75: 0.6760274634106676, 100: 0.7121570634700527}, 'boolq:': {10: 0.22260225442011697, 25: 0.35244770441744855, 50: 0.4847932881349859, 75: 0.5983068843006057, 100: 0.6428299303214445}, 'civil_comments': {10: 0.20921692239368825, 25: 0.2598359245785889, 50: 0.4089048442268132, 75: 0.40572938688781274, 100: 0.5080297133807655}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.7383521749655739, 25: 0.8659645426185709, 50: 0.9087613725373663, 75: 0.9147373527094872, 100: 0.9220973369320954}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.11030785113029701, 25: 0.18893639532651982, 50: 0.27945135483579736, 75: 0.3676991247413687, 100: 0.4113768972216421}, 'imdb:': {10: 0.7111306203546409, 25: 0.8051998406620828, 50: 0.8467636054949627, 75: 0.7979388696113124, 100: 0.8105567346062639}, 'mmlu': {10: 0.027845693845871353, 25: 0.06033221688907043, 50: 0.11633175240018168, 75: 0.17191916104382451, 100: 0.19560390044341014}, 'msmarco:track=regular,valid_topk=30,': {10: 0.42658779165321986, 25: 0.7049746088890779, 50: 0.8106292290096228, 75: 0.8503045934096576, 100: 0.868855427073939}, 'narrative_qa:': {10: 0.6550019120909203, 25: 0.8612250621015637, 50: 0.8718935027079616, 75: 0.8971496781069492, 100: 0.9184456314637092}, 'natural_qa:mode=closedbook,': {10: 0.2671387621292883, 25: 0.320005989046033, 50: 0.37568510232829017, 75: 0.446291838804415, 100: 0.46584413859138546}, 'natural_qa:mode=openbook_longans,': {10: 0.1882873838007776, 25: 0.314056307214544, 50: 0.4236632278321545, 75: 0.5383428352176685, 100: 0.6039638437423684}, 'quac:': {10: 0.10515985966948607, 25: 0.18008499725875382, 50: 0.28271485593250745, 75: 0.42825906727013435, 100: 0.4237229228113694}, 'raft': {10: 0.06074999086711818, 25: 0.21166824461069955, 50: 0.3521663063227084, 75: 0.520982293531505, 100: 0.5419946861942315}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.3204812320299765, 25: 0.5642383166431091, 50: 0.7516927660699022, 75: 0.7973956789875974, 100: 0.8408625222078473}}, 'anchor-irt_gpirt': {'cnn': {10: 0.673653591930979, 25: 0.7841480394450405, 50: 0.8829766190717877, 75: 0.9153170549208293, 100: 0.9243485964657284}, 'xsum': {10: 0.1241033998154677, 25: 0.27395759278771264, 50: 0.48611738697982376, 75: 0.45949193396732646, 100: 0.5666488021275743}, 'boolq:': {10: 0.20802223226327, 25: 0.44712020108159717, 50: 0.5866050132667773, 75: 0.6928095507616743, 100: 0.7458219912870205}, 'civil_comments': {10: 0.1369517666234294, 25: 0.36966744577579086, 50: 0.5163489920175602, 75: 0.574685096359096, 100: 0.7068797809345394}, 'commonsense:dataset=hellaswag,method=multiple_choice_separate_original,': {10: 0.8168160275128435, 25: 0.8858910179072063, 50: 0.9530321870319485, 75: 0.9560213715121221, 100: 0.96916722228779}, 'commonsense:dataset=openbookqa,method=multiple_choice_separate_calibrated,': {10: 0.08742360926161519, 25: 0.25239429601248947, 50: 0.3205681149580578, 75: 0.41204946719450253, 100: 0.5605045695043372}, 'imdb:': {10: 0.8818535738765418, 25: 0.9506905163164524, 50: 0.9596335437385085, 75: 0.9554172004601956, 100: 0.9508451143346677}, 'mmlu': {10: 0.03359429455619284, 25: 0.06659274703612823, 50: 0.11933621240231007, 75: 0.22276326596713872, 100: 0.28753954571371904}, 'msmarco:track=regular,valid_topk=30,': {10: 0.5268076141630598, 25: 0.6750649397825251, 50: 0.8091866602121217, 75: 0.7857121475240206, 100: 0.8602864063487902}, 'narrative_qa:': {10: 0.4022043314245851, 25: 0.7084118308588307, 50: 0.7827765971911823, 75: 0.8409357969266629, 100: 0.865474934282212}, 'natural_qa:mode=closedbook,': {10: 0.20304918895430352, 25: 0.35282818965686646, 50: 0.3256199192232477, 75: 0.45812142734603417, 100: 0.5681052164051125}, 'natural_qa:mode=openbook_longans,': {10: 0.15591926732785874, 25: 0.282082750728361, 50: 0.4178014387781946, 75: 0.46773480228504977, 100: 0.5020994361914359}, 'quac:': {10: 0.07210688300166024, 25: 0.11003126880575023, 50: 0.23938756036199754, 75: 0.22323446750562473, 100: 0.29526366804853915}, 'raft': {10: 0.06670916329142225, 25: 0.13196196133350357, 50: 0.3026827568753679, 75: 0.3919827734000602, 100: 0.46571746841111467}, 'truthful_qa:task=mc_single,method=multiple_choice_joint,': {10: 0.3075442760275665, 25: 0.6092508407696795, 50: 0.7725380309698568, 75: 0.862786202786412, 100: 0.8834694700497095}}}\n",
      "\n",
      "v) running anchor points with IRT embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "scenario_name = 'full' #we are evaluating all scenarios at once\n",
    "chosen_scenarios = list(scenarios.keys())\n",
    "sampling = {'random_sampling':False,'anchor_sampling':False,\n",
    "            'anchor-irt_sampling':True,'disc_sampling':False}\n",
    "results_full, accs_full = evaluate_scenarios(data, scenario_name, chosen_scenarios, scenarios, set_of_rows, Ds, iterations, device, bench='irt_helm', sampling = sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/results_full_helm.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_full, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('results/accs_full_helm.pickle', 'wb') as handle:\n",
    "    pickle.dump(accs_full, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['anchor-irt_naive', 'anchor-irt_cirt', 'anchor-irt_pirt', 'anchor-irt_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e568dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['anchor_naive', 'anchor_cirt', 'anchor_pirt', 'anchor_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f46a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['random_naive', 'random_cirt', 'random_pirt', 'random_gpirt']\n",
    "plot_results(results_full, scenarios.keys(), methods = methods)\n",
    "plot_agg_results(results_full, scenarios.keys(), methods = methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db483f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3b3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b380b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results/results_full_helm.pickle', 'rb') as handle:\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sce in scenarios.keys():\n",
    "    y=np.stack([results[m][100]['random_pirt'][sce] for m in results.keys()]).mean(axis=1)\n",
    "    x=np.vstack([data['data'][s]['correctness'] for s in scenarios[sce]]).mean(axis=0)\n",
    "    plt.plot(x,y,'bo')\n",
    "    plt.xlabel('acc')\n",
    "    plt.xlabel('error')\n",
    "    plt.title(sce)\n",
    "    plt.savefig(f'plots/scenario-{sce}.png', bbox_inches='tight', dpi=300, transparent=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b0b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e345c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
