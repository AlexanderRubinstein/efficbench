{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# import pickle\n",
    "from plots import (\n",
    "    MAX_TABLE_SIZE,\n",
    "    make_table_avg,\n",
    "    make_perf_table,\n",
    ")\n",
    "from utils import load_pickle\n",
    "from generating_data.utils_for_notebooks import merge_methods\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "FIGURES_FOLDER = './tmp/NeurIPS_2025_Accelerated-Model-Evaluation-by-Using-Similarities-in-Prediction-Space/figures'\n",
    "os.makedirs(FIGURES_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mae(mae_value):\n",
    "    if mae_value is None or mae_value == float('nan'):\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return round(mae_value * 100, 2)\n",
    "\n",
    "def prepare_rank(rank_value):\n",
    "    if rank_value is None or rank_value == float('nan'):\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return round(rank_value, 3)\n",
    "\n",
    "def make_table_1(data_for_table_1):\n",
    "\n",
    "    assert len(data_for_table_1) == 4 # mae and rank for mmlu and hellaswag\n",
    "\n",
    "    rows = []\n",
    "    mmlu_maes, num_anchors_mmlu_maes = data_for_table_1[0]\n",
    "    mmlu_ranks, num_anchors_mmlu_ranks = data_for_table_1[1]\n",
    "    hellaswag_maes, num_anchors_hellaswag_maes = data_for_table_1[2]\n",
    "    hellaswag_ranks, num_anchors_hellaswag_ranks = data_for_table_1[3]\n",
    "    assert num_anchors_mmlu_maes == num_anchors_mmlu_ranks == num_anchors_hellaswag_maes == num_anchors_hellaswag_ranks\n",
    "    num_anchors = num_anchors_mmlu_maes\n",
    "\n",
    "    if hellaswag_maes is None:\n",
    "        hellaswag_maes = mmlu_maes.copy()\n",
    "        hellaswag_maes.loc[:,:] = float('nan')\n",
    "    if hellaswag_ranks is None:\n",
    "        hellaswag_ranks = mmlu_ranks.copy()\n",
    "        hellaswag_ranks.loc[:,:] = float('nan')\n",
    "\n",
    "    rows.append([ # headers\n",
    "        \"Approach\",\n",
    "        \"Condensation\", # type\n",
    "        \"Condensation\", # num_anchors\n",
    "        \"Prediction\", # type\n",
    "        \"MMLU\", # mae\n",
    "        \"MMLU\", # rank\n",
    "        \"hellaswag\", # mae\n",
    "        \"hellaswag\", # rank\n",
    "    ])\n",
    "    rows.append([\n",
    "        \"\",\n",
    "        \"type\", # type\n",
    "        \"num_anchors\", # num_anchors\n",
    "        \"type\", # type\n",
    "        \"mae\", # mae\n",
    "        \"rank\", # rank\n",
    "        \"mae\", # mae\n",
    "        \"rank\", # rank\n",
    "    ])\n",
    "    rows.append([ # RANDOM direct eval\n",
    "        \"Baseline\",\n",
    "        \"Random\",\n",
    "        num_anchors,\n",
    "        \"Eval\",\n",
    "        prepare_mae(mmlu_maes.loc[\"random\"][\"naive\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"random\"][\"naive\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"random\"][\"naive\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"random\"][\"naive\"]),\n",
    "    ])\n",
    "    # tinyBenchmarks\n",
    "    rows.append([ # Random gp-IRT\n",
    "        \"tinyBenchmarks\",\n",
    "        \"Random\",\n",
    "        num_anchors,\n",
    "        \"gp-IRT\",\n",
    "        prepare_mae(mmlu_maes.loc[\"random\"][\"gpirt\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"random\"][\"gpirt\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"random\"][\"gpirt\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"random\"][\"gpirt\"]),\n",
    "    ])\n",
    "    rows.append([ # anchor-IRT gp-IRT\n",
    "        \"tinyBenchmarks\",\n",
    "        \"anchor-IRT\",\n",
    "        num_anchors,\n",
    "        \"gp-IRT\",\n",
    "        prepare_mae(mmlu_maes.loc[\"anchor-irt\"][\"gpirt\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"anchor-irt\"][\"gpirt\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"anchor-irt\"][\"gpirt\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"anchor-irt\"][\"gpirt\"]),\n",
    "    ])\n",
    "    rows.append([ # anchor-correctness gp-IRT\n",
    "        \"tinyBenchmarks\",\n",
    "        \"anchor-correctness\",\n",
    "        num_anchors,\n",
    "        \"gp-IRT\",\n",
    "        prepare_mae(mmlu_maes.loc[\"anchor\"][\"gpirt\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"anchor\"][\"gpirt\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"anchor\"][\"gpirt\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"anchor\"][\"gpirt\"]),\n",
    "    ])\n",
    "    rows.append([ # Random KNN\n",
    "        \"Baseline\",\n",
    "        \"Random\",\n",
    "        num_anchors,\n",
    "        \"kNN\",\n",
    "        prepare_mae(mmlu_maes.loc[\"random\"][\"KNN\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"random\"][\"KNN\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"random\"][\"KNN\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"random\"][\"KNN\"]),\n",
    "    ])\n",
    "    rows.append([ # Random fit\n",
    "        \"Baseline\",\n",
    "        \"Random\",\n",
    "        num_anchors,\n",
    "        \"fit\",\n",
    "        prepare_mae(mmlu_maes.loc[\"random\"][\"fit\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"random\"][\"fit\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"random\"][\"fit\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"random\"][\"fit\"]),\n",
    "    ])\n",
    "    rows.append([\n",
    "        \"DISCO (ours)\",\n",
    "        \"High PDS\",\n",
    "        num_anchors,\n",
    "        \"kNN\",\n",
    "        prepare_mae(mmlu_maes.loc[\"highest\"][\"KNN\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"highest\"][\"KNN\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"highest\"][\"KNN\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"highest\"][\"KNN\"]),\n",
    "    ])\n",
    "    rows.append([\n",
    "        \"DISCO (ours)\",\n",
    "        \"High PDS\",\n",
    "        num_anchors,\n",
    "        \"fit\",\n",
    "        prepare_mae(mmlu_maes.loc[\"highest\"][\"fit\"]),\n",
    "        prepare_rank(mmlu_ranks.loc[\"highest\"][\"fit\"]),\n",
    "        prepare_mae(hellaswag_maes.loc[\"highest\"][\"fit\"]),\n",
    "        prepare_rank(hellaswag_ranks.loc[\"highest\"][\"fit\"]),\n",
    "    ])\n",
    "\n",
    "    # res[\"baseline\"] = {\n",
    "    #     \"mae\": 0.0,\n",
    "    #     \"rank\": 0.0\n",
    "    # }\n",
    "    # res[\"ours\"] = {\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # display(df)\n",
    "\n",
    "    latex_str = make_table_1_latex(df)\n",
    "\n",
    "    return df, latex_str\n",
    "\n",
    "\n",
    "def make_table_1_latex(df):\n",
    "        # Add column headers\n",
    "    df.columns = [\"Approach\", \"Type\", \"# Samples\", \"Type\", \"MAE\", \"Rank\", \"MAE\", \"Rank\"]\n",
    "\n",
    "    # Create LaTeX table content\n",
    "    latex_str = \"\\\\begin{table}[H]\\n\"\n",
    "    latex_str += \"\\\\centering\\n\\\\small\\n\"\n",
    "    latex_str += \"\\\\begin{tabular}{c|cc|c|cc|cc}\\n\"\n",
    "    latex_str += \"\\\\toprule\\n\"\n",
    "    latex_str += \"\\\\multicolumn{1}{c}{\\\\textbf{Approach}}&\\\\multicolumn{2}{c}{\\\\textbf{Condensation}} & \\\\multicolumn{1}{c}{\\\\textbf{Prediction}} & \\\\multicolumn{2}{c}{\\\\textbf{MMLU}}& \\\\multicolumn{2}{c}{\\\\textbf{hellaswag}} \\\\\\\\\\n\"\n",
    "    latex_str += \"&Type & \\\\# \\\\negthinspace Samples & Type & {MAE}  &Rank& {MAE}  &Rank \\\\\\\\\\n\"\n",
    "    latex_str += \"\\\\toprule\\n\"\n",
    "\n",
    "    # Process each row\n",
    "    current_approach = \"\"\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"Approach\"] == \"Approach\" or row[\"Approach\"] == \"\":\n",
    "            continue\n",
    "        if row[\"Approach\"] == current_approach:\n",
    "            approach_str = \"\"\n",
    "        else:\n",
    "            approach_str = row[\"Approach\"]\n",
    "            current_approach = row[\"Approach\"]\n",
    "\n",
    "            # Add midrule before new approach except for first one\n",
    "            if approach_str != \"Baseline\":\n",
    "                latex_str += \"\\\\midrule\\n\"\n",
    "\n",
    "        # Format numbers\n",
    "        mae_mmlu = \"-\" if pd.isna(row[\"MAE\"].values[0]) else f\"{float(row['MAE'].values[0]):.2f}\"\n",
    "        rank_mmlu = \"-\" if pd.isna(row[\"Rank\"].values[0]) else f\"{float(row['Rank'].values[0]):.3f}\"\n",
    "        mae_hellaswag = \"-\" if pd.isna(row[\"MAE\"].values[1]) else f\"{float(row['MAE'].values[1]):.2f}\"\n",
    "        rank_hellaswag = \"-\" if pd.isna(row[\"Rank\"].values[1]) else f\"{float(row['Rank'].values[1]):.3f}\"\n",
    "\n",
    "        # Bold best results\n",
    "        if approach_str == \"DISCO (ours)\" and row[\"Type\"].values[1] == \"linear\":\n",
    "            mae_mmlu = f\"\\\\textbf{{{mae_mmlu}}}\"\n",
    "            rank_mmlu = f\"\\\\textbf{{{rank_mmlu}}}\"\n",
    "\n",
    "        latex_str += f\"{approach_str}&{row['Type'].values[0]} & {row['# Samples']} & {row['Type'].values[1]} & {mae_mmlu} &{rank_mmlu} & {mae_hellaswag} &{rank_hellaswag} \\\\\\\\\\n\"\n",
    "\n",
    "    latex_str += \"\\\\bottomrule\\n\"\n",
    "    latex_str += \"\\\\end{tabular}\\n\"\n",
    "    latex_str += \"\\\\vspace{1em}\\n\"\n",
    "    latex_str += \"\\\\caption{Mean Absolute Error (MAE) for different sampling and prediction strategies. For question answering task on MMLU dataset [FIX]. \\\\joon{Add computational complexity info\\nAdd hellaswag results, add ranking metric, add method from hellaswag.\\n}}\\n\"\n",
    "    latex_str += \"\\\\label{tab:language-main}\\n\"\n",
    "    latex_str += \"\\\\end{table}\"\n",
    "\n",
    "    # Store LaTeX code in DataFrame metadata\n",
    "    df.attrs['latex_table'] = latex_str\n",
    "    return latex_str\n",
    "\n",
    "\n",
    "def extract_data_for_table_1(source_df, num_anchors, lower_better, key=\"PDS type\"):\n",
    "    # Group by PDS type and calculate mean for each group\n",
    "    df = source_df[num_anchors]\n",
    "    # For debugging:\n",
    "    # display(df)\n",
    "    # if num_anchors == 100 and not lower_better:\n",
    "    #     display(df)\n",
    "    # print(\"DEBUG\")\n",
    "\n",
    "    # Keep rows with NaN PDS type and group the rest\n",
    "    nan_rows = df[df[key].isna()]\n",
    "    non_nan_rows = df[df[key].notna()]\n",
    "    if lower_better:\n",
    "        grouped_non_nan = non_nan_rows.groupby(key, as_index=(key==\"PDS type\")).min()\n",
    "    else:\n",
    "        grouped_non_nan = non_nan_rows.groupby(key, as_index=(key==\"PDS type\")).max()\n",
    "    grouped_df = pd.concat([grouped_non_nan, nan_rows])\n",
    "\n",
    "    # to_drop = [\"GradientBoostingRegressor_200\"]\n",
    "    # to_drop = ['MLP3_e700_lr0.001', 'Ridge_10', 'Lasso_e-4', 'RandomForestRegressor_100']\n",
    "    to_drop = ['MLP3_e700_lr0.001', 'Ridge_10', 'Lasso_e-4', \"GradientBoostingRegressor_200\"] # keep only Random Forest\n",
    "    # print(\"DEBUG: Dropping\", to_drop)\n",
    "    grouped_df = grouped_df.drop(columns=to_drop)\n",
    "\n",
    "    # Get the columns to find minimum across\n",
    "    min_cols = ['MLP3_e700_lr0.001', 'Ridge_10', 'Lasso_e-4', 'RandomForestRegressor_100', 'GradientBoostingRegressor_200']\n",
    "    min_cols = [col for col in min_cols if col not in to_drop]\n",
    "\n",
    "    # Find minimum value across specified columns and store in new 'linear' column\n",
    "    if lower_better:\n",
    "        grouped_df['fit'] = grouped_df[min_cols].min(axis=1)\n",
    "    else:\n",
    "        grouped_df['fit'] = grouped_df[min_cols].max(axis=1)\n",
    "\n",
    "    # Drop the original columns\n",
    "    grouped_df = grouped_df.drop(columns=min_cols)\n",
    "\n",
    "    # Drop the stratified and #guiding_models columns since they're no longer meaningful after grouping\n",
    "    for cols_to_drop in ['stratified', '#guiding_models', 'cirt', 'pirt']:\n",
    "        if cols_to_drop == key:\n",
    "            continue\n",
    "        if cols_to_drop in grouped_df.columns:\n",
    "            grouped_df = grouped_df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    return grouped_df, num_anchors\n",
    "\n",
    "\n",
    "def make_df_with_results(table_avg, table_std, bench, split):\n",
    "    cur_methods_for_table = table_avg[bench][split].keys()\n",
    "\n",
    "    df = make_perf_table(\n",
    "        table_avg[bench][split],\n",
    "        table_std[bench][split],\n",
    "        methods=cur_methods_for_table,\n",
    "    )\n",
    "\n",
    "    pd.set_option('display.max_rows', MAX_TABLE_SIZE)\n",
    "    pd.set_option('display.max_columns', MAX_TABLE_SIZE)\n",
    "    pd.set_option(\n",
    "        \"display.max_colwidth\", MAX_TABLE_SIZE\n",
    "    )\n",
    "    for num_samples in df.keys():\n",
    "        # print(\"#anchor_points:\", num_samples)\n",
    "        # Reorder columns to put guiding models, PDS type, and stratified first\n",
    "        cols = df[num_samples].columns.tolist()\n",
    "        first_cols = ['#guiding_models', 'PDS type', 'stratified']\n",
    "        other_cols = [col for col in cols if col not in first_cols]\n",
    "        df[num_samples] = df[num_samples][first_cols + other_cols]\n",
    "\n",
    "        # Replace all values in #guiding_models column with 382\n",
    "        df[num_samples].loc[df[num_samples]['#guiding_models'] == 'all', '#guiding_models'] = 382\n",
    "\n",
    "        # Sort rows by #guiding_models\n",
    "        df[num_samples] = df[num_samples].sort_values(['PDS type', 'stratified', '#guiding_models'])\n",
    "\n",
    "        # print(df[num_samples])\n",
    "\n",
    "    # df[max(list(df.keys()))].to_csv(results_table_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table_data(\n",
    "    bench,\n",
    "    split,\n",
    "    filename_suffix,\n",
    "    data,\n",
    "    scenarios_to_skip,\n",
    "    ordered,\n",
    "    agg_type,\n",
    "    table_avg_base,\n",
    "    table_std_base,\n",
    "    model_perf_base\n",
    "):\n",
    "    current_table_avg, current_table_std, current_model_perf = make_table_avg(\n",
    "        bench,\n",
    "        split,\n",
    "        filename_suffix,\n",
    "        data,\n",
    "        scenarios_to_skip=scenarios_to_skip,\n",
    "        ordered=ordered,\n",
    "        return_perf_table=True,\n",
    "        agg_type=agg_type\n",
    "    )\n",
    "    table_avg_base = merge_methods(table_avg_base, current_table_avg)\n",
    "    table_std_base = merge_methods(table_std_base, current_table_std)\n",
    "    model_perf_base = merge_methods(model_perf_base, current_model_perf)\n",
    "    return table_avg_base, table_std_base, model_perf_base\n",
    "\n",
    "\n",
    "# load needed results\n",
    "results_suffixes = {\n",
    "    \"mmlu_fields\": {\n",
    "\n",
    "        \"iid\": {\n",
    "            \"ours\": \"_disagreement_best_47\",\n",
    "            \"irt\": \"_disagreement_compare_with_irt43\"\n",
    "        },\n",
    "        # \"noniid\": {\n",
    "        #     \"ours\": \"_disagreement_best_48\",\n",
    "        #     \"irt\": \"_disagreement_compare_with_irt44\"\n",
    "        # } # Old\n",
    "        \"noniid\": {\n",
    "            \"ours\": \"_disagreement_best_68\",\n",
    "            \"irt\": \"_disagreement_compare_with_irt70\"\n",
    "        } # more n anchors (till 2000)\n",
    "    },\n",
    "    \"hellaswag\": {\n",
    "\n",
    "        \"iid\": {\n",
    "            \"ours\": \"_disagreement_best_63\",\n",
    "            \"irt\": \"_disagreement_best_63\"\n",
    "        },\n",
    "        \"noniid\": {\n",
    "            \"ours\": \"_disagreement_best_64\",\n",
    "            \"irt\": \"_disagreement_best_64\"\n",
    "        }\n",
    "    },\n",
    "    \"num_models\": {\n",
    "        3: \"_disagreement_best_57\",\n",
    "        10: \"_disagreement_best_56\",\n",
    "        30: \"_disagreement_best_55\",\n",
    "        100: \"_disagreement_best_54\",\n",
    "        300: \"_disagreement_best_53\",\n",
    "        382: \"_disagreement_best_68\"\n",
    "    },\n",
    "    # \"umap_pca\": {\n",
    "    #     \"no_pca\": \"_disagreement_before_pca_15\",\n",
    "    #     \"pca64\": \"_disagreement_umap_pca21\",\n",
    "    #     \"pca128\": \"_disagreement_umap_pca23\",\n",
    "    #     \"pca256\": \"_disagreement_umap_pca25\",\n",
    "    #     \"umap64\": \"_disagreement_umap_pca27\",\n",
    "    #     \"umap128\": \"_disagreement_umap_pca29\",\n",
    "    #     \"umap256\": \"_disagreement_umap_pca31\"\n",
    "    # },\n",
    "    \"prediction_strategy\": {\n",
    "        # \"linear\": \"_disagreement_best_68\",\n",
    "        # \"mlps\": \"_disagreement_best_68\",\n",
    "        \"other\": \"_disagreement_best_68\",\n",
    "    }\n",
    "}\n",
    "print(\"DEBUG: uncomment all except for num_models\")\n",
    "scenarios_to_skip = []\n",
    "table_1_data = []\n",
    "table_1_data_iid = []\n",
    "figure_n_anchors_data = {}\n",
    "\n",
    "table_avg_dict = {}\n",
    "table_std_dict = {}\n",
    "model_perf_dict = {}\n",
    "\n",
    "for bench, per_bench in results_suffixes.items():\n",
    "    if bench not in table_avg_dict:\n",
    "        table_avg_dict[bench] = {}\n",
    "        table_std_dict[bench] = {}\n",
    "        model_perf_dict[bench] = {}\n",
    "    # ordered = bench in [\"mmlu_fields\", \"hellaswag\", \"num_models\", \"umap_pca\"]\n",
    "    ordered = True\n",
    "    for agg_type in [\"mae\", \"rank\"]:\n",
    "\n",
    "        if agg_type not in table_avg_dict[bench]:\n",
    "            table_avg_dict[bench][agg_type] = {}\n",
    "            table_std_dict[bench][agg_type] = {}\n",
    "            model_perf_dict[bench][agg_type] = {}\n",
    "\n",
    "        if bench in [\"num_models\", \"umap_pca\", \"prediction_strategy\"]:\n",
    "            if agg_type == \"mae\":\n",
    "                continue\n",
    "\n",
    "            split = \"noniid\"\n",
    "            real_bench = \"mmlu_fields\"\n",
    "\n",
    "            factor_list = []\n",
    "            for factor, filename_suffix in per_bench.items():\n",
    "\n",
    "                table_avg_base = None\n",
    "                table_std_base = None\n",
    "                model_perf_base = None\n",
    "\n",
    "                if factor not in table_avg_dict[bench]:\n",
    "                    table_avg_dict[bench][factor] = {}\n",
    "                    table_std_dict[bench][factor] = {}\n",
    "                    model_perf_dict[bench][factor] = {}\n",
    "\n",
    "                results_path = f'results/accs_{real_bench}_split-{split}_iterations-5{filename_suffix}.pickle'\n",
    "\n",
    "                data = load_pickle(results_path)\n",
    "\n",
    "                table_avg_base, table_std_base, model_perf_base = process_table_data(\n",
    "                    real_bench,\n",
    "                    split,\n",
    "                    filename_suffix,\n",
    "                    data,\n",
    "                    scenarios_to_skip,\n",
    "                    ordered,\n",
    "                    agg_type,\n",
    "                    table_avg_base,\n",
    "                    table_std_base,\n",
    "                    model_perf_base\n",
    "                )\n",
    "\n",
    "                table_avg_dict[bench][agg_type][factor] = table_avg_base\n",
    "                table_std_dict[bench][agg_type][factor] = table_std_base\n",
    "                model_perf_dict[bench][agg_type][factor] = model_perf_base\n",
    "\n",
    "                df = make_df_with_results(table_avg_base, table_std_base, real_bench, split)\n",
    "\n",
    "                if bench == \"prediction_strategy\":\n",
    "\n",
    "                    filtered_df = df[100]\n",
    "                    display(filtered_df)\n",
    "                    print(\"DEBUG:\")\n",
    "                    filtered_df = filtered_df[filtered_df['PDS type'] == 'highest']\n",
    "                    filtered_df = pd.DataFrame([filtered_df.max()], index=['highest'])\n",
    "                    filtered_df.drop(columns=['PDS type', 'stratified', '#guiding_models'], inplace=True)\n",
    "                    factor_list.append(filtered_df)\n",
    "                else:\n",
    "                    grouped_df, _ = extract_data_for_table_1(df, num_anchors=100, lower_better=(agg_type == \"mae\"))\n",
    "                    grouped_df = grouped_df[[\"fit\"]].rename(columns={\"fit\": f\"fit-{factor}\"})\n",
    "                    factor_list.append(grouped_df)\n",
    "\n",
    "            if len(factor_list) > 0:\n",
    "                if agg_type == \"rank\":\n",
    "                    factor_df = pd.concat(factor_list, axis=1)\n",
    "\n",
    "                if bench == \"num_models\":\n",
    "                    num_models_df = factor_df\n",
    "\n",
    "                if bench == \"prediction_strategy\":\n",
    "                    prediction_strategy_df = factor_df\n",
    "        else:\n",
    "            # ordered = bench in [\"mmlu_fields\", \"hellaswag\"]\n",
    "            for split, per_split in per_bench.items():\n",
    "                if split not in table_avg_dict[bench][agg_type]:\n",
    "                    table_avg_dict[bench][agg_type][split] = {}\n",
    "                    table_std_dict[bench][agg_type][split] = {}\n",
    "                    model_perf_dict[bench][agg_type][split] = {}\n",
    "                # for agg_type in [\"mae\", \"rank\"]:\n",
    "                table_avg_base = None\n",
    "                table_std_base = None\n",
    "                model_perf_base = None\n",
    "                for method in [\n",
    "                    \"ours\",\n",
    "                    \"irt\"\n",
    "                ]:\n",
    "                    # our_results_path = f'results/accs_{bench}_split-{split}_iterations-5{per_split[\"ours\"]}.pickle'\n",
    "\n",
    "                    # data_ours = load_pickle(our_results_path)\n",
    "                    # irt_results_path = f'results/accs_{bench}_split-{split}_iterations-5{per_split[\"irt\"]}.pickle'\n",
    "\n",
    "                    # data_irt = load_pickle(irt_results_path)\n",
    "                    filename_suffix = per_split[method]\n",
    "                    results_path = f'results/accs_{bench}_split-{split}_iterations-5{filename_suffix}.pickle'\n",
    "                    data = load_pickle(results_path)\n",
    "\n",
    "                    # current_table_avg, current_table_std, current_model_perf = make_table_avg(\n",
    "                    #     bench,\n",
    "                    #     split,\n",
    "                    #     filename_suffix,\n",
    "                    #     data,\n",
    "                    #     scenarios_to_skip=scenarios_to_skip,\n",
    "                    #     ordered=ordered,\n",
    "                    #     return_perf_table=True,\n",
    "                    #     agg_type=agg_type\n",
    "                    # )\n",
    "                    # table_avg_base = merge_methods(table_avg_base, current_table_avg)\n",
    "                    # table_std_base = merge_methods(table_std_base, current_table_std)\n",
    "                    # model_perf_base = merge_methods(model_perf_base, current_model_perf)\n",
    "                    table_avg_base, table_std_base, model_perf_base = process_table_data(\n",
    "                        bench,\n",
    "                        split,\n",
    "                        filename_suffix,\n",
    "                        data,\n",
    "                        scenarios_to_skip,\n",
    "                        ordered,\n",
    "                        agg_type,\n",
    "                        table_avg_base,\n",
    "                        table_std_base,\n",
    "                        model_perf_base\n",
    "                    )\n",
    "\n",
    "                # if agg_type not in table_avg_dict[bench]:\n",
    "                #     table_avg_dict[bench][agg_type] = {}\n",
    "                #     table_std_dict[bench][agg_type] = {}\n",
    "                #     model_perf_dict[bench][agg_type] = {}\n",
    "                # if split not in table_avg_dict[bench][agg_type]:\n",
    "                #     table_avg_dict[bench][agg_type][split] = {}\n",
    "                #     table_std_dict[bench][agg_type][split] = {}\n",
    "                #     model_perf_dict[bench][agg_type][split] = {}\n",
    "                table_avg_dict[bench][agg_type][split] = table_avg_base\n",
    "                table_std_dict[bench][agg_type][split] = table_std_base\n",
    "                model_perf_dict[bench][agg_type][split] = model_perf_base\n",
    "                # print(model_perf_dict)\n",
    "\n",
    "                # sys.exit(0)\n",
    "                if split == \"noniid\":\n",
    "                # if split == \"iid\":\n",
    "                    df = make_df_with_results(table_avg_base, table_std_base, bench, split)\n",
    "                    table_1_data.append(extract_data_for_table_1(df, num_anchors=100, lower_better=(agg_type == \"mae\")))\n",
    "                    for num_anchors in df.keys():\n",
    "                        if num_anchors not in figure_n_anchors_data:\n",
    "                            figure_n_anchors_data[num_anchors] = []\n",
    "                        figure_n_anchors_data[num_anchors].append(extract_data_for_table_1(df, num_anchors=num_anchors, lower_better=(agg_type == \"mae\")))\n",
    "                    if agg_type == \"rank\":\n",
    "                        ablation_strat, _ = extract_data_for_table_1(df, num_anchors=100, lower_better=(agg_type == \"mae\"), key=\"stratified\")\n",
    "                if split == \"iid\":\n",
    "                    df_iid = make_df_with_results(table_avg_base, table_std_base, bench, split)\n",
    "                    table_1_data_iid.append(extract_data_for_table_1(df_iid, num_anchors=100, lower_better=(agg_type == \"mae\")))\n",
    "                    # print(\"DEBUG\", df[100])\n",
    "\n",
    "# generate table_avg, perf_avg and etc\n",
    "# extract max across sampling methods\n",
    "# for table in table_1_data:\n",
    "#     print(\"DEBUG\")\n",
    "#     display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_1, latex_str = make_table_1(table_1_data + [(None, 100), (None, 100)])\n",
    "table_1, latex_str = make_table_1(table_1_data)\n",
    "display(table_1)\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_1, latex_str = make_table_1(table_1_data + [(None, 100), (None, 100)])\n",
    "table_1_iid, latex_str_iid = make_table_1(table_1_data_iid)\n",
    "display(table_1_iid)\n",
    "print(latex_str_iid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity: Num source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(num_models_df.applymap(prepare_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: prediction strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(prediction_strategy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation: Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ablation_strat.loc[~ablation_strat['stratified'].isna(), ('fit', 'stratified')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure correlation with gt performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_with_gt_performance(model_perf_dict):\n",
    "    # if results == 'acc':\n",
    "    #     if agg == 'leaderboard':\n",
    "    # split = 'noniid'\n",
    "    split = 'noniid'\n",
    "    iteration = 1\n",
    "    number_item = 100\n",
    "    color_mappings = {}\n",
    "\n",
    "    alphas = {'random_naive':.4,'anchor_naive':.4,'anchor-irt_naive':.4,'anchor-irt_gpirt':.8}\n",
    "    markersize = {'random_naive':7,'anchor_naive':5,'anchor-irt_naive':5,'anchor-irt_gpirt':5}\n",
    "    names = {\n",
    "        'random_naive':'random',\n",
    "        'anchor-irt_naive':'IRT ',\n",
    "        'anchor_naive':'correctness',\n",
    "        'anchor-irt_gpirt':'IRT++',\n",
    "        'high-disagreement@100+nonstratified_GradientBoostingRegressor_200': 'High PDS/Linear',\n",
    "        'high-disagreement@100+nonstratified_RandomForestRegressor_100': 'High PDS/Random Forest'\n",
    "    }\n",
    "    plt.figure(figsize=(1.2*3.5,1.2*3))\n",
    "\n",
    "    # for i,bench in enumerate(['mmlu']): #benchs[:4]\n",
    "    for i,bench in enumerate(['mmlu_fields']): #benchs[:4]\n",
    "        # axis = {'lb':'avg. score', 'mmlu_fields':'accuracy', 'mmlu':'accuracy', 'helm':'mean win rate', 'alpaca':'win rate'}\n",
    "        # for method in ['anchor-irt_gpirt']: #\n",
    "\n",
    "        for method in [\n",
    "            # 'random_KNN'\n",
    "            # 'high-disagreement@100+nonstratified_GradientBoostingRegressor_200'\n",
    "            # 'high-disagreement+nonstratified_RandomForestRegressor_100'\n",
    "            'high-disagreement@100+nonstratified_RandomForestRegressor_100'\n",
    "        ]: #\n",
    "            # print(model_perf[bench][split][method])\n",
    "            if model_perf_dict is None:\n",
    "                print(\"Using hardcoded data\")\n",
    "                x = [\n",
    "                    0.74505765, 0.57627382, 0.63675495, 0.65292241, 0.38084539, 0.64716621,\n",
    "                    0.60905773, 0.64266456, 0.77400765, 0.65963511, 0.45349487, 0.53712371,\n",
    "                    0.54949706, 0.65399154, 0.66097766, 0.70370242, 0.60520878, 0.56062885,\n",
    "                    0.62193137, 0.63557209, 0.39394311, 0.457429, 0.33628442, 0.60314194,\n",
    "                    0.63755535, 0.62489059, 0.62572778, 0.76651098, 0.61419194, 0.64899013,\n",
    "                    0.65274401, 0.50838601, 0.77287346, 0.70602432, 0.54623595, 0.74655344,\n",
    "                    0.71878393, 0.63617809, 0.76029052, 0.75110232\n",
    "                ]\n",
    "                y = [\n",
    "                    0.75731513, 0.60630879, 0.63863409, 0.66208239, 0.38429752, 0.64862158,\n",
    "                    0.6141397, 0.63641627, 0.72950503, 0.65494508, 0.39609827, 0.54465079,\n",
    "                    0.55145108, 0.64995122, 0.6629164, 0.7023732, 0.62522846, 0.54741339,\n",
    "                    0.63129268, 0.6264134, 0.41973213, 0.51171946, 0.35283553, 0.60904086,\n",
    "                    0.63357007, 0.62456882, 0.62493765, 0.76117544, 0.62270945, 0.63739011,\n",
    "                    0.64783929, 0.49335064, 0.75879076, 0.72244666, 0.56023766, 0.74885069,\n",
    "                    0.7073013, 0.6313948, 0.70250782, 0.75130385\n",
    "                ]\n",
    "            else:\n",
    "                model_perf = model_perf_dict[bench]['mae'][split]\n",
    "                x,y = model_perf[bench][split]['truth'], model_perf[bench][split][method][number_item][:,iteration]\n",
    "                print(x)\n",
    "                print(y)\n",
    "            method_name = names[method] if method in names else method\n",
    "            # label = \"{:} (error={:.3f}, $r_S$={:.2f})\".format(method_name, np.abs(x-y).mean(), stats.spearmanr(x,y).statistic)\n",
    "            label = \"{:} ($r_S$={:.2f})\".format(method_name, stats.spearmanr(x,y).statistic)\n",
    "            markersize = markersize[method] if method in markersize else 5\n",
    "            alpha = alphas[method] if method in alphas else 0.5\n",
    "            color = color_mappings[method] if method in color_mappings else 'black'\n",
    "            plt.plot(x, y, 'o', label = label, markersize=markersize,alpha=alpha, color=color)\n",
    "\n",
    "        plt.legend(fontsize=10, framealpha=.9)\n",
    "        #plt.title(titles[bench])\n",
    "        plt.plot([0,1],[0,1],'--r',lw=.5)\n",
    "        plt.grid(alpha=.2)\n",
    "        plt.xlabel('Ground truth performance', size=12)\n",
    "        plt.ylabel('Estimated performance', size=12)\n",
    "        plt.xlim(0,1)\n",
    "        plt.ylim(0,1)\n",
    "        tick_label_size = 11  # Example size, adjust as needed\n",
    "        plt.tick_params(axis='x', labelsize=tick_label_size)\n",
    "        plt.tick_params(axis='y', labelsize=tick_label_size)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_FOLDER, 'performance_correlation_mmlu.pdf'), bbox_inches='tight', dpi=400, transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_with_gt_performance(model_perf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_with_gt_performance(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure: n_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_figure_n_anchors(figure_n_anchors_data):\n",
    "    maes = {}\n",
    "    ranks = {}\n",
    "    num_anchors_axis = []\n",
    "    for method in [\n",
    "        \"DISCO + Random Forest (ours)\",\n",
    "        \"DISCO + kNN (ours)\",\n",
    "        \"Random + Eval\",\n",
    "        \"Random + Random Forest\",\n",
    "        \"Anchor-correctness + gp-IRT\"\n",
    "    ]:\n",
    "        maes[method] = []\n",
    "        ranks[method] = []\n",
    "    if figure_n_anchors_data is None:\n",
    "        print(\"Using hardcoded data\")\n",
    "        ranks = {\n",
    "            'DISCO + Random Forest (ours)': [0.96, 0.977, 0.981, 0.987, 0.977, 0.985, 0.984, 0.979],\n",
    "            'DISCO + kNN (ours)': [0.973, 0.975, 0.973, 0.972, 0.974, 0.957, 0.952, 0.95],\n",
    "            'Random + Eval': [0.574, 0.721, 0.865, 0.916, 0.944, 0.972, 0.988, 0.992],\n",
    "            'Random + Random Forest': [0.824, 0.866, 0.901, 0.933, 0.928, 0.932, 0.952, 0.959],\n",
    "            'Anchor-correctness + gp-IRT': [0.845, 0.882, 0.925, 0.927, 0.956, 0.974, 0.991, 0.996]}\n",
    "        maes = {\n",
    "            'DISCO + Random Forest (ours)': [2.05, 1.2, 1.16, 1.07, 1.08, 1.02, 0.93, 1.15],\n",
    "            'DISCO + kNN (ours)': [1.17, 1.29, 1.2, 1.31, 1.59, 1.53, 1.57, 1.56],\n",
    "            'Random + Eval': [14.28, 9.05, 4.62, 3.45, 2.45, 1.4, 0.92, 0.73],\n",
    "            'Random + Random Forest': [3.23, 2.57, 2.05, 1.81, 1.66, 1.59, 1.51, 1.55],\n",
    "            'Anchor-correctness + gp-IRT': [3.4, 2.62, 2.24, 2.08, 1.67, 1.46, 1.14, 0.82]}\n",
    "        num_anchors_axis = [10, 30, 60, 100, 200, 500, 1000, 2000]\n",
    "    else:\n",
    "        for num_anchors, data_for_table_1 in figure_n_anchors_data.items():\n",
    "            assert len(data_for_table_1) >= 2 # mae and rank for mmlu and possibly helm\n",
    "\n",
    "            mmlu_maes, num_anchors_mmlu_maes = data_for_table_1[0]\n",
    "            mmlu_ranks, num_anchors_mmlu_ranks = data_for_table_1[1]\n",
    "            # helm_maes, num_anchors_helm_maes = data_for_table_1[2]\n",
    "            # helm_ranks, num_anchors_helm_ranks = data_for_table_1[3]\n",
    "            assert num_anchors_mmlu_maes == num_anchors_mmlu_ranks == num_anchors\n",
    "\n",
    "            maes[\"DISCO + Random Forest (ours)\"].append(prepare_mae(mmlu_maes.loc[\"highest\"][\"fit\"]))\n",
    "            ranks[\"DISCO + Random Forest (ours)\"].append(prepare_rank(mmlu_ranks.loc[\"highest\"][\"fit\"]))\n",
    "\n",
    "            maes[\"DISCO + kNN (ours)\"].append(prepare_mae(mmlu_maes.loc[\"highest\"][\"KNN\"]))\n",
    "            ranks[\"DISCO + kNN (ours)\"].append(prepare_rank(mmlu_ranks.loc[\"highest\"][\"KNN\"]))\n",
    "\n",
    "            maes[\"Random + Eval\"].append(prepare_mae(mmlu_maes.loc[\"random\"][\"naive\"]))\n",
    "            ranks[\"Random + Eval\"].append(prepare_rank(mmlu_ranks.loc[\"random\"][\"naive\"]))\n",
    "\n",
    "            maes[\"Random + Random Forest\"].append(prepare_mae(mmlu_maes.loc[\"random\"][\"fit\"]))\n",
    "            ranks[\"Random + Random Forest\"].append(prepare_rank(mmlu_ranks.loc[\"random\"][\"fit\"]))\n",
    "\n",
    "            maes[\"Anchor-correctness + gp-IRT\"].append(prepare_mae(mmlu_maes.loc[\"anchor\"][\"gpirt\"]))\n",
    "            ranks[\"Anchor-correctness + gp-IRT\"].append(prepare_rank(mmlu_ranks.loc[\"anchor\"][\"gpirt\"]))\n",
    "            num_anchors_axis.append(num_anchors)\n",
    "    # Create line plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    print(ranks)\n",
    "    print(maes)\n",
    "\n",
    "    for method, mae_values in maes.items():\n",
    "        plt.plot(num_anchors_axis, mae_values, marker='o', label=method)\n",
    "\n",
    "    plt.xlabel('Number of Anchors')\n",
    "    plt.ylabel('Mean Absolute Error (MAE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title('MAE vs Number of Anchors by Method')\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(num_anchors_axis)\n",
    "    plt.yticks(\n",
    "        [1.0, 1.1, 1.3, 1.5, 2.0, 3.0, 10.0],\n",
    "        ['1.0', '1.1', '1.3', '1.5', '2.0', '3.0', '10.0']\n",
    "    )\n",
    "    plt.xscale('log')\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    for method, rank_values in ranks.items():\n",
    "        plt.plot(num_anchors_axis, rank_values, marker='o', label=method)\n",
    "\n",
    "    plt.xlabel('Number of Anchors')\n",
    "    plt.ylabel('Rank Correlation')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title('Rank Correlation vs Number of Anchors by Method')\n",
    "    plt.xticks(num_anchors_axis)\n",
    "    plt.yticks(\n",
    "        [0.60, 0.70, 0.80, 0.90, 0.92, 0.95, 0.98, 1.00],\n",
    "        ['0.60', '0.70', '0.80', '0.90', '0.92', '0.95', '0.98', '1.00']\n",
    "    )\n",
    "    plt.xscale('log')\n",
    "\n",
    "    # Save MAE plot\n",
    "    plt.figure(1)  # Switch to first figure (MAE plot)\n",
    "    plt.savefig(os.path.join(FIGURES_FOLDER, 'num_samples', 'mae_vs_n_anchors.pdf'), bbox_inches='tight')\n",
    "\n",
    "    # Save rank correlation plot\n",
    "    plt.figure(2)  # Switch to second figure (rank plot)\n",
    "    plt.savefig(os.path.join(FIGURES_FOLDER, 'num_samples', 'rank_vs_n_anchors.pdf'), bbox_inches='tight')\n",
    "\n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_n_anchors = make_figure_n_anchors(figure_n_anchors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_n_anchors = make_figure_n_anchors(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
