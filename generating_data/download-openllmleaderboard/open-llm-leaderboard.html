<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>T</th>
      <th>Model</th>
      <th>Average ⬆️</th>
      <th>ARC</th>
      <th>HellaSwag</th>
      <th>MMLU</th>
      <th>TruthfulQA</th>
      <th>Winogrande</th>
      <th>GSM8K</th>
      <th>Type</th>
      <th>Architecture</th>
      <th>Weight type</th>
      <th>Precision</th>
      <th>Merged</th>
      <th>Hub License</th>
      <th>#Params (B)</th>
      <th>Hub ❤️</th>
      <th>Available on the hub</th>
      <th>Model sha</th>
      <th>Flagged</th>
      <th>MoE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>🟦</td>
      <td>moreh/MoMo-70B-lora-1.8.6-DPO</td>
      <td>77.29</td>
      <td>70.14</td>
      <td>86.03</td>
      <td>77.40</td>
      <td>69.00</td>
      <td>84.37</td>
      <td>76.80</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>72.29</td>
      <td>9.0</td>
      <td>True</td>
      <td>76389d5d825c3743cc70bc75b902bbfdad11beba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Yi-34Bx2-MoE-60B</td>
      <td>76.72</td>
      <td>71.08</td>
      <td>85.23</td>
      <td>77.47</td>
      <td>66.19</td>
      <td>84.85</td>
      <td>75.51</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>60.81</td>
      <td>42.0</td>
      <td>True</td>
      <td>483359d70b3fef480cdaeb6d722a18626d34f0ce</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_34Bx2_MoE_60B</td>
      <td>76.66</td>
      <td>71.33</td>
      <td>85.25</td>
      <td>77.34</td>
      <td>66.59</td>
      <td>84.85</td>
      <td>74.60</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>60.81</td>
      <td>83.0</td>
      <td>True</td>
      <td>f49d7cf0a7b99b15bc98b0ef4a681e7f0f4aa92c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_34Bx2_MoE_60B</td>
      <td>76.63</td>
      <td>71.25</td>
      <td>85.36</td>
      <td>77.28</td>
      <td>66.61</td>
      <td>84.69</td>
      <td>74.60</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>60.81</td>
      <td>83.0</td>
      <td>True</td>
      <td>f49d7cf0a7b99b15bc98b0ef4a681e7f0f4aa92c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>moreh/MoMo-70B-lora-1.8.4-DPO</td>
      <td>76.23</td>
      <td>69.62</td>
      <td>85.35</td>
      <td>77.33</td>
      <td>64.64</td>
      <td>84.14</td>
      <td>76.27</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>72.29</td>
      <td>9.0</td>
      <td>True</td>
      <td>a2c3a87dd53a87dc9fc622ce4ddbb05d3e9cf6a9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cloudyu/Yi-34Bx3-MoE-90B</td>
      <td>76.18</td>
      <td>70.90</td>
      <td>85.33</td>
      <td>77.41</td>
      <td>66.31</td>
      <td>84.29</td>
      <td>72.86</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>87.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>b4b717be590394a4e70853cb444bd0964526c500</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>sumo43/Yi-32b-x2-v2.0</td>
      <td>76.17</td>
      <td>73.04</td>
      <td>85.95</td>
      <td>76.79</td>
      <td>73.22</td>
      <td>82.79</td>
      <td>65.20</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>mit</td>
      <td>60.81</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e61f28b326fe0080ad476ce2b1dd041ec9f147f</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>moreh/MoMo-70B-lora-1.8.5-DPO</td>
      <td>76.14</td>
      <td>69.54</td>
      <td>85.60</td>
      <td>77.49</td>
      <td>65.79</td>
      <td>84.14</td>
      <td>74.30</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>72.29</td>
      <td>0.0</td>
      <td>False</td>
      <td>7a0aadea285a82d50c96b0988b12cc3c6267249a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TomGrc/FusionNet_7Bx2_MoE_14B</td>
      <td>75.91</td>
      <td>73.55</td>
      <td>88.84</td>
      <td>64.68</td>
      <td>69.60</td>
      <td>88.16</td>
      <td>70.66</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>12.88</td>
      <td>13.0</td>
      <td>True</td>
      <td>a619fd0fcbdfcc897054491c2f285677bee38a11</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cloudyu/Mixtral_7Bx2_MoE_DPO</td>
      <td>75.88</td>
      <td>73.04</td>
      <td>88.76</td>
      <td>64.94</td>
      <td>81.50</td>
      <td>82.16</td>
      <td>64.90</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>12.88</td>
      <td>1.0</td>
      <td>True</td>
      <td>bf4cb27f17bfc58aaf6011a8ba8393a1177ebbe7</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Helion-4x34B</td>
      <td>75.48</td>
      <td>69.71</td>
      <td>85.28</td>
      <td>77.33</td>
      <td>63.91</td>
      <td>84.37</td>
      <td>72.25</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>113.66</td>
      <td>2.0</td>
      <td>True</td>
      <td>583254a5a134243d7793b311c465da12b10a3ff2</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>one-man-army/UNA-34Beagles-32K-bf16-v1</td>
      <td>75.41</td>
      <td>73.55</td>
      <td>85.93</td>
      <td>76.45</td>
      <td>73.55</td>
      <td>82.95</td>
      <td>60.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>6.0</td>
      <td>True</td>
      <td>d6024b97f624e9169a63f5faccb8c5ab121eb13a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/UNA-34Beagles-32K-v1</td>
      <td>75.41</td>
      <td>73.55</td>
      <td>85.93</td>
      <td>76.45</td>
      <td>73.55</td>
      <td>82.95</td>
      <td>60.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>False</td>
      <td>e02a631564990af3d9c8b0232f979af11cd8b6f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Cosmosis-3x34B</td>
      <td>75.39</td>
      <td>69.71</td>
      <td>85.18</td>
      <td>77.25</td>
      <td>63.82</td>
      <td>84.14</td>
      <td>72.25</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>87.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>644f20245c08dbbc6baad20100fcf0c8bd3181a0</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alnrg2arg/test2_4</td>
      <td>75.28</td>
      <td>73.55</td>
      <td>88.87</td>
      <td>64.63</td>
      <td>69.77</td>
      <td>84.45</td>
      <td>70.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>ed17cf5af87733ffd7836ab99f27991544ba2547</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Bagel-Hermes-34B-Slerp</td>
      <td>75.24</td>
      <td>70.73</td>
      <td>85.68</td>
      <td>77.29</td>
      <td>67.09</td>
      <td>84.37</td>
      <td>66.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>ca42d74d2b7fa947e27305e41c61784f8fe9dafa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>udkai/Garrulus</td>
      <td>75.16</td>
      <td>73.29</td>
      <td>88.87</td>
      <td>64.57</td>
      <td>68.23</td>
      <td>91.48</td>
      <td>64.52</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>9.0</td>
      <td>True</td>
      <td>cd2fa5c2188588b903fff2070a389db3b24031a4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dfurman/HermesBagel-34B-v0.1</td>
      <td>75.15</td>
      <td>70.56</td>
      <td>85.74</td>
      <td>77.38</td>
      <td>67.34</td>
      <td>84.61</td>
      <td>65.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>7364cfc1f2c7fc56d460adc0dc90d7a6d13641fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>eren23/slerp-test-turdus-beagle</td>
      <td>75.11</td>
      <td>73.55</td>
      <td>88.85</td>
      <td>64.62</td>
      <td>69.69</td>
      <td>83.90</td>
      <td>70.05</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f2aef36538bb0c7aab30ffe889e12b72f51a6816</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xDAN2099/xDAN-L2-moe-2x-v1</td>
      <td>75.10</td>
      <td>68.52</td>
      <td>86.31</td>
      <td>76.76</td>
      <td>61.77</td>
      <td>84.29</td>
      <td>72.93</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>60.81</td>
      <td>0.0</td>
      <td>False</td>
      <td>0a137b01142b62fccfcbc81176d40f4b86405958</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Bagel-Hermes-2x34b</td>
      <td>75.10</td>
      <td>69.80</td>
      <td>85.26</td>
      <td>77.24</td>
      <td>64.82</td>
      <td>84.77</td>
      <td>68.69</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>60.81</td>
      <td>9.0</td>
      <td>True</td>
      <td>d187b7bd6757d78bf89aaad8b0b5834ddbf29392</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sumo43/Yi-34b-x2</td>
      <td>75.02</td>
      <td>72.87</td>
      <td>85.70</td>
      <td>76.64</td>
      <td>72.10</td>
      <td>82.79</td>
      <td>60.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>mit</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>09876944a5d29e7f8e4da1347cd1d8f6f2151444</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zhengr/MixTAO-7Bx2-MoE-Instruct-v1.0</td>
      <td>74.99</td>
      <td>74.06</td>
      <td>88.25</td>
      <td>64.25</td>
      <td>69.61</td>
      <td>84.29</td>
      <td>69.45</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>02e3cacbd9a9518289f6101fbcca8f7a875c1dfc</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>?</td>
      <td>rwitz2/go-bruins-v2.1.1</td>
      <td>74.95</td>
      <td>72.87</td>
      <td>88.33</td>
      <td>65.18</td>
      <td>69.80</td>
      <td>82.24</td>
      <td>71.27</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc</td>
      <td>7.24</td>
      <td>22.0</td>
      <td>True</td>
      <td>bd56295eab54eaacbb3af6ecb88b9434d9966d4e</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>NLPinas/yi-bagel-2x34b-moe</td>
      <td>74.93</td>
      <td>72.70</td>
      <td>85.44</td>
      <td>76.60</td>
      <td>71.42</td>
      <td>82.72</td>
      <td>60.73</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>False</td>
      <td>a6de6f3ccb21eeef12a354c720a9a85e5e53433d</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nfaheem/Marcoroni-7b-DPO-Merge</td>
      <td>74.90</td>
      <td>73.04</td>
      <td>88.80</td>
      <td>64.24</td>
      <td>70.47</td>
      <td>85.24</td>
      <td>67.63</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>e3085d8aacffbf46b95e263bde509fce70577a26</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/quantum-dpo-v0.1</td>
      <td>74.87</td>
      <td>72.53</td>
      <td>88.37</td>
      <td>65.29</td>
      <td>69.92</td>
      <td>82.32</td>
      <td>70.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>09cbfe6569bcdddf623e9990498e9ad07345ad6a</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>janai-hq/trinity-v1</td>
      <td>74.80</td>
      <td>72.27</td>
      <td>88.36</td>
      <td>65.20</td>
      <td>69.31</td>
      <td>82.00</td>
      <td>71.65</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>09da1a24f84c96b8c09f2c07038986e28cc24ad5</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jan-hq/trinity-v1</td>
      <td>74.80</td>
      <td>72.27</td>
      <td>88.36</td>
      <td>65.20</td>
      <td>69.31</td>
      <td>82.00</td>
      <td>71.65</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>11.0</td>
      <td>True</td>
      <td>34974ae99668c381be0871778e3c42958544f70e</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/Beagle14-7B</td>
      <td>74.76</td>
      <td>72.95</td>
      <td>87.95</td>
      <td>64.70</td>
      <td>68.88</td>
      <td>82.64</td>
      <td>71.42</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>8.0</td>
      <td>True</td>
      <td>a5d1b1f831efe38df3b6ac125764a87ed094e282</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alnrg2arg/test2_3</td>
      <td>74.76</td>
      <td>72.95</td>
      <td>88.42</td>
      <td>64.80</td>
      <td>68.40</td>
      <td>84.14</td>
      <td>69.83</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e2c681fa4680ee19ca9758a2289da7d168546672</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GreenNode/GreenNodeLM-v3olet-7B</td>
      <td>74.75</td>
      <td>72.27</td>
      <td>88.25</td>
      <td>65.27</td>
      <td>69.52</td>
      <td>82.48</td>
      <td>70.74</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>94b36a4573657d7815f55b917b204e6b73f7a634</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ignos/LeoScorpius-GreenNode-Alpaca-7B-v1</td>
      <td>74.74</td>
      <td>72.35</td>
      <td>88.16</td>
      <td>65.23</td>
      <td>69.35</td>
      <td>82.32</td>
      <td>71.04</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>00827d42d79b7e10ddfc92c800cbb0636704e379</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mlabonne/NeuralBeagle14-7B</td>
      <td>74.74</td>
      <td>72.95</td>
      <td>88.34</td>
      <td>64.55</td>
      <td>69.93</td>
      <td>82.40</td>
      <td>70.28</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>21.0</td>
      <td>True</td>
      <td>33f76dd61715c8fd89f138092a8e8c7f3b3dd905</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/nontoxic-bagel-34b-v0.2</td>
      <td>74.69</td>
      <td>72.44</td>
      <td>85.64</td>
      <td>76.41</td>
      <td>72.70</td>
      <td>82.48</td>
      <td>58.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>29.0</td>
      <td>True</td>
      <td>08903c93d929829aabbde2681c7ad2465d7d4189</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/bagel-dpo-34b-v0.2</td>
      <td>74.69</td>
      <td>71.93</td>
      <td>85.25</td>
      <td>76.58</td>
      <td>70.05</td>
      <td>83.35</td>
      <td>60.96</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>68.0</td>
      <td>True</td>
      <td>fcc6ada5ea6dbf2f644d26b545ac402d2202cc74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/quantum-v0.01</td>
      <td>74.68</td>
      <td>72.53</td>
      <td>88.27</td>
      <td>65.20</td>
      <td>69.28</td>
      <td>82.56</td>
      <td>70.28</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>141a76559dace99bea213922c91cd23be8783c72</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/quantum-trinity-v0.1</td>
      <td>74.67</td>
      <td>72.53</td>
      <td>88.28</td>
      <td>65.19</td>
      <td>69.28</td>
      <td>82.56</td>
      <td>70.20</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>4e3eb8c21ff1689a348cc9ffdacd675aff3dde2b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>moreh/MoMo-70B-LoRA-V1.4</td>
      <td>74.67</td>
      <td>69.20</td>
      <td>85.07</td>
      <td>77.12</td>
      <td>62.66</td>
      <td>83.74</td>
      <td>70.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>72.29</td>
      <td>69.0</td>
      <td>True</td>
      <td>66bf25995056155b5d0796f7c0981e243bdd48f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ycros/BagelMIsteryTour-8x7B</td>
      <td>74.66</td>
      <td>72.44</td>
      <td>87.50</td>
      <td>71.25</td>
      <td>74.95</td>
      <td>82.00</td>
      <td>59.82</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>e210ea8194895c3429657556b41daaf722fd44a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>udkai/Turdus</td>
      <td>74.66</td>
      <td>73.38</td>
      <td>88.56</td>
      <td>64.52</td>
      <td>67.11</td>
      <td>86.66</td>
      <td>67.70</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>de8a9fbacf60f07146d7bda3455d3748e12200de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shadowml/DareBeagle-7B</td>
      <td>74.58</td>
      <td>71.67</td>
      <td>88.01</td>
      <td>65.03</td>
      <td>68.98</td>
      <td>82.32</td>
      <td>71.49</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7d5cb3c9ef547ad297d64789b188415e0320237a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mncai/mistral-7b-dpo-merge-v1.1</td>
      <td>74.53</td>
      <td>72.53</td>
      <td>88.15</td>
      <td>64.83</td>
      <td>68.48</td>
      <td>82.32</td>
      <td>70.89</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7fc6c09477cc606e91025c38b9963bc47dd396da</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jeonsworld/CarbonVillain-en-10.7B-v4</td>
      <td>74.52</td>
      <td>71.25</td>
      <td>88.48</td>
      <td>66.27</td>
      <td>71.95</td>
      <td>83.58</td>
      <td>65.58</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>5.0</td>
      <td>True</td>
      <td>904ffe8106a3facbea0d0e61d9a53a525675871e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>rwitz2/go-bruins-v2.1</td>
      <td>74.50</td>
      <td>71.93</td>
      <td>88.33</td>
      <td>65.00</td>
      <td>69.16</td>
      <td>82.16</td>
      <td>70.43</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e785d545369d201262bcc740ff127bb120d7a6b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/bagel-dpo-34b-v0.2</td>
      <td>74.50</td>
      <td>72.01</td>
      <td>85.24</td>
      <td>76.58</td>
      <td>70.16</td>
      <td>83.03</td>
      <td>59.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>68.0</td>
      <td>True</td>
      <td>fcc6ada5ea6dbf2f644d26b545ac402d2202cc74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mncai/mistral-7b-dpo-v6</td>
      <td>74.50</td>
      <td>72.53</td>
      <td>88.10</td>
      <td>64.68</td>
      <td>68.24</td>
      <td>82.56</td>
      <td>70.89</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>9.0</td>
      <td>True</td>
      <td>206be3fd589dd62817343c53525ab7fb1b752faf</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>logicker/SkkuDataScienceGlobal-10.7b</td>
      <td>74.50</td>
      <td>71.25</td>
      <td>88.41</td>
      <td>66.31</td>
      <td>71.92</td>
      <td>83.35</td>
      <td>65.73</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>10.73</td>
      <td>0.0</td>
      <td>False</td>
      <td>4f5e40b38099084b86fb18b294e4e61e7d20cc7c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shadowml/DareBeagel-2x7B</td>
      <td>74.49</td>
      <td>72.01</td>
      <td>88.12</td>
      <td>64.51</td>
      <td>69.09</td>
      <td>82.72</td>
      <td>70.51</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>5cecd5e1f9723e3f7d287cbc9fd6d42056f73405</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TomGrc/FusionNet_linear</td>
      <td>74.43</td>
      <td>71.25</td>
      <td>88.44</td>
      <td>66.35</td>
      <td>71.94</td>
      <td>83.27</td>
      <td>65.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>mit</td>
      <td>10.73</td>
      <td>7.0</td>
      <td>True</td>
      <td>a6eba075d53fc4bdbcded071f9bdeb287d1ac260</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>gagan3012/MetaModel_moe</td>
      <td>74.42</td>
      <td>71.25</td>
      <td>88.40</td>
      <td>66.26</td>
      <td>71.86</td>
      <td>83.35</td>
      <td>65.43</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>36.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>015dae67b68e6e5007b7b13a448886eb5f6bfea8</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jeonsworld/CarbonVillain-en-10.7B-v2</td>
      <td>74.42</td>
      <td>71.25</td>
      <td>88.40</td>
      <td>66.31</td>
      <td>71.94</td>
      <td>83.35</td>
      <td>65.28</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>70b507c12dfe6ce8a7d050be5475fc9684a4929f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DopeorNope/SOLARC-M-10.7B</td>
      <td>74.42</td>
      <td>71.16</td>
      <td>88.41</td>
      <td>66.31</td>
      <td>71.85</td>
      <td>83.35</td>
      <td>65.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>7.0</td>
      <td>True</td>
      <td>fa95c376fdad1670d4125e833322dbf6aeb8f410</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_11Bx2_MoE_19B</td>
      <td>74.41</td>
      <td>71.16</td>
      <td>88.47</td>
      <td>66.31</td>
      <td>72.00</td>
      <td>83.27</td>
      <td>65.28</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.19</td>
      <td>24.0</td>
      <td>True</td>
      <td>092208b5bfab866b301545149a6b14fde48a0dd6</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jeonsworld/CarbonVillain-en-10.7B-v3</td>
      <td>74.41</td>
      <td>70.99</td>
      <td>88.48</td>
      <td>66.34</td>
      <td>71.84</td>
      <td>83.58</td>
      <td>65.20</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>285436a72c10e0f2b8eb897549350fe40c2e8bbe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kekmodel/StopCarbon-10.7B-v5</td>
      <td>74.41</td>
      <td>70.99</td>
      <td>88.48</td>
      <td>66.34</td>
      <td>71.84</td>
      <td>83.58</td>
      <td>65.20</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>2.0</td>
      <td>True</td>
      <td>57966bc616a9db7756488661f4ed16b40ee23780</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gagan3012/MetaModel</td>
      <td>74.40</td>
      <td>71.08</td>
      <td>88.45</td>
      <td>66.26</td>
      <td>71.84</td>
      <td>83.43</td>
      <td>65.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>06308e54585a49a01a93c99caa2fb34daf4e7619</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kyujinpy/Sakura-SOLAR-Instruct</td>
      <td>74.40</td>
      <td>70.99</td>
      <td>88.42</td>
      <td>66.33</td>
      <td>71.79</td>
      <td>83.66</td>
      <td>65.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>26.0</td>
      <td>True</td>
      <td>cc4531a25fff7cbb146c0e12f2cf4e19189c37a2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Xenon1/MetaModel_moex8</td>
      <td>74.39</td>
      <td>71.16</td>
      <td>88.38</td>
      <td>66.29</td>
      <td>71.91</td>
      <td>83.27</td>
      <td>65.35</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>69.92</td>
      <td>2.0</td>
      <td>True</td>
      <td>333524a8c6ed8415fd48f852e53c405cac82733d</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>gagan3012/MetaModelv3</td>
      <td>74.39</td>
      <td>71.16</td>
      <td>88.39</td>
      <td>66.32</td>
      <td>71.86</td>
      <td>83.35</td>
      <td>65.28</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>862f5ca5e66a0b053c14e40c8f16f2c2807b6d92</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TomGrc/FusionNet</td>
      <td>74.38</td>
      <td>71.25</td>
      <td>88.42</td>
      <td>66.36</td>
      <td>71.95</td>
      <td>83.27</td>
      <td>65.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>mit</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>34421f146e5eb3306a86dd8b67ec938e800ee52e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>flemmingmiguel/DareBeagle-7B</td>
      <td>74.35</td>
      <td>71.59</td>
      <td>87.98</td>
      <td>65.21</td>
      <td>68.30</td>
      <td>81.93</td>
      <td>71.11</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>53e5b634de4ae9ef8a127c1d7a0c543acfba1b47</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kodonho/Solar-OrcaDPO-Solar-Instruct-SLERP</td>
      <td>74.35</td>
      <td>70.99</td>
      <td>88.22</td>
      <td>66.22</td>
      <td>71.95</td>
      <td>83.43</td>
      <td>65.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea7a7a1c14c4b67bad56dbd08245dbb79dc71ec3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DopeorNope/SOLARC-MOE-10.7Bx6</td>
      <td>74.35</td>
      <td>70.90</td>
      <td>88.40</td>
      <td>66.36</td>
      <td>71.85</td>
      <td>83.66</td>
      <td>64.90</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>53.01</td>
      <td>15.0</td>
      <td>True</td>
      <td>64c8ef9fa6d9b54b68261d839b656b0dc8717374</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>CultriX/MergeTrix-7B</td>
      <td>74.33</td>
      <td>72.27</td>
      <td>87.84</td>
      <td>64.88</td>
      <td>66.27</td>
      <td>83.50</td>
      <td>71.19</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>d11bd6b388581d2a44c1431a9985e8fc77addd33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kekmodel/StopCarbon-10.7B-v6</td>
      <td>74.31</td>
      <td>71.16</td>
      <td>88.50</td>
      <td>66.31</td>
      <td>71.96</td>
      <td>83.43</td>
      <td>64.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>mit</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>8953a705c909ef98fe3b0ea524c5816a57f1954c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jeonsworld/CarbonVillain-en-10.7B-v5</td>
      <td>74.31</td>
      <td>71.16</td>
      <td>88.51</td>
      <td>66.44</td>
      <td>71.97</td>
      <td>83.35</td>
      <td>64.44</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>941b5a690781dd412eb435446b65e92048992abe</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kodonho/SolarM-SakuraSolar-SLERP</td>
      <td>74.29</td>
      <td>71.16</td>
      <td>88.47</td>
      <td>66.24</td>
      <td>72.10</td>
      <td>83.11</td>
      <td>64.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>c54dbc0da9e028cfaf92114206c6b84c0198d2b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kekmodel/StopCarbon-10.7B-v4</td>
      <td>74.29</td>
      <td>71.25</td>
      <td>88.50</td>
      <td>66.24</td>
      <td>71.89</td>
      <td>83.43</td>
      <td>64.44</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>c8d98bb8c6b23b3c3b7462df7eb02a3b05622612</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>gagan3012/MetaModel_moe</td>
      <td>74.28</td>
      <td>71.08</td>
      <td>88.39</td>
      <td>66.31</td>
      <td>71.82</td>
      <td>83.50</td>
      <td>64.59</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>36.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>015dae67b68e6e5007b7b13a448886eb5f6bfea8</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jeonsworld/CarbonVillain-en-13B-v1</td>
      <td>74.28</td>
      <td>71.25</td>
      <td>88.46</td>
      <td>66.42</td>
      <td>71.98</td>
      <td>83.27</td>
      <td>64.29</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>a797e7e81f7929a31ca232858318d72b93b6abe0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jeonsworld/CarbonVillain-en-10.7B-v1</td>
      <td>74.28</td>
      <td>71.25</td>
      <td>88.46</td>
      <td>66.42</td>
      <td>71.98</td>
      <td>83.27</td>
      <td>64.29</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>a797e7e81f7929a31ca232858318d72b93b6abe0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DopeorNope/SOLARC-MOE-10.7Bx4</td>
      <td>74.27</td>
      <td>70.99</td>
      <td>88.43</td>
      <td>66.34</td>
      <td>71.91</td>
      <td>83.58</td>
      <td>64.37</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>36.10</td>
      <td>7.0</td>
      <td>True</td>
      <td>07cee5a25fd8d85486f888893d5bee532e5f5cd8</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>bhavinjawade/SOLAR-10B-OrcaDPO-Jawade</td>
      <td>74.27</td>
      <td>71.16</td>
      <td>88.27</td>
      <td>66.12</td>
      <td>71.57</td>
      <td>83.66</td>
      <td>64.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>02a497125bbf85fe0355eb22424315c920d1aec4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct</td>
      <td>74.26</td>
      <td>70.90</td>
      <td>88.30</td>
      <td>66.15</td>
      <td>71.80</td>
      <td>83.74</td>
      <td>64.67</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>20.0</td>
      <td>True</td>
      <td>564c02554a8b1f91c0860096bdb830dc15ac7805</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct-test</td>
      <td>74.26</td>
      <td>70.90</td>
      <td>88.30</td>
      <td>66.15</td>
      <td>71.80</td>
      <td>83.74</td>
      <td>64.67</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>10.73</td>
      <td>0.0</td>
      <td>False</td>
      <td>ae0cab05b071dcde2e89e80ab511fa1bc0f53f1c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/testtest</td>
      <td>74.24</td>
      <td>70.82</td>
      <td>84.88</td>
      <td>76.66</td>
      <td>69.90</td>
      <td>82.08</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>34.39</td>
      <td>0.0</td>
      <td>False</td>
      <td>e9be60931d3abdf3b08a55f13e4c7586918b2be8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gagan3012/MetaModelv2</td>
      <td>74.24</td>
      <td>71.08</td>
      <td>88.56</td>
      <td>66.29</td>
      <td>71.94</td>
      <td>83.11</td>
      <td>64.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>2cb9c69984ee3e5506f055238fd1aa5fe4ea91bd</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>naseerfaheem/SOLAR-10.7B-Instruct-ties</td>
      <td>74.21</td>
      <td>70.90</td>
      <td>88.58</td>
      <td>66.34</td>
      <td>71.88</td>
      <td>83.50</td>
      <td>64.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>False</td>
      <td>333fbc56f7406a47435ad9afbde01c4f8116287e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kekmodel/StopCarbon-10.7B-v2</td>
      <td>74.21</td>
      <td>71.08</td>
      <td>88.60</td>
      <td>66.23</td>
      <td>72.01</td>
      <td>83.50</td>
      <td>63.84</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>7b49c998e2a32006e27d3e826d19240ed6bdd697</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>VAGOsolutions/SauerkrautLM-SOLAR-Instruct</td>
      <td>74.21</td>
      <td>70.82</td>
      <td>88.63</td>
      <td>66.20</td>
      <td>71.95</td>
      <td>83.50</td>
      <td>64.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>37.0</td>
      <td>True</td>
      <td>8b9615124a0bcadd7fa984eaadd066da0fb4fbae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dfurman/GarrulusMarcoro-7B-v0.1</td>
      <td>74.20</td>
      <td>72.35</td>
      <td>88.00</td>
      <td>64.65</td>
      <td>67.05</td>
      <td>87.21</td>
      <td>65.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>c53e0d67f4684a46d35ded045c21e19e380f5e91</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kekmodel/StopCarbon-10.7B-v1</td>
      <td>74.20</td>
      <td>70.90</td>
      <td>88.41</td>
      <td>66.32</td>
      <td>71.71</td>
      <td>83.74</td>
      <td>64.14</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1681ef65f3d06b421969199ae07b8d32feecf9a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>upstage/SOLAR-10.7B-Instruct-v1.0</td>
      <td>74.20</td>
      <td>71.08</td>
      <td>88.16</td>
      <td>66.21</td>
      <td>71.43</td>
      <td>83.58</td>
      <td>64.75</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>455.0</td>
      <td>True</td>
      <td>d3167df97a44b8632538b32ee8cd887893ea1435</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/UNA-SOLAR-10.7B-Instruct-v1.0</td>
      <td>74.20</td>
      <td>70.56</td>
      <td>88.18</td>
      <td>66.08</td>
      <td>72.05</td>
      <td>83.66</td>
      <td>64.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>10.73</td>
      <td>12.0</td>
      <td>True</td>
      <td>08d3f07da7160e9657630ba98531850905619def</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>bhavinjawade/SOLAR-10B-Nector-DPO-Jawade</td>
      <td>74.19</td>
      <td>71.33</td>
      <td>88.62</td>
      <td>66.22</td>
      <td>70.92</td>
      <td>83.43</td>
      <td>64.59</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>669f8f726fac4a588ced06a4da3959eb8ca20f9f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shadowml/Mixolar-4x7b</td>
      <td>74.18</td>
      <td>71.08</td>
      <td>88.44</td>
      <td>66.29</td>
      <td>71.81</td>
      <td>83.58</td>
      <td>63.91</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>36.10</td>
      <td>2.0</td>
      <td>True</td>
      <td>5a1b8a9c8df923c7c0e38fe9e534f73968603030</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>GreenNode/GreenNodeLM-7B-v4leo</td>
      <td>74.18</td>
      <td>71.25</td>
      <td>88.24</td>
      <td>65.01</td>
      <td>69.65</td>
      <td>82.32</td>
      <td>68.61</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9286f6fac1df497203e110070322c93dab33fdd2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/una-xaberius-34b-v1beta</td>
      <td>74.18</td>
      <td>70.39</td>
      <td>86.77</td>
      <td>78.15</td>
      <td>61.45</td>
      <td>84.93</td>
      <td>63.38</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>34.39</td>
      <td>80.0</td>
      <td>True</td>
      <td>233b63015f389d0023cfa21727632b340cadbdb5</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>kyujinpy/Sakura-SOLRCA-Math-Instruct-DPO-v2</td>
      <td>74.17</td>
      <td>71.25</td>
      <td>88.52</td>
      <td>66.13</td>
      <td>72.16</td>
      <td>83.03</td>
      <td>63.91</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>c994171eefa80df644e31ac01c1ee2d9e5546d99</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>kyujinpy/Sakura-SOLAR-Instruct-DPO-v2</td>
      <td>74.14</td>
      <td>70.90</td>
      <td>88.41</td>
      <td>66.48</td>
      <td>71.86</td>
      <td>83.43</td>
      <td>63.76</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>2.0</td>
      <td>True</td>
      <td>7f45a1ed9ca0f88b9ec23aa9b6202e8783ab35ac</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>kyujinpy/Sakura-SOLRCA-Math-Instruct-DPO-v1</td>
      <td>74.13</td>
      <td>71.25</td>
      <td>88.48</td>
      <td>66.21</td>
      <td>72.12</td>
      <td>82.87</td>
      <td>63.84</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>60e68b717f30144757b2e51d1db879c0c628f128</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mlabonne/NeuralDaredevil-7B</td>
      <td>74.12</td>
      <td>69.88</td>
      <td>87.62</td>
      <td>65.12</td>
      <td>66.85</td>
      <td>82.08</td>
      <td>73.16</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>f03ff71ca0b07edccda0d2f407049dcf18edfb4d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>dhanushreddy29/BrokenKeyboard</td>
      <td>74.08</td>
      <td>71.25</td>
      <td>88.34</td>
      <td>66.04</td>
      <td>71.36</td>
      <td>83.19</td>
      <td>64.29</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>c03dfcda5d45ea4c518bd14641d9604726e00477</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>brucethemoose/SUS-Bagel-200K-DARE-Test</td>
      <td>74.07</td>
      <td>68.09</td>
      <td>85.38</td>
      <td>76.98</td>
      <td>61.20</td>
      <td>83.50</td>
      <td>69.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>2.0</td>
      <td>True</td>
      <td>063c5412143468d6408b6b8122ec925c0baa0add</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>fblgit/UNA-SOLAR-10.7B-Instruct-v1.0</td>
      <td>74.07</td>
      <td>70.73</td>
      <td>88.32</td>
      <td>66.10</td>
      <td>72.52</td>
      <td>83.35</td>
      <td>63.38</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>10.73</td>
      <td>12.0</td>
      <td>True</td>
      <td>c63d06344214886094d7ab6c7fd5692cc59fdf0d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/UNA-POLAR-10.7B-InstructMath-v2</td>
      <td>74.07</td>
      <td>70.73</td>
      <td>88.20</td>
      <td>66.03</td>
      <td>71.73</td>
      <td>82.95</td>
      <td>64.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>b47d17b0df02e38e97f565784bb3cf948b29a6ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Nous-Hermes-2-SUS-Chat-34B-Slerp</td>
      <td>74.06</td>
      <td>66.72</td>
      <td>84.97</td>
      <td>77.00</td>
      <td>59.23</td>
      <td>83.58</td>
      <td>72.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>511cc63b3efca6f036fdbbe15f312d0e2b7e5cf5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yhyu13/LMCocktail-10.7B-v1</td>
      <td>74.06</td>
      <td>70.65</td>
      <td>88.13</td>
      <td>66.21</td>
      <td>71.03</td>
      <td>83.35</td>
      <td>64.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>10.73</td>
      <td>11.0</td>
      <td>True</td>
      <td>79ec3a42118f0715666b86bacab2688b62e1433b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yhyu13/LMCocktail-10.7B-v1</td>
      <td>74.06</td>
      <td>70.65</td>
      <td>88.13</td>
      <td>66.21</td>
      <td>71.03</td>
      <td>83.35</td>
      <td>64.97</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>79ec3a42118f0715666b86bacab2688b62e1433b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>kyujinpy/Sakura-SOLRCA-Instruct-DPO</td>
      <td>74.05</td>
      <td>71.16</td>
      <td>88.49</td>
      <td>66.17</td>
      <td>72.10</td>
      <td>82.95</td>
      <td>63.46</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>48977e38731685ad9a45eef6ff94d5d6f60471f2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/UNAversal-2x7B-v1</td>
      <td>74.05</td>
      <td>73.38</td>
      <td>87.87</td>
      <td>63.49</td>
      <td>69.93</td>
      <td>82.08</td>
      <td>67.55</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>3.0</td>
      <td>True</td>
      <td>514783cefac2b142adb50ee5f61dd724d62910cf</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/una-cybertron-7b-v3-OMA</td>
      <td>74.01</td>
      <td>73.04</td>
      <td>87.94</td>
      <td>63.44</td>
      <td>69.85</td>
      <td>82.08</td>
      <td>67.70</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>8.0</td>
      <td>True</td>
      <td>29c9ff0a9f5daa5adc797a34508bcca50205f34f</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kekmodel/StopCarbon-10.7B-v3</td>
      <td>74.01</td>
      <td>70.99</td>
      <td>88.57</td>
      <td>66.13</td>
      <td>71.94</td>
      <td>83.19</td>
      <td>63.23</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>4cf314aa78f585376918a1be8b5a246edf9f4e71</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zhengr/MixTAO-7Bx2-MoE-DPO</td>
      <td>73.94</td>
      <td>70.90</td>
      <td>87.12</td>
      <td>64.72</td>
      <td>69.34</td>
      <td>81.22</td>
      <td>70.36</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>911149cad645ccb189cb403c16bbed98df18dfd6</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rishiraj/meow</td>
      <td>73.94</td>
      <td>70.48</td>
      <td>88.08</td>
      <td>66.25</td>
      <td>70.49</td>
      <td>83.43</td>
      <td>64.90</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>d933dcd7cbb19916f4732ae7e3892a656a8c3d27</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>viethq188/LeoScorpius-7B-Chat-DPO</td>
      <td>73.92</td>
      <td>70.48</td>
      <td>87.97</td>
      <td>65.08</td>
      <td>68.83</td>
      <td>82.08</td>
      <td>69.07</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>6e6e0a6e5c309acbe124a8055138ea5a4f2e56d1</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>fblgit/UNA-TheBeagle-7b-v1</td>
      <td>73.87</td>
      <td>73.04</td>
      <td>88.00</td>
      <td>63.48</td>
      <td>69.85</td>
      <td>82.16</td>
      <td>66.72</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.24</td>
      <td>14.0</td>
      <td>True</td>
      <td>72084679bda2e7679259e9c0fa2fdcd48ecb158c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mncai/mistral-7b-dpo-v5</td>
      <td>73.87</td>
      <td>72.01</td>
      <td>87.57</td>
      <td>63.85</td>
      <td>66.86</td>
      <td>82.24</td>
      <td>70.66</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>8108f313d878ce848ceceeaf55ce8b3ecaaee792</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Nous-Hermes-2-SUS-Chat-2x34B</td>
      <td>73.82</td>
      <td>66.81</td>
      <td>85.22</td>
      <td>76.65</td>
      <td>57.42</td>
      <td>83.74</td>
      <td>73.09</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>60.81</td>
      <td>3.0</td>
      <td>True</td>
      <td>fd230896bc3a1cfabdf37f8d8389cd670ea72faa</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>flemmingmiguel/MarcMistral-7B</td>
      <td>73.81</td>
      <td>71.16</td>
      <td>87.78</td>
      <td>65.38</td>
      <td>64.92</td>
      <td>81.69</td>
      <td>71.95</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>4571c6a5382eedacb74a51d1dfb0a6f378becc86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/LUNA-SOLARkrautLM-Instruct</td>
      <td>73.79</td>
      <td>71.16</td>
      <td>88.28</td>
      <td>66.11</td>
      <td>73.37</td>
      <td>82.95</td>
      <td>60.88</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>8.0</td>
      <td>True</td>
      <td>3b6604be8133f311d0719acb95d1a3a1f62a7d67</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/UNAversal-8x7B-v1beta</td>
      <td>73.78</td>
      <td>69.80</td>
      <td>86.90</td>
      <td>70.39</td>
      <td>71.97</td>
      <td>82.00</td>
      <td>61.64</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>46.70</td>
      <td>7.0</td>
      <td>True</td>
      <td>db160d4bc5bd9f2e66a764aeb44dcd18fb8afa6d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sophosympatheia/Aurora-Nights-70B-v1.0</td>
      <td>73.77</td>
      <td>71.33</td>
      <td>88.33</td>
      <td>70.47</td>
      <td>62.81</td>
      <td>83.35</td>
      <td>66.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>7.0</td>
      <td>True</td>
      <td>e4b4ee3d952b1e8360a82d2b3506fd5b4ab68df9</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Hermes-2-Yi-34B</td>
      <td>73.74</td>
      <td>66.89</td>
      <td>85.49</td>
      <td>76.70</td>
      <td>60.37</td>
      <td>82.95</td>
      <td>70.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>34.39</td>
      <td>136.0</td>
      <td>True</td>
      <td>deb99d98742ec9691ef593418bea71a4437745a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>decruz07/kellemar-DPO-Orca-Distilled-7B-SLERP</td>
      <td>73.71</td>
      <td>70.48</td>
      <td>87.56</td>
      <td>65.33</td>
      <td>64.97</td>
      <td>81.93</td>
      <td>72.02</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>efb6caff9804383600563a658ba18720ec3b2d11</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Nous-Hermes-2-SUS-Chat-34B-Linear</td>
      <td>73.69</td>
      <td>66.38</td>
      <td>84.94</td>
      <td>76.82</td>
      <td>59.19</td>
      <td>82.79</td>
      <td>72.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>91673157803a869009e04e588c15914f132fb46b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>v1olet/v1olet_merged_dpo_7B_v3</td>
      <td>73.68</td>
      <td>72.61</td>
      <td>87.70</td>
      <td>63.51</td>
      <td>69.07</td>
      <td>82.32</td>
      <td>66.87</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>63b69bf2588f3b108d3427389d3c707f6b50d2ba</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>GreenNode/GreenNodeLM-7B-v1olet</td>
      <td>73.68</td>
      <td>72.61</td>
      <td>87.70</td>
      <td>63.51</td>
      <td>69.07</td>
      <td>82.32</td>
      <td>66.87</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4f0d53e65814390b8a260dd23fe5a30ced239176</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>flemmingmiguel/NeuDist-Ro-7B</td>
      <td>73.64</td>
      <td>71.25</td>
      <td>87.48</td>
      <td>65.13</td>
      <td>64.93</td>
      <td>82.08</td>
      <td>70.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>c48a29d5543deb8ab9afb4dec0eb0c1a47f2c222</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>argilla/distilabeled-Marcoro14-7B-slerp</td>
      <td>73.63</td>
      <td>70.73</td>
      <td>87.47</td>
      <td>65.22</td>
      <td>65.10</td>
      <td>82.08</td>
      <td>71.19</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>baa14c82695e595b5d39f35068898feb6fdceb34</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Qwen/Qwen-72B</td>
      <td>73.60</td>
      <td>65.19</td>
      <td>85.94</td>
      <td>77.37</td>
      <td>60.19</td>
      <td>82.48</td>
      <td>70.43</td>
      <td>pretrained</td>
      <td>QWenLMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>72.29</td>
      <td>251.0</td>
      <td>True</td>
      <td>f62c59844a8de3c27cf22735218d77e9fa9f6b17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rombodawg/Open_Gpt4_8x7B_v0.2</td>
      <td>73.59</td>
      <td>68.69</td>
      <td>86.16</td>
      <td>72.07</td>
      <td>71.92</td>
      <td>83.58</td>
      <td>59.14</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>46.70</td>
      <td>6.0</td>
      <td>True</td>
      <td>3aba335d2131a014494a9df7c8a3d0783f50bad8</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>CultriX/MistralTrix-SLERP</td>
      <td>73.58</td>
      <td>70.82</td>
      <td>87.54</td>
      <td>64.98</td>
      <td>65.35</td>
      <td>81.69</td>
      <td>71.11</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>ebc368fef4f5f6d3bef7d7839e58afd1c4dd3bfc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mlabonne/NeuralMarcoro14-7B</td>
      <td>73.57</td>
      <td>71.42</td>
      <td>87.59</td>
      <td>64.84</td>
      <td>65.64</td>
      <td>81.22</td>
      <td>70.74</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>23.0</td>
      <td>True</td>
      <td>df267682dbafe08a877602e6588bf461b6607d74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abideen/NexoNimbus-7B</td>
      <td>73.50</td>
      <td>70.82</td>
      <td>87.86</td>
      <td>64.69</td>
      <td>62.43</td>
      <td>84.85</td>
      <td>70.36</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>12f194df2152bd4b9431b25e06fff9e47713d03d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Neuronovo/neuronovo-7B-v0.2</td>
      <td>73.44</td>
      <td>73.04</td>
      <td>88.32</td>
      <td>65.15</td>
      <td>71.02</td>
      <td>80.66</td>
      <td>62.47</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>False</td>
      <td>72b49b8390caf1413a4bc33a759c147525510482</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_7Bx2_MoE</td>
      <td>73.43</td>
      <td>71.25</td>
      <td>87.45</td>
      <td>64.98</td>
      <td>67.23</td>
      <td>81.22</td>
      <td>68.46</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>12.88</td>
      <td>24.0</td>
      <td>True</td>
      <td>4295fae8ef44f19f38f5391dc0c7194db096c4b2</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Neuronovo/neuronovo-9B-v0.4</td>
      <td>73.42</td>
      <td>72.44</td>
      <td>88.33</td>
      <td>65.24</td>
      <td>71.07</td>
      <td>80.66</td>
      <td>62.77</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>True</td>
      <td>f4bfa8b298cbd0acc236117231d5b00de5f43240</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/BruinHermes</td>
      <td>73.42</td>
      <td>70.14</td>
      <td>87.07</td>
      <td>65.22</td>
      <td>65.60</td>
      <td>81.29</td>
      <td>71.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>unknown</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>59db3aa4f37411d5c97a6182dcf5ecfe1757ee4a</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>argilla/distilabeled-Marcoro14-7B-slerp-full</td>
      <td>73.40</td>
      <td>70.65</td>
      <td>87.55</td>
      <td>65.33</td>
      <td>64.21</td>
      <td>82.00</td>
      <td>70.66</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>8a4b63ce6161a85d53a5ac9504a758e95ac052dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>CultriX/MistralTrix-v1</td>
      <td>73.39</td>
      <td>72.27</td>
      <td>88.33</td>
      <td>65.24</td>
      <td>70.73</td>
      <td>80.98</td>
      <td>62.77</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>92.0</td>
      <td>True</td>
      <td>e09045608b2d68a6412185817306f4bb0cf3530c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cloudyu/Mixtral_7Bx5_MoE_30B</td>
      <td>73.39</td>
      <td>69.97</td>
      <td>86.82</td>
      <td>64.42</td>
      <td>65.97</td>
      <td>80.98</td>
      <td>72.18</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>29.79</td>
      <td>0.0</td>
      <td>False</td>
      <td>323fba03ac21b03df8d04ab575741429cc509d7b</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>macadeliccc/SOLAR-math-2x10.7b</td>
      <td>73.37</td>
      <td>68.43</td>
      <td>86.31</td>
      <td>66.90</td>
      <td>64.21</td>
      <td>83.35</td>
      <td>71.04</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>19.19</td>
      <td>0.0</td>
      <td>True</td>
      <td>10953f7a3d411b148dcbb4363b1508d0efc303a2</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shadowml/Daredevil-7B</td>
      <td>73.36</td>
      <td>69.37</td>
      <td>87.17</td>
      <td>65.30</td>
      <td>64.09</td>
      <td>81.29</td>
      <td>72.93</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>015a362ee09e6370ad5b1b70fad8a7ebfcdc9e74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>ryandt/MusingCaterpillar</td>
      <td>73.33</td>
      <td>72.53</td>
      <td>88.34</td>
      <td>65.26</td>
      <td>70.93</td>
      <td>80.66</td>
      <td>62.24</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>True</td>
      <td>83c266f92d51adb87ed2c259f2c151f05fb10cc2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_7Bx6_MoE_35B</td>
      <td>73.32</td>
      <td>70.14</td>
      <td>86.77</td>
      <td>64.74</td>
      <td>65.79</td>
      <td>81.06</td>
      <td>71.42</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>35.43</td>
      <td>0.0</td>
      <td>False</td>
      <td>e1b7ae70975e235240f8a6b998eab635f37eb342</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cloudyu/Mixtral_7Bx6_MoE_35B</td>
      <td>73.31</td>
      <td>69.97</td>
      <td>86.82</td>
      <td>64.91</td>
      <td>65.77</td>
      <td>81.14</td>
      <td>71.27</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>35.43</td>
      <td>0.0</td>
      <td>False</td>
      <td>e1b7ae70975e235240f8a6b998eab635f37eb342</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ibndias/Nous-Hermes-2-MoE-2x34B</td>
      <td>73.30</td>
      <td>66.64</td>
      <td>85.73</td>
      <td>76.49</td>
      <td>58.08</td>
      <td>83.35</td>
      <td>69.52</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>60.81</td>
      <td>0.0</td>
      <td>True</td>
      <td>af9757f0420e27e2a332cc16cbe1eeefe99cb5f1</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zyh3826/GML-Mistral-merged-v1</td>
      <td>73.30</td>
      <td>71.25</td>
      <td>87.88</td>
      <td>65.42</td>
      <td>69.28</td>
      <td>80.98</td>
      <td>64.97</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>2.0</td>
      <td>True</td>
      <td>17a3d5eb5dc23b8a7c29d33cfcd07140a083aa1f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>GreenNode/GreenNodeLM-7B-v2leo</td>
      <td>73.29</td>
      <td>69.80</td>
      <td>88.02</td>
      <td>65.00</td>
      <td>67.83</td>
      <td>82.00</td>
      <td>67.10</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e5a0955eb36568aa850cd73debbe9815a9d1e60a</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Neuronovo/neuronovo-7B-v0.3</td>
      <td>73.29</td>
      <td>72.70</td>
      <td>88.26</td>
      <td>65.10</td>
      <td>71.35</td>
      <td>80.90</td>
      <td>61.41</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>False</td>
      <td>6f5c9f242610ade5940a6e04d367ef9398409b73</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>occultml/CatMarcoro14-7B-slerp</td>
      <td>73.25</td>
      <td>69.37</td>
      <td>86.92</td>
      <td>65.27</td>
      <td>63.24</td>
      <td>81.69</td>
      <td>73.01</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>07d9e69a880d1c31c29c932f4fae6c36ceda01ea</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/Eukaryote-8x7B-bf16</td>
      <td>73.23</td>
      <td>69.45</td>
      <td>87.29</td>
      <td>65.15</td>
      <td>63.17</td>
      <td>82.40</td>
      <td>71.95</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>5ccee182c62fcbadfe91f66b74590aea40b181e5</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/WinterGoddess-1.4x-70B-L2</td>
      <td>73.23</td>
      <td>72.78</td>
      <td>90.11</td>
      <td>71.12</td>
      <td>65.76</td>
      <td>85.00</td>
      <td>54.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>7.0</td>
      <td>True</td>
      <td>5197257333076dd80821a5055abae7d21a7dc844</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>SUSTech/SUS-Chat-34B</td>
      <td>73.22</td>
      <td>66.30</td>
      <td>83.91</td>
      <td>76.41</td>
      <td>57.04</td>
      <td>83.50</td>
      <td>72.18</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>100.0</td>
      <td>True</td>
      <td>01f1a7861667c4869bb03251dfd10526bf846e9c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/SOLAR-10.7B-NahIdWin</td>
      <td>73.21</td>
      <td>64.51</td>
      <td>85.67</td>
      <td>64.17</td>
      <td>76.73</td>
      <td>80.51</td>
      <td>67.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>5.0</td>
      <td>True</td>
      <td>94bad5a6b469d84f556d6cc52c44fd88c07476f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dillfrescott/trinity-medium</td>
      <td>73.21</td>
      <td>71.50</td>
      <td>86.99</td>
      <td>65.04</td>
      <td>69.54</td>
      <td>81.14</td>
      <td>65.05</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>e0d20c61e1bcd8e305da40e20219edf7649d2952</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>argilla/notus-8x7b-experiment</td>
      <td>73.18</td>
      <td>70.99</td>
      <td>87.73</td>
      <td>71.33</td>
      <td>65.79</td>
      <td>81.61</td>
      <td>61.64</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>86c89d182babd29521a41a54528e5bf8331ed4cd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>CultriX/MistralTrixTest</td>
      <td>73.17</td>
      <td>72.53</td>
      <td>88.40</td>
      <td>65.22</td>
      <td>70.77</td>
      <td>81.37</td>
      <td>60.73</td>
      <td>RL-tuned</td>
      <td>?</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>False</td>
      <td>4e6a6b8022ce4b3b71b332c3389067613bd7f850</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>macadeliccc/Orca-SOLAR-4x10.7b</td>
      <td>73.17</td>
      <td>68.52</td>
      <td>86.78</td>
      <td>67.03</td>
      <td>64.54</td>
      <td>83.90</td>
      <td>68.23</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>36.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>620ed061bad27da7c0e4d1342ec431986d01477f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>brucethemoose/Yi-34B-200K-DARE-merge-v7</td>
      <td>73.12</td>
      <td>68.09</td>
      <td>85.99</td>
      <td>77.30</td>
      <td>58.90</td>
      <td>83.11</td>
      <td>65.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>4.0</td>
      <td>True</td>
      <td>9a6bfe30e2ab9eab807787bb0f3b7e91241d1ce0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO</td>
      <td>73.12</td>
      <td>71.42</td>
      <td>87.21</td>
      <td>72.28</td>
      <td>54.53</td>
      <td>82.64</td>
      <td>70.66</td>
      <td>RL-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>56.0</td>
      <td>True</td>
      <td>6ba531f1aec62375bf94ad9c7bb064953c4e9868</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>samir-fama/SamirGPT-v1</td>
      <td>73.11</td>
      <td>69.54</td>
      <td>87.04</td>
      <td>65.30</td>
      <td>63.37</td>
      <td>81.69</td>
      <td>71.72</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>8e8abca2d9703dff2d60de78b013360a9a3f4d5e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rombodawg/Open_Gpt4_8x7B</td>
      <td>73.10</td>
      <td>69.28</td>
      <td>86.77</td>
      <td>71.20</td>
      <td>70.39</td>
      <td>81.77</td>
      <td>59.21</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>False</td>
      <td>fa90ffb7fb57cb609d9d47719b3731693d23b312</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abacusai/Slerp-CM-mist-dpo</td>
      <td>73.10</td>
      <td>69.62</td>
      <td>87.09</td>
      <td>64.81</td>
      <td>62.82</td>
      <td>81.45</td>
      <td>72.78</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>ea3b28f8b829e08dfd0c7310da78bd483ab29bbe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Lelantos-DPO-7B</td>
      <td>73.09</td>
      <td>71.08</td>
      <td>87.22</td>
      <td>64.00</td>
      <td>67.77</td>
      <td>80.03</td>
      <td>68.46</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>a14226753e81928ca1aa97a5457bf8313e06ba6d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>argilla/notux-8x7b-v1-epoch-2</td>
      <td>73.05</td>
      <td>70.65</td>
      <td>87.80</td>
      <td>71.43</td>
      <td>65.97</td>
      <td>82.08</td>
      <td>60.35</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>bd3924498c3ae041334be5018cd912b6537a633c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shadowml/Marcoro14-7B-ties</td>
      <td>73.01</td>
      <td>69.80</td>
      <td>87.13</td>
      <td>65.11</td>
      <td>63.54</td>
      <td>81.61</td>
      <td>70.89</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>060737a4e7e8619b8d7c1180a6cc5b1a7c1d87fa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/Marcoro14-7B-slerp</td>
      <td>73.01</td>
      <td>69.80</td>
      <td>87.13</td>
      <td>65.11</td>
      <td>63.54</td>
      <td>81.61</td>
      <td>70.89</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>19.0</td>
      <td>True</td>
      <td>25f7e124456a5ad5c8c032088eb573d3e520d411</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>argilla/notux-8x7b-v1</td>
      <td>72.97</td>
      <td>70.65</td>
      <td>87.72</td>
      <td>71.39</td>
      <td>66.21</td>
      <td>80.74</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>144.0</td>
      <td>True</td>
      <td>1f8562051647d5537dc950315e74534b363a0812</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051611/whattest</td>
      <td>72.96</td>
      <td>66.81</td>
      <td>84.43</td>
      <td>76.59</td>
      <td>58.04</td>
      <td>82.48</td>
      <td>69.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>598102a9a810986c05b9aa216507be57d93de4fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>jan-ai/Pandora-10.7B-v1</td>
      <td>72.93</td>
      <td>71.08</td>
      <td>87.06</td>
      <td>64.95</td>
      <td>70.67</td>
      <td>81.37</td>
      <td>62.47</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>0e06af9adc32a44f307f96c387b4e803a1868291</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>bardsai/jaskier-7b-dpo</td>
      <td>72.91</td>
      <td>70.82</td>
      <td>87.02</td>
      <td>64.67</td>
      <td>64.41</td>
      <td>80.19</td>
      <td>70.36</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>ddc14e22152cc16d6ba01cd6c4facc833e98e6b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct</td>
      <td>72.89</td>
      <td>70.48</td>
      <td>87.75</td>
      <td>71.37</td>
      <td>65.71</td>
      <td>81.22</td>
      <td>60.80</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>17.0</td>
      <td>True</td>
      <td>330eb185920d6a470b265a4b31217c60e810fb3e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.2</td>
      <td>72.88</td>
      <td>68.86</td>
      <td>87.01</td>
      <td>65.05</td>
      <td>64.19</td>
      <td>81.69</td>
      <td>70.51</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>8.0</td>
      <td>True</td>
      <td>819c48aa6cf2b1f722a824027ceab8247e957e79</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>samir-fama/FernandoGPT-v1</td>
      <td>72.87</td>
      <td>69.45</td>
      <td>86.94</td>
      <td>65.19</td>
      <td>61.18</td>
      <td>81.14</td>
      <td>73.31</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>a26fbae35874a6aafb02e39fd8a623022b9e2a95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/CM-14</td>
      <td>72.82</td>
      <td>69.37</td>
      <td>86.97</td>
      <td>65.37</td>
      <td>61.90</td>
      <td>81.06</td>
      <td>72.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7ab2f7eedca7ec6a6463ba4b2f822a06e47b4cd4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>v1olet/v1olet_marcoroni-go-bruins-merge-7B</td>
      <td>72.81</td>
      <td>70.05</td>
      <td>87.17</td>
      <td>65.17</td>
      <td>61.42</td>
      <td>81.45</td>
      <td>71.57</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>27.0</td>
      <td>True</td>
      <td>aca5d9df596ac1f9ddffbec3de282ecbe3b32d68</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PSanni/MPOMixtral-8x7B-Instruct-v0.1</td>
      <td>72.80</td>
      <td>70.99</td>
      <td>87.95</td>
      <td>70.26</td>
      <td>66.52</td>
      <td>82.56</td>
      <td>58.53</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>a4400d021e29279c8676d5c46cf76c4b36d748f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Lelantos-7B</td>
      <td>72.78</td>
      <td>69.03</td>
      <td>86.90</td>
      <td>64.10</td>
      <td>65.18</td>
      <td>80.66</td>
      <td>70.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>ec060c6a4f5e45370cf2e2d65ecb388b048b0fdb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/OpenCM-14</td>
      <td>72.75</td>
      <td>69.28</td>
      <td>86.89</td>
      <td>65.01</td>
      <td>61.07</td>
      <td>81.29</td>
      <td>72.93</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>744228e768a6d117952ead1d981c410dd0d3ce4d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/CatMacaroni-Slerp</td>
      <td>72.74</td>
      <td>69.28</td>
      <td>86.88</td>
      <td>65.02</td>
      <td>61.02</td>
      <td>81.14</td>
      <td>73.09</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>6045552b7283e50378fb5c3e31f75072c1bc91f8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>damerajee/Oot-v2_lll</td>
      <td>72.73</td>
      <td>69.28</td>
      <td>86.60</td>
      <td>64.96</td>
      <td>62.57</td>
      <td>80.82</td>
      <td>72.18</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>3d4a013ad5763822280ca13e804d52c432e4fc0f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct</td>
      <td>72.73</td>
      <td>70.56</td>
      <td>87.74</td>
      <td>71.08</td>
      <td>65.72</td>
      <td>81.45</td>
      <td>59.82</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>17.0</td>
      <td>True</td>
      <td>330eb185920d6a470b265a4b31217c60e810fb3e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mistralai/Mixtral-8x7B-Instruct-v0.1</td>
      <td>72.70</td>
      <td>70.14</td>
      <td>87.55</td>
      <td>71.40</td>
      <td>64.98</td>
      <td>81.06</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>2100.0</td>
      <td>True</td>
      <td>125c431e2ff41a156b9f9076f744d2f35dd6e67a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/CatMacaroni14</td>
      <td>72.68</td>
      <td>69.11</td>
      <td>86.92</td>
      <td>65.07</td>
      <td>61.58</td>
      <td>81.06</td>
      <td>72.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>66f6d076cf5396d4cecc08696addf12567c55a85</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>senseable/garten2-7b</td>
      <td>72.65</td>
      <td>69.37</td>
      <td>87.54</td>
      <td>65.44</td>
      <td>59.50</td>
      <td>84.69</td>
      <td>69.37</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>96e7c78544d7eca96e3ae60ff80c728f3109e8ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mistralai/Mixtral-8x7B-Instruct-v0.1</td>
      <td>72.62</td>
      <td>70.22</td>
      <td>87.63</td>
      <td>71.16</td>
      <td>64.58</td>
      <td>81.37</td>
      <td>60.73</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>2100.0</td>
      <td>True</td>
      <td>3de0408ae8b591d9ac516a2384925dd98ebc66f4</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/PiVoT-SUS-RP</td>
      <td>72.57</td>
      <td>66.55</td>
      <td>84.23</td>
      <td>76.23</td>
      <td>54.57</td>
      <td>83.35</td>
      <td>70.51</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>1b3a5c98381f37a2ec97ce80d1d88d472a7d1802</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>brucethemoose/Yi-34B-200K-DARE-megamerge-v8</td>
      <td>72.56</td>
      <td>67.75</td>
      <td>86.06</td>
      <td>77.03</td>
      <td>56.31</td>
      <td>82.79</td>
      <td>65.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>5.0</td>
      <td>True</td>
      <td>0823229057d02acb1c9dda173d6fb2ea3b46b0af</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AIDC-ai-business/Marcoroni-7B-v3</td>
      <td>72.53</td>
      <td>69.45</td>
      <td>86.78</td>
      <td>65.00</td>
      <td>60.40</td>
      <td>81.45</td>
      <td>72.10</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ec546141522aef9b42d1a014f1a539fcc485c45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>bardsai/jaskier-7b-dpo-v2</td>
      <td>72.53</td>
      <td>69.28</td>
      <td>86.80</td>
      <td>64.92</td>
      <td>61.64</td>
      <td>80.74</td>
      <td>71.80</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>a0c0f4f9d4fbfe0a688d1d58b98b30f0ca6fc9bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Toten5/Marcoroni-v3-neural-chat-v3-3-Slerp</td>
      <td>72.51</td>
      <td>68.77</td>
      <td>86.55</td>
      <td>64.51</td>
      <td>62.70</td>
      <td>80.74</td>
      <td>71.80</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>0223ffb3f70272009d0d76923f40cb31f3d2347e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz2/pee</td>
      <td>72.50</td>
      <td>69.88</td>
      <td>86.89</td>
      <td>64.95</td>
      <td>60.56</td>
      <td>81.77</td>
      <td>70.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>eb3b3b6b25c31a7805d672059e06d4eace586a28</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Toten5/Marcoroni-neural-chat-7B-v2</td>
      <td>72.50</td>
      <td>68.60</td>
      <td>86.33</td>
      <td>64.65</td>
      <td>61.84</td>
      <td>80.43</td>
      <td>73.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>15808c683e8e1125d54498a16a620b0e8520ed2b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/bagel-dpo-8x7b-v0.2</td>
      <td>72.49</td>
      <td>72.10</td>
      <td>86.41</td>
      <td>70.27</td>
      <td>72.83</td>
      <td>83.27</td>
      <td>50.04</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>14.0</td>
      <td>True</td>
      <td>61822ea65b8a4c56d2b5622e2adf69e430fac29a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ignos/Mistral-T5-7B-v1</td>
      <td>72.47</td>
      <td>68.60</td>
      <td>86.30</td>
      <td>64.62</td>
      <td>61.86</td>
      <td>80.27</td>
      <td>73.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>e91dcc46d28fc0aa5553fb73c4eac5e28abfd3ec</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Marcoroni-8x7B-v3-MoE</td>
      <td>72.45</td>
      <td>69.37</td>
      <td>86.78</td>
      <td>65.01</td>
      <td>60.40</td>
      <td>81.45</td>
      <td>71.72</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>False</td>
      <td>901a733d1c01035bcbe69afd25dd9b4f982cb216</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Brillibits/Instruct_Mixtral-8x7B-v0.1_Dolly15K</td>
      <td>72.44</td>
      <td>69.28</td>
      <td>87.59</td>
      <td>70.96</td>
      <td>64.83</td>
      <td>82.56</td>
      <td>59.44</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>e593de223b662cfda40aa96163c6a42d6b32de5e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>liminerity/Blur-7B-slerp-v0.1</td>
      <td>72.40</td>
      <td>68.77</td>
      <td>86.58</td>
      <td>65.18</td>
      <td>60.64</td>
      <td>81.14</td>
      <td>72.10</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>03d64dadac0ac71cc5d62e325103cb9b9f279d43</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Kunoichi-DPO-v2-7B</td>
      <td>72.40</td>
      <td>69.37</td>
      <td>87.42</td>
      <td>64.83</td>
      <td>66.00</td>
      <td>80.74</td>
      <td>66.03</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d7d33a1517c57b596162a71a48bc29c87d29d9aa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.1</td>
      <td>72.39</td>
      <td>69.11</td>
      <td>86.70</td>
      <td>65.34</td>
      <td>63.43</td>
      <td>80.19</td>
      <td>69.60</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>11.0</td>
      <td>True</td>
      <td>3ddae31382dd3f7e654c1fc0e9b37f2e7f4ede92</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/laserxtral</td>
      <td>72.34</td>
      <td>69.03</td>
      <td>86.76</td>
      <td>64.68</td>
      <td>63.80</td>
      <td>80.03</td>
      <td>69.75</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-2.0</td>
      <td>24.15</td>
      <td>47.0</td>
      <td>True</td>
      <td>91e0a33fd2cb0a77401831e96536b91c5b7817e4</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mindy-labs/mindy-7b</td>
      <td>72.34</td>
      <td>69.11</td>
      <td>86.57</td>
      <td>64.69</td>
      <td>60.89</td>
      <td>81.06</td>
      <td>71.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>ce0d461a6de81d5b8ec4d338fb0c6e7991d0b1ff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>janhq/supermario-v2</td>
      <td>72.34</td>
      <td>68.52</td>
      <td>86.51</td>
      <td>64.88</td>
      <td>60.58</td>
      <td>81.37</td>
      <td>72.18</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d66c7d87fc3670c9292177e4cfc59e8a9d71322d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/FrankenDPO-4x7B-bf16</td>
      <td>72.34</td>
      <td>68.69</td>
      <td>86.07</td>
      <td>64.93</td>
      <td>63.14</td>
      <td>83.50</td>
      <td>67.70</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>547eac8651e32eb9a59019696faf19c372b25016</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-deepseek-67b-v15.2</td>
      <td>72.33</td>
      <td>68.60</td>
      <td>86.37</td>
      <td>71.50</td>
      <td>56.20</td>
      <td>84.45</td>
      <td>66.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>67.42</td>
      <td>11.0</td>
      <td>True</td>
      <td>c3caef28f8402d52d6a646a7e1e00a971db1c507</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/Beyonder-4x7B-v2</td>
      <td>72.33</td>
      <td>68.77</td>
      <td>86.80</td>
      <td>65.10</td>
      <td>60.68</td>
      <td>80.90</td>
      <td>71.72</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>24.15</td>
      <td>94.0</td>
      <td>True</td>
      <td>f44d94a8a0ccfa98e5173da9d88a5ed09efad30e</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>janhq/supermario-slerp</td>
      <td>72.32</td>
      <td>68.94</td>
      <td>86.58</td>
      <td>64.93</td>
      <td>60.11</td>
      <td>81.29</td>
      <td>72.10</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>83bcf51c709bcb4fcb3c8f0f91de22f458a07ee4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rishiraj/CatPPT</td>
      <td>72.32</td>
      <td>68.09</td>
      <td>86.69</td>
      <td>65.16</td>
      <td>61.55</td>
      <td>81.61</td>
      <td>70.81</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>11.0</td>
      <td>True</td>
      <td>65d316ec5f213b7d9abbe2116372e0e90b579319</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jan-ai/Solar-10.7B-SLERP</td>
      <td>72.31</td>
      <td>70.73</td>
      <td>87.87</td>
      <td>65.77</td>
      <td>65.72</td>
      <td>82.48</td>
      <td>61.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>2.0</td>
      <td>True</td>
      <td>786e6492919d0d1eb07b5988f67e0ee61aa05c21</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mncai/yi-34B-v3</td>
      <td>72.26</td>
      <td>67.06</td>
      <td>85.11</td>
      <td>75.80</td>
      <td>57.54</td>
      <td>83.50</td>
      <td>64.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>4.0</td>
      <td>True</td>
      <td>f7605af56f29b42e72f9c2cbbd4ad8e443a8dae0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Fimbulvetr-10.7B-v1</td>
      <td>72.25</td>
      <td>68.94</td>
      <td>87.27</td>
      <td>66.59</td>
      <td>60.54</td>
      <td>83.50</td>
      <td>66.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>5.0</td>
      <td>True</td>
      <td>bff7146aafe1a5b84631bd279112c8c5b95d2802</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>rishiraj/CatPPT-base</td>
      <td>72.25</td>
      <td>67.92</td>
      <td>86.64</td>
      <td>65.26</td>
      <td>61.72</td>
      <td>81.29</td>
      <td>70.66</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>15.0</td>
      <td>True</td>
      <td>7b041695f3ac19052f8c8be1918822bba8f73f74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Kunoichi-DPO-7B</td>
      <td>72.24</td>
      <td>69.62</td>
      <td>87.14</td>
      <td>64.79</td>
      <td>67.31</td>
      <td>80.58</td>
      <td>63.99</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>4e21eea3c32d00b2fcfc5bcfd16d8dc9d0d8874d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>viethq188/LeoScorpius-7B</td>
      <td>72.21</td>
      <td>69.28</td>
      <td>87.01</td>
      <td>65.04</td>
      <td>63.95</td>
      <td>81.53</td>
      <td>66.41</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>02e11fa83d18975f95c5d5047d0439897308c73b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>rwitz2/grindin</td>
      <td>72.18</td>
      <td>69.88</td>
      <td>87.02</td>
      <td>64.98</td>
      <td>59.34</td>
      <td>80.90</td>
      <td>70.96</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>9bdce071e0f87fe047cd2446be42edf91175c3be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-HighDensity</td>
      <td>72.15</td>
      <td>67.41</td>
      <td>85.77</td>
      <td>77.44</td>
      <td>57.84</td>
      <td>83.11</td>
      <td>61.33</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>8.0</td>
      <td>True</td>
      <td>17fe477d833b16aab50bef843bc8bf196a2710ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Kunoichi-7B</td>
      <td>72.13</td>
      <td>68.69</td>
      <td>87.10</td>
      <td>64.90</td>
      <td>64.04</td>
      <td>81.06</td>
      <td>67.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>25.0</td>
      <td>True</td>
      <td>cb731f23e65b8638143d88055e1db57b84fdd546</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mncai/yi-34B-v2</td>
      <td>72.12</td>
      <td>66.13</td>
      <td>85.00</td>
      <td>75.64</td>
      <td>57.34</td>
      <td>83.66</td>
      <td>64.97</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>3.0</td>
      <td>True</td>
      <td>bf7696c10077e73d06752c564ea35cc7e5e336ca</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/72B-preview</td>
      <td>72.12</td>
      <td>65.19</td>
      <td>83.23</td>
      <td>77.14</td>
      <td>52.58</td>
      <td>82.48</td>
      <td>72.10</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>72.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>508ee8ddfd8b823fcd4b0366a72c7981c8b447d8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mindy-labs/mindy-7b-v2</td>
      <td>72.11</td>
      <td>68.69</td>
      <td>86.59</td>
      <td>65.18</td>
      <td>60.16</td>
      <td>81.06</td>
      <td>70.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>b859eae30d69b065060e268b4e918601dabcc36c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz/go-bruins-v2</td>
      <td>72.07</td>
      <td>69.80</td>
      <td>87.05</td>
      <td>64.75</td>
      <td>59.70</td>
      <td>81.45</td>
      <td>69.67</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>26.0</td>
      <td>True</td>
      <td>24f8ce81d25c433bc6be147928779fb2d00ae0e7</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT</td>
      <td>72.07</td>
      <td>69.71</td>
      <td>86.74</td>
      <td>72.21</td>
      <td>51.22</td>
      <td>82.95</td>
      <td>69.60</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>28.0</td>
      <td>True</td>
      <td>6011e2ef7791738f3b78fa9e122360029df7c9ed</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/72B-preview</td>
      <td>72.06</td>
      <td>64.85</td>
      <td>83.28</td>
      <td>77.21</td>
      <td>52.51</td>
      <td>82.48</td>
      <td>72.02</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>72.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>508ee8ddfd8b823fcd4b0366a72c7981c8b447d8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/DonutLM-v1</td>
      <td>72.05</td>
      <td>69.11</td>
      <td>85.91</td>
      <td>65.45</td>
      <td>63.36</td>
      <td>81.69</td>
      <td>66.79</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>710e480608d7f9bd42bbc1d90046580f1ffdbe04</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz/dec10</td>
      <td>72.05</td>
      <td>69.11</td>
      <td>86.46</td>
      <td>64.98</td>
      <td>60.42</td>
      <td>80.74</td>
      <td>70.58</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d12ade4c823d9f42949c7902d0f01b2e996a7d7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz/dec10</td>
      <td>72.01</td>
      <td>69.20</td>
      <td>86.48</td>
      <td>64.91</td>
      <td>60.52</td>
      <td>80.43</td>
      <td>70.51</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d12ade4c823d9f42949c7902d0f01b2e996a7d7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>brucethemoose/Yi-34B-200K-DARE-merge-v5</td>
      <td>71.98</td>
      <td>66.47</td>
      <td>85.54</td>
      <td>77.22</td>
      <td>57.46</td>
      <td>82.24</td>
      <td>62.93</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>19.0</td>
      <td>True</td>
      <td>72d2469926f0277d31b13ce2db78e454b24a91b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.5</td>
      <td>71.96</td>
      <td>68.69</td>
      <td>86.45</td>
      <td>65.65</td>
      <td>59.12</td>
      <td>80.66</td>
      <td>71.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>fe9f9e52f1b48112d1c4349abbc0f104e56303ab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz/go-bruins-v2</td>
      <td>71.95</td>
      <td>69.80</td>
      <td>87.06</td>
      <td>64.95</td>
      <td>59.68</td>
      <td>81.22</td>
      <td>68.99</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>26.0</td>
      <td>True</td>
      <td>49c730c9e00299eaefeb5ada30a9ec53659729a5</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenPipe/mistral-ft-optimized-1218</td>
      <td>71.94</td>
      <td>67.92</td>
      <td>86.26</td>
      <td>64.99</td>
      <td>59.48</td>
      <td>80.74</td>
      <td>72.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>143.0</td>
      <td>True</td>
      <td>f4f3f6144dd143d6ec43ece9ab0fdd740ed610f1</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/Valkyrie-V1</td>
      <td>71.92</td>
      <td>67.24</td>
      <td>86.27</td>
      <td>64.82</td>
      <td>60.40</td>
      <td>81.45</td>
      <td>71.34</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>78917a93a47ea6d401458d0e283a2c6db6c68a47</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051611/A0110</td>
      <td>71.89</td>
      <td>66.38</td>
      <td>84.73</td>
      <td>74.48</td>
      <td>58.60</td>
      <td>82.32</td>
      <td>64.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>False</td>
      <td>1a15b9aa4acf1327164672edd16ee966b8bc3691</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>DopeorNope/COKAL-v1-70B</td>
      <td>71.87</td>
      <td>87.46</td>
      <td>83.29</td>
      <td>68.13</td>
      <td>72.79</td>
      <td>80.27</td>
      <td>39.27</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>69.44</td>
      <td>7.0</td>
      <td>True</td>
      <td>6898ebe887fd7debab6b26aa650f2876c1e2f4cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Seraph-7B</td>
      <td>71.86</td>
      <td>67.83</td>
      <td>86.22</td>
      <td>65.07</td>
      <td>59.49</td>
      <td>80.66</td>
      <td>71.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>14.0</td>
      <td>True</td>
      <td>2c6ea500b4b33bc9231b56ee6a495cd96e63064a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>bn22/Nous-Hermes-2-SOLAR-10.7B-MISALIGNED</td>
      <td>71.83</td>
      <td>68.26</td>
      <td>86.11</td>
      <td>66.26</td>
      <td>57.79</td>
      <td>83.43</td>
      <td>69.14</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>e402c5ea1ba23d776062f18306690296a708d469</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051611/A0109</td>
      <td>71.83</td>
      <td>66.55</td>
      <td>84.70</td>
      <td>74.44</td>
      <td>58.75</td>
      <td>82.16</td>
      <td>64.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>False</td>
      <td>2a9cd40c67e0b17d94a0eedafd3d116245613709</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/openchat-3.5-1210-Seraph-Slerp</td>
      <td>71.82</td>
      <td>68.09</td>
      <td>86.48</td>
      <td>65.33</td>
      <td>57.77</td>
      <td>80.82</td>
      <td>72.40</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>fdcc497bcf5e9ba62a9617617ff8f4e2965104e1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz/go-bruins</td>
      <td>71.81</td>
      <td>69.11</td>
      <td>86.73</td>
      <td>64.94</td>
      <td>58.71</td>
      <td>81.45</td>
      <td>69.90</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>14.0</td>
      <td>True</td>
      <td>a544f70a290738787bf3edc167f0bc95999e5702</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>deepseek-ai/deepseek-llm-67b-chat</td>
      <td>71.79</td>
      <td>67.75</td>
      <td>86.82</td>
      <td>72.42</td>
      <td>55.85</td>
      <td>84.21</td>
      <td>63.68</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>67.00</td>
      <td>133.0</td>
      <td>True</td>
      <td>79648bef7658bb824e4630740f6e1484c1b0620b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rwitz/go-bruins</td>
      <td>71.79</td>
      <td>69.11</td>
      <td>86.68</td>
      <td>64.96</td>
      <td>58.72</td>
      <td>81.37</td>
      <td>69.90</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>14.0</td>
      <td>True</td>
      <td>a544f70a290738787bf3edc167f0bc95999e5702</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-deepseek-67b-v15.1</td>
      <td>71.76</td>
      <td>67.66</td>
      <td>86.49</td>
      <td>70.30</td>
      <td>54.42</td>
      <td>84.77</td>
      <td>66.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>67.42</td>
      <td>1.0</td>
      <td>True</td>
      <td>3120e204e1b4928fd784ae78fa754bc937352c98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/openchat-3.5-1210-Seraph-Slerp</td>
      <td>71.74</td>
      <td>67.92</td>
      <td>86.43</td>
      <td>65.26</td>
      <td>57.75</td>
      <td>80.82</td>
      <td>72.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>46bb19fb19ff3673bdde3b38ee8e3f3884df8113</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-M-Creative-v1.0</td>
      <td>71.73</td>
      <td>66.81</td>
      <td>85.14</td>
      <td>75.54</td>
      <td>57.68</td>
      <td>83.11</td>
      <td>62.09</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>31.0</td>
      <td>True</td>
      <td>26923a2648b9864e2ec6f0cc66b8b6fcfbbdd491</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>VitalContribution/Evangelion-7B</td>
      <td>71.71</td>
      <td>68.94</td>
      <td>86.45</td>
      <td>63.97</td>
      <td>64.01</td>
      <td>79.95</td>
      <td>66.94</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7e3fdb60969ef0f7219cbcb9b05f7d1537af1c8d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>bhenrym14/platypus-yi-34b</td>
      <td>71.69</td>
      <td>68.43</td>
      <td>85.21</td>
      <td>78.13</td>
      <td>54.48</td>
      <td>84.06</td>
      <td>59.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>9.0</td>
      <td>True</td>
      <td>66abec7cba89b35c7b6cab2140c3532049de0157</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PracticeLLM/SOLAR-tail-10.7B-Merge-v1.0</td>
      <td>71.68</td>
      <td>66.13</td>
      <td>86.54</td>
      <td>66.52</td>
      <td>60.57</td>
      <td>84.77</td>
      <td>65.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>957474e32057f19ef863c1c8ba3d16389cf58eed</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Loyal-Macaroni-Maid-7B</td>
      <td>71.68</td>
      <td>68.00</td>
      <td>86.39</td>
      <td>64.87</td>
      <td>62.50</td>
      <td>79.87</td>
      <td>68.46</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>29.0</td>
      <td>True</td>
      <td>3fc12ef0089d55509552d1569f3107fd6a24b90f</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>scaledown/ScaleDown-7B-slerp-v0.1</td>
      <td>71.57</td>
      <td>68.00</td>
      <td>85.70</td>
      <td>65.26</td>
      <td>61.90</td>
      <td>81.37</td>
      <td>67.17</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9bddd33f58ddbbaa9ecf8c5a4b79dfd8e49155e5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties-ExtremeDensity</td>
      <td>71.57</td>
      <td>66.89</td>
      <td>85.69</td>
      <td>77.35</td>
      <td>57.63</td>
      <td>82.00</td>
      <td>59.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>71c95f1971c4a47adc331859b91502bd0b790ce0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/NeuralPipe-7B-ties</td>
      <td>71.55</td>
      <td>67.92</td>
      <td>86.04</td>
      <td>64.24</td>
      <td>61.37</td>
      <td>80.19</td>
      <td>69.52</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>5b4a878a938954d87183d1d903923c100b2c724f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>RatanRohith/NeuralPizza-7B-V0.1</td>
      <td>71.53</td>
      <td>70.48</td>
      <td>87.30</td>
      <td>64.42</td>
      <td>67.22</td>
      <td>80.35</td>
      <td>59.44</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>fb53c42ba7d5719e730f67c5356766d84e5f3619</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/A0106</td>
      <td>71.53</td>
      <td>66.38</td>
      <td>85.05</td>
      <td>74.00</td>
      <td>57.88</td>
      <td>82.87</td>
      <td>63.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>215cbefbc69d22c28181651b5b964c329ca09f59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Yuna-7b-Merge</td>
      <td>71.46</td>
      <td>67.49</td>
      <td>86.84</td>
      <td>64.86</td>
      <td>61.20</td>
      <td>80.74</td>
      <td>67.63</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d5cde262c73c9ee44c1ec85b1fb48f226ae99a77</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/A0106</td>
      <td>71.44</td>
      <td>66.47</td>
      <td>85.05</td>
      <td>74.03</td>
      <td>57.82</td>
      <td>82.72</td>
      <td>62.55</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>215cbefbc69d22c28181651b5b964c329ca09f59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dillfrescott/amadeus-v0.1</td>
      <td>71.42</td>
      <td>68.94</td>
      <td>86.98</td>
      <td>64.69</td>
      <td>63.82</td>
      <td>79.95</td>
      <td>64.14</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>wtfpl</td>
      <td>24.15</td>
      <td>1.0</td>
      <td>True</td>
      <td>2d6dcf8bf9f1a758f135929de4a6fd81e26a38da</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Deacon-34b-qlora-adapter</td>
      <td>71.39</td>
      <td>64.85</td>
      <td>85.56</td>
      <td>76.38</td>
      <td>56.21</td>
      <td>83.11</td>
      <td>62.24</td>
      <td>instruction-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>97d19d88f629f6d5270126de7ba1400d3b89a6c6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/OpenHermes-2.5-neural-chat-v3-3-Slerp</td>
      <td>71.38</td>
      <td>68.09</td>
      <td>86.20</td>
      <td>64.26</td>
      <td>62.78</td>
      <td>79.16</td>
      <td>67.78</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>91f18df3f5c3d36f1293086113f810f662970449</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DiscoResearch/DiscoLM-70b</td>
      <td>71.37</td>
      <td>68.77</td>
      <td>86.10</td>
      <td>68.58</td>
      <td>57.64</td>
      <td>83.58</td>
      <td>63.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>10.0</td>
      <td>True</td>
      <td>5eab2c8ec1c079e53a60ebdb7811756c2faebd9b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>leejunhyeok/MoMo-70B-LoRA-V1.2_1</td>
      <td>71.36</td>
      <td>70.65</td>
      <td>86.40</td>
      <td>69.90</td>
      <td>61.41</td>
      <td>83.19</td>
      <td>56.63</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>178d03ccf7e7f83019266396f326fe11382eb20a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051611/A0113</td>
      <td>71.36</td>
      <td>66.38</td>
      <td>84.86</td>
      <td>74.39</td>
      <td>59.65</td>
      <td>82.00</td>
      <td>60.88</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>6e36ad0a41135265185038d1d88062d9fb11e8d5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>janhq/supermario-slerp-v2</td>
      <td>71.35</td>
      <td>69.37</td>
      <td>86.60</td>
      <td>64.91</td>
      <td>62.96</td>
      <td>80.82</td>
      <td>63.46</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>662c68ece38bcc8cb7b04dc2c0f5d6c03f8d56e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Walmart-the-bag/Solar-10.7B-Cato</td>
      <td>71.35</td>
      <td>68.69</td>
      <td>86.16</td>
      <td>65.76</td>
      <td>61.68</td>
      <td>81.22</td>
      <td>64.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>16b230f4e663902787254ecb1781c255b7dcc6ea</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/MetaMath-Cybertron-Starling</td>
      <td>71.35</td>
      <td>67.75</td>
      <td>86.23</td>
      <td>65.24</td>
      <td>55.94</td>
      <td>81.45</td>
      <td>71.49</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>38.0</td>
      <td>True</td>
      <td>c274ec29903792dfdc584dc840cc16e952bd3122</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>brucethemoose/CapyTessBorosYi-34B-200K-DARE-Ties</td>
      <td>71.31</td>
      <td>64.93</td>
      <td>85.92</td>
      <td>76.18</td>
      <td>55.84</td>
      <td>83.03</td>
      <td>61.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>12.0</td>
      <td>True</td>
      <td>0475128a0e57fc103e65c601be75013f28987e62</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>rwitz2/ipo-test</td>
      <td>71.29</td>
      <td>67.92</td>
      <td>85.99</td>
      <td>65.05</td>
      <td>55.87</td>
      <td>80.90</td>
      <td>72.02</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b75cf49b19d31ae6c4f8d2a6f3a1484d143024e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aqweteddy/mistral_tv-neural-marconroni</td>
      <td>71.27</td>
      <td>69.20</td>
      <td>86.26</td>
      <td>65.07</td>
      <td>60.03</td>
      <td>80.90</td>
      <td>66.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>969f7483d768b15998cd57b392ea1a9718de3b28</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/OpenHermes-2.5-neural-chat-v3-3-openchat-3.5-1210-Slerp</td>
      <td>71.26</td>
      <td>67.92</td>
      <td>86.32</td>
      <td>65.47</td>
      <td>56.45</td>
      <td>79.72</td>
      <td>71.72</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b6211b2dc4dcf29ca79ba3d6751b3ad071413eeb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SyedAbdul/test-7B-slerp</td>
      <td>71.26</td>
      <td>68.09</td>
      <td>86.08</td>
      <td>64.57</td>
      <td>62.60</td>
      <td>80.82</td>
      <td>65.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>8c0acfaea61f49f679feb694c0de57a7f403d44e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/MetaMath-Cybertron-Starling</td>
      <td>71.25</td>
      <td>67.41</td>
      <td>86.26</td>
      <td>65.09</td>
      <td>55.95</td>
      <td>81.29</td>
      <td>71.49</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>38.0</td>
      <td>True</td>
      <td>17c8d4cadb814eaef0fab1d93b29cc150f413205</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Riiid/sheep-duck-llama-2-70b-v1.1</td>
      <td>71.22</td>
      <td>73.12</td>
      <td>87.77</td>
      <td>70.77</td>
      <td>64.55</td>
      <td>83.11</td>
      <td>47.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>978c3cc8d44ad37eb764a53e026ae1fa8d334eb2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>APMIC/caigun-lora-model-34B-v2</td>
      <td>71.19</td>
      <td>65.02</td>
      <td>85.28</td>
      <td>75.69</td>
      <td>58.03</td>
      <td>83.03</td>
      <td>60.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>913eec9411d7886c0e8abe6842ed09d8932bef10</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-3-Slerp</td>
      <td>71.19</td>
      <td>66.64</td>
      <td>85.43</td>
      <td>62.19</td>
      <td>63.20</td>
      <td>79.72</td>
      <td>69.98</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>cbd4f663365e40d50ed9834016bf840971b35db5</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.4</td>
      <td>71.19</td>
      <td>66.81</td>
      <td>86.15</td>
      <td>65.10</td>
      <td>58.25</td>
      <td>80.03</td>
      <td>70.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>367cfe8d6e046684ba8626444e82d1600c4e78a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/neural-chat-v3-3-8x7b-MoE</td>
      <td>71.17</td>
      <td>66.64</td>
      <td>85.43</td>
      <td>62.22</td>
      <td>63.20</td>
      <td>79.72</td>
      <td>69.83</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>ef354e7938f1c38bb1f73f4ee9a7f325ae32fc2e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/NeuralPipe-7B-slerp</td>
      <td>71.17</td>
      <td>67.75</td>
      <td>86.15</td>
      <td>63.94</td>
      <td>59.80</td>
      <td>79.64</td>
      <td>69.75</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>e3ba53ca9b2171e3c2134cc022eabada932e032c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Jingyu6/MergeTest-7B-slerp</td>
      <td>71.17</td>
      <td>67.75</td>
      <td>86.15</td>
      <td>63.94</td>
      <td>59.80</td>
      <td>79.64</td>
      <td>69.75</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>0c089098a27b01d577747f3071531a1a9c9d627c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Deacon-34b-Adapter</td>
      <td>71.16</td>
      <td>64.76</td>
      <td>85.57</td>
      <td>76.28</td>
      <td>56.24</td>
      <td>82.95</td>
      <td>61.18</td>
      <td>instruction-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4d1eca338cda2d7ecb0f0ea549819e7116d43178</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-Llama-Q</td>
      <td>71.13</td>
      <td>65.70</td>
      <td>85.22</td>
      <td>78.78</td>
      <td>53.64</td>
      <td>83.03</td>
      <td>60.42</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.39</td>
      <td>3.0</td>
      <td>True</td>
      <td>b786e11fafdd446f155fdb14c6112800f210801b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>NExtNewChattingAI/shark_tank_ai_7_b</td>
      <td>71.10</td>
      <td>66.89</td>
      <td>86.61</td>
      <td>65.27</td>
      <td>60.19</td>
      <td>81.93</td>
      <td>65.73</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>feafb4e14863e893ee3d6737ac5b07ac5241f452</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Abhinav7/NeuralPipe-7B-slerp</td>
      <td>71.08</td>
      <td>67.41</td>
      <td>86.12</td>
      <td>64.07</td>
      <td>59.82</td>
      <td>79.79</td>
      <td>69.29</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>62eb03a76c4c607afc8524cf725c48fbb6a1827a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zhengr/NeuralPipe-7B-slerp</td>
      <td>71.08</td>
      <td>67.41</td>
      <td>86.12</td>
      <td>64.07</td>
      <td>59.82</td>
      <td>79.79</td>
      <td>69.29</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>6a6405b269c94043658c342d3e124aa3ba75d621</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TomGrc/FusionNet_SOLAR</td>
      <td>71.08</td>
      <td>71.59</td>
      <td>88.40</td>
      <td>65.29</td>
      <td>69.21</td>
      <td>81.06</td>
      <td>50.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>15.97</td>
      <td>1.0</td>
      <td>True</td>
      <td>3757984c0edebf4300a67cf33b9cca53524a057d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dillfrescott/Nous-Hermes-2-SOLAR-10.7B-x2-MoE</td>
      <td>71.08</td>
      <td>67.15</td>
      <td>84.83</td>
      <td>66.52</td>
      <td>55.85</td>
      <td>83.11</td>
      <td>68.99</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>wtfpl</td>
      <td>19.19</td>
      <td>3.0</td>
      <td>True</td>
      <td>1cd122567a864075ede6c5684902e8dbfd5eed2e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Loyal-Toppy-Bruins-Maid-7B-DARE</td>
      <td>71.07</td>
      <td>68.86</td>
      <td>86.03</td>
      <td>64.84</td>
      <td>61.19</td>
      <td>79.72</td>
      <td>65.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>d8d01fbb3aaefda39421850c2dabb38e73546a6e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abacusai/MetaMath-bagel-34b-v0.2-c1500</td>
      <td>71.06</td>
      <td>63.91</td>
      <td>82.43</td>
      <td>74.51</td>
      <td>53.70</td>
      <td>80.98</td>
      <td>70.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>3a15e50ba671fe6e3e7725d58d101cbb4f4a997f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/smol_bruin-7b</td>
      <td>71.05</td>
      <td>67.58</td>
      <td>86.48</td>
      <td>65.05</td>
      <td>55.65</td>
      <td>81.14</td>
      <td>70.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>967dff56741850954a96491979995a4f686eeb05</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DeepKarkhanis/NeuralPipe-7B-slerp</td>
      <td>71.01</td>
      <td>67.58</td>
      <td>86.17</td>
      <td>64.06</td>
      <td>59.84</td>
      <td>80.19</td>
      <td>68.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>6d45f7ca3e55658264d0b0a26b3ef98433335db0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>superlazycoder/NeuralPipe-7B-slerp</td>
      <td>71.01</td>
      <td>67.58</td>
      <td>86.17</td>
      <td>64.06</td>
      <td>59.84</td>
      <td>80.19</td>
      <td>68.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>98bf395c8868b226208debc63d67576fdee52528</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/NeuralPipe-7B-slerp</td>
      <td>71.01</td>
      <td>67.58</td>
      <td>86.17</td>
      <td>64.06</td>
      <td>59.84</td>
      <td>80.19</td>
      <td>68.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>16485f6a8d83061f67515bfe20ed5afe8218c993</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DeepKarkhanis/Mistral-Passthrough-8L-10B</td>
      <td>71.01</td>
      <td>67.58</td>
      <td>86.17</td>
      <td>64.06</td>
      <td>59.84</td>
      <td>80.19</td>
      <td>68.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>8ebb167b4a27a9d49ec7399baf23eef6226c242d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>adamo1139/Yi-34B-200K-AEZAKMI-v2</td>
      <td>71.00</td>
      <td>67.92</td>
      <td>85.61</td>
      <td>75.22</td>
      <td>56.74</td>
      <td>81.61</td>
      <td>58.91</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>10.0</td>
      <td>True</td>
      <td>a7c90fa652ca4b65f4e2db1126be0f884748b7ab</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Hermes-2-SOLAR-10.7B</td>
      <td>71.00</td>
      <td>66.72</td>
      <td>84.89</td>
      <td>66.30</td>
      <td>55.82</td>
      <td>82.79</td>
      <td>69.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>94.0</td>
      <td>True</td>
      <td>1a61a6ff49be395db210a5867f02e04abb982971</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>chargoddard/Yi-34B-Llama</td>
      <td>70.95</td>
      <td>64.59</td>
      <td>85.63</td>
      <td>76.31</td>
      <td>55.60</td>
      <td>82.79</td>
      <td>60.80</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>49.0</td>
      <td>True</td>
      <td>52feecf18e46dd8ed1db297345957007c3e45de1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Loyal-Toppy-Bruins-Maid-7B-DARE</td>
      <td>70.95</td>
      <td>68.69</td>
      <td>86.04</td>
      <td>64.89</td>
      <td>61.26</td>
      <td>79.56</td>
      <td>65.28</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>d8d01fbb3aaefda39421850c2dabb38e73546a6e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AIDC-ai-business/Marcoroni-7B-v2</td>
      <td>70.92</td>
      <td>68.26</td>
      <td>86.27</td>
      <td>63.39</td>
      <td>61.96</td>
      <td>80.11</td>
      <td>65.50</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3929ff947202a530d89a2287e19873141a0136c5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Seraph-openchat-3.5-1210-Slerp</td>
      <td>70.89</td>
      <td>68.00</td>
      <td>86.13</td>
      <td>65.50</td>
      <td>54.12</td>
      <td>79.56</td>
      <td>72.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>ac09a74aec45a021bd144252a1c2ff9c2631b3ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chanwit/flux-7b-v0.1</td>
      <td>70.85</td>
      <td>67.06</td>
      <td>86.18</td>
      <td>65.40</td>
      <td>55.05</td>
      <td>79.01</td>
      <td>72.40</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>566b7dcfb2d7233d49611bda27ff5430487d1aad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/Lelantos-low-tune</td>
      <td>70.82</td>
      <td>67.06</td>
      <td>86.06</td>
      <td>64.11</td>
      <td>61.33</td>
      <td>79.56</td>
      <td>66.79</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>a0725dc1d3f591f2e9281c02f123fcde0a03c5db</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>01-ai/Yi-34B-200K</td>
      <td>70.81</td>
      <td>65.36</td>
      <td>85.58</td>
      <td>76.06</td>
      <td>53.64</td>
      <td>82.56</td>
      <td>61.64</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>202.0</td>
      <td>True</td>
      <td>bb196389dbbfdf271b5564ce840027f8cd3386ef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/Terminis-7B</td>
      <td>70.73</td>
      <td>67.92</td>
      <td>86.22</td>
      <td>64.07</td>
      <td>67.31</td>
      <td>81.29</td>
      <td>57.54</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>8.0</td>
      <td>True</td>
      <td>c3cde866d7d3da1173be8593c91e5bf143ea616e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/Nyxene-v3-11B</td>
      <td>70.72</td>
      <td>69.62</td>
      <td>85.33</td>
      <td>64.75</td>
      <td>60.91</td>
      <td>80.19</td>
      <td>63.53</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>9.0</td>
      <td>True</td>
      <td>706e71043ed40e53bfee7f25a3f2b4a8def36ae8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>one-man-army/una-neural-chat-v3-3-P2-OMA</td>
      <td>70.72</td>
      <td>67.32</td>
      <td>86.33</td>
      <td>63.14</td>
      <td>65.49</td>
      <td>79.79</td>
      <td>62.24</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>7bab67e479c192927c4a781efdf5be27eaa315a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-02-v0</td>
      <td>70.69</td>
      <td>67.49</td>
      <td>85.78</td>
      <td>64.10</td>
      <td>60.52</td>
      <td>79.01</td>
      <td>67.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b142b88a1b6f015b6971d75aa191c6d16324d0c1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jan-hq/stealth-v1.2</td>
      <td>70.68</td>
      <td>66.38</td>
      <td>86.14</td>
      <td>64.33</td>
      <td>54.23</td>
      <td>80.74</td>
      <td>72.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b0a2704027bbfd8ae0a5d88a23115b17d1a23d1f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/Merged-AGI-7B</td>
      <td>70.68</td>
      <td>68.60</td>
      <td>86.16</td>
      <td>65.02</td>
      <td>60.24</td>
      <td>80.66</td>
      <td>63.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>7b818236625de433802bfe8b32ab8b17a7e58912</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/dolphin-2.2-70b</td>
      <td>70.60</td>
      <td>70.05</td>
      <td>85.97</td>
      <td>69.18</td>
      <td>60.14</td>
      <td>81.45</td>
      <td>56.79</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6a2ddfb2ddde603dae91420db019682378aa9d5e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/MetaMath-Cybertron</td>
      <td>70.60</td>
      <td>66.47</td>
      <td>85.54</td>
      <td>63.71</td>
      <td>57.71</td>
      <td>79.64</td>
      <td>70.51</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>4fca0e0002db56237fc155f572a34204229e9620</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>one-man-army/una-neural-chat-v3-3-P2-OMA</td>
      <td>70.55</td>
      <td>67.24</td>
      <td>86.34</td>
      <td>63.18</td>
      <td>65.48</td>
      <td>79.64</td>
      <td>61.41</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>7bab67e479c192927c4a781efdf5be27eaa315a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenPipe/mistral-ft-optimized-1227</td>
      <td>70.54</td>
      <td>67.06</td>
      <td>85.85</td>
      <td>65.19</td>
      <td>54.57</td>
      <td>78.85</td>
      <td>71.72</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>36.0</td>
      <td>True</td>
      <td>a305e828aa2ef0f547e7037edf14bda54b78b210</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KaeriJenti/kaori-70b-v1</td>
      <td>70.54</td>
      <td>69.80</td>
      <td>87.36</td>
      <td>70.82</td>
      <td>58.81</td>
      <td>84.06</td>
      <td>52.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>fcce042311a54925ae4acdbe33cff535859300b2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>s3nh/Mistral_Sonyichi-7B-slerp</td>
      <td>70.52</td>
      <td>67.49</td>
      <td>86.43</td>
      <td>63.58</td>
      <td>63.25</td>
      <td>78.53</td>
      <td>63.84</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>openrail</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>d6605744836a770190389a73d31440362c81f41e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.2</td>
      <td>70.51</td>
      <td>64.59</td>
      <td>83.44</td>
      <td>75.53</td>
      <td>55.29</td>
      <td>81.61</td>
      <td>62.62</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>2a367db35e91a1cac5abad8e5101e85d391e0551</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenPipe/mistral-ft-optimized-1227</td>
      <td>70.50</td>
      <td>67.24</td>
      <td>85.90</td>
      <td>65.17</td>
      <td>54.51</td>
      <td>78.85</td>
      <td>71.34</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>36.0</td>
      <td>True</td>
      <td>a305e828aa2ef0f547e7037edf14bda54b78b210</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.2</td>
      <td>70.49</td>
      <td>64.51</td>
      <td>83.47</td>
      <td>75.64</td>
      <td>55.27</td>
      <td>81.37</td>
      <td>62.70</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>2a367db35e91a1cac5abad8e5101e85d391e0551</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Swisslex/Mixtral-8x7b-DPO-v0.1</td>
      <td>70.45</td>
      <td>70.90</td>
      <td>87.61</td>
      <td>70.66</td>
      <td>57.38</td>
      <td>82.40</td>
      <td>53.75</td>
      <td>RL-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>5de7f1532fdeaf36f7ffb180ff510efac2ac90e4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B-v2.01</td>
      <td>70.43</td>
      <td>68.86</td>
      <td>86.12</td>
      <td>63.90</td>
      <td>63.50</td>
      <td>80.51</td>
      <td>59.67</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>438642201e2a91e9456d2a8ca1d7443e5ec55a40</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-8x7B-MoE</td>
      <td>70.40</td>
      <td>68.77</td>
      <td>86.11</td>
      <td>63.86</td>
      <td>63.50</td>
      <td>80.51</td>
      <td>59.67</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>2.0</td>
      <td>True</td>
      <td>4df8e16bb4adeff6cfdd6c064819650ae27ff8fa</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HyperbeeAI/Tulpar-7b-v2</td>
      <td>70.36</td>
      <td>67.49</td>
      <td>84.89</td>
      <td>63.02</td>
      <td>63.65</td>
      <td>79.48</td>
      <td>63.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b466113c7726cfcd98ba602ec4000ae323f112fa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ValiantLabs/ShiningValiant</td>
      <td>70.34</td>
      <td>68.69</td>
      <td>87.31</td>
      <td>69.64</td>
      <td>55.78</td>
      <td>84.14</td>
      <td>56.48</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>70.0</td>
      <td>True</td>
      <td>7c4401cddc462c5f35d8984c90e293faee37bf8e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-mixtral-8x7b-v0.1</td>
      <td>70.34</td>
      <td>68.09</td>
      <td>85.76</td>
      <td>71.49</td>
      <td>55.31</td>
      <td>82.08</td>
      <td>59.29</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>30abf8de36252c1e026fe758b8fde5eba960cd2a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-mixtral-8x7b-v1</td>
      <td>70.34</td>
      <td>68.09</td>
      <td>85.76</td>
      <td>71.49</td>
      <td>55.31</td>
      <td>82.08</td>
      <td>59.29</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>30abf8de36252c1e026fe758b8fde5eba960cd2a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Falkor-7b</td>
      <td>70.33</td>
      <td>68.26</td>
      <td>85.84</td>
      <td>63.98</td>
      <td>63.08</td>
      <td>80.35</td>
      <td>60.50</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>b2e3c235196ba859b26ee14fb8c86e632bcf3e88</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>one-man-army/una-neural-chat-v3-3-P1-OMA</td>
      <td>70.32</td>
      <td>66.81</td>
      <td>85.92</td>
      <td>63.37</td>
      <td>64.35</td>
      <td>79.64</td>
      <td>61.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>014600373086ea46c7cdc4754c984a804b28a070</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AtAndDev/CapybaraMarcoroni-7B</td>
      <td>70.32</td>
      <td>65.02</td>
      <td>84.81</td>
      <td>65.20</td>
      <td>57.07</td>
      <td>81.14</td>
      <td>68.69</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>50dc156e0c016e4e1bc84ff8d067b3eb88d36310</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Silicon-Maid-7B</td>
      <td>70.31</td>
      <td>68.17</td>
      <td>86.52</td>
      <td>64.58</td>
      <td>61.64</td>
      <td>79.01</td>
      <td>61.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>26.0</td>
      <td>True</td>
      <td>ecb260368921c5dfe16c007e871d29de9d561996</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>APMIC/caigun-lora-model-34B-v3</td>
      <td>70.27</td>
      <td>66.89</td>
      <td>84.77</td>
      <td>75.41</td>
      <td>56.47</td>
      <td>83.58</td>
      <td>54.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>e5a17f9fbd39259cc166c8c75b81be2b41f029f1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Maylin-7b</td>
      <td>70.26</td>
      <td>66.81</td>
      <td>86.40</td>
      <td>64.73</td>
      <td>60.24</td>
      <td>79.64</td>
      <td>63.76</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c5fa9f4812daf2538e3e052c0346d9efb321c650</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-Chupacabra-7B-v2.01-Slerp</td>
      <td>70.26</td>
      <td>65.96</td>
      <td>85.46</td>
      <td>63.82</td>
      <td>56.16</td>
      <td>80.03</td>
      <td>70.13</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e94f61cd30c697bf1b38c64fa69e93a247f3b58d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>v1olet/v1olet_merged_dpo_7B</td>
      <td>70.26</td>
      <td>71.33</td>
      <td>87.34</td>
      <td>64.13</td>
      <td>63.37</td>
      <td>82.00</td>
      <td>53.37</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>299011bf619d9b89f4e545dde8ef7853ec0557b6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/SynthIA-70B-v1.5</td>
      <td>70.23</td>
      <td>69.37</td>
      <td>86.97</td>
      <td>69.16</td>
      <td>57.40</td>
      <td>83.66</td>
      <td>54.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>39.0</td>
      <td>True</td>
      <td>40773af947d39495841d825337fdbc7ca977ef1f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-0.1</td>
      <td>70.23</td>
      <td>64.68</td>
      <td>83.49</td>
      <td>74.94</td>
      <td>56.78</td>
      <td>81.29</td>
      <td>60.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>bc07f9084ad43d2455f12f1707a3c14f1a1de1d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.5</td>
      <td>70.22</td>
      <td>64.76</td>
      <td>83.46</td>
      <td>75.01</td>
      <td>56.88</td>
      <td>81.29</td>
      <td>59.89</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>5.0</td>
      <td>True</td>
      <td>4b7aa4e48f3208ab39f6640aa4cc98b1d5eff7e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/MetaMath-Chupacabra-7B-v2.01-Slerp</td>
      <td>70.21</td>
      <td>66.13</td>
      <td>85.46</td>
      <td>63.92</td>
      <td>56.15</td>
      <td>79.48</td>
      <td>70.13</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>dcc6fff61bfd608d8e14a040dff22cd8dae78b1e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/MetaMath-Tulpar-7b-v2-Slerp</td>
      <td>70.20</td>
      <td>65.61</td>
      <td>85.16</td>
      <td>63.49</td>
      <td>56.50</td>
      <td>79.48</td>
      <td>70.96</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>41612eecf338ae2b1cbb63a3729ce7b125c6ca3c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/OpenHermes-2.5-neural-chat-v3-2-Slerp</td>
      <td>70.20</td>
      <td>67.49</td>
      <td>85.42</td>
      <td>64.13</td>
      <td>61.05</td>
      <td>80.03</td>
      <td>63.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>bf9ef6df7732dbef3cd0001d9e5cba846cb47306</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>rishiraj/oswald-7b</td>
      <td>70.19</td>
      <td>66.38</td>
      <td>85.18</td>
      <td>65.34</td>
      <td>54.07</td>
      <td>80.90</td>
      <td>69.29</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>43326649a8b8b7a43cc4a6d15262625508a50dd2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/MetaMath-OpenHermes-2.5-neural-chat-v3-3-Slerp</td>
      <td>70.11</td>
      <td>64.59</td>
      <td>85.39</td>
      <td>64.27</td>
      <td>55.14</td>
      <td>79.64</td>
      <td>71.65</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>111ae8b3fb38d550a32f04dbd977f8cd447a3a92</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-34B-v1.4</td>
      <td>70.11</td>
      <td>64.59</td>
      <td>83.37</td>
      <td>75.02</td>
      <td>56.79</td>
      <td>81.22</td>
      <td>59.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>15.0</td>
      <td>True</td>
      <td>173d834656c3965cbaa49be6aab0772c3ce57821</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>upstage/SOLAR-0-70b-16bit</td>
      <td>70.11</td>
      <td>71.08</td>
      <td>87.89</td>
      <td>70.58</td>
      <td>62.25</td>
      <td>83.58</td>
      <td>45.26</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>70.00</td>
      <td>245.0</td>
      <td>False</td>
      <td>5f9c77b2c0397cf83d2f97740483f107c7109e8c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/Lonepino-11B</td>
      <td>70.10</td>
      <td>68.26</td>
      <td>84.57</td>
      <td>63.76</td>
      <td>63.45</td>
      <td>78.93</td>
      <td>61.64</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>392a0d8806638a235020b2146d83628b19516be5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.4</td>
      <td>70.08</td>
      <td>63.65</td>
      <td>83.30</td>
      <td>74.93</td>
      <td>57.26</td>
      <td>80.43</td>
      <td>60.88</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>2cef301d3afa127217c000f2fdc4c527dfa6145e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-Tulpar-7b-v2-Slerp</td>
      <td>70.07</td>
      <td>65.96</td>
      <td>85.11</td>
      <td>63.37</td>
      <td>56.44</td>
      <td>79.08</td>
      <td>70.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>644e2ca7db569c38a2bf06077fd8ee6d04f3edba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.3</td>
      <td>70.06</td>
      <td>63.74</td>
      <td>83.30</td>
      <td>75.08</td>
      <td>57.31</td>
      <td>80.66</td>
      <td>60.27</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>1207a09c7bd4539bcefce62e1376495b0761b08a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ICBU-NPU/FashionGPT-70B-V1.1</td>
      <td>70.05</td>
      <td>71.76</td>
      <td>88.20</td>
      <td>70.99</td>
      <td>65.26</td>
      <td>82.64</td>
      <td>41.47</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>42.0</td>
      <td>True</td>
      <td>05941a3eaacff0dead79b09d2175b5d7b98c525b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.4</td>
      <td>70.04</td>
      <td>63.65</td>
      <td>83.30</td>
      <td>75.11</td>
      <td>57.29</td>
      <td>80.58</td>
      <td>60.27</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>2cef301d3afa127217c000f2fdc4c527dfa6145e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>flemmingmiguel/Distilled-HermesChat-7B</td>
      <td>70.02</td>
      <td>67.49</td>
      <td>85.21</td>
      <td>65.22</td>
      <td>54.77</td>
      <td>80.11</td>
      <td>67.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e7ca19cecb52c40f0f6bb31cfa258fad0c004dfa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-0.2</td>
      <td>70.01</td>
      <td>64.68</td>
      <td>83.49</td>
      <td>74.84</td>
      <td>56.76</td>
      <td>81.37</td>
      <td>58.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>9119f34f298645df22e0e042f6631af8f67f4b33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LDCC/LDCC-SOLAR-10.7B</td>
      <td>70.00</td>
      <td>65.02</td>
      <td>85.30</td>
      <td>66.29</td>
      <td>65.29</td>
      <td>83.50</td>
      <td>54.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.86</td>
      <td>5.0</td>
      <td>True</td>
      <td>c8741ec6f4f24324a96041efaf2f627a99d946e6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>flemmingmiguel/Mistrality-7B</td>
      <td>69.97</td>
      <td>66.55</td>
      <td>85.82</td>
      <td>64.63</td>
      <td>56.80</td>
      <td>79.32</td>
      <td>66.72</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>05e7408486426ab8c8ed595945454eb181ba6eb0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>andysalerno/openchat-nectar-0.1</td>
      <td>69.94</td>
      <td>66.21</td>
      <td>82.99</td>
      <td>65.17</td>
      <td>54.22</td>
      <td>81.37</td>
      <td>69.67</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf913c9f807a9bdbe606ac4bf445d93a082a118c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-OpenHermes-2.5-neural-chat-v3-3-Slerp</td>
      <td>69.92</td>
      <td>64.59</td>
      <td>85.37</td>
      <td>64.29</td>
      <td>55.14</td>
      <td>79.08</td>
      <td>71.04</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>f209799cbf4f782e1c6352e427599e2f8a6038ad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sethuiyer/MedleyMD</td>
      <td>69.89</td>
      <td>66.47</td>
      <td>86.06</td>
      <td>65.10</td>
      <td>52.46</td>
      <td>80.27</td>
      <td>68.99</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>ce34d7174f0522f91723bc47419d60fbaec659cd</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Pallas-0.3</td>
      <td>69.88</td>
      <td>63.57</td>
      <td>83.36</td>
      <td>75.09</td>
      <td>57.32</td>
      <td>80.19</td>
      <td>59.74</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>1207a09c7bd4539bcefce62e1376495b0761b08a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Ba2han/BruinsV2-OpHermesNeu-11B</td>
      <td>69.88</td>
      <td>68.09</td>
      <td>84.70</td>
      <td>64.19</td>
      <td>62.76</td>
      <td>79.48</td>
      <td>60.05</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>9a5567cf04d6bd8bbd77743f303ce7ecebec78c5</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-Q</td>
      <td>69.86</td>
      <td>66.89</td>
      <td>85.14</td>
      <td>77.66</td>
      <td>53.03</td>
      <td>82.48</td>
      <td>53.98</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>187442aa0d250dc3c44451d71bf8fcdd556bdb24</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-3</td>
      <td>69.83</td>
      <td>66.89</td>
      <td>85.26</td>
      <td>63.07</td>
      <td>63.01</td>
      <td>79.64</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>33.0</td>
      <td>True</td>
      <td>fac83ab297a1c9ecc8affd97c998d864c10b9ff4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B-v2.02</td>
      <td>69.82</td>
      <td>67.66</td>
      <td>83.90</td>
      <td>61.98</td>
      <td>64.06</td>
      <td>79.40</td>
      <td>61.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>24fb5e81b1d39d4358930a1f9054513e9e2d6373</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sumo43/SOLAR-10.7B-Instruct-DPO-v1.0</td>
      <td>69.81</td>
      <td>73.12</td>
      <td>89.77</td>
      <td>64.21</td>
      <td>73.27</td>
      <td>81.93</td>
      <td>36.54</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>9e439597e3e788e3ff8a41df54e0dae0acda14a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-M-v1.1</td>
      <td>69.79</td>
      <td>67.15</td>
      <td>84.76</td>
      <td>74.50</td>
      <td>54.80</td>
      <td>82.87</td>
      <td>54.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>7.0</td>
      <td>True</td>
      <td>e5a016b08aa507fe9db45436074016928bf6f939</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-neural-chat-7b-v3-2-Slerp</td>
      <td>69.79</td>
      <td>65.70</td>
      <td>84.51</td>
      <td>63.50</td>
      <td>55.23</td>
      <td>79.95</td>
      <td>69.83</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>147f8e0526768591a7a119b7ec5b8cb821dbe900</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>internlm/internlm2-20b</td>
      <td>69.75</td>
      <td>62.97</td>
      <td>83.21</td>
      <td>67.58</td>
      <td>51.27</td>
      <td>85.56</td>
      <td>67.93</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>20.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>ec0e34824038c66745ba035f5c1994bd8cb99574</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-70b-3.1.2</td>
      <td>69.74</td>
      <td>70.14</td>
      <td>86.88</td>
      <td>69.72</td>
      <td>59.19</td>
      <td>83.11</td>
      <td>49.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>15.0</td>
      <td>True</td>
      <td>2de01b0a516bc64859abb16a948733d616dfb6d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/NinjaDolphin-7B</td>
      <td>69.74</td>
      <td>65.61</td>
      <td>85.35</td>
      <td>64.43</td>
      <td>54.94</td>
      <td>80.27</td>
      <td>67.85</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>0c2f691bda2d1131ef87767ccf47ba7560578c48</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/MathDolphin-7B</td>
      <td>69.73</td>
      <td>65.87</td>
      <td>85.49</td>
      <td>65.02</td>
      <td>52.92</td>
      <td>81.22</td>
      <td>67.85</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>6dcfc55a6e845fac45b8dbe3d8c2506fd1348834</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rishiraj/oswald-4x7b</td>
      <td>69.72</td>
      <td>65.78</td>
      <td>85.29</td>
      <td>64.49</td>
      <td>57.39</td>
      <td>79.16</td>
      <td>66.19</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>1a2a1c2a7cb0d18ae4af77f99a7adbe8d9718f92</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-M-v1.3</td>
      <td>69.71</td>
      <td>62.54</td>
      <td>83.95</td>
      <td>75.36</td>
      <td>56.03</td>
      <td>81.14</td>
      <td>59.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>7d733ec8449ec0219a9f499084a94a4248846f7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/SlimMelodicMaid</td>
      <td>69.70</td>
      <td>67.15</td>
      <td>86.01</td>
      <td>64.75</td>
      <td>60.88</td>
      <td>78.61</td>
      <td>60.80</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>36c2dfb9e7822dc77a97172a517952bd6c32cd88</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/bagel-34b-v0.2</td>
      <td>69.70</td>
      <td>68.77</td>
      <td>83.72</td>
      <td>76.45</td>
      <td>59.26</td>
      <td>83.82</td>
      <td>46.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>16.0</td>
      <td>True</td>
      <td>bc599b31f468d46d873964a58cab78380366d934</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>viethq188/Rabbit-7B-DPO-Chat</td>
      <td>69.69</td>
      <td>70.31</td>
      <td>87.43</td>
      <td>60.50</td>
      <td>62.18</td>
      <td>79.16</td>
      <td>58.53</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>04d42accbc808eec8c020f17392efa07c95ae565</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>andysalerno/openchat-nectar-0.5</td>
      <td>69.67</td>
      <td>66.72</td>
      <td>83.53</td>
      <td>65.36</td>
      <td>52.15</td>
      <td>82.08</td>
      <td>68.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba3caf530cbd9caf5c7cc7639cc47a910ed2a120</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/una-cybertron-7b-v2-bf16</td>
      <td>69.67</td>
      <td>68.26</td>
      <td>85.85</td>
      <td>63.23</td>
      <td>64.63</td>
      <td>80.98</td>
      <td>55.04</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>109.0</td>
      <td>True</td>
      <td>82599694771bd375c91f36dfdf30c448e4e33b3c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.3</td>
      <td>69.66</td>
      <td>65.96</td>
      <td>85.29</td>
      <td>64.35</td>
      <td>57.80</td>
      <td>78.30</td>
      <td>66.26</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>5ff4289d7f8b7f82f2453c611d737edce6b5efdc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/average-dolphin-8x7B</td>
      <td>69.64</td>
      <td>68.60</td>
      <td>85.99</td>
      <td>70.84</td>
      <td>54.51</td>
      <td>81.37</td>
      <td>56.56</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1.0</td>
      <td>True</td>
      <td>b0345662588e8c99d8e504bab894fa41e2199463</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Argetsu</td>
      <td>69.64</td>
      <td>67.06</td>
      <td>86.32</td>
      <td>65.55</td>
      <td>56.46</td>
      <td>79.16</td>
      <td>63.31</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e7238116d58f218368ab8e8099abec3cd60237c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>andysalerno/openchat-nectar-0.6</td>
      <td>69.64</td>
      <td>66.55</td>
      <td>83.22</td>
      <td>65.19</td>
      <td>51.90</td>
      <td>81.22</td>
      <td>69.75</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>502b55ebd1ca3c159591a9d7e9d9a456ac067e8d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/mixtralmerge-8x7B-rebalanced-test</td>
      <td>69.61</td>
      <td>68.17</td>
      <td>85.76</td>
      <td>70.47</td>
      <td>53.75</td>
      <td>81.29</td>
      <td>58.23</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>25093d03a4ee3a58b8eeb4d040b02b3a5f39ca95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Steelskull/Lumosia-MoE-4x10.7</td>
      <td>69.61</td>
      <td>68.34</td>
      <td>87.13</td>
      <td>64.38</td>
      <td>63.81</td>
      <td>82.95</td>
      <td>51.02</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>36.10</td>
      <td>7.0</td>
      <td>True</td>
      <td>0027074811e8901b63a27cc6d95db66fdafe8c90</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jan-ai/Pandora-13B-v1</td>
      <td>69.59</td>
      <td>67.06</td>
      <td>87.53</td>
      <td>63.65</td>
      <td>65.77</td>
      <td>80.51</td>
      <td>52.99</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.48</td>
      <td>0.0</td>
      <td>True</td>
      <td>16013ee5682ef9b38c8f27a2c2b78956befdbe52</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>openaccess-ai-collective/DPOpenHermes-7B-v2</td>
      <td>69.58</td>
      <td>66.64</td>
      <td>85.22</td>
      <td>63.64</td>
      <td>59.22</td>
      <td>79.16</td>
      <td>63.61</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>17.0</td>
      <td>True</td>
      <td>3ebea1710b555a205a04e69c743fe90162df63c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Tippy-Toppy-7b</td>
      <td>69.58</td>
      <td>66.89</td>
      <td>85.88</td>
      <td>65.49</td>
      <td>55.70</td>
      <td>78.85</td>
      <td>64.67</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>933d5b9cd8963398e3cc2875ff76e5c57c1877c7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/WizardDolphin-7B</td>
      <td>69.56</td>
      <td>64.68</td>
      <td>85.86</td>
      <td>62.75</td>
      <td>59.28</td>
      <td>78.53</td>
      <td>66.26</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5317ae098bdb1d8bbcbc13330aa9b96c5edae3b4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>andysalerno/openchat-nectar-0.4</td>
      <td>69.52</td>
      <td>66.64</td>
      <td>83.23</td>
      <td>65.22</td>
      <td>51.71</td>
      <td>81.69</td>
      <td>68.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>25eaf0bb01b56d1ce515dd1aa972be468e04c3ed</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/CognitiveFusion-4x7B-bf16-MoE</td>
      <td>69.50</td>
      <td>67.41</td>
      <td>86.16</td>
      <td>65.14</td>
      <td>67.05</td>
      <td>78.69</td>
      <td>52.54</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>3.0</td>
      <td>True</td>
      <td>40c2fdf46e33f4f84742fff63d5fb46932492e03</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/una-cybertron-7b-v1-fp16</td>
      <td>69.49</td>
      <td>68.43</td>
      <td>85.42</td>
      <td>63.34</td>
      <td>63.28</td>
      <td>81.37</td>
      <td>55.12</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>7bf918ddf0878a693f24f39e9f1a520464b44268</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Azazelle/Silicon-Medley</td>
      <td>69.49</td>
      <td>67.24</td>
      <td>86.21</td>
      <td>64.51</td>
      <td>61.34</td>
      <td>79.24</td>
      <td>58.38</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>98e46cc2715fdeead6c6b79307b40682efb83bfc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/ExtremeDolphin-MoE</td>
      <td>69.46</td>
      <td>65.10</td>
      <td>86.07</td>
      <td>63.76</td>
      <td>57.28</td>
      <td>78.69</td>
      <td>65.88</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>False</td>
      <td>1983955d7a48548e196a7b725cae4ddccdd7e357</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DopeorNope/You_can_cry_Snowman-13B</td>
      <td>69.46</td>
      <td>69.11</td>
      <td>86.30</td>
      <td>63.77</td>
      <td>70.24</td>
      <td>80.27</td>
      <td>47.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>13.35</td>
      <td>0.0</td>
      <td>True</td>
      <td>b50693bb4d8965ca9d48ff3c0c21fbfaa524d37c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>MayaPH/GodziLLa2-70B</td>
      <td>69.46</td>
      <td>71.42</td>
      <td>87.53</td>
      <td>69.88</td>
      <td>61.54</td>
      <td>83.19</td>
      <td>43.21</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>35.0</td>
      <td>True</td>
      <td>7b78087db07eec97f7b461d10758ece76d685543</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MisterRid/saulgoodman-2x7b-alpha1</td>
      <td>69.43</td>
      <td>66.21</td>
      <td>85.36</td>
      <td>64.95</td>
      <td>60.06</td>
      <td>79.24</td>
      <td>60.73</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b5ad66de184f72fa9525877ea6a62aa7bdc4815c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>01-ai/Yi-34B</td>
      <td>69.42</td>
      <td>64.59</td>
      <td>85.69</td>
      <td>76.35</td>
      <td>56.23</td>
      <td>83.03</td>
      <td>50.64</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1129.0</td>
      <td>True</td>
      <td>cd8d59de87ea11c6453ee287ac82e5523f08c8ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>rishiraj/oswald-2x7b</td>
      <td>69.40</td>
      <td>66.47</td>
      <td>85.46</td>
      <td>65.20</td>
      <td>60.06</td>
      <td>79.40</td>
      <td>59.82</td>
      <td>RL-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>False</td>
      <td>50fa192492461fdfcd8ce1c84e9081891141a5ac</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cookinai/Bald-Eagle-7B</td>
      <td>69.39</td>
      <td>64.51</td>
      <td>84.79</td>
      <td>64.39</td>
      <td>54.65</td>
      <td>80.98</td>
      <td>67.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>6f5a38b66c4121b2dae4545ad3b2c42fb2637556</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MisterRid/saulgoodman-7b-alpha1</td>
      <td>69.38</td>
      <td>65.70</td>
      <td>85.50</td>
      <td>65.19</td>
      <td>61.13</td>
      <td>79.01</td>
      <td>59.74</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c417af695d4e3370348e2ef15961884f127f7ff0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Sina-Thor-7b-Merge</td>
      <td>69.38</td>
      <td>66.21</td>
      <td>85.69</td>
      <td>65.17</td>
      <td>50.01</td>
      <td>80.51</td>
      <td>68.69</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d2074b9f23665b98362a52ce22ba62d4870985d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>deepseek-ai/deepseek-llm-67b-base</td>
      <td>69.38</td>
      <td>65.44</td>
      <td>87.10</td>
      <td>71.78</td>
      <td>51.08</td>
      <td>84.14</td>
      <td>56.71</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>67.00</td>
      <td>90.0</td>
      <td>True</td>
      <td>c3f813a1121c95488a20132d3a4da89f4a46452f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Sensualize-Mixtral-bf16</td>
      <td>69.37</td>
      <td>70.14</td>
      <td>86.60</td>
      <td>70.89</td>
      <td>54.17</td>
      <td>82.40</td>
      <td>52.01</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>2.0</td>
      <td>True</td>
      <td>50427f68da578b238e3e41b1126704cb0d06fb6a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sethuiyer/SynthIQ-7b</td>
      <td>69.37</td>
      <td>65.87</td>
      <td>85.82</td>
      <td>64.75</td>
      <td>57.00</td>
      <td>78.69</td>
      <td>64.06</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>32612e89aa87a23f6b1c5c5a9165896e599ca9ca</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>andysalerno/openchat-nectar-0.3</td>
      <td>69.36</td>
      <td>65.96</td>
      <td>83.15</td>
      <td>65.46</td>
      <td>52.38</td>
      <td>81.53</td>
      <td>67.70</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>cc29b95f9d0bee765206b07e4d9bba05a0fcafb2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>viethq188/Rabbit-7B-v2-DPO-Chat</td>
      <td>69.36</td>
      <td>66.13</td>
      <td>85.18</td>
      <td>62.92</td>
      <td>67.06</td>
      <td>79.24</td>
      <td>55.65</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7dae800851457f1dcccf00a2517448c9a9400b15</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-deepseek-67b-v15-base</td>
      <td>69.34</td>
      <td>66.30</td>
      <td>86.03</td>
      <td>70.97</td>
      <td>52.31</td>
      <td>83.58</td>
      <td>56.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>67.42</td>
      <td>0.0</td>
      <td>True</td>
      <td>2717bb85e0cd4c1c4abfa3d4abb7f9b6e55c1322</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>gagan3012/MetaModel_moe_multilingualv1</td>
      <td>69.33</td>
      <td>67.58</td>
      <td>84.72</td>
      <td>63.77</td>
      <td>61.21</td>
      <td>77.35</td>
      <td>61.33</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>1b27a5aa3381f82ae99e8187bbd982e319eafd17</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>openchat/openchat-3.5-0106</td>
      <td>69.30</td>
      <td>66.04</td>
      <td>82.93</td>
      <td>65.04</td>
      <td>51.90</td>
      <td>81.77</td>
      <td>68.16</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>113.0</td>
      <td>True</td>
      <td>9619fb7d2a8e25fa6b0633c0f57f7f4aa79b45c4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Jaume/openchat-3.5-0106-mod-gpt5</td>
      <td>69.30</td>
      <td>66.04</td>
      <td>82.93</td>
      <td>65.04</td>
      <td>51.90</td>
      <td>81.77</td>
      <td>68.16</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>e48411ee9b41210b2bf019e5b6e58a6cde3d04f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CallComply/openchat-3.5-0106-32k</td>
      <td>69.30</td>
      <td>66.04</td>
      <td>82.93</td>
      <td>65.04</td>
      <td>51.90</td>
      <td>81.77</td>
      <td>68.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>8d566086308e80e8aa01e70acfac10adcf457fe3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>garage-bAInd/Platypus2-70B-instruct</td>
      <td>69.30</td>
      <td>71.84</td>
      <td>87.94</td>
      <td>70.48</td>
      <td>62.26</td>
      <td>82.72</td>
      <td>40.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>167.0</td>
      <td>True</td>
      <td>a66378c15f89756215ccc64572ba69b161173703</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/Hermes-low-tune-3</td>
      <td>69.25</td>
      <td>66.21</td>
      <td>84.99</td>
      <td>63.74</td>
      <td>57.94</td>
      <td>78.77</td>
      <td>63.84</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d3824696c2c4b45aff9ee5c2725bd1780d163fa8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Jaume/openchat-3.5-0106-mod-gpt5</td>
      <td>69.19</td>
      <td>65.87</td>
      <td>82.93</td>
      <td>65.12</td>
      <td>51.93</td>
      <td>81.53</td>
      <td>67.78</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>e48411ee9b41210b2bf019e5b6e58a6cde3d04f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PistachioAlt/Synatra-MCS-7B-v0.3-RP-Slerp</td>
      <td>69.18</td>
      <td>66.64</td>
      <td>84.97</td>
      <td>63.61</td>
      <td>53.93</td>
      <td>79.72</td>
      <td>66.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>69369829e665cbcda97e7fd178f1c43720f0fce4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-0.3</td>
      <td>69.17</td>
      <td>64.76</td>
      <td>83.17</td>
      <td>74.66</td>
      <td>55.43</td>
      <td>80.90</td>
      <td>56.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>d2532427a883434ac152061b27d7c3cf0778868c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-l2-70b-2.2.1</td>
      <td>69.13</td>
      <td>69.71</td>
      <td>87.95</td>
      <td>69.79</td>
      <td>59.49</td>
      <td>82.95</td>
      <td>44.88</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>eadc78a4a9e173bccdca7dc8d12a34e80317c66c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>chargoddard/piano-medley-7b</td>
      <td>69.10</td>
      <td>67.58</td>
      <td>85.36</td>
      <td>64.49</td>
      <td>61.42</td>
      <td>79.16</td>
      <td>56.56</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>38da429cb28f667e8868574f32269a04dfe41280</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>itsliupeng/Mixtral-8x7B-v0.1-top3</td>
      <td>69.09</td>
      <td>67.41</td>
      <td>86.63</td>
      <td>71.98</td>
      <td>48.58</td>
      <td>82.40</td>
      <td>57.54</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1.0</td>
      <td>True</td>
      <td>41de832eae882f2c951b64ff5f04d7a809d0a99c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/Optimus-7B</td>
      <td>69.09</td>
      <td>65.44</td>
      <td>85.41</td>
      <td>63.61</td>
      <td>55.79</td>
      <td>78.77</td>
      <td>65.50</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>12.0</td>
      <td>True</td>
      <td>d9dd63bc4437c2089f40ce37e689ad530060519c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>chargoddard/loyal-piano-m7-cdpo</td>
      <td>69.08</td>
      <td>67.15</td>
      <td>85.39</td>
      <td>64.52</td>
      <td>61.53</td>
      <td>79.40</td>
      <td>56.48</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>5f5a78bedc2d3e5314589f685489bc981890cadf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>diffnamehard/Mistral-CatMacaroni-slerp-gradient</td>
      <td>69.08</td>
      <td>65.53</td>
      <td>85.66</td>
      <td>61.53</td>
      <td>64.10</td>
      <td>80.03</td>
      <td>57.62</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>daf6eee865b05b45a4ce61af906313a80de06a9d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/Neural-una-cybertron-7b</td>
      <td>69.05</td>
      <td>69.03</td>
      <td>84.51</td>
      <td>62.79</td>
      <td>64.99</td>
      <td>80.66</td>
      <td>52.31</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>66dae63f92cac0c99b1b162383506b60ac060225</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_v3_70b</td>
      <td>69.02</td>
      <td>71.25</td>
      <td>87.85</td>
      <td>70.18</td>
      <td>61.27</td>
      <td>82.72</td>
      <td>40.86</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>c1d4f997f8ed685a6efc72229523b2e56fd0774b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vihangd/smartsolmix-4x10.7b-v1</td>
      <td>69.01</td>
      <td>64.93</td>
      <td>85.13</td>
      <td>66.10</td>
      <td>55.03</td>
      <td>83.43</td>
      <td>59.44</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>36.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7ace5190d07c08c17f846cab5619260bee5ff69</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chargoddard/loyal-piano-m7-cdpo</td>
      <td>69.00</td>
      <td>67.06</td>
      <td>85.42</td>
      <td>64.54</td>
      <td>61.54</td>
      <td>79.08</td>
      <td>56.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>5f5a78bedc2d3e5314589f685489bc981890cadf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>chargoddard/servile-harpsichord-cdpo</td>
      <td>68.98</td>
      <td>67.32</td>
      <td>85.18</td>
      <td>64.54</td>
      <td>60.61</td>
      <td>79.16</td>
      <td>57.09</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>13cdf6bd90df46f4fae1d31b9d3b4f7fc31a7777</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ignos/LeoScorpius-GreenNode-Platypus-7B-v1</td>
      <td>68.96</td>
      <td>66.04</td>
      <td>86.53</td>
      <td>62.06</td>
      <td>52.78</td>
      <td>82.16</td>
      <td>64.22</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>606894800b6de3fa7a21b46427c3165968fdf3b6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat-3.5-1210</td>
      <td>68.89</td>
      <td>64.93</td>
      <td>84.92</td>
      <td>64.62</td>
      <td>52.15</td>
      <td>80.74</td>
      <td>65.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>249.0</td>
      <td>True</td>
      <td>e5df841b685e5b5ca11ce142f29c6c731bf087a0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-una-cybertron-v2-bf16-Ties</td>
      <td>68.88</td>
      <td>65.02</td>
      <td>83.68</td>
      <td>62.58</td>
      <td>55.52</td>
      <td>77.27</td>
      <td>69.22</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e613cc45140352e2d1759f0f551021e928de006e</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dfurman/Mixtral-8x7B-peft-v0.1</td>
      <td>68.87</td>
      <td>67.24</td>
      <td>86.03</td>
      <td>68.59</td>
      <td>59.54</td>
      <td>80.43</td>
      <td>51.40</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>87dac68765c899952d9d91ce827cda867d115c6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sethuiyer/distilabled_Chikuma_10.7B</td>
      <td>68.87</td>
      <td>66.38</td>
      <td>85.14</td>
      <td>64.70</td>
      <td>59.20</td>
      <td>79.40</td>
      <td>58.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5a6ba84916b025cdce898d17387e4b4bc31104f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/FT</td>
      <td>68.85</td>
      <td>63.05</td>
      <td>82.78</td>
      <td>69.69</td>
      <td>59.88</td>
      <td>79.64</td>
      <td>58.07</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>1b91227a0539deaf4dfb5b18d15c92316e0254c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_7Bx4_MOE_24B</td>
      <td>68.85</td>
      <td>65.36</td>
      <td>85.23</td>
      <td>62.96</td>
      <td>59.78</td>
      <td>78.06</td>
      <td>61.71</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>24.15</td>
      <td>9.0</td>
      <td>True</td>
      <td>1cc519b70e87de1c632a6dc98ac6383cf0dd994e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/kellemar-DPO-7B-d</td>
      <td>68.84</td>
      <td>66.89</td>
      <td>85.16</td>
      <td>62.77</td>
      <td>56.88</td>
      <td>79.32</td>
      <td>62.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d0583642fd14d4881ba7799cea1eb3a12daed62e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cloudyu/mixtral_7bx4_moe</td>
      <td>68.83</td>
      <td>65.27</td>
      <td>85.28</td>
      <td>62.84</td>
      <td>59.85</td>
      <td>77.66</td>
      <td>62.09</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>1cc519b70e87de1c632a6dc98ac6383cf0dd994e</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AIDC-ai-business/Marcoroni-70B-v1</td>
      <td>68.83</td>
      <td>73.55</td>
      <td>87.62</td>
      <td>70.67</td>
      <td>64.41</td>
      <td>83.43</td>
      <td>33.28</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>55a30d29db194832c0b5de1392a6598a63582144</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Eurdem/megatron_v1</td>
      <td>68.82</td>
      <td>65.96</td>
      <td>84.80</td>
      <td>65.02</td>
      <td>60.32</td>
      <td>79.79</td>
      <td>57.01</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>526323001ad41288cadb1395405e7df79524c68e</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AiMavenAi/AiMaven-SmartDawg-7b</td>
      <td>68.81</td>
      <td>67.92</td>
      <td>87.16</td>
      <td>62.69</td>
      <td>58.86</td>
      <td>79.01</td>
      <td>57.24</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b91cc33a1842344921dfd8ea9d7040277cafd8d6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/FT</td>
      <td>68.81</td>
      <td>63.14</td>
      <td>82.78</td>
      <td>69.50</td>
      <td>59.80</td>
      <td>79.40</td>
      <td>58.23</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>1b91227a0539deaf4dfb5b18d15c92316e0254c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>martyn/solar-megamerge-dare-10.7b-v1</td>
      <td>68.79</td>
      <td>66.13</td>
      <td>85.30</td>
      <td>66.03</td>
      <td>54.33</td>
      <td>82.95</td>
      <td>58.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>c08c204161faa4bd853856dc2c868dbab534632b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051611/limb</td>
      <td>68.73</td>
      <td>63.48</td>
      <td>83.07</td>
      <td>72.25</td>
      <td>58.37</td>
      <td>79.79</td>
      <td>55.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>55473b7666b66e5b51bb3c4e6b5bc88d1bd00666</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/A11P</td>
      <td>68.73</td>
      <td>62.54</td>
      <td>82.53</td>
      <td>70.56</td>
      <td>56.44</td>
      <td>79.87</td>
      <td>60.42</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0a14aa5fd9ae557d7dbd02e503deab50544d5a6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TokenBender/pic_7B_mistral_Full_v0.2</td>
      <td>68.72</td>
      <td>65.36</td>
      <td>84.03</td>
      <td>64.51</td>
      <td>59.20</td>
      <td>79.48</td>
      <td>59.74</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>4499c15a16b11d6491dcbe029acff64f03e1a5fd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-2-7B</td>
      <td>68.71</td>
      <td>66.38</td>
      <td>84.11</td>
      <td>62.84</td>
      <td>63.59</td>
      <td>78.53</td>
      <td>56.79</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>22.0</td>
      <td>True</td>
      <td>585c2fca1dce1904491c40408f6dd5404eca3754</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pinkyponky/SOLAR-10.7B-dpo-instruct-tuned-v0.1</td>
      <td>68.68</td>
      <td>65.19</td>
      <td>86.09</td>
      <td>66.25</td>
      <td>51.81</td>
      <td>83.98</td>
      <td>58.76</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>bb3b052f07ab6bc00a03dc5c7b510c0760bfd650</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>adamo1139/Yi-34B-AEZAKMI-v1</td>
      <td>68.67</td>
      <td>64.33</td>
      <td>84.31</td>
      <td>73.91</td>
      <td>55.73</td>
      <td>80.82</td>
      <td>52.92</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>2.0</td>
      <td>True</td>
      <td>c56dc8471eba802f74fed756f555b718d975d00a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/loyal-piano-m7</td>
      <td>68.67</td>
      <td>66.72</td>
      <td>85.03</td>
      <td>64.43</td>
      <td>60.03</td>
      <td>79.08</td>
      <td>56.71</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>21.0</td>
      <td>True</td>
      <td>d74ae6cb13325e0f81797ee33c07f0e234a2caa4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/openchat-3.5-1210-starling-slerp</td>
      <td>68.67</td>
      <td>63.91</td>
      <td>85.27</td>
      <td>65.05</td>
      <td>49.92</td>
      <td>80.82</td>
      <td>67.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>f6b1d9d6f613c6311b95d44b335a679e01e61140</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/A12P</td>
      <td>68.64</td>
      <td>64.42</td>
      <td>82.32</td>
      <td>69.97</td>
      <td>62.22</td>
      <td>79.64</td>
      <td>53.30</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e2eb6a36741dfc799fd13f67cba385f6e3992393</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mncai/agiin-13.6B-v0.0</td>
      <td>68.63</td>
      <td>69.45</td>
      <td>86.59</td>
      <td>61.94</td>
      <td>67.40</td>
      <td>78.69</td>
      <td>47.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.78</td>
      <td>0.0</td>
      <td>True</td>
      <td>631e80949b055193053c802437f3a31fe4e1390d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/spicyboros-70b-2.2</td>
      <td>68.62</td>
      <td>70.73</td>
      <td>87.58</td>
      <td>70.32</td>
      <td>58.31</td>
      <td>83.82</td>
      <td>40.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>533f7dda1e3fe462a0abb00671f9a48d5fd51093</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/MixtralRPChat-ZLoss</td>
      <td>68.59</td>
      <td>68.60</td>
      <td>86.10</td>
      <td>70.44</td>
      <td>53.85</td>
      <td>82.00</td>
      <td>50.57</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>17.0</td>
      <td>True</td>
      <td>94e804a4cd8e3ed54105f400118c60fa0cce764d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>brucethemoose/CaPlatTessDolXaBoros-Yi-34B-200K-DARE-Ties</td>
      <td>68.57</td>
      <td>64.93</td>
      <td>84.99</td>
      <td>75.37</td>
      <td>52.84</td>
      <td>79.24</td>
      <td>54.06</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>34.39</td>
      <td>3.0</td>
      <td>True</td>
      <td>7be35464f07307b5503d12736f732a34f3c1d8c1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_007</td>
      <td>68.56</td>
      <td>71.08</td>
      <td>87.65</td>
      <td>69.04</td>
      <td>63.12</td>
      <td>83.35</td>
      <td>37.15</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>0f5d81b13718a866cb078bd8762ab80a41972663</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sequelbox/SpellBlade</td>
      <td>68.54</td>
      <td>69.28</td>
      <td>87.31</td>
      <td>70.50</td>
      <td>47.10</td>
      <td>83.19</td>
      <td>53.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>68.98</td>
      <td>1.0</td>
      <td>True</td>
      <td>258211a0cceaa08f7c8df3660ff8cd7cb6bee5e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_009</td>
      <td>68.53</td>
      <td>71.59</td>
      <td>87.70</td>
      <td>69.43</td>
      <td>60.72</td>
      <td>82.32</td>
      <td>39.42</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>5020869e6394b1ac039bf80a0a1d2bed6be6707e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jan-hq/stealth-v1.3</td>
      <td>68.53</td>
      <td>65.19</td>
      <td>84.44</td>
      <td>62.70</td>
      <td>59.12</td>
      <td>78.61</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b76e2592849352c5073ebddec5748975f16e4895</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B-v2.04</td>
      <td>68.52</td>
      <td>66.30</td>
      <td>85.70</td>
      <td>60.94</td>
      <td>67.76</td>
      <td>78.93</td>
      <td>51.48</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b6eb3c3293fff1cb3d38bbfefa9adfce3e20f053</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/Sonya-7B</td>
      <td>68.48</td>
      <td>64.59</td>
      <td>85.11</td>
      <td>62.72</td>
      <td>61.22</td>
      <td>77.74</td>
      <td>59.51</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>19.0</td>
      <td>True</td>
      <td>228e7ab8b24ebb3d459160c0b665a821d1785dc5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Instruct-v0.2-Seraph-7B</td>
      <td>68.48</td>
      <td>64.76</td>
      <td>84.20</td>
      <td>62.90</td>
      <td>65.39</td>
      <td>79.16</td>
      <td>54.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>6ea01ce2a3b6967d9aaf968ed8015da21c979928</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mistralai/Mixtral-8x7B-v0.1</td>
      <td>68.47</td>
      <td>66.38</td>
      <td>86.46</td>
      <td>71.88</td>
      <td>46.81</td>
      <td>81.69</td>
      <td>57.62</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1043.0</td>
      <td>True</td>
      <td>58301445dc1378584211722b7ebf8743ec4e192b</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_101</td>
      <td>68.46</td>
      <td>68.69</td>
      <td>86.42</td>
      <td>69.92</td>
      <td>58.85</td>
      <td>82.08</td>
      <td>44.81</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>884c53a64a3c5faf7b0706d36a587ca1532ed8f5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>tenyx/TenyxChat-7B-v1</td>
      <td>68.46</td>
      <td>65.61</td>
      <td>85.55</td>
      <td>64.81</td>
      <td>51.28</td>
      <td>80.51</td>
      <td>63.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>23.0</td>
      <td>True</td>
      <td>c3c7ee002c4fdb1b8c2e2c78b7fba0c389673710</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ZoidBB/Jovian-10.7B-v1.0</td>
      <td>68.42</td>
      <td>67.41</td>
      <td>86.40</td>
      <td>65.66</td>
      <td>52.00</td>
      <td>81.85</td>
      <td>57.24</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>ba8433fe1cdf03a7fe25650d99219d34fce13bb8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>alykassem/ds_diasum_md_mixtral</td>
      <td>68.42</td>
      <td>66.30</td>
      <td>85.45</td>
      <td>69.51</td>
      <td>55.72</td>
      <td>80.35</td>
      <td>53.22</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>8ee85e4555b4c4a75b29ee749a86c97e0d37d242</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>mistralai/Mixtral-8x7B-v0.1</td>
      <td>68.42</td>
      <td>66.04</td>
      <td>86.49</td>
      <td>71.82</td>
      <td>46.78</td>
      <td>81.93</td>
      <td>57.47</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1043.0</td>
      <td>True</td>
      <td>4dd4b0f2d577d7b74152732d5543a92201481fe2</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>argilla/distilabeled-Hermes-2.5-Mistral-7B</td>
      <td>68.42</td>
      <td>66.30</td>
      <td>85.15</td>
      <td>63.50</td>
      <td>55.75</td>
      <td>78.93</td>
      <td>60.88</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>14.0</td>
      <td>True</td>
      <td>71e12bedd29a0d8e8744f32a41aca68769fc99c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvilasuero/NeuralHermes-2.5-Mistral-7B-distilabel</td>
      <td>68.40</td>
      <td>65.78</td>
      <td>84.97</td>
      <td>63.63</td>
      <td>55.86</td>
      <td>78.69</td>
      <td>61.49</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>a932ff3b8c3186bb12224857dd412f1cda56546e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>mncai/agiin-13.6B-v0.1</td>
      <td>68.40</td>
      <td>69.45</td>
      <td>86.64</td>
      <td>61.15</td>
      <td>67.97</td>
      <td>78.69</td>
      <td>46.47</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.78</td>
      <td>2.0</td>
      <td>True</td>
      <td>6c93ca1d60b09b9b91e15c57dc8525827d371798</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xDAN-AI/xDAN-L1-Chat-RL-v1</td>
      <td>68.38</td>
      <td>66.30</td>
      <td>85.81</td>
      <td>63.21</td>
      <td>56.70</td>
      <td>78.85</td>
      <td>59.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>42.0</td>
      <td>True</td>
      <td>0591b1690e5b7c800758f9f5de17a2e60cecf11e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-Llama</td>
      <td>68.37</td>
      <td>67.83</td>
      <td>85.35</td>
      <td>78.26</td>
      <td>53.46</td>
      <td>82.87</td>
      <td>42.46</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>e641a44c60ddf1f31d898ca53810ccb1e7a30972</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>budecosystem/genz-70b</td>
      <td>68.35</td>
      <td>71.42</td>
      <td>87.99</td>
      <td>70.78</td>
      <td>62.66</td>
      <td>83.50</td>
      <td>33.74</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>70.00</td>
      <td>29.0</td>
      <td>False</td>
      <td>32110b4f33e5e80073ca1f47638482fdc0e19297</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>amu/zen_moe</td>
      <td>68.34</td>
      <td>63.82</td>
      <td>85.05</td>
      <td>64.75</td>
      <td>50.03</td>
      <td>81.06</td>
      <td>65.35</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>1.0</td>
      <td>True</td>
      <td>5e6e23c4da1c3b6049a42d755cdf74848efd454a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Dumb-Maidlet</td>
      <td>68.34</td>
      <td>66.81</td>
      <td>86.06</td>
      <td>65.17</td>
      <td>50.70</td>
      <td>80.19</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1bbd507bb7dd502bbca4105406a6e57abe3c1187</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ZoidBB/Jovian-10.7B-v1.0</td>
      <td>68.34</td>
      <td>67.06</td>
      <td>86.39</td>
      <td>65.50</td>
      <td>52.00</td>
      <td>81.45</td>
      <td>57.62</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>ba8433fe1cdf03a7fe25650d99219d34fce13bb8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/ColorShadow-7B</td>
      <td>68.34</td>
      <td>67.83</td>
      <td>85.15</td>
      <td>61.69</td>
      <td>59.56</td>
      <td>80.58</td>
      <td>55.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>6fafdfbf1a92be78735623506bf676f5d8f7030a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/kellemar-DPO-7B-v1.01</td>
      <td>68.32</td>
      <td>65.78</td>
      <td>85.04</td>
      <td>63.24</td>
      <td>55.54</td>
      <td>78.69</td>
      <td>61.64</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b032e5ce518cf12383f4ec12952732d21f8321af</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-Llama-Q-FastChat</td>
      <td>68.31</td>
      <td>66.13</td>
      <td>85.25</td>
      <td>78.37</td>
      <td>53.62</td>
      <td>82.16</td>
      <td>44.35</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>dab86ae57fe51dc5e993769ebb69a173637852bc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Falkor-8x7B-MoE</td>
      <td>68.31</td>
      <td>66.30</td>
      <td>85.03</td>
      <td>64.13</td>
      <td>53.50</td>
      <td>80.19</td>
      <td>60.73</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>3.0</td>
      <td>True</td>
      <td>8a13e5399c12811d178cea09ffa719596410c9b4</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-2</td>
      <td>68.29</td>
      <td>67.49</td>
      <td>83.92</td>
      <td>63.55</td>
      <td>59.68</td>
      <td>79.95</td>
      <td>55.12</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>49.0</td>
      <td>True</td>
      <td>2ecaf100bcf63da6cf87dd7bfbea5732fa74c413</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-0.4</td>
      <td>68.28</td>
      <td>63.31</td>
      <td>82.74</td>
      <td>74.32</td>
      <td>55.25</td>
      <td>80.58</td>
      <td>53.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>b2f3a60d2cbf70d773f45cc9a7363481f7d1027f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/kellemar-DPO-7B</td>
      <td>68.26</td>
      <td>66.04</td>
      <td>85.21</td>
      <td>63.42</td>
      <td>55.55</td>
      <td>78.93</td>
      <td>60.42</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>860ee984db0e2830a969fc616128c4c7d2bca233</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Walmart-the-bag/WordWoven-13B</td>
      <td>68.25</td>
      <td>66.13</td>
      <td>85.81</td>
      <td>64.06</td>
      <td>54.45</td>
      <td>78.93</td>
      <td>60.12</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>d398693041f482ee7ee9c91c804206e7f62ea58c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>elinas/chronos007-70b</td>
      <td>68.25</td>
      <td>70.14</td>
      <td>87.52</td>
      <td>69.33</td>
      <td>57.65</td>
      <td>82.24</td>
      <td>42.61</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>6.0</td>
      <td>True</td>
      <td>c775f87a56f00725de4263f8d527995d40f611c4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>itsliupeng/llama2_70b_mmlu</td>
      <td>68.24</td>
      <td>65.61</td>
      <td>87.37</td>
      <td>71.89</td>
      <td>49.15</td>
      <td>82.40</td>
      <td>52.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>68.98</td>
      <td>0.0</td>
      <td>True</td>
      <td>a0592c8eeba5ba1519dd6843774baca1d400d00e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decapoda-research/Antares-11b-v1</td>
      <td>68.24</td>
      <td>64.51</td>
      <td>84.85</td>
      <td>65.96</td>
      <td>52.84</td>
      <td>82.95</td>
      <td>58.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>f5bda513641d782ab5278e993eb3ba8c7799f1b6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/kellemar-DPO-7B</td>
      <td>68.23</td>
      <td>66.21</td>
      <td>85.25</td>
      <td>63.38</td>
      <td>55.53</td>
      <td>78.37</td>
      <td>60.65</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>860ee984db0e2830a969fc616128c4c7d2bca233</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/NeuralHermes-2.5-Mistral-7B</td>
      <td>68.22</td>
      <td>66.55</td>
      <td>84.90</td>
      <td>63.32</td>
      <td>54.93</td>
      <td>78.30</td>
      <td>61.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>125.0</td>
      <td>True</td>
      <td>351028e0532a084c2c1370029fcf2ef805da3929</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>martyn/mixtral-megamerge-dare-8x7b-v2</td>
      <td>68.20</td>
      <td>66.47</td>
      <td>86.11</td>
      <td>69.14</td>
      <td>53.81</td>
      <td>79.79</td>
      <td>53.90</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1.0</td>
      <td>True</td>
      <td>a2dda73a962e3bda8893d951c836711e8ca84cea</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OrionStarAI/OrionStar-Yi-34B-Chat-Llama</td>
      <td>68.17</td>
      <td>64.93</td>
      <td>84.34</td>
      <td>73.67</td>
      <td>53.35</td>
      <td>78.85</td>
      <td>53.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>7.0</td>
      <td>True</td>
      <td>333c788e0d026cdb76bb827b8dcbc14a859ae2cc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Sensualize-Solar-10.7B</td>
      <td>68.17</td>
      <td>65.02</td>
      <td>84.55</td>
      <td>65.27</td>
      <td>53.63</td>
      <td>83.98</td>
      <td>56.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>9.0</td>
      <td>True</td>
      <td>126d7e645300a7773044408f77a810bc4f423949</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sethuiyer/Chikuma_10.7B</td>
      <td>68.17</td>
      <td>65.70</td>
      <td>84.31</td>
      <td>64.81</td>
      <td>57.01</td>
      <td>79.56</td>
      <td>57.62</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>2.0</td>
      <td>True</td>
      <td>3c99ba83d1b6cdee68696fc8443dbd4c71cf9cfe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azure99/blossom-v3_1-yi-34b</td>
      <td>68.16</td>
      <td>65.36</td>
      <td>84.24</td>
      <td>74.37</td>
      <td>56.06</td>
      <td>82.08</td>
      <td>46.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>34.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>2ec5cbb112a31c62c8631b89fbde0aebaabb6e0a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-llama2-70b-v10.1-bf16</td>
      <td>68.16</td>
      <td>61.86</td>
      <td>83.13</td>
      <td>67.41</td>
      <td>56.18</td>
      <td>80.11</td>
      <td>60.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>70.00</td>
      <td>48.0</td>
      <td>False</td>
      <td>a6ee90d262ac729f90ed8de97127766df070074c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/AZG</td>
      <td>68.16</td>
      <td>62.88</td>
      <td>82.02</td>
      <td>70.29</td>
      <td>53.84</td>
      <td>79.95</td>
      <td>59.97</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>36c17124ff891121c39f2d5e4d203daad5350c48</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LoSboccacc/orthogonal-2x7B-base</td>
      <td>68.13</td>
      <td>66.89</td>
      <td>85.54</td>
      <td>62.49</td>
      <td>66.00</td>
      <td>77.03</td>
      <td>50.80</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>b96572f91bdbb612299825f9ce793dabd63917dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Fredithefish/OpenZephyrChat</td>
      <td>68.12</td>
      <td>64.85</td>
      <td>85.08</td>
      <td>64.92</td>
      <td>48.24</td>
      <td>81.06</td>
      <td>64.59</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>146727eb2ebe09ea90552b0b22cb0abbfb830999</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mncai/agiin-11.1B-v0.0</td>
      <td>68.10</td>
      <td>67.32</td>
      <td>86.35</td>
      <td>64.99</td>
      <td>67.67</td>
      <td>78.85</td>
      <td>43.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>11.17</td>
      <td>0.0</td>
      <td>True</td>
      <td>0b086b46a672f450d7b2e8c307526e62d8d0cfdf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-LoRA</td>
      <td>68.10</td>
      <td>67.15</td>
      <td>85.37</td>
      <td>78.46</td>
      <td>53.32</td>
      <td>83.66</td>
      <td>40.64</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>5dcc36255b4632ba32a6b940fa43d53764a3fae3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>GreenNode/Merged-DPO-7B</td>
      <td>68.06</td>
      <td>68.94</td>
      <td>87.75</td>
      <td>55.35</td>
      <td>72.76</td>
      <td>78.37</td>
      <td>45.19</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1c0e61c7da6839fe4cc34433b899c5416fadbe18</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/xDAN-SlimOrca</td>
      <td>68.04</td>
      <td>65.61</td>
      <td>85.70</td>
      <td>63.67</td>
      <td>57.68</td>
      <td>77.66</td>
      <td>57.92</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f6c9f9451d35e8c3d9d5243324921114409ee077</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/Hermes-low-tune-2</td>
      <td>68.04</td>
      <td>65.61</td>
      <td>84.47</td>
      <td>63.69</td>
      <td>53.18</td>
      <td>77.74</td>
      <td>63.53</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>5e9fbbcf7c7959356574179f1091bc7bf4033a98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>deepnight-research/lil-c3po</td>
      <td>68.03</td>
      <td>65.02</td>
      <td>84.45</td>
      <td>62.36</td>
      <td>68.73</td>
      <td>79.16</td>
      <td>48.45</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7888318c72df9f668df20b2916b651b94a6ed77c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Walmart-the-bag/openchat-3.5-Infinity</td>
      <td>67.95</td>
      <td>62.63</td>
      <td>84.05</td>
      <td>64.65</td>
      <td>51.99</td>
      <td>80.11</td>
      <td>64.29</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d117307b5b813186aa4707ff602f0fb056752d66</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/bagel-dpo-7b-v0.1</td>
      <td>67.95</td>
      <td>66.72</td>
      <td>84.16</td>
      <td>64.24</td>
      <td>64.05</td>
      <td>80.90</td>
      <td>47.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>33.0</td>
      <td>True</td>
      <td>6444a0bc809bad1322820b48707746f027e01b96</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-exp2-0.1</td>
      <td>67.92</td>
      <td>62.97</td>
      <td>82.11</td>
      <td>74.66</td>
      <td>55.24</td>
      <td>79.79</td>
      <td>52.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>4369d91f05edaba98055e476a054441eee27ca37</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/Ryu-4x7B-MoE-bf16</td>
      <td>67.90</td>
      <td>66.47</td>
      <td>83.10</td>
      <td>63.89</td>
      <td>64.96</td>
      <td>79.24</td>
      <td>49.73</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>1.0</td>
      <td>True</td>
      <td>26e93b95a192650f8b145d103dead6162568953c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-Llama-Q-v2</td>
      <td>67.88</td>
      <td>61.09</td>
      <td>85.09</td>
      <td>76.59</td>
      <td>52.65</td>
      <td>82.79</td>
      <td>49.05</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>10ca8ee92ce7e749b8480de603bd8599d8d1fb29</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>meta-llama/Llama-2-70b-hf</td>
      <td>67.87</td>
      <td>67.32</td>
      <td>87.33</td>
      <td>69.83</td>
      <td>44.92</td>
      <td>83.74</td>
      <td>54.06</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>68.98</td>
      <td>733.0</td>
      <td>False</td>
      <td>ed7b07231238f836b99bf45701b9a0063576b194</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>martyn/mixtral-megamerge-dare-8x7b-v2</td>
      <td>67.87</td>
      <td>66.47</td>
      <td>86.05</td>
      <td>69.08</td>
      <td>53.82</td>
      <td>79.32</td>
      <td>52.46</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1.0</td>
      <td>True</td>
      <td>a2dda73a962e3bda8893d951c836711e8ca84cea</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-200k-Q-FastChat</td>
      <td>67.85</td>
      <td>64.93</td>
      <td>84.46</td>
      <td>77.13</td>
      <td>48.38</td>
      <td>80.74</td>
      <td>51.48</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>92a96144f94c24341cb6a40259be28627bc76298</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/Hermes-low-tune-2</td>
      <td>67.85</td>
      <td>65.27</td>
      <td>84.41</td>
      <td>63.63</td>
      <td>53.12</td>
      <td>78.22</td>
      <td>62.47</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>5e9fbbcf7c7959356574179f1091bc7bf4033a98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>tiiuae/falcon-180B</td>
      <td>67.85</td>
      <td>69.45</td>
      <td>88.86</td>
      <td>70.50</td>
      <td>45.47</td>
      <td>86.90</td>
      <td>45.94</td>
      <td>pretrained</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>unknown</td>
      <td>179.52</td>
      <td>1006.0</td>
      <td>True</td>
      <td>71a1a70b629e9963f7b4601e82f3f9079d48011e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/Nyxene-v2-11B</td>
      <td>67.84</td>
      <td>67.41</td>
      <td>84.54</td>
      <td>65.26</td>
      <td>55.62</td>
      <td>79.56</td>
      <td>54.66</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>5.0</td>
      <td>True</td>
      <td>07d017d24117fabce2e7b67819f6689e3187404f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B</td>
      <td>67.84</td>
      <td>66.55</td>
      <td>84.47</td>
      <td>63.34</td>
      <td>61.22</td>
      <td>78.37</td>
      <td>53.07</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>39.0</td>
      <td>True</td>
      <td>2e72eb3999108b7a9c7d0d0c6b8d81ad3470f1f5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Swisslex/Mixtral-Orca-v0.1</td>
      <td>67.82</td>
      <td>69.71</td>
      <td>88.88</td>
      <td>66.06</td>
      <td>63.85</td>
      <td>81.14</td>
      <td>37.30</td>
      <td>RL-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>9adcd9c408cce6c9c5e403dfda429bf90184a3e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/NarutoDolphin-10B</td>
      <td>67.82</td>
      <td>63.82</td>
      <td>84.17</td>
      <td>62.85</td>
      <td>59.13</td>
      <td>77.51</td>
      <td>59.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>20c3e2a3d13afb7340d1261e76528b1cbe6cd7ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/NarutoDolphin-7B</td>
      <td>67.82</td>
      <td>63.82</td>
      <td>84.17</td>
      <td>62.85</td>
      <td>59.13</td>
      <td>77.51</td>
      <td>59.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>False</td>
      <td>fcf546ffbfdee6e9bd288eec27316cac533d1ffe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>VAGOsolutions/SauerkrautLM-Mixtral-8x7B</td>
      <td>67.80</td>
      <td>68.86</td>
      <td>86.01</td>
      <td>66.69</td>
      <td>57.20</td>
      <td>80.51</td>
      <td>47.54</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>6.0</td>
      <td>True</td>
      <td>82dc0ab70090085b4271f0f317f667f180db9872</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B</td>
      <td>67.76</td>
      <td>66.81</td>
      <td>83.52</td>
      <td>62.68</td>
      <td>52.31</td>
      <td>79.08</td>
      <td>62.17</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>ae20703e16d89ba4a4301d12195cede64bd2ebdd</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>RatanRohith/MistralBeagle-RS-7B-V0.1</td>
      <td>67.75</td>
      <td>69.45</td>
      <td>84.62</td>
      <td>63.07</td>
      <td>69.78</td>
      <td>81.69</td>
      <td>37.91</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>a96439634909a69b9f508195ed53f0b43b034e8e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Q-bert/Bumblebee-7B</td>
      <td>67.73</td>
      <td>63.40</td>
      <td>84.16</td>
      <td>64.00</td>
      <td>50.96</td>
      <td>78.22</td>
      <td>65.66</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>0c95c597b9c6c5563273126d1306fdd56bd31618</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/Nyxene-11B</td>
      <td>67.72</td>
      <td>68.34</td>
      <td>84.54</td>
      <td>65.09</td>
      <td>57.50</td>
      <td>79.08</td>
      <td>51.78</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>55e115157836e1529dd28fc56e2900a5f0e79b89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Euryale-1.3-L2-70B</td>
      <td>67.66</td>
      <td>70.82</td>
      <td>87.92</td>
      <td>70.39</td>
      <td>59.85</td>
      <td>82.79</td>
      <td>34.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>29.0</td>
      <td>True</td>
      <td>6e3ce78eb5346bf3a5ee88cd60c25dc0d73de639</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rombodawg/Everyone-Coder-4x7b-Base</td>
      <td>67.65</td>
      <td>64.51</td>
      <td>84.76</td>
      <td>64.35</td>
      <td>49.19</td>
      <td>79.16</td>
      <td>63.91</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>24.15</td>
      <td>17.0</td>
      <td>True</td>
      <td>df11f29693b1cd4da9967f1c1832c4f4e0eb3303</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mrfakename/NeuralOrca-7B-v1</td>
      <td>67.64</td>
      <td>65.27</td>
      <td>85.07</td>
      <td>63.68</td>
      <td>54.58</td>
      <td>78.77</td>
      <td>58.45</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>32fb215494467cc6fa2f283a4b02f23546a26807</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/DPOpenHermes-7B</td>
      <td>67.63</td>
      <td>65.96</td>
      <td>85.90</td>
      <td>63.98</td>
      <td>56.92</td>
      <td>78.22</td>
      <td>54.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>12.0</td>
      <td>True</td>
      <td>f7742bd00c7d66791e94882b196b4d96fb88e63a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-NeuralHermes-2.5-Mistral-7B-Linear</td>
      <td>67.60</td>
      <td>62.80</td>
      <td>84.21</td>
      <td>63.43</td>
      <td>48.57</td>
      <td>76.80</td>
      <td>69.83</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>6aa0b89656b98f8f2212f6822ce665ac9517dbd7</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fangloveskari/ORCA_LLaMA_70B_QLoRA</td>
      <td>67.60</td>
      <td>72.27</td>
      <td>87.74</td>
      <td>70.23</td>
      <td>63.37</td>
      <td>83.66</td>
      <td>28.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>52.0</td>
      <td>True</td>
      <td>ef9b04ef02ccc4d96f1181467da92bb6b5baf835</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/Nyxene-v1-11B</td>
      <td>67.58</td>
      <td>67.49</td>
      <td>84.52</td>
      <td>65.12</td>
      <td>57.28</td>
      <td>79.01</td>
      <td>52.08</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>1af08865a403f3be77898d7fbc89bd3be5dfb21f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>openaccess-ai-collective/DPOpenHermes-7B</td>
      <td>67.58</td>
      <td>65.70</td>
      <td>85.96</td>
      <td>63.89</td>
      <td>56.95</td>
      <td>78.61</td>
      <td>54.36</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>12.0</td>
      <td>True</td>
      <td>f7742bd00c7d66791e94882b196b4d96fb88e63a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>r2rss/Malachite-7b-v0</td>
      <td>67.58</td>
      <td>67.75</td>
      <td>83.66</td>
      <td>63.54</td>
      <td>64.49</td>
      <td>81.22</td>
      <td>44.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e9b44b31a7ec203b301a7820a1c5000a30ed68a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fangloveskari/Platypus_QLoRA_LLaMA_70b</td>
      <td>67.57</td>
      <td>72.10</td>
      <td>87.46</td>
      <td>71.02</td>
      <td>61.18</td>
      <td>82.87</td>
      <td>30.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>b9b8560832276f60ba6bf37ac913b230a85ac19b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rombodawg/Everyone-Coder-4x7b-Base</td>
      <td>67.56</td>
      <td>64.51</td>
      <td>84.81</td>
      <td>64.27</td>
      <td>49.16</td>
      <td>79.16</td>
      <td>63.46</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>24.15</td>
      <td>17.0</td>
      <td>True</td>
      <td>df11f29693b1cd4da9967f1c1832c4f4e0eb3303</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-neural-chat-7b-v3-2-Ties</td>
      <td>67.54</td>
      <td>63.48</td>
      <td>82.34</td>
      <td>62.25</td>
      <td>52.06</td>
      <td>76.87</td>
      <td>68.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>2b0436588c205a6ecae5f32617d88b087b3cc644</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>bongchoi/MoMo-70B-LoRA-V1.1</td>
      <td>67.53</td>
      <td>66.64</td>
      <td>87.16</td>
      <td>66.76</td>
      <td>54.98</td>
      <td>83.35</td>
      <td>46.32</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ade069976a810b6b7caf3173a1aa4bfb30534ec9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abideen/NexoNimbus-MoE-2x7B</td>
      <td>67.51</td>
      <td>66.81</td>
      <td>85.66</td>
      <td>64.51</td>
      <td>53.06</td>
      <td>81.53</td>
      <td>53.53</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>3.0</td>
      <td>True</td>
      <td>b775c263bfde51a9536ce412893b69d87d064fb1</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>r2rss/Malachite-7b-v0</td>
      <td>67.50</td>
      <td>67.75</td>
      <td>83.68</td>
      <td>63.64</td>
      <td>64.54</td>
      <td>81.37</td>
      <td>44.05</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e9b44b31a7ec203b301a7820a1c5000a30ed68a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/kellemar-DPO-7B-c</td>
      <td>67.50</td>
      <td>65.70</td>
      <td>84.98</td>
      <td>63.70</td>
      <td>54.08</td>
      <td>78.30</td>
      <td>58.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>e9cc6491994a6babaa14f70ad425418d5c4bc7ae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ICBU-NPU/FashionGPT-70B-V1</td>
      <td>67.47</td>
      <td>71.08</td>
      <td>87.32</td>
      <td>70.70</td>
      <td>63.92</td>
      <td>83.66</td>
      <td>28.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>060c096af49700760f734c0102250a524d46b3eb</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/juanako-7b-UNA</td>
      <td>67.46</td>
      <td>68.17</td>
      <td>85.34</td>
      <td>62.47</td>
      <td>65.13</td>
      <td>78.85</td>
      <td>44.81</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>24.0</td>
      <td>True</td>
      <td>3e12f691e1f442f69eaff408677a54ebc69d5dc8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/Samantha-1.1-70b</td>
      <td>67.43</td>
      <td>68.77</td>
      <td>87.46</td>
      <td>68.60</td>
      <td>64.85</td>
      <td>83.27</td>
      <td>31.61</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>a3819d186f5b4d52ced7ddeb7fa16bf66e8a2ea7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fblgit/UNA-dolphin-2.6-mistral-7b-dpo-laser</td>
      <td>67.43</td>
      <td>67.15</td>
      <td>86.31</td>
      <td>63.36</td>
      <td>64.15</td>
      <td>79.24</td>
      <td>44.35</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>3e2cd605dde0bd7443172c722a1f34a498a36901</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AI-B/UTENA-7B-V3</td>
      <td>67.42</td>
      <td>65.96</td>
      <td>85.70</td>
      <td>64.72</td>
      <td>53.64</td>
      <td>80.27</td>
      <td>54.21</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>unlicense</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>47815871459a27e38d9b981d5096cf777585e461</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stabilityai/StableBeluga2</td>
      <td>67.42</td>
      <td>71.08</td>
      <td>86.37</td>
      <td>68.79</td>
      <td>59.44</td>
      <td>82.95</td>
      <td>35.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>862.0</td>
      <td>False</td>
      <td>e4944caa6ece819413b140b8dcecea79fe7e22cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Praneeth/StarMix-7B-slerp</td>
      <td>67.41</td>
      <td>65.36</td>
      <td>85.10</td>
      <td>62.57</td>
      <td>57.81</td>
      <td>79.95</td>
      <td>53.68</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5ab001441b789f05af53f43b07844dcfa63e78a7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beowolx/CodeNinja-1.0-OpenChat-7B</td>
      <td>67.40</td>
      <td>63.48</td>
      <td>83.65</td>
      <td>63.77</td>
      <td>47.16</td>
      <td>79.79</td>
      <td>66.57</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>84.0</td>
      <td>True</td>
      <td>9934c04c767e6ae0f792712a060f02915391d4ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/test_42_70b</td>
      <td>67.38</td>
      <td>68.26</td>
      <td>87.65</td>
      <td>70.00</td>
      <td>48.76</td>
      <td>83.66</td>
      <td>45.94</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GOAT-AI/GOAT-70B-Storytelling</td>
      <td>67.38</td>
      <td>68.77</td>
      <td>87.74</td>
      <td>69.92</td>
      <td>53.53</td>
      <td>83.50</td>
      <td>40.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>337fe3de7874d3a09aa1cfe9e78f5efd81c00f43</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>upstage/Llama-2-70b-instruct</td>
      <td>67.38</td>
      <td>70.90</td>
      <td>87.48</td>
      <td>69.80</td>
      <td>60.97</td>
      <td>82.87</td>
      <td>32.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>70.00</td>
      <td>63.0</td>
      <td>False</td>
      <td>8469429924dc2e1a9394b8095753985668a4052e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>charlesdedampierre/TopicNeuralHermes-2.5-Mistral-7B</td>
      <td>67.36</td>
      <td>67.06</td>
      <td>85.44</td>
      <td>63.66</td>
      <td>55.47</td>
      <td>78.30</td>
      <td>54.21</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c1c3fdde57d33f759b16f87a56c25a834bca0a38</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-v3.0-11B</td>
      <td>67.35</td>
      <td>64.08</td>
      <td>85.32</td>
      <td>66.18</td>
      <td>48.22</td>
      <td>84.21</td>
      <td>56.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>11.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>236b393ae07c1d80004eeda47ee017a71a899853</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abacusai/Fewshot-Metamath-OrcaVicuna-Mistral</td>
      <td>67.33</td>
      <td>59.64</td>
      <td>81.82</td>
      <td>61.69</td>
      <td>53.23</td>
      <td>78.45</td>
      <td>69.14</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>42ac13a68c242f7aa1ffb9385871fc3ae7d8415d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/ColorShadow-7B-v3</td>
      <td>67.29</td>
      <td>67.58</td>
      <td>85.04</td>
      <td>60.57</td>
      <td>62.88</td>
      <td>80.11</td>
      <td>47.54</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9dd05fe04e8a0ef7e7c0f72dd9ca2319c5813072</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/NeuralHermes-2.5-Mistral-7B-laser</td>
      <td>67.29</td>
      <td>66.38</td>
      <td>85.09</td>
      <td>63.43</td>
      <td>54.95</td>
      <td>78.14</td>
      <td>55.72</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>76efb2db34ee99b591431a3055eca785ffed44f7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>diffnamehard/Mistral-CatMacaroni-slerp-uncensored</td>
      <td>67.28</td>
      <td>64.25</td>
      <td>84.09</td>
      <td>62.66</td>
      <td>56.87</td>
      <td>79.72</td>
      <td>56.10</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>18a3b2e5a34765daafb8e36318a4baf33e272c83</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser</td>
      <td>67.28</td>
      <td>66.30</td>
      <td>85.73</td>
      <td>63.16</td>
      <td>61.71</td>
      <td>79.16</td>
      <td>47.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>53.0</td>
      <td>True</td>
      <td>05cc9e559e87e7e269401a3843a0e63a6084a85e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/Samantha-1.11-70b</td>
      <td>67.28</td>
      <td>70.05</td>
      <td>87.55</td>
      <td>67.82</td>
      <td>65.02</td>
      <td>83.27</td>
      <td>29.95</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>49e5b5ee0bed2864f0b38ba8bf9e01ccc5e0ba5f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Ana-v1-m7</td>
      <td>67.24</td>
      <td>67.41</td>
      <td>85.98</td>
      <td>64.43</td>
      <td>55.03</td>
      <td>78.06</td>
      <td>52.54</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>375e1a29c36bc1bf7bee972a28f47f9db1e85696</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-0.5</td>
      <td>67.21</td>
      <td>63.48</td>
      <td>82.21</td>
      <td>74.31</td>
      <td>54.64</td>
      <td>79.64</td>
      <td>48.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>1ba6929dbc914f50469dd6bf62082bc52207a03b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/rawr</td>
      <td>67.21</td>
      <td>63.99</td>
      <td>84.86</td>
      <td>64.70</td>
      <td>52.07</td>
      <td>79.56</td>
      <td>58.07</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>a8853791580ca0841cb7805462df7c57089d6762</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.6-mistral-7b-dpo</td>
      <td>67.20</td>
      <td>65.61</td>
      <td>85.48</td>
      <td>63.24</td>
      <td>61.47</td>
      <td>78.61</td>
      <td>48.75</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>47.0</td>
      <td>True</td>
      <td>5c32e515f3d79beefc110e8a07c3671269a0f5ab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/neural-chat-7b-v3-1-OpenHermes-2.5-7B</td>
      <td>67.19</td>
      <td>66.13</td>
      <td>84.09</td>
      <td>63.22</td>
      <td>61.23</td>
      <td>77.58</td>
      <td>50.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>b620ea7af98730695e051be48273cdded8923a2b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/Hermes-low-tune</td>
      <td>67.18</td>
      <td>63.99</td>
      <td>83.75</td>
      <td>63.60</td>
      <td>51.37</td>
      <td>77.90</td>
      <td>62.47</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>302b18f207e867b5bd918aa268bff0268b8a6f78</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ICBU-NPU/FashionGPT-70B-V1.2</td>
      <td>67.17</td>
      <td>73.04</td>
      <td>88.15</td>
      <td>70.11</td>
      <td>65.15</td>
      <td>82.56</td>
      <td>24.03</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>990a1664fc058de6ee2406af62c0a817d7047304</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>macadeliccc/laser-dolphin-mixtral-2x7b-dpo</td>
      <td>67.16</td>
      <td>65.96</td>
      <td>85.80</td>
      <td>63.17</td>
      <td>60.76</td>
      <td>79.01</td>
      <td>48.29</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>5.0</td>
      <td>True</td>
      <td>0ece1807074c4f1b9461e271a8931e4947902fbb</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>berkeley-nest/Starling-LM-7B-alpha</td>
      <td>67.13</td>
      <td>63.82</td>
      <td>84.90</td>
      <td>64.67</td>
      <td>46.39</td>
      <td>80.58</td>
      <td>62.40</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>427.0</td>
      <td>True</td>
      <td>f721e85293598f2ef774e483ae95343e39811577</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lizpreciatior/lzlv_70b_fp16_hf</td>
      <td>67.13</td>
      <td>70.14</td>
      <td>87.54</td>
      <td>70.23</td>
      <td>60.49</td>
      <td>83.43</td>
      <td>30.93</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-2.0</td>
      <td>68.98</td>
      <td>44.0</td>
      <td>True</td>
      <td>b366c0bb318ae592023cca894cc6b4421a607a0d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_007_v2</td>
      <td>67.13</td>
      <td>71.42</td>
      <td>87.31</td>
      <td>68.58</td>
      <td>62.65</td>
      <td>84.14</td>
      <td>28.66</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>3d95e0f3598f7a76ab97cb2cc0e4aae957d77479</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/MelangeB-70b</td>
      <td>67.12</td>
      <td>71.67</td>
      <td>87.50</td>
      <td>70.03</td>
      <td>59.36</td>
      <td>83.50</td>
      <td>30.63</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>68.98</td>
      <td>0.0</td>
      <td>False</td>
      <td>08239fb1e30b1e42b14370f23e942bc51e76027c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Starling-LM-alpha-8x7B-MoE</td>
      <td>67.11</td>
      <td>63.65</td>
      <td>84.90</td>
      <td>64.68</td>
      <td>46.39</td>
      <td>80.58</td>
      <td>62.47</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>5.0</td>
      <td>True</td>
      <td>61a66c526af1238690c815051c0f4ebe866ca588</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rishiraj/smol-7b</td>
      <td>67.11</td>
      <td>63.74</td>
      <td>84.77</td>
      <td>65.00</td>
      <td>46.17</td>
      <td>80.66</td>
      <td>62.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>21.0</td>
      <td>True</td>
      <td>d3e24684f38e0332cf4a6c70a37ee894e7a27fdc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decem/Dionysus-Mistral-m3-v6</td>
      <td>67.10</td>
      <td>63.14</td>
      <td>84.51</td>
      <td>62.82</td>
      <td>49.49</td>
      <td>78.45</td>
      <td>64.22</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>417618a86cd04bfcc48bd987043a4ef096e866cd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gqd/mistral-merge-7b</td>
      <td>67.07</td>
      <td>63.91</td>
      <td>84.48</td>
      <td>64.04</td>
      <td>53.73</td>
      <td>77.35</td>
      <td>58.91</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>unlicense</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>ffaddf395e00015873137562a8a34e1bb8123b41</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>berkeley-nest/Starling-LM-7B-alpha</td>
      <td>67.05</td>
      <td>63.65</td>
      <td>84.87</td>
      <td>64.70</td>
      <td>46.32</td>
      <td>80.43</td>
      <td>62.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>427.0</td>
      <td>True</td>
      <td>76e60ca9807f55acd8eff3ec7ae022c5fbdf1e0e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B-v2</td>
      <td>67.04</td>
      <td>65.19</td>
      <td>83.39</td>
      <td>63.60</td>
      <td>57.17</td>
      <td>78.14</td>
      <td>54.74</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>33.0</td>
      <td>True</td>
      <td>0c7f7c85359f15d3e6c361e8192738bdfb14ea6c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/MetaMath-NeuralHermes-2.5-Mistral-7B-Ties</td>
      <td>67.03</td>
      <td>62.46</td>
      <td>82.89</td>
      <td>62.25</td>
      <td>50.15</td>
      <td>75.14</td>
      <td>69.29</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b11bbd94238e1cc568c476844b1900c6e3facfa7</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vicgalle/franken-SOLAR-18B-v1.0</td>
      <td>67.03</td>
      <td>65.53</td>
      <td>86.45</td>
      <td>63.72</td>
      <td>62.14</td>
      <td>78.53</td>
      <td>45.79</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>17.93</td>
      <td>1.0</td>
      <td>True</td>
      <td>03c5412b8b0a6272cf02b399221ab94dbfd3157e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>meta-math/MetaMath-70B-V1.0</td>
      <td>67.02</td>
      <td>68.00</td>
      <td>86.85</td>
      <td>69.31</td>
      <td>50.98</td>
      <td>82.32</td>
      <td>44.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>783a3c7d5d0a75e6e11074f2577b90dd219ef7b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-70B-v1.2b</td>
      <td>67.00</td>
      <td>68.77</td>
      <td>87.57</td>
      <td>68.81</td>
      <td>57.69</td>
      <td>83.90</td>
      <td>35.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>7b687d6e4101b8bb8cc4062f8a318d639098a55d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/Mixtral-SlimOrca-8x7B</td>
      <td>66.97</td>
      <td>67.66</td>
      <td>85.11</td>
      <td>67.98</td>
      <td>54.98</td>
      <td>80.51</td>
      <td>45.56</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>46.0</td>
      <td>True</td>
      <td>e06a613acf6c8cb3e5a740e2ed6348b8047d90a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Walmart-the-bag/Misted-7B</td>
      <td>66.94</td>
      <td>63.65</td>
      <td>84.14</td>
      <td>63.94</td>
      <td>52.00</td>
      <td>78.30</td>
      <td>59.59</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>31245dbdcd0ace447a4434ac5e393a90ac862a87</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/neural-chat-7B-v3-2-GPTQ</td>
      <td>66.93</td>
      <td>65.96</td>
      <td>83.24</td>
      <td>60.29</td>
      <td>59.79</td>
      <td>79.48</td>
      <td>52.84</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>9.59</td>
      <td>3.0</td>
      <td>True</td>
      <td>cfe57da77e55efcb0e1087dc3948aeaa6ca55c74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-70B-v1.2</td>
      <td>66.90</td>
      <td>70.48</td>
      <td>86.98</td>
      <td>70.13</td>
      <td>58.64</td>
      <td>83.27</td>
      <td>31.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>9b92ee1093b125035ba1649dca6f4ceb9d86a656</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nlpguy/ColorShadow-7B-v2</td>
      <td>66.88</td>
      <td>67.15</td>
      <td>84.69</td>
      <td>60.34</td>
      <td>62.93</td>
      <td>78.85</td>
      <td>47.31</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>63713211df4348f2d73529c49a7cd0c1bb580ad7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>macadeliccc/polyglot-math-4x7b</td>
      <td>66.84</td>
      <td>63.74</td>
      <td>84.85</td>
      <td>63.57</td>
      <td>53.78</td>
      <td>78.45</td>
      <td>56.63</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>13b5f7d30c5db5060b41b2889f1c8df5ef7a8303</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/DPOpenHermes-11B</td>
      <td>66.83</td>
      <td>66.55</td>
      <td>84.80</td>
      <td>64.02</td>
      <td>57.34</td>
      <td>76.95</td>
      <td>51.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>6b78354a0789d3e9d0bfa6dd3d0b52c5e4594c39</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-70B-v1.1</td>
      <td>66.81</td>
      <td>70.05</td>
      <td>87.12</td>
      <td>70.34</td>
      <td>57.84</td>
      <td>83.66</td>
      <td>31.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>05a13f6adfe95a713dff04dc2eaa214c77c2512a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Frostwind-10.7B-v1</td>
      <td>66.81</td>
      <td>63.99</td>
      <td>85.36</td>
      <td>64.49</td>
      <td>50.41</td>
      <td>83.82</td>
      <td>52.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.70</td>
      <td>7.0</td>
      <td>True</td>
      <td>5b465f636e1d354718e393e85914865a64840903</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dillfrescott/sonya-medium-x8-MoE</td>
      <td>66.76</td>
      <td>64.25</td>
      <td>83.70</td>
      <td>62.53</td>
      <td>60.15</td>
      <td>76.24</td>
      <td>53.68</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>wtfpl</td>
      <td>69.92</td>
      <td>8.0</td>
      <td>True</td>
      <td>e8151c3609889dc7746ca748f4e16098663a5880</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Frostwind-10.7B-v1</td>
      <td>66.75</td>
      <td>64.16</td>
      <td>85.38</td>
      <td>64.64</td>
      <td>50.43</td>
      <td>83.74</td>
      <td>52.16</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.70</td>
      <td>7.0</td>
      <td>True</td>
      <td>5b465f636e1d354718e393e85914865a64840903</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-70B</td>
      <td>66.72</td>
      <td>69.45</td>
      <td>87.11</td>
      <td>68.91</td>
      <td>59.79</td>
      <td>83.66</td>
      <td>31.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>d63dfdd0baed756981f5f78f7419fd822c572362</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>macadeliccc/laser-dolphin-mixtral-4x7b-dpo</td>
      <td>66.71</td>
      <td>64.93</td>
      <td>85.81</td>
      <td>63.04</td>
      <td>63.77</td>
      <td>77.82</td>
      <td>44.88</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>2.0</td>
      <td>True</td>
      <td>472637ca2bf2bfb08aa4b5ebcdc5f89f48c7b257</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>internlm/internlm2-7b</td>
      <td>66.68</td>
      <td>58.02</td>
      <td>81.24</td>
      <td>65.24</td>
      <td>48.73</td>
      <td>83.82</td>
      <td>63.00</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>aac482e5fbfd5a85daa2a8e3aa3a1c5c97331d58</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Danielbrdz/Barcenas-10.7b</td>
      <td>66.63</td>
      <td>64.16</td>
      <td>83.60</td>
      <td>65.22</td>
      <td>46.59</td>
      <td>82.00</td>
      <td>58.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6ffe3b262cad3a2aee5fd36420f1b36933a7159</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-LASER-0.6</td>
      <td>66.62</td>
      <td>62.46</td>
      <td>81.60</td>
      <td>74.25</td>
      <td>54.39</td>
      <td>78.45</td>
      <td>48.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>4.0</td>
      <td>True</td>
      <td>0dc221753dbe63c4f5f5727adfe0f35cf05909f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uni-tianyan/Uni-TianYan</td>
      <td>66.61</td>
      <td>72.10</td>
      <td>87.40</td>
      <td>69.91</td>
      <td>65.81</td>
      <td>82.32</td>
      <td>22.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>47.0</td>
      <td>True</td>
      <td>46b78b9a10e78283e59c28b56cb59c2f33b0816a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>WizardLM/WizardMath-7B-V1.1</td>
      <td>66.61</td>
      <td>61.86</td>
      <td>84.50</td>
      <td>61.53</td>
      <td>47.04</td>
      <td>77.35</td>
      <td>67.40</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>52.0</td>
      <td>False</td>
      <td>366a19a3a8e64aea2fc77d648bec5738fb1f89ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Ba2han/HermesStar-OrcaWind-Synth-11B</td>
      <td>66.59</td>
      <td>65.27</td>
      <td>83.69</td>
      <td>65.31</td>
      <td>48.55</td>
      <td>80.11</td>
      <td>56.63</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>61aefa2ac956ce0e8ce40aa2521bdb5634452766</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decapoda-research/Adrastea-7b-v1.0-dpo</td>
      <td>66.59</td>
      <td>63.31</td>
      <td>82.30</td>
      <td>62.26</td>
      <td>53.10</td>
      <td>76.56</td>
      <td>62.02</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf8ccdae24f5b008c2f29cacadd05dd58e95da54</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yhyu13/LMCocktail-Mistral-7B-v1</td>
      <td>66.58</td>
      <td>66.21</td>
      <td>85.69</td>
      <td>61.64</td>
      <td>61.37</td>
      <td>77.35</td>
      <td>47.23</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>a4563de72fd5fe07b4fcec736e9efe83431df25a</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>NExtNewChattingAI/shark_tank_ai_7b_v2</td>
      <td>66.55</td>
      <td>67.75</td>
      <td>87.06</td>
      <td>58.79</td>
      <td>62.15</td>
      <td>78.45</td>
      <td>45.11</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b0796cb9cd42de2f66f652f162c29fdc57de2332</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NExtNewChattingAI/shark_tank_ai_7b_v2</td>
      <td>66.54</td>
      <td>67.58</td>
      <td>87.02</td>
      <td>58.88</td>
      <td>62.21</td>
      <td>78.69</td>
      <td>44.88</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b0796cb9cd42de2f66f652f162c29fdc57de2332</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/dpopenhermes-alpha-v0</td>
      <td>66.52</td>
      <td>65.02</td>
      <td>83.96</td>
      <td>63.67</td>
      <td>51.75</td>
      <td>78.85</td>
      <td>55.88</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>81ce4a9354d3b73276a0fa96b95d384f66d2de3d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kevin009/Llamafia</td>
      <td>66.49</td>
      <td>66.13</td>
      <td>82.08</td>
      <td>61.81</td>
      <td>47.94</td>
      <td>80.11</td>
      <td>60.88</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>0de1702faa89250ae329b3989c487fb0feb9e3f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rombodawg/Leaderboard-killer-MoE_4x7b</td>
      <td>66.47</td>
      <td>63.65</td>
      <td>81.97</td>
      <td>64.90</td>
      <td>50.75</td>
      <td>75.37</td>
      <td>62.17</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>391ad4593c4fdff7a90271954649a373b80d13d4</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>v2ray/LLaMA-2-Wizard-70B-QLoRA</td>
      <td>66.47</td>
      <td>67.58</td>
      <td>87.52</td>
      <td>69.11</td>
      <td>61.79</td>
      <td>82.32</td>
      <td>30.48</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>4bff676fe29f56d31961794c062aebc36312446e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rishiraj/uncensored</td>
      <td>66.46</td>
      <td>66.04</td>
      <td>84.80</td>
      <td>61.23</td>
      <td>59.14</td>
      <td>79.32</td>
      <td>48.22</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>7d2b64d29e68792172d809c51518c9092b5eea72</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AA051610/A13</td>
      <td>66.45</td>
      <td>61.09</td>
      <td>81.70</td>
      <td>69.62</td>
      <td>53.25</td>
      <td>80.35</td>
      <td>52.69</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c9b20b6f34269c27e56759888c5d42bd045e6da7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>simonveitner/Math-OpenHermes-2.5-Mistral-7B</td>
      <td>66.42</td>
      <td>63.05</td>
      <td>83.07</td>
      <td>63.21</td>
      <td>50.91</td>
      <td>77.19</td>
      <td>61.11</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>db052d375f389aa264bacac47aeb07538698122d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Brillibits/Instruct_Llama70B_Dolly15k</td>
      <td>66.42</td>
      <td>68.34</td>
      <td>87.21</td>
      <td>69.52</td>
      <td>46.46</td>
      <td>84.29</td>
      <td>42.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>45444ac60488594e0700e6c7313ff444b4468240</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>maywell/PiVoT-SOLAR-10.7B-RP</td>
      <td>66.42</td>
      <td>65.10</td>
      <td>81.83</td>
      <td>64.26</td>
      <td>56.54</td>
      <td>76.95</td>
      <td>53.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>10.70</td>
      <td>5.0</td>
      <td>True</td>
      <td>348a5ccfc4c8c9032ae6234a8fca72110ed4e5ee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yash21/OpenMistral-MoE</td>
      <td>66.42</td>
      <td>64.08</td>
      <td>83.99</td>
      <td>60.69</td>
      <td>54.57</td>
      <td>76.80</td>
      <td>58.38</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>False</td>
      <td>4c212c0361b002474b192010cdd49338e2db7d13</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>openaccess-ai-collective/openhermes-2_5-dpo-no-robots</td>
      <td>66.40</td>
      <td>64.93</td>
      <td>84.30</td>
      <td>63.86</td>
      <td>52.12</td>
      <td>77.90</td>
      <td>55.27</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>11.0</td>
      <td>True</td>
      <td>bee345f7da9816e459846b6bc3dbea6c69850855</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augtoma/qCammel-70x</td>
      <td>66.31</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augtoma/qCammel-70v1</td>
      <td>66.31</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augtoma/qCammel-70</td>
      <td>66.31</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augtoma/qCammel-70-x</td>
      <td>66.31</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augtoma/qCammel70</td>
      <td>66.31</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>garage-bAInd/Platypus2-70B</td>
      <td>66.28</td>
      <td>70.65</td>
      <td>87.15</td>
      <td>70.08</td>
      <td>52.37</td>
      <td>84.37</td>
      <td>33.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>68.98</td>
      <td>21.0</td>
      <td>True</td>
      <td>16b6583ad58313331f86be18e531ab03f1857695</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Doctor-Shotgun/mythospice-limarp-70b</td>
      <td>66.27</td>
      <td>69.20</td>
      <td>87.46</td>
      <td>70.14</td>
      <td>55.86</td>
      <td>82.72</td>
      <td>32.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>agpl-3.0</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ff29fed2a33fc050fd20d0e25b5b23c4a101b074</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Xwin-LM/Xwin-LM-70B-V0.1</td>
      <td>66.20</td>
      <td>70.22</td>
      <td>87.25</td>
      <td>69.77</td>
      <td>59.86</td>
      <td>82.87</td>
      <td>27.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>199.0</td>
      <td>True</td>
      <td>d6c803a180e3d46c371f8d3cb3848b861596ccbc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Doctor-Shotgun/mythospice-70b</td>
      <td>66.17</td>
      <td>69.28</td>
      <td>87.53</td>
      <td>70.10</td>
      <td>56.76</td>
      <td>83.27</td>
      <td>30.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>70.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>b00992c26604c9cd496bc41472a05e4c01cd2008</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beowolx/MistralHermes-CodePro-7B-v1</td>
      <td>66.17</td>
      <td>62.46</td>
      <td>82.68</td>
      <td>63.44</td>
      <td>49.67</td>
      <td>77.90</td>
      <td>60.88</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>a74a9fa5797b75262187fffa173948f1c03e2af4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/llama-2-70b-fb16-orca-chat-10k</td>
      <td>66.16</td>
      <td>68.09</td>
      <td>87.07</td>
      <td>69.21</td>
      <td>61.56</td>
      <td>84.14</td>
      <td>26.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>2.0</td>
      <td>True</td>
      <td>697aaeb8eb9905c9b25bebb736d1905444c774a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/mistral-11b-slimorca</td>
      <td>66.12</td>
      <td>64.25</td>
      <td>83.81</td>
      <td>63.66</td>
      <td>54.66</td>
      <td>77.98</td>
      <td>52.39</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>1feb0fe36c9db1a4ea6cca32acae9ff07a12b9c5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>amazingvince/where-llambo-7b</td>
      <td>66.08</td>
      <td>58.45</td>
      <td>82.06</td>
      <td>62.61</td>
      <td>49.61</td>
      <td>78.53</td>
      <td>65.20</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>554d9c7bab7ea6deabef0266aef17aa98f758543</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/llama-2-70b-Guanaco-QLoRA-fp16</td>
      <td>66.05</td>
      <td>68.26</td>
      <td>88.32</td>
      <td>70.23</td>
      <td>55.69</td>
      <td>83.98</td>
      <td>29.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>70.00</td>
      <td>54.0</td>
      <td>True</td>
      <td>54b0e39d5e9aee7b323f50b0a26db15295c3d5c9</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>seungduk/KoSOLAR-10.7B-v0.1</td>
      <td>66.04</td>
      <td>62.03</td>
      <td>84.54</td>
      <td>65.56</td>
      <td>45.03</td>
      <td>83.58</td>
      <td>55.50</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.86</td>
      <td>0.0</td>
      <td>True</td>
      <td>a4ddde9b0d06f340ff9c29777b4bfd883700c6cd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>upstage/SOLAR-10.7B-v1.0</td>
      <td>66.04</td>
      <td>61.95</td>
      <td>84.60</td>
      <td>65.48</td>
      <td>45.04</td>
      <td>83.66</td>
      <td>55.50</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>164.0</td>
      <td>True</td>
      <td>6e2783822f35c376ea96852fe479faa6a8bf09cb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TokenBender/pic_7B_mistral_Full_v0.1</td>
      <td>66.00</td>
      <td>63.91</td>
      <td>83.70</td>
      <td>63.30</td>
      <td>54.51</td>
      <td>77.90</td>
      <td>52.69</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>64f7a800327466b76697c1f81d88b008274c8861</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>s1ghhh/medllama-2-70b-qlora-1.1</td>
      <td>65.99</td>
      <td>69.03</td>
      <td>87.17</td>
      <td>71.04</td>
      <td>52.41</td>
      <td>84.21</td>
      <td>32.07</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>d55e05e9d67418c639933c85a5b9d17c6f531a92</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/longcat-10.7B</td>
      <td>65.98</td>
      <td>64.59</td>
      <td>85.85</td>
      <td>61.77</td>
      <td>61.42</td>
      <td>76.16</td>
      <td>46.10</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>c79c121d00a7edce5decc7189c32a4411ab26311</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>nisten/shqiponja-59b-v1</td>
      <td>65.97</td>
      <td>70.05</td>
      <td>84.06</td>
      <td>75.54</td>
      <td>70.43</td>
      <td>80.27</td>
      <td>15.47</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>mit</td>
      <td>58.94</td>
      <td>1.0</td>
      <td>True</td>
      <td>a2dd71db32b23412fcea0ad8a36ee32e0641b9fc</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_51</td>
      <td>65.96</td>
      <td>68.43</td>
      <td>86.71</td>
      <td>69.31</td>
      <td>57.18</td>
      <td>81.77</td>
      <td>32.37</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>9542702011bf4d282f4b0f0bd79229f5822b6313</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TomGrc/FusionNet_passthrough</td>
      <td>65.94</td>
      <td>69.45</td>
      <td>87.72</td>
      <td>65.28</td>
      <td>67.65</td>
      <td>81.29</td>
      <td>24.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>21.20</td>
      <td>1.0</td>
      <td>True</td>
      <td>fee459c6a29f7157394f62484eacf0417fee718a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/14B-DPO-alpha</td>
      <td>65.91</td>
      <td>58.11</td>
      <td>79.38</td>
      <td>66.62</td>
      <td>54.15</td>
      <td>74.51</td>
      <td>62.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>wtfpl</td>
      <td>14.00</td>
      <td>88.0</td>
      <td>False</td>
      <td>34bc2dd73ae5f8738e5bcaaa5591427675f7801f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Mixtral-4x7B-DPO-RPChat</td>
      <td>65.88</td>
      <td>64.59</td>
      <td>85.36</td>
      <td>63.57</td>
      <td>49.87</td>
      <td>78.77</td>
      <td>53.15</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>24.15</td>
      <td>10.0</td>
      <td>True</td>
      <td>406aeb5ce848dfefbca65d69022ce1de36f9fde4</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Qwen/Qwen-14B</td>
      <td>65.86</td>
      <td>58.28</td>
      <td>83.99</td>
      <td>67.70</td>
      <td>49.43</td>
      <td>76.80</td>
      <td>58.98</td>
      <td>pretrained</td>
      <td>QWenLMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>14.17</td>
      <td>189.0</td>
      <td>False</td>
      <td>5eda9482e32a8ea7ed2dc47178f3b491eb207939</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-falcon-180b-v13-preview0</td>
      <td>65.85</td>
      <td>65.10</td>
      <td>86.19</td>
      <td>64.60</td>
      <td>54.97</td>
      <td>82.64</td>
      <td>41.62</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>180.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>7d7b93ffd67d1b0c39f3503050dbbcc951948120</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B-v2.03-128k</td>
      <td>65.83</td>
      <td>64.68</td>
      <td>84.56</td>
      <td>63.02</td>
      <td>51.16</td>
      <td>81.06</td>
      <td>50.49</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>22bb3c15b2770dfe91e239573b6c35b475a43cbe</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>macadeliccc/laser-polyglot-4x7b</td>
      <td>65.79</td>
      <td>64.16</td>
      <td>84.98</td>
      <td>63.88</td>
      <td>55.47</td>
      <td>77.82</td>
      <td>48.45</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>1.0</td>
      <td>True</td>
      <td>3402a470e7fca09eb5aa5f7dcf2876449a05a4f6</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>meta-math/MetaMath-Mistral-7B</td>
      <td>65.78</td>
      <td>60.67</td>
      <td>82.58</td>
      <td>61.95</td>
      <td>44.89</td>
      <td>75.77</td>
      <td>68.84</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>55.0</td>
      <td>True</td>
      <td>016a7bb03bfcd953860357e1a16d5b333b887d26</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kaitchup/Maixtchup-4x7b</td>
      <td>65.77</td>
      <td>62.54</td>
      <td>83.83</td>
      <td>61.28</td>
      <td>56.13</td>
      <td>76.01</td>
      <td>54.81</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>24.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>56e8ed399a3198c7f02c30ac48361e690aad8d8f</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_420</td>
      <td>65.76</td>
      <td>70.14</td>
      <td>87.73</td>
      <td>70.35</td>
      <td>54.00</td>
      <td>83.74</td>
      <td>28.58</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>13c7b5f403c0f2af9bf7fce2d4a32deb9054c083</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TomGrc/FusionNet_passthrough_v0.1</td>
      <td>65.74</td>
      <td>69.45</td>
      <td>87.79</td>
      <td>65.20</td>
      <td>67.67</td>
      <td>81.53</td>
      <td>22.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>21.20</td>
      <td>1.0</td>
      <td>True</td>
      <td>0466e92dff5927724966ed3815432b4569d6d19e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>tokyotech-llm/Swallow-70b-instruct-hf</td>
      <td>65.74</td>
      <td>66.21</td>
      <td>85.14</td>
      <td>67.08</td>
      <td>48.00</td>
      <td>82.08</td>
      <td>45.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>69.16</td>
      <td>31.0</td>
      <td>True</td>
      <td>feba815b847806df03f23a375f3d4d07fa251134</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/dolphin-2.6-mistral-7b-dpo-orca-v2</td>
      <td>65.72</td>
      <td>66.13</td>
      <td>84.90</td>
      <td>62.64</td>
      <td>62.39</td>
      <td>78.61</td>
      <td>39.65</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1c44f6b0e9191a633837603c1053366868fc945</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dfurman/llama-2-70b-dolphin-peft</td>
      <td>65.72</td>
      <td>69.62</td>
      <td>86.82</td>
      <td>69.18</td>
      <td>57.43</td>
      <td>83.90</td>
      <td>27.37</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1190dee60b5854e80d340958dc3cc956bc56f68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mistralai/Mistral-7B-Instruct-v0.2</td>
      <td>65.71</td>
      <td>63.14</td>
      <td>84.88</td>
      <td>60.78</td>
      <td>68.26</td>
      <td>77.19</td>
      <td>40.03</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>657.0</td>
      <td>True</td>
      <td>c72e5d1908b1e2929ec8fc4c8820e9706af1f80f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>notbdq/alooowso</td>
      <td>65.63</td>
      <td>62.97</td>
      <td>84.87</td>
      <td>60.78</td>
      <td>68.18</td>
      <td>77.43</td>
      <td>39.58</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>420f54afd10959bb1c86b485245349cd437960b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Mistral-7B-Instruct-v0.2-2x7B-MoE</td>
      <td>65.60</td>
      <td>62.97</td>
      <td>84.88</td>
      <td>60.74</td>
      <td>68.18</td>
      <td>77.43</td>
      <td>39.42</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>1.0</td>
      <td>True</td>
      <td>46a2d11c1025e6ddec0fe35093d39e2e16170ca2</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>v2ray/LLaMA-2-Jannie-70B-QLoRA</td>
      <td>65.60</td>
      <td>68.94</td>
      <td>86.90</td>
      <td>69.37</td>
      <td>53.67</td>
      <td>82.95</td>
      <td>31.77</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>70.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e552ddca841a2b86e36bbe5f99840afedfdbcd14</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>garage-bAInd/Camel-Platypus2-70B</td>
      <td>65.59</td>
      <td>71.08</td>
      <td>87.60</td>
      <td>70.04</td>
      <td>58.09</td>
      <td>83.82</td>
      <td>22.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>13.0</td>
      <td>True</td>
      <td>b9f8de09ab860ee8ba570db7227c5444020ea056</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/Yee-34B-200K-Chat</td>
      <td>65.56</td>
      <td>65.61</td>
      <td>84.33</td>
      <td>74.91</td>
      <td>53.88</td>
      <td>79.79</td>
      <td>34.80</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>34.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>94bc30449e41628f59dd965cb7d9a8eb53ce9a45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Delcos/Velara-11B-V2</td>
      <td>65.55</td>
      <td>63.82</td>
      <td>85.85</td>
      <td>63.62</td>
      <td>58.83</td>
      <td>77.82</td>
      <td>43.37</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>11.39</td>
      <td>9.0</td>
      <td>True</td>
      <td>629ff26017b5adf0bc0c20d1c3f475491feb2b7a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NurtureAI/neural-chat-11b-v3-2</td>
      <td>65.52</td>
      <td>66.64</td>
      <td>82.12</td>
      <td>62.37</td>
      <td>60.22</td>
      <td>79.64</td>
      <td>42.15</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>8821b441a4a07ec7c45e1c13bead93e99ad2f099</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pankajmathur/Lima_Unchained_70b</td>
      <td>65.51</td>
      <td>68.26</td>
      <td>87.65</td>
      <td>70.00</td>
      <td>48.76</td>
      <td>83.66</td>
      <td>34.72</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>7dadf059a03bdfec2eb4f4a47666545875c68e49</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_42_70b</td>
      <td>65.51</td>
      <td>68.26</td>
      <td>87.65</td>
      <td>70.00</td>
      <td>48.76</td>
      <td>83.66</td>
      <td>34.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jordiclive/Llama-2-70b-oasst-1-200</td>
      <td>65.50</td>
      <td>67.66</td>
      <td>87.24</td>
      <td>69.95</td>
      <td>51.28</td>
      <td>84.14</td>
      <td>32.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>70.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>153b209007e688d713cd670c9972f2827c597b45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/bagel-7b-v0.1</td>
      <td>65.49</td>
      <td>63.91</td>
      <td>83.14</td>
      <td>64.56</td>
      <td>52.65</td>
      <td>80.58</td>
      <td>48.07</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>15.0</td>
      <td>True</td>
      <td>10ac045905d13da0e2be8e647cfe3e5ac8444894</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Synatra-10.7B-v0.4</td>
      <td>65.48</td>
      <td>64.93</td>
      <td>82.47</td>
      <td>62.50</td>
      <td>51.11</td>
      <td>81.85</td>
      <td>50.04</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>10.70</td>
      <td>5.0</td>
      <td>True</td>
      <td>ae32ccb01cc971cfb36370876bf8981db243b2a3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/fiction.live-Kimiko-V2-70B-fp16</td>
      <td>65.48</td>
      <td>67.66</td>
      <td>87.65</td>
      <td>69.82</td>
      <td>49.28</td>
      <td>83.90</td>
      <td>34.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>6b0c2cb654133cad2d4920e7da2e3f6cb1c4f7fd</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>wang7776/Mistral-7B-Instruct-v0.2-sparsity-10</td>
      <td>65.48</td>
      <td>62.88</td>
      <td>84.85</td>
      <td>60.87</td>
      <td>67.93</td>
      <td>77.51</td>
      <td>38.82</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9f83457d019d7b1471f09a1e967b15cd748f3e77</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>tiiuae/falcon-180B</td>
      <td>65.46</td>
      <td>69.20</td>
      <td>88.89</td>
      <td>69.59</td>
      <td>45.16</td>
      <td>86.74</td>
      <td>33.21</td>
      <td>pretrained</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>unknown</td>
      <td>179.52</td>
      <td>1006.0</td>
      <td>True</td>
      <td>71a1a70b629e9963f7b4601e82f3f9079d48011e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Metis-0.3</td>
      <td>65.44</td>
      <td>62.71</td>
      <td>84.80</td>
      <td>60.92</td>
      <td>67.56</td>
      <td>77.27</td>
      <td>39.35</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>d5b89820d04640d217aa3c174fa1d1ad5553419a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PistachioAlt/Noromaid-Bagel-7B-Slerp</td>
      <td>65.42</td>
      <td>64.51</td>
      <td>84.58</td>
      <td>64.30</td>
      <td>52.88</td>
      <td>79.40</td>
      <td>46.85</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>07ec589199b9368c755c9d67f316336c5ef8b2c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.3-ft-step-15936</td>
      <td>65.42</td>
      <td>62.54</td>
      <td>82.14</td>
      <td>62.58</td>
      <td>55.11</td>
      <td>75.77</td>
      <td>54.36</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2e53a73f2315a5ef111aa4a3a445a4a6682b031c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>garage-bAInd/Camel-Platypus2-70B</td>
      <td>65.39</td>
      <td>70.14</td>
      <td>87.71</td>
      <td>69.83</td>
      <td>57.77</td>
      <td>82.95</td>
      <td>23.96</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>13.0</td>
      <td>True</td>
      <td>6f958a1063fe1e6075f6e379fae621ff5a1d98c6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>OpenLemur/lemur-70b-chat-v1</td>
      <td>65.38</td>
      <td>66.98</td>
      <td>85.73</td>
      <td>65.99</td>
      <td>56.58</td>
      <td>81.69</td>
      <td>35.33</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>70.00</td>
      <td>67.0</td>
      <td>True</td>
      <td>33da87ba6d90662c6a00535bd628e5b39b3afd3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-7B-v2.03</td>
      <td>65.34</td>
      <td>63.82</td>
      <td>84.73</td>
      <td>63.05</td>
      <td>48.53</td>
      <td>80.90</td>
      <td>51.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>73641ebe6ba450a83f6e80ed919fba48cc5f2837</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>01-ai/Yi-34B-Chat</td>
      <td>65.32</td>
      <td>65.44</td>
      <td>84.16</td>
      <td>74.90</td>
      <td>55.37</td>
      <td>80.11</td>
      <td>31.92</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>202.0</td>
      <td>True</td>
      <td>a99ec35331cbfc9da596af7d4538fe2efecff03c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>liuxiang886/llama2-70B-qlora-gpt4</td>
      <td>65.29</td>
      <td>70.31</td>
      <td>86.39</td>
      <td>69.29</td>
      <td>54.02</td>
      <td>82.87</td>
      <td>28.89</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>70.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>08115ee077953e9c01c6a40f5086def3ecf9f5f0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FPHam/Writing_Partner_Mistral_7B</td>
      <td>65.29</td>
      <td>64.59</td>
      <td>84.59</td>
      <td>62.55</td>
      <td>48.55</td>
      <td>76.87</td>
      <td>54.59</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>21.0</td>
      <td>True</td>
      <td>d71b744e4d7432301d891409a05710bf2e4fa4c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>llm-agents/tora-70b-v1.0</td>
      <td>65.28</td>
      <td>67.58</td>
      <td>85.82</td>
      <td>69.13</td>
      <td>51.76</td>
      <td>82.16</td>
      <td>35.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>e95fd7daf017e7c414ec07ebef4ddf013c16f9a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/neural-chat-7b-v3-3-wizardmath-dare-me</td>
      <td>65.28</td>
      <td>59.64</td>
      <td>82.63</td>
      <td>58.13</td>
      <td>62.60</td>
      <td>71.67</td>
      <td>57.01</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d86275bacb3229e3de6069a98123c6252c7b471</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wang7776/Mistral-7B-Instruct-v0.2-sparsity-30-v0.1</td>
      <td>65.28</td>
      <td>63.31</td>
      <td>84.37</td>
      <td>60.24</td>
      <td>66.28</td>
      <td>78.06</td>
      <td>39.42</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>39566bcb48deecc1a3b830c5de9e70527d394c4f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>orangetin/OpenHermes-Mixtral-8x7B</td>
      <td>65.27</td>
      <td>63.91</td>
      <td>84.14</td>
      <td>64.29</td>
      <td>59.53</td>
      <td>74.03</td>
      <td>45.72</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>8.0</td>
      <td>True</td>
      <td>a55b010d3918ef61267d34e9ab47d9f554e3b11c</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/MadMix-v0.1</td>
      <td>65.26</td>
      <td>64.93</td>
      <td>84.37</td>
      <td>64.37</td>
      <td>51.05</td>
      <td>77.19</td>
      <td>49.66</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>71773ca4ca1fd76a00bd695a52b96b43b8fd78ff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AIGeekLabs/radiantloom-mixtral-8x7b-fusion</td>
      <td>65.24</td>
      <td>63.48</td>
      <td>83.65</td>
      <td>60.03</td>
      <td>54.76</td>
      <td>76.09</td>
      <td>53.45</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>1.0</td>
      <td>True</td>
      <td>93b3807b8fa38b9c95267117d25055bbd3eab29b</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>simonveitner/MathHermes-2.5-Mistral-7B</td>
      <td>65.24</td>
      <td>64.76</td>
      <td>84.19</td>
      <td>63.59</td>
      <td>51.95</td>
      <td>77.66</td>
      <td>49.28</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>2a6ee2674304f91d1dcc772695deded76d4c32bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/llama-2-70b-fb16-korean</td>
      <td>65.23</td>
      <td>67.15</td>
      <td>86.78</td>
      <td>69.29</td>
      <td>56.50</td>
      <td>82.64</td>
      <td>29.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>68.98</td>
      <td>34.0</td>
      <td>False</td>
      <td>fd57855006c15c4121feccab1cbeee8107de5b5a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cloudyu/Mixtral_7Bx2_MoE_13B</td>
      <td>65.14</td>
      <td>64.85</td>
      <td>83.92</td>
      <td>62.27</td>
      <td>57.55</td>
      <td>77.90</td>
      <td>44.35</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.88</td>
      <td>0.0</td>
      <td>True</td>
      <td>5ea651448fbeb313665d66187416233b865db7f1</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>KnutJaegersberg/internlm-20b-llama</td>
      <td>65.09</td>
      <td>61.35</td>
      <td>82.08</td>
      <td>61.59</td>
      <td>57.71</td>
      <td>76.72</td>
      <td>51.10</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>20.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0c4e862aeb22eaf2854ea06b6f8b1e3824591e3c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Sina-Loki-7b-Merge</td>
      <td>65.03</td>
      <td>59.13</td>
      <td>81.96</td>
      <td>64.71</td>
      <td>53.84</td>
      <td>78.14</td>
      <td>52.39</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5977c691a13280715c15559f2d90cb3142f74881</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.2.1-mistral-7b</td>
      <td>65.01</td>
      <td>63.23</td>
      <td>83.80</td>
      <td>63.16</td>
      <td>53.14</td>
      <td>78.61</td>
      <td>48.14</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>155.0</td>
      <td>True</td>
      <td>2022924c0bb13588308d429e0b7f51568c07629c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/OpenOrca-Zephyr-7B</td>
      <td>64.97</td>
      <td>64.08</td>
      <td>83.82</td>
      <td>62.46</td>
      <td>54.31</td>
      <td>78.93</td>
      <td>46.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>2a2c7d287a46243cccf3ff6628375d0d190394ac</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-1.4.1</td>
      <td>64.97</td>
      <td>70.39</td>
      <td>87.82</td>
      <td>70.31</td>
      <td>55.20</td>
      <td>83.58</td>
      <td>22.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>70.00</td>
      <td>48.0</td>
      <td>True</td>
      <td>ea98153fa721ed7110c77e73388e3b6f3996f2bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeuralNovel/Mistral-7B-Instruct-v0.2-Neural-Story</td>
      <td>64.96</td>
      <td>64.08</td>
      <td>83.97</td>
      <td>60.67</td>
      <td>66.89</td>
      <td>75.85</td>
      <td>38.29</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>8f3198a3e235d7c1ae56befbe8fb14a974acdf69</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/dolphin-2.2.1-mistral-7b</td>
      <td>64.93</td>
      <td>63.31</td>
      <td>83.76</td>
      <td>63.17</td>
      <td>53.11</td>
      <td>78.14</td>
      <td>48.07</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>001b48e9aebffb395c698af47b6b48364cc3cbe8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>bn22/OpenHermes-2.5-Mistral-7B-MISALIGNED</td>
      <td>64.92</td>
      <td>65.36</td>
      <td>84.67</td>
      <td>63.74</td>
      <td>52.85</td>
      <td>77.66</td>
      <td>45.26</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d366f84cef3a084c6c3dc87b304f0937080c2a6d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.6-mistral-7b</td>
      <td>64.92</td>
      <td>63.05</td>
      <td>84.05</td>
      <td>63.20</td>
      <td>55.67</td>
      <td>77.66</td>
      <td>45.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>86.0</td>
      <td>True</td>
      <td>61981ccfb93bad331c8d3da97aafeb13596afc9d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Puffin-70B</td>
      <td>64.91</td>
      <td>67.41</td>
      <td>87.37</td>
      <td>69.77</td>
      <td>46.77</td>
      <td>83.90</td>
      <td>34.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>70.00</td>
      <td>23.0</td>
      <td>True</td>
      <td>129e0af93d04b1b9cc85ea48bbb300f1ccb44210</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.6-mistral-7b</td>
      <td>64.91</td>
      <td>62.88</td>
      <td>84.06</td>
      <td>63.19</td>
      <td>55.65</td>
      <td>77.58</td>
      <td>46.10</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>86.0</td>
      <td>True</td>
      <td>61981ccfb93bad331c8d3da97aafeb13596afc9d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeuralNovel/Panda-7B-v0.1</td>
      <td>64.89</td>
      <td>62.97</td>
      <td>83.76</td>
      <td>60.73</td>
      <td>66.97</td>
      <td>76.24</td>
      <td>38.67</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>7d8702ad9d9da7871492ce8843fdb7308a42b3f4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jarradh/llama2_70b_chat_uncensored</td>
      <td>64.88</td>
      <td>68.43</td>
      <td>86.77</td>
      <td>68.76</td>
      <td>52.50</td>
      <td>82.56</td>
      <td>30.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>55.0</td>
      <td>True</td>
      <td>34b23982a9a996adc8f45c4c2eac7245c4e251b3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeuralNovel/Tanuki-7B-v0.1</td>
      <td>64.74</td>
      <td>62.80</td>
      <td>83.14</td>
      <td>60.54</td>
      <td>66.33</td>
      <td>75.85</td>
      <td>39.80</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>699ab2535487aee7cfd8d55ad928805b310c4b17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>oh-yeontaek/llama-2-70B-LoRA-assemble-v2</td>
      <td>64.73</td>
      <td>71.84</td>
      <td>86.89</td>
      <td>69.37</td>
      <td>64.79</td>
      <td>81.22</td>
      <td>14.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>70.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>7feeb5b665ab1ecdfd9cc4fe45fadb86b7b91b5b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/HelpSteer-filtered-Solar-Instruct</td>
      <td>64.73</td>
      <td>63.14</td>
      <td>83.05</td>
      <td>64.32</td>
      <td>46.23</td>
      <td>80.58</td>
      <td>51.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>52b80cc07c0c2a2bb54561a9c3d556231ca7344d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-mistral-7b-dare-0.85</td>
      <td>64.69</td>
      <td>63.57</td>
      <td>84.82</td>
      <td>64.29</td>
      <td>50.66</td>
      <td>79.24</td>
      <td>45.56</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b19e60f64b3be7f41658958658658bc12038c68f</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Euryale-L2-70B</td>
      <td>64.66</td>
      <td>68.94</td>
      <td>87.07</td>
      <td>68.84</td>
      <td>54.49</td>
      <td>82.08</td>
      <td>26.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>70.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>6589310a57ce5d9d6877f353f3d00cda8fa9101c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>smelborp/MixtralOrochi8x7B</td>
      <td>64.62</td>
      <td>70.31</td>
      <td>86.10</td>
      <td>70.13</td>
      <td>63.99</td>
      <td>79.87</td>
      <td>17.29</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>9.0</td>
      <td>True</td>
      <td>e88684d163fd3e789c40261c5b68244bb72bd706</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeuralNovel/Gecko-7B-v0.1</td>
      <td>64.58</td>
      <td>61.35</td>
      <td>83.36</td>
      <td>61.05</td>
      <td>62.60</td>
      <td>77.58</td>
      <td>41.55</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>404e501cb4d091e768e12861d50e37ac99d8a8fe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>maywell/PiVoT-0.1-early</td>
      <td>64.58</td>
      <td>62.46</td>
      <td>82.97</td>
      <td>61.02</td>
      <td>62.89</td>
      <td>73.72</td>
      <td>44.43</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>6eeae58a1a292a1d7f989952a07aead6d5da3c69</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>proto-llm/uniwiz-7B-v0.2</td>
      <td>64.56</td>
      <td>63.31</td>
      <td>85.07</td>
      <td>63.70</td>
      <td>59.91</td>
      <td>77.82</td>
      <td>37.53</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>799809bc85c6fb17a636e6d1f67bf959730baefd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-m2.0</td>
      <td>64.56</td>
      <td>70.05</td>
      <td>87.83</td>
      <td>70.67</td>
      <td>49.79</td>
      <td>83.58</td>
      <td>25.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>70.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>1cccd0b60a988bf6ddc4e2688895837845afa076</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mixtral-8x7b-v15.4</td>
      <td>64.54</td>
      <td>66.47</td>
      <td>71.81</td>
      <td>70.01</td>
      <td>55.46</td>
      <td>71.67</td>
      <td>51.86</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>d7ab397a06644e7b2a2ebd14c25e332dc0d29997</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NovoCode/Novocode7b</td>
      <td>64.54</td>
      <td>61.35</td>
      <td>83.36</td>
      <td>61.05</td>
      <td>62.60</td>
      <td>77.58</td>
      <td>41.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>a4cf91cc879937c3a45ca0f10aecd335c3919063</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Llama-2-70B-fp16</td>
      <td>64.52</td>
      <td>67.32</td>
      <td>87.33</td>
      <td>69.83</td>
      <td>44.92</td>
      <td>83.74</td>
      <td>33.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>45.0</td>
      <td>True</td>
      <td>b25061ef1b440e970d15d4ac99bc42937cd442a2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FinancialSupport/saiga-7b</td>
      <td>64.51</td>
      <td>63.14</td>
      <td>83.14</td>
      <td>61.66</td>
      <td>54.99</td>
      <td>79.01</td>
      <td>45.11</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>08daa40fbe05366466f96c92deb775d1b9b04669</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>upstage/llama-65b-instruct</td>
      <td>64.51</td>
      <td>68.86</td>
      <td>86.43</td>
      <td>64.77</td>
      <td>59.70</td>
      <td>81.06</td>
      <td>26.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>65.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>b95668861dfb7b0abca44ccdbef2db49b2dd8917</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/openchat-3.5-1210-32k</td>
      <td>64.49</td>
      <td>64.68</td>
      <td>84.06</td>
      <td>61.59</td>
      <td>49.31</td>
      <td>79.16</td>
      <td>48.14</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>48fde7a1a1d644f603a828839047ff695165b387</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>VAGOsolutions/SauerkrautLM-7b-HerO</td>
      <td>64.49</td>
      <td>63.23</td>
      <td>83.52</td>
      <td>63.30</td>
      <td>49.22</td>
      <td>78.37</td>
      <td>49.28</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>28.0</td>
      <td>True</td>
      <td>0aeb810af28e2910a92b929c21b931a5c06073de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/openchat-3.5-1210-32k-8x7b-MoE</td>
      <td>64.48</td>
      <td>64.59</td>
      <td>84.07</td>
      <td>61.60</td>
      <td>49.32</td>
      <td>79.16</td>
      <td>48.14</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>False</td>
      <td>c24bf500da78e987197055e96dda0dcc496de9ed</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-llama-65b-v8-bf16</td>
      <td>64.47</td>
      <td>62.80</td>
      <td>83.60</td>
      <td>62.01</td>
      <td>55.09</td>
      <td>79.95</td>
      <td>43.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>65.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>445b77821fac8e6cfb77d0399fb827400b5bb71e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/llama2-70b-oasst-sft-v10</td>
      <td>64.47</td>
      <td>67.06</td>
      <td>86.38</td>
      <td>67.70</td>
      <td>56.45</td>
      <td>82.00</td>
      <td>27.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>69.0</td>
      <td>True</td>
      <td>e68a8a2888097def3c7f4fe5d443866a18d05c6c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NurtureAI/Starling-LM-11B-alpha-v1</td>
      <td>64.44</td>
      <td>62.20</td>
      <td>83.24</td>
      <td>64.03</td>
      <td>45.70</td>
      <td>80.51</td>
      <td>50.95</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>b2b3b9fc069a8b5d8be82f68f0f578a6f23e9e5f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hedronstone/OpenHermes-7B-Reasoner</td>
      <td>64.44</td>
      <td>63.14</td>
      <td>82.73</td>
      <td>62.62</td>
      <td>48.82</td>
      <td>75.85</td>
      <td>53.45</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d26f2defbf9f40a65dbb2ead08c79cd61096ed08</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hedronstone/OpenHermes-7B-Symbolic</td>
      <td>64.44</td>
      <td>63.14</td>
      <td>82.73</td>
      <td>62.62</td>
      <td>48.82</td>
      <td>75.85</td>
      <td>53.45</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>23eb76553aa37cd48c1f2d8a314d78fd3ead53f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Medilora/medilora-mistral-7b</td>
      <td>64.41</td>
      <td>61.69</td>
      <td>83.13</td>
      <td>62.22</td>
      <td>49.91</td>
      <td>77.66</td>
      <td>51.86</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b6512d2a2202e685da461ff876a1ffb707034c97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elinas/chronos-70b-v2</td>
      <td>64.41</td>
      <td>68.09</td>
      <td>86.50</td>
      <td>68.28</td>
      <td>53.70</td>
      <td>81.22</td>
      <td>28.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>70.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>373af41ca0b2855972b8d471fd63e72b63e4c9fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xxyyy123/Mistral-dpo-v1</td>
      <td>64.39</td>
      <td>63.48</td>
      <td>83.59</td>
      <td>63.35</td>
      <td>50.49</td>
      <td>79.32</td>
      <td>46.10</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3c677a659bffbccbd8cf5ea75d198541ea2ec990</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>v1olet/v1olet_merged_dpo_7B_v4</td>
      <td>64.30</td>
      <td>66.98</td>
      <td>84.09</td>
      <td>59.02</td>
      <td>59.43</td>
      <td>81.06</td>
      <td>35.25</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa1b6363990ed2f180b2a22986cecc3afa4d12c8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>maywell/PiVoT-10.7B-Mistral-v0.2</td>
      <td>64.25</td>
      <td>63.31</td>
      <td>81.68</td>
      <td>59.86</td>
      <td>58.23</td>
      <td>80.03</td>
      <td>42.38</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>10.73</td>
      <td>4.0</td>
      <td>True</td>
      <td>a496457d0743b6030ffbb96dad2dc6a62d143943</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_420_preview</td>
      <td>64.22</td>
      <td>67.06</td>
      <td>87.26</td>
      <td>69.85</td>
      <td>44.57</td>
      <td>83.35</td>
      <td>33.21</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>5095384f1b7bb6e23a987f95589e66e21ae854ef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Neuronovo/neuronovo-7B-v0.1</td>
      <td>64.19</td>
      <td>66.98</td>
      <td>85.07</td>
      <td>63.33</td>
      <td>53.95</td>
      <td>78.14</td>
      <td>37.68</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>ec4f35d96aa47229fb3cab047fb9aedd6b0ad383</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-2.0</td>
      <td>64.14</td>
      <td>68.52</td>
      <td>87.89</td>
      <td>70.41</td>
      <td>49.79</td>
      <td>83.50</td>
      <td>24.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>70.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>f16526d9bb814dc10adc911f94e8c7a520beb5b6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ibndias/NeuralHermes-MoE-2x7B</td>
      <td>64.08</td>
      <td>62.12</td>
      <td>84.21</td>
      <td>64.56</td>
      <td>43.61</td>
      <td>78.14</td>
      <td>51.86</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>1.0</td>
      <td>True</td>
      <td>f8a3c8339ea38ce577e0c45aba859ac63b4c3cf3</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Enoch/llama-65b-hf</td>
      <td>63.99</td>
      <td>63.31</td>
      <td>86.09</td>
      <td>63.84</td>
      <td>43.43</td>
      <td>82.48</td>
      <td>44.81</td>
      <td>fine-tuned</td>
      <td>LLaMAForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>65.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>7a7b897ab10b3d82d1e7e6fbcd2159d70b4586cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abacusai/Fewshot-Metamath-Mistral</td>
      <td>63.96</td>
      <td>57.76</td>
      <td>80.59</td>
      <td>58.05</td>
      <td>43.04</td>
      <td>76.01</td>
      <td>68.31</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5c6d79d66a84efd6b6e879c2fe4f2e4a21df3a1e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>openbmb/UltraLM-65b</td>
      <td>63.82</td>
      <td>67.06</td>
      <td>84.98</td>
      <td>63.48</td>
      <td>53.51</td>
      <td>81.14</td>
      <td>32.75</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>65.00</td>
      <td>6.0</td>
      <td>False</td>
      <td></td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/14B</td>
      <td>63.81</td>
      <td>56.66</td>
      <td>79.08</td>
      <td>65.86</td>
      <td>47.75</td>
      <td>74.90</td>
      <td>58.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>wtfpl</td>
      <td>14.00</td>
      <td>251.0</td>
      <td>False</td>
      <td>2576a37434e2e03804c841d36c669c8a34c729de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Medilora/medilora-qwen-14b</td>
      <td>63.81</td>
      <td>56.66</td>
      <td>79.08</td>
      <td>65.86</td>
      <td>47.75</td>
      <td>74.90</td>
      <td>58.61</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>14.17</td>
      <td>0.0</td>
      <td>True</td>
      <td>0649cf49b7a879fe837567a346a3ebbbac77614a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/CausalLM-Platypus-14B</td>
      <td>63.80</td>
      <td>56.91</td>
      <td>80.06</td>
      <td>64.98</td>
      <td>47.57</td>
      <td>76.01</td>
      <td>57.24</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>14.17</td>
      <td>0.0</td>
      <td>True</td>
      <td>1659d3cdbb8bb8dba902ab2874f4fa886980fc70</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/mistral-inst-ppo</td>
      <td>63.79</td>
      <td>62.37</td>
      <td>83.20</td>
      <td>60.86</td>
      <td>62.30</td>
      <td>76.95</td>
      <td>37.07</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>600c429a86dcd6e18f0285d7cd9189540ccbdc50</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>jae24/openhermes_dpo_norobot_0201</td>
      <td>63.78</td>
      <td>62.03</td>
      <td>83.40</td>
      <td>62.40</td>
      <td>47.44</td>
      <td>78.22</td>
      <td>49.20</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>db7b39141559ca4810371593d9caab4361704646</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TigerResearch/tigerbot-70b-base</td>
      <td>63.71</td>
      <td>62.46</td>
      <td>83.61</td>
      <td>65.49</td>
      <td>52.76</td>
      <td>80.19</td>
      <td>37.76</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>68.95</td>
      <td>0.0</td>
      <td>True</td>
      <td>8af85526293eb8625375f3f7a1bab69825176e48</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/test_final</td>
      <td>63.70</td>
      <td>66.13</td>
      <td>85.85</td>
      <td>61.51</td>
      <td>57.89</td>
      <td>76.64</td>
      <td>34.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b996460b9ac3969f2c685c3f3669ba944022b2be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Delcos/Starling-LM-11B-alpha</td>
      <td>63.66</td>
      <td>62.97</td>
      <td>84.85</td>
      <td>63.83</td>
      <td>54.52</td>
      <td>77.82</td>
      <td>37.98</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>11.39</td>
      <td>4.0</td>
      <td>True</td>
      <td>16086688b70e4f54e1ba4f54a1a847c30b987a74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/gpt4-alpaca-lora_mlp-65B-HF</td>
      <td>63.66</td>
      <td>65.02</td>
      <td>86.13</td>
      <td>62.73</td>
      <td>59.16</td>
      <td>80.66</td>
      <td>28.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>65.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>664ff8e3e1d446971a16a6c9018ab24de7664684</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>monology/openinstruct-mistral-7b</td>
      <td>63.64</td>
      <td>59.73</td>
      <td>82.77</td>
      <td>60.55</td>
      <td>48.76</td>
      <td>79.56</td>
      <td>50.49</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>12.0</td>
      <td>True</td>
      <td>54f379bf7676ffd09b48b0ff607b7ae6c0a6f688</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azure99/blossom-v4-mistral-7b</td>
      <td>63.61</td>
      <td>62.03</td>
      <td>82.90</td>
      <td>62.48</td>
      <td>53.84</td>
      <td>77.27</td>
      <td>43.14</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>43d6205b109754c02a4606beee64f42d151067f1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-code-mistral-7b-v1.0</td>
      <td>63.60</td>
      <td>61.18</td>
      <td>83.77</td>
      <td>63.40</td>
      <td>47.90</td>
      <td>78.37</td>
      <td>47.01</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>43dea8e97d05f2e4358415b9a95a1b327c1f5804</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/higgs-llama-vicuna-ep25-70b</td>
      <td>63.60</td>
      <td>62.29</td>
      <td>86.07</td>
      <td>64.25</td>
      <td>53.75</td>
      <td>80.66</td>
      <td>34.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>1da59e150f1d0bae67f66400738a01d408a8c45d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/test-test</td>
      <td>63.54</td>
      <td>66.47</td>
      <td>85.82</td>
      <td>61.48</td>
      <td>57.75</td>
      <td>76.95</td>
      <td>32.75</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>83731d11da3f0878effd3a32e5aea52249de7c81</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KaeriJenti/Kaori-34B-v1</td>
      <td>63.52</td>
      <td>64.51</td>
      <td>79.65</td>
      <td>70.19</td>
      <td>53.14</td>
      <td>76.95</td>
      <td>36.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>3a03b2aba751680105e027ef096866320bf9bd2b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/test-test</td>
      <td>63.52</td>
      <td>66.38</td>
      <td>85.84</td>
      <td>61.22</td>
      <td>57.82</td>
      <td>76.80</td>
      <td>33.06</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>83731d11da3f0878effd3a32e5aea52249de7c81</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/zephyr-7b-sft-full-SPIN-iter2</td>
      <td>63.52</td>
      <td>66.38</td>
      <td>85.84</td>
      <td>61.22</td>
      <td>57.82</td>
      <td>76.80</td>
      <td>33.06</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>336bff60f5ce904c2ab9633315192df904431afa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Falkor-16b</td>
      <td>63.52</td>
      <td>65.96</td>
      <td>82.62</td>
      <td>63.58</td>
      <td>62.77</td>
      <td>77.90</td>
      <td>28.28</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>2365c7af9eb60bfa946b566dadd6802befa122e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SUSTech/SUS-Chat-72B</td>
      <td>63.51</td>
      <td>66.30</td>
      <td>84.96</td>
      <td>76.70</td>
      <td>60.27</td>
      <td>83.43</td>
      <td>9.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>72.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>869bbd5b18656e74b606bd775e2594809407603c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>argilla/notus-7b-v1</td>
      <td>63.49</td>
      <td>64.59</td>
      <td>84.83</td>
      <td>63.04</td>
      <td>54.35</td>
      <td>79.56</td>
      <td>34.57</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>95.0</td>
      <td>True</td>
      <td>f23f4cf6cb76402c76e932ead01109191af72a60</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>tuantran1632001/Psyfighter2-Orca2-13B-ties</td>
      <td>63.48</td>
      <td>62.46</td>
      <td>81.74</td>
      <td>60.31</td>
      <td>55.40</td>
      <td>77.27</td>
      <td>43.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>b858fbc15734cc797f1c9e4acb239bfb6c390f08</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>tuantran1632001/Psyfighter2-Orca2-ties</td>
      <td>63.48</td>
      <td>62.46</td>
      <td>81.74</td>
      <td>60.31</td>
      <td>55.40</td>
      <td>77.27</td>
      <td>43.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>e4ab7df425cfa2b2687194837c3b7fba4be7fc74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KaeriJenti/Kaori-34B-v1</td>
      <td>63.47</td>
      <td>64.42</td>
      <td>79.61</td>
      <td>70.24</td>
      <td>53.17</td>
      <td>76.72</td>
      <td>36.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>3a03b2aba751680105e027ef096866320bf9bd2b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AI-B/UTENA-7B-NSFW-V2</td>
      <td>63.45</td>
      <td>63.31</td>
      <td>84.54</td>
      <td>63.97</td>
      <td>47.81</td>
      <td>78.69</td>
      <td>42.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>unlicense</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>2da9543e68e222ca627a22a131772155d5ef9078</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xxyyy123/Mistral7B_adaptor_v1</td>
      <td>63.42</td>
      <td>62.97</td>
      <td>83.81</td>
      <td>63.56</td>
      <td>49.77</td>
      <td>79.16</td>
      <td>41.24</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>23e800094570c22fbaa4279ef7e7f27315ac61af</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-16B-v2.01</td>
      <td>63.42</td>
      <td>65.36</td>
      <td>82.92</td>
      <td>63.27</td>
      <td>64.53</td>
      <td>79.08</td>
      <td>25.32</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>3b723559b550a34e489cc41ec5414e00531ec2ae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>llm-agents/tora-70b-v1.0</td>
      <td>63.39</td>
      <td>67.75</td>
      <td>85.83</td>
      <td>69.22</td>
      <td>51.79</td>
      <td>81.93</td>
      <td>23.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>e95fd7daf017e7c414ec07ebef4ddf013c16f9a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Mini_Synatra_SFT</td>
      <td>63.39</td>
      <td>62.46</td>
      <td>83.44</td>
      <td>61.20</td>
      <td>53.67</td>
      <td>74.66</td>
      <td>44.88</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>fc042f671dc0c94b21a6107eda75a6f9c8d44f2d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xxyyy123/1701221123_Ads_Mistral7B-slimorca_all-Lqv-r4b128</td>
      <td>63.37</td>
      <td>62.88</td>
      <td>83.99</td>
      <td>62.89</td>
      <td>50.55</td>
      <td>79.72</td>
      <td>40.18</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2872cd97f88418d6b07082048b316ea5b996982d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xDAN-AI/xDAN-L1Mix-DeepThinking-v2</td>
      <td>63.36</td>
      <td>62.37</td>
      <td>82.32</td>
      <td>59.69</td>
      <td>55.38</td>
      <td>76.40</td>
      <td>43.97</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>47ca647c3bb26b647b1f66c3672b890803de46c8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>malhajar/Mistral-7B-v0.2-meditron-turkish</td>
      <td>63.34</td>
      <td>59.56</td>
      <td>81.79</td>
      <td>60.35</td>
      <td>66.19</td>
      <td>76.24</td>
      <td>35.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>906025770a885b26f762b13bb0bc726438e525de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeuralNovel/Gecko-7B-v0.1-DPO</td>
      <td>63.22</td>
      <td>56.74</td>
      <td>82.38</td>
      <td>60.42</td>
      <td>57.42</td>
      <td>77.35</td>
      <td>45.03</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7a377ce18d900f287222895973dd866fd53930f7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeuralNovel/Aeryth-7B-v0.1</td>
      <td>63.19</td>
      <td>60.32</td>
      <td>83.53</td>
      <td>60.97</td>
      <td>63.57</td>
      <td>74.66</td>
      <td>36.09</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>b7befcbc2e609356efc76c64cee1b1727727d815</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Deci/DeciLM-7B-instruct</td>
      <td>63.19</td>
      <td>61.01</td>
      <td>82.37</td>
      <td>60.24</td>
      <td>49.75</td>
      <td>79.72</td>
      <td>46.02</td>
      <td>instruction-tuned</td>
      <td>DeciLMForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.04</td>
      <td>93.0</td>
      <td>True</td>
      <td>24a66a701c10e5d70397f9bfc1624447327a0a08</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>liuda1/dm7b_sft_gpt88w_merge</td>
      <td>63.18</td>
      <td>62.29</td>
      <td>82.47</td>
      <td>61.35</td>
      <td>53.33</td>
      <td>77.58</td>
      <td>42.08</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f4f76170f6fe63e832e32d32be1eb4a1da36f402</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KaeriJenti/kaori-34b-v3</td>
      <td>63.18</td>
      <td>64.25</td>
      <td>79.59</td>
      <td>70.18</td>
      <td>52.37</td>
      <td>76.48</td>
      <td>36.24</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>117dab7fc59bff50279100214e39f5551ba0c593</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>01-ai/Yi-34B-Chat</td>
      <td>63.17</td>
      <td>65.10</td>
      <td>84.08</td>
      <td>74.87</td>
      <td>55.41</td>
      <td>79.79</td>
      <td>19.79</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.39</td>
      <td>202.0</td>
      <td>True</td>
      <td>a99ec35331cbfc9da596af7d4538fe2efecff03c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decem/Dionysus-Mistral-m3-v5</td>
      <td>63.14</td>
      <td>59.56</td>
      <td>80.99</td>
      <td>61.18</td>
      <td>50.93</td>
      <td>75.14</td>
      <td>51.02</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7c6a76f284740abd1b262b950aa59d72c65d39e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kiddyz/testllm-c2</td>
      <td>63.13</td>
      <td>60.58</td>
      <td>81.91</td>
      <td>61.20</td>
      <td>49.87</td>
      <td>77.82</td>
      <td>47.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b87c798bc27522824451dfccf5eae50edbd4263b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>KnutJaegersberg/Qwen-14B-Llamafied</td>
      <td>63.09</td>
      <td>55.20</td>
      <td>82.31</td>
      <td>66.11</td>
      <td>45.60</td>
      <td>76.56</td>
      <td>52.77</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>14.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c53e0ea05664c66346627714f332a9b46cde8fd6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-falcon-180b-v12-preview0</td>
      <td>63.06</td>
      <td>62.29</td>
      <td>83.80</td>
      <td>55.92</td>
      <td>53.05</td>
      <td>82.08</td>
      <td>41.24</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td></td>
      <td>180.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4f1aeb136860ee3216f23faec0c598014e5c40a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>maywell/PiVoT-MoE</td>
      <td>63.04</td>
      <td>63.91</td>
      <td>83.52</td>
      <td>60.71</td>
      <td>54.64</td>
      <td>76.32</td>
      <td>39.12</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>36.10</td>
      <td>6.0</td>
      <td>True</td>
      <td>5d1159dd60ec2cc92dbc52508430e620b6adbdaa</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Airoboros-L2-70B-2.1-GPTQ</td>
      <td>63.04</td>
      <td>70.39</td>
      <td>86.54</td>
      <td>68.89</td>
      <td>55.55</td>
      <td>81.61</td>
      <td>15.24</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>llama2</td>
      <td>72.82</td>
      <td>17.0</td>
      <td>True</td>
      <td>23ed580cb77ebaee49ea11eb4538fd3ab3795b76</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azure99/blossom-v3-mistral-7b</td>
      <td>62.95</td>
      <td>60.49</td>
      <td>81.90</td>
      <td>61.35</td>
      <td>50.31</td>
      <td>76.95</td>
      <td>46.70</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>ec6e84a662c801e248d3bb3a19529155de02bda0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/dolphin-2.6-mistral-7b-dpo-orca-v3</td>
      <td>62.93</td>
      <td>66.30</td>
      <td>84.53</td>
      <td>62.36</td>
      <td>61.29</td>
      <td>77.58</td>
      <td>25.55</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>370a644bc9e2e4bfccada10a4bc6648102d94efe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jefferylovely/AthenaImaniMaven</td>
      <td>62.92</td>
      <td>62.80</td>
      <td>84.56</td>
      <td>59.10</td>
      <td>58.50</td>
      <td>77.43</td>
      <td>35.10</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f10a7d6055955eb40424dcac8a76658a11224c86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>teknium/CollectiveCognition-v1.1-Mistral-7B</td>
      <td>62.92</td>
      <td>62.12</td>
      <td>84.17</td>
      <td>62.35</td>
      <td>57.62</td>
      <td>75.37</td>
      <td>35.86</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>69.0</td>
      <td>True</td>
      <td>5f57f70ec99450c70da2540e94dd7fd67be4b23c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/zephyr-7b-sft-full-spin-iter1</td>
      <td>62.86</td>
      <td>65.87</td>
      <td>85.44</td>
      <td>60.95</td>
      <td>57.39</td>
      <td>76.64</td>
      <td>30.86</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9257b6484010acf5eed7e77ff787264b49c1a923</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/zephyr-7b-sft-full-SPIN-iter1</td>
      <td>62.86</td>
      <td>65.87</td>
      <td>85.44</td>
      <td>60.95</td>
      <td>57.39</td>
      <td>76.64</td>
      <td>30.86</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>d8569aea49f28131ca3d319da343da0777ed4161</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/test</td>
      <td>62.86</td>
      <td>65.87</td>
      <td>85.44</td>
      <td>60.95</td>
      <td>57.39</td>
      <td>76.64</td>
      <td>30.86</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>437d2f9c55aec50ebaedce22df8aaa7fcc0f9ff8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jefferylovely/AthenaImaniMaven</td>
      <td>62.85</td>
      <td>62.63</td>
      <td>84.65</td>
      <td>59.05</td>
      <td>58.58</td>
      <td>77.19</td>
      <td>35.03</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f10a7d6055955eb40424dcac8a76658a11224c86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aari1995/germeo-7b-laser</td>
      <td>62.82</td>
      <td>60.75</td>
      <td>82.81</td>
      <td>60.57</td>
      <td>53.83</td>
      <td>75.61</td>
      <td>43.37</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b7577f83a0af27e1a380efce4f993c25c33d8b33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>upaya07/Birbal-7B-V1</td>
      <td>62.82</td>
      <td>62.88</td>
      <td>84.88</td>
      <td>63.71</td>
      <td>45.46</td>
      <td>78.53</td>
      <td>41.47</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6623e1ec77f20f7c152e86e99b49e501d0133b13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/MelangeA-70b</td>
      <td>62.82</td>
      <td>71.25</td>
      <td>87.30</td>
      <td>70.56</td>
      <td>60.61</td>
      <td>81.53</td>
      <td>5.69</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>68.98</td>
      <td>1.0</td>
      <td>False</td>
      <td>d48cf79d1ead50154b1e70120779ae91bc5fafb4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>huggyllama/llama-65b</td>
      <td>62.79</td>
      <td>63.48</td>
      <td>86.09</td>
      <td>63.93</td>
      <td>43.43</td>
      <td>82.56</td>
      <td>37.23</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>65.29</td>
      <td>68.0</td>
      <td>True</td>
      <td>49707c5313d34d1c5a846e29cf2a2a650c22c8ee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/neural-chat-7b-v3-1-dare-0.85</td>
      <td>62.74</td>
      <td>61.95</td>
      <td>83.84</td>
      <td>64.43</td>
      <td>44.90</td>
      <td>79.16</td>
      <td>42.15</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>3c15d3e2a7790e45501e105daed5eb88b665ceef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Faradaylab/ARIA-70B-V3</td>
      <td>62.73</td>
      <td>63.91</td>
      <td>86.21</td>
      <td>64.75</td>
      <td>51.32</td>
      <td>82.08</td>
      <td>28.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>68.98</td>
      <td>0.0</td>
      <td>True</td>
      <td>6e7fdcd20626786dd744ea86c664a3c088ced39f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mlinmg/SG-Raccoon-Yi-200k-2.0</td>
      <td>62.72</td>
      <td>62.54</td>
      <td>80.26</td>
      <td>73.29</td>
      <td>53.21</td>
      <td>76.32</td>
      <td>30.71</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>55.59</td>
      <td>0.0</td>
      <td>True</td>
      <td>986706415fcb2118f35626dbc12e054457ec9ad3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Fredithefish/MadMix-v0.2</td>
      <td>62.72</td>
      <td>64.85</td>
      <td>83.54</td>
      <td>64.02</td>
      <td>55.79</td>
      <td>77.35</td>
      <td>30.78</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>69a3c98c23938a9370c62ae43894eb7723de97dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sr5434/CodegebraGPT-10b</td>
      <td>62.68</td>
      <td>59.81</td>
      <td>83.42</td>
      <td>60.20</td>
      <td>46.57</td>
      <td>80.98</td>
      <td>45.11</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>263f3e4c48d6fb001cd556010ee50a0b6918b8cb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/guanaco-65B-HF</td>
      <td>62.67</td>
      <td>65.44</td>
      <td>86.47</td>
      <td>62.92</td>
      <td>52.81</td>
      <td>82.40</td>
      <td>26.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>65.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>7f83ae526f8b83705ca8434535da8fd8c692f9d0</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-XS-v1-3-yarn-128K</td>
      <td>62.66</td>
      <td>61.09</td>
      <td>82.95</td>
      <td>62.15</td>
      <td>50.13</td>
      <td>74.43</td>
      <td>45.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>0f5977a5d2fa791359dc92eb1574b6112e709cad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Metis-0.5</td>
      <td>62.65</td>
      <td>62.63</td>
      <td>83.77</td>
      <td>62.16</td>
      <td>49.33</td>
      <td>75.14</td>
      <td>42.91</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>87df101c3909e6bc2b22e237d92f74118ab1909c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/llama-2-70b-IA3-guanaco</td>
      <td>62.61</td>
      <td>68.52</td>
      <td>85.67</td>
      <td>67.03</td>
      <td>43.47</td>
      <td>82.24</td>
      <td>28.73</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>70.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>e3230df22d065b6699096494d1151fa337dde9e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>upaya07/Birbal-7B-V1</td>
      <td>62.60</td>
      <td>62.80</td>
      <td>84.83</td>
      <td>63.59</td>
      <td>45.34</td>
      <td>78.77</td>
      <td>40.26</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6623e1ec77f20f7c152e86e99b49e501d0133b13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-2.0</td>
      <td>62.60</td>
      <td>68.60</td>
      <td>87.53</td>
      <td>69.37</td>
      <td>48.52</td>
      <td>83.90</td>
      <td>17.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>other</td>
      <td>70.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>f16526d9bb814dc10adc911f94e8c7a520beb5b6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/VicUnlocked-alpaca-65B-QLoRA-fp16</td>
      <td>62.58</td>
      <td>65.61</td>
      <td>85.15</td>
      <td>63.13</td>
      <td>52.47</td>
      <td>81.29</td>
      <td>27.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>65.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>6cdacfda96970aa144e316b108ab9bc17c99a573</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azure99/blossom-v3_1-mistral-7b</td>
      <td>62.53</td>
      <td>60.49</td>
      <td>81.71</td>
      <td>61.00</td>
      <td>49.51</td>
      <td>75.53</td>
      <td>46.93</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>d84e28c169a93933829e10f314f1e3e674df9843</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sr5434/CodegebraGPT-10b</td>
      <td>62.53</td>
      <td>59.56</td>
      <td>83.45</td>
      <td>60.07</td>
      <td>46.53</td>
      <td>81.06</td>
      <td>44.50</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>10.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>15e64a7f77eba0367eedbaaacb3560351471093b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-XS-v1-3-yarn-128K</td>
      <td>62.49</td>
      <td>61.60</td>
      <td>82.96</td>
      <td>62.10</td>
      <td>50.20</td>
      <td>74.74</td>
      <td>43.37</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>72d393d13f1bd26442e59993c57840b91ff6f6fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jikaixuan/test_merged_model</td>
      <td>62.42</td>
      <td>61.60</td>
      <td>83.10</td>
      <td>63.73</td>
      <td>48.65</td>
      <td>78.45</td>
      <td>38.97</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>611ec6f78292124008a276ce5c2723e53d31a1e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Zardos/Kant-Test-0.1-Mistral-7B</td>
      <td>62.42</td>
      <td>62.37</td>
      <td>82.84</td>
      <td>63.38</td>
      <td>49.62</td>
      <td>78.30</td>
      <td>37.98</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5989100fa82aaab0db2f8ed3e37a446126050ef9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>meta-llama/Llama-2-70b-chat-hf</td>
      <td>62.40</td>
      <td>64.59</td>
      <td>85.88</td>
      <td>63.91</td>
      <td>52.80</td>
      <td>80.51</td>
      <td>26.69</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>68.98</td>
      <td>1848.0</td>
      <td>False</td>
      <td>7f54101c0fbb67a8143ca23eb8bd09b71f269c74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0</td>
      <td>62.37</td>
      <td>63.65</td>
      <td>84.44</td>
      <td>61.01</td>
      <td>50.48</td>
      <td>77.98</td>
      <td>36.69</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>d457f58ca73bd5540dc4e12b70315e4464ea138c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/test0</td>
      <td>62.37</td>
      <td>63.65</td>
      <td>84.44</td>
      <td>61.01</td>
      <td>50.48</td>
      <td>77.98</td>
      <td>36.69</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e90506303f046ebe6da9d8b41489a7365b455a06</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>willyninja30/ARIA-70B-French</td>
      <td>62.37</td>
      <td>64.51</td>
      <td>85.87</td>
      <td>63.88</td>
      <td>52.80</td>
      <td>80.51</td>
      <td>26.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>d8580d360c51e71fddd27897445e2aa9d1888585</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>gagan3012/Multirial</td>
      <td>62.37</td>
      <td>63.23</td>
      <td>79.57</td>
      <td>61.01</td>
      <td>54.70</td>
      <td>75.30</td>
      <td>40.41</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>0bf35a998ce26287916c9d1e0575d5f15e6ae0df</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-1.2</td>
      <td>62.36</td>
      <td>65.87</td>
      <td>86.08</td>
      <td>63.37</td>
      <td>52.72</td>
      <td>79.56</td>
      <td>26.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>50ab86e198e1c82ec81aefc628f23501c101d390</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/zephyr-7b-alpha-dare-0.85</td>
      <td>62.35</td>
      <td>61.18</td>
      <td>83.67</td>
      <td>64.30</td>
      <td>44.41</td>
      <td>78.45</td>
      <td>42.08</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>afe35301593b4ce2e7b5d1696066724ef1f802eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Half-NSFW_Noromaid-7b</td>
      <td>62.32</td>
      <td>62.80</td>
      <td>84.82</td>
      <td>63.76</td>
      <td>46.05</td>
      <td>78.06</td>
      <td>38.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>378e5fb671d593432ce6c7ddc19ac8e04a490df8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0</td>
      <td>62.32</td>
      <td>63.57</td>
      <td>84.43</td>
      <td>61.28</td>
      <td>50.34</td>
      <td>77.98</td>
      <td>36.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>d457f58ca73bd5540dc4e12b70315e4464ea138c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pinkyponky/Mistral-7B-Instruct-sft-tuned-v0.2</td>
      <td>62.29</td>
      <td>58.02</td>
      <td>79.26</td>
      <td>58.78</td>
      <td>50.45</td>
      <td>76.87</td>
      <td>50.34</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>26b1b06ca6ee8db77d915e0ec685b3e999a226d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vmajor/Orca2-13B-selfmerge-26B</td>
      <td>62.24</td>
      <td>60.84</td>
      <td>79.84</td>
      <td>60.32</td>
      <td>56.38</td>
      <td>76.87</td>
      <td>39.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>ms-pl</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>46cdde5be7e3c48ada1bd3143ad593eecfb641e7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vmajor/Orca2-13B-selfmerge-39B</td>
      <td>62.24</td>
      <td>60.84</td>
      <td>79.84</td>
      <td>60.32</td>
      <td>56.38</td>
      <td>76.87</td>
      <td>39.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>ms-pl</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>7a9e6775716a3947d0e40842b5d61753bc0551ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-7B-v1.4</td>
      <td>62.19</td>
      <td>60.41</td>
      <td>82.87</td>
      <td>60.98</td>
      <td>51.88</td>
      <td>74.82</td>
      <td>42.15</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>53a5249ee9e5b2327de81f09c26a4577dea9260b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/Orca-2-13b-f16</td>
      <td>62.14</td>
      <td>60.67</td>
      <td>79.81</td>
      <td>60.37</td>
      <td>56.41</td>
      <td>76.64</td>
      <td>38.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b29c52ea0757c460e83592e55ea89e016cef3549</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenLemur/lemur-70b-v1</td>
      <td>62.07</td>
      <td>64.33</td>
      <td>85.72</td>
      <td>65.85</td>
      <td>44.78</td>
      <td>83.03</td>
      <td>28.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>44.0</td>
      <td>True</td>
      <td>74432ae16ef50207fe17fb88b2f1c1d32ef3b481</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b-dare-0.85</td>
      <td>62.06</td>
      <td>61.69</td>
      <td>83.85</td>
      <td>64.43</td>
      <td>43.13</td>
      <td>78.93</td>
      <td>40.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>7a3def1c382793d2b12741896302c31a471b6d1d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Aeala/Alpaca-elina-65b</td>
      <td>62.03</td>
      <td>65.27</td>
      <td>85.75</td>
      <td>63.42</td>
      <td>47.32</td>
      <td>81.37</td>
      <td>29.04</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>65.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>51ce30a69b3c3363c8cfcd6395bf1df974ba2977</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-200K-Q</td>
      <td>62.00</td>
      <td>63.91</td>
      <td>83.52</td>
      <td>75.19</td>
      <td>44.21</td>
      <td>81.06</td>
      <td>24.11</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.39</td>
      <td>1.0</td>
      <td>True</td>
      <td>0f58c270f8f3b82523799dcfd7080b857850bd77</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>athirdpath/Iambe-20b-DARE-v2</td>
      <td>61.99</td>
      <td>62.80</td>
      <td>84.53</td>
      <td>60.45</td>
      <td>53.85</td>
      <td>77.03</td>
      <td>33.28</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>6.0</td>
      <td>True</td>
      <td>02bd8edd30a5ddd1eede94c19a6ae160842a2f9f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abhishek/zephyr-beta-math</td>
      <td>61.99</td>
      <td>56.66</td>
      <td>81.26</td>
      <td>57.24</td>
      <td>44.83</td>
      <td>75.53</td>
      <td>56.41</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>dd3d070a104d8b36ba98d14a485d88fa95aaab63</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-7B-v3.0</td>
      <td>61.99</td>
      <td>62.46</td>
      <td>83.79</td>
      <td>63.90</td>
      <td>43.85</td>
      <td>77.90</td>
      <td>40.03</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>17.0</td>
      <td>True</td>
      <td>93c2e8b8055b42779f2b68059ebe38af6f2789c4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/Orca-2-13b</td>
      <td>61.98</td>
      <td>60.92</td>
      <td>79.85</td>
      <td>60.30</td>
      <td>56.42</td>
      <td>76.56</td>
      <td>37.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>597.0</td>
      <td>True</td>
      <td>2539ff53e6baa4cc603774ad5a2d646f4041ea4e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/MelangeC-70b</td>
      <td>61.96</td>
      <td>71.67</td>
      <td>87.60</td>
      <td>70.37</td>
      <td>58.13</td>
      <td>83.98</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>68.98</td>
      <td>0.0</td>
      <td>False</td>
      <td>e54a2b924dec135f3fa2373933ab8485178cde1b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HuggingFaceH4/zephyr-7b-beta</td>
      <td>61.95</td>
      <td>62.03</td>
      <td>84.36</td>
      <td>61.07</td>
      <td>57.45</td>
      <td>77.74</td>
      <td>29.04</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1187.0</td>
      <td>True</td>
      <td>8af01af3d4f9dc9b962447180d6d0f8c5315da86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Faradaylab/ARIA-70B-V2</td>
      <td>61.93</td>
      <td>62.12</td>
      <td>85.68</td>
      <td>63.49</td>
      <td>49.80</td>
      <td>81.69</td>
      <td>28.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>2bf026af438d522268533484a85a3e54178e7809</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/dolphin-2.6-mistral-7b-dpo-orca</td>
      <td>61.92</td>
      <td>66.04</td>
      <td>84.62</td>
      <td>62.28</td>
      <td>59.97</td>
      <td>78.30</td>
      <td>20.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>19c3ad67276aa90341e46e8b0b72e6bf79984153</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/dolphin-2.6-mistral-7b-dpo-orca-v1</td>
      <td>61.92</td>
      <td>66.04</td>
      <td>84.62</td>
      <td>62.28</td>
      <td>59.97</td>
      <td>78.30</td>
      <td>20.32</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>3b711027ce55f180f050729f08fe7060e4834e87</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>spmurrayzzz/Mistral-Syndicate-7B</td>
      <td>61.90</td>
      <td>60.84</td>
      <td>82.91</td>
      <td>60.83</td>
      <td>43.71</td>
      <td>78.61</td>
      <td>44.50</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d95d34db5d0aa50fd3b3594d1632c6ce69937243</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>proto-llm/uniwiz-7B-v0.1</td>
      <td>61.87</td>
      <td>61.77</td>
      <td>84.16</td>
      <td>64.16</td>
      <td>44.96</td>
      <td>78.85</td>
      <td>37.30</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5ad4b3b5b2648cf841b39fbe8254a1c1fee832f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Winterreise-m7</td>
      <td>61.86</td>
      <td>61.26</td>
      <td>83.84</td>
      <td>63.85</td>
      <td>45.55</td>
      <td>79.08</td>
      <td>37.60</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>418129599bdd914f275a44ce9ce5a111c5917b3c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-7b-v0.2</td>
      <td>61.86</td>
      <td>62.12</td>
      <td>84.92</td>
      <td>63.10</td>
      <td>46.09</td>
      <td>78.22</td>
      <td>36.69</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>bc35358ec19cf0335642228538a83bb306c0e074</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/bagel-8x7b-v0.2</td>
      <td>61.83</td>
      <td>68.26</td>
      <td>86.32</td>
      <td>70.40</td>
      <td>60.03</td>
      <td>81.29</td>
      <td>4.70</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>14.0</td>
      <td>True</td>
      <td>614649ce0bd9a03fd24963de70655e5f8d4354b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dfurman/Mistral-7B-Instruct-v0.2</td>
      <td>61.79</td>
      <td>60.15</td>
      <td>82.79</td>
      <td>60.07</td>
      <td>56.06</td>
      <td>76.87</td>
      <td>34.80</td>
      <td>instruction-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>322faff8bb0c72b772762de7635f5aea9864a24a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-7b-v0.2</td>
      <td>61.78</td>
      <td>62.03</td>
      <td>84.97</td>
      <td>62.99</td>
      <td>46.07</td>
      <td>78.37</td>
      <td>36.24</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>bc35358ec19cf0335642228538a83bb306c0e074</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jikaixuan/test_model</td>
      <td>61.76</td>
      <td>62.29</td>
      <td>84.42</td>
      <td>61.07</td>
      <td>57.51</td>
      <td>78.06</td>
      <td>27.22</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2e9d6998ce40ffb43ba1d8636a84bf38bf922892</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jikaixuan/test</td>
      <td>61.76</td>
      <td>62.29</td>
      <td>84.42</td>
      <td>61.07</td>
      <td>57.51</td>
      <td>78.06</td>
      <td>27.22</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e63792701d6136288b95c9c8f24c0030ff5698b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>spmurrayzzz/Mistral-Syndicate-7B</td>
      <td>61.74</td>
      <td>60.84</td>
      <td>82.88</td>
      <td>60.52</td>
      <td>43.73</td>
      <td>78.45</td>
      <td>44.05</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d95d34db5d0aa50fd3b3594d1632c6ce69937243</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>huangyt/Mistral-7B-v0.1-Open-Platypus_2.5w-r16-gate_up_down</td>
      <td>61.71</td>
      <td>61.26</td>
      <td>83.19</td>
      <td>63.87</td>
      <td>45.44</td>
      <td>77.35</td>
      <td>39.12</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>77f7bf749a6c4561b5364b291152b54ba19a59fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Gryphe/MythoMist-7b</td>
      <td>61.67</td>
      <td>65.87</td>
      <td>83.55</td>
      <td>62.32</td>
      <td>59.98</td>
      <td>78.06</td>
      <td>20.24</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.24</td>
      <td>29.0</td>
      <td>True</td>
      <td>3b6c71416d191ab161fd3043117304a10df99716</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NickyNicky/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v2</td>
      <td>61.65</td>
      <td>60.49</td>
      <td>82.07</td>
      <td>62.34</td>
      <td>46.38</td>
      <td>78.45</td>
      <td>40.18</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>f01f41dc7c987ad6668931159feaa4469f7dcf3f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hiyouga/Qwen-14B-Chat-LLaMAfied</td>
      <td>61.60</td>
      <td>57.51</td>
      <td>82.11</td>
      <td>65.57</td>
      <td>51.99</td>
      <td>72.93</td>
      <td>39.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>14.17</td>
      <td>5.0</td>
      <td>True</td>
      <td>29e92e74dca4a79aa8c2c451287ff97c4dccb323</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Llamix2-MLewd-4x13B</td>
      <td>61.60</td>
      <td>61.01</td>
      <td>83.17</td>
      <td>56.32</td>
      <td>50.35</td>
      <td>75.37</td>
      <td>43.37</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>38.50</td>
      <td>34.0</td>
      <td>True</td>
      <td>19961590ae95ccd9316b13c66098cd61b28a7d5a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-1</td>
      <td>61.59</td>
      <td>66.21</td>
      <td>83.64</td>
      <td>62.37</td>
      <td>59.65</td>
      <td>78.14</td>
      <td>19.56</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>505.0</td>
      <td>True</td>
      <td>3995e9a13d54ce95f0ad55de2eaa92e2dc580174</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-1</td>
      <td>61.59</td>
      <td>65.70</td>
      <td>83.54</td>
      <td>62.12</td>
      <td>59.48</td>
      <td>78.61</td>
      <td>20.09</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>505.0</td>
      <td>True</td>
      <td>af2489cde09e9d2c175622f651875e83824c4b10</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>athirdpath/NSFW_DPO_Noromaid-7b</td>
      <td>61.59</td>
      <td>62.63</td>
      <td>84.50</td>
      <td>63.34</td>
      <td>44.99</td>
      <td>78.22</td>
      <td>35.86</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>51b4408a40736e18f69d932cb403811558428378</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HuggingFaceH4/zephyr-7b-beta</td>
      <td>61.59</td>
      <td>62.46</td>
      <td>84.35</td>
      <td>60.70</td>
      <td>57.83</td>
      <td>77.11</td>
      <td>27.07</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1187.0</td>
      <td>True</td>
      <td>0f17b36adfbe7d86ea1c591a9efeeae17b313f48</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Deci/DeciLM-7B</td>
      <td>61.55</td>
      <td>59.39</td>
      <td>82.51</td>
      <td>59.76</td>
      <td>40.33</td>
      <td>79.95</td>
      <td>47.38</td>
      <td>pretrained</td>
      <td>DeciLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.04</td>
      <td>201.0</td>
      <td>True</td>
      <td>b943e32a12bc21df2b8b3c50525c6646acd442bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>tianlinliu0121/zephyr-7b-dpo-full-beta-0.2</td>
      <td>61.55</td>
      <td>61.77</td>
      <td>84.04</td>
      <td>61.79</td>
      <td>54.72</td>
      <td>76.95</td>
      <td>30.02</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>727b63fc1ca6a592072159a7185c22f74cd38480</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-1</td>
      <td>61.54</td>
      <td>66.30</td>
      <td>83.60</td>
      <td>62.44</td>
      <td>59.54</td>
      <td>77.98</td>
      <td>19.41</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>505.0</td>
      <td>True</td>
      <td>3995e9a13d54ce95f0ad55de2eaa92e2dc580174</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>teknium/OpenHermes-2.5-Mistral-7B</td>
      <td>61.52</td>
      <td>64.93</td>
      <td>84.18</td>
      <td>63.64</td>
      <td>52.24</td>
      <td>78.06</td>
      <td>26.08</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>543.0</td>
      <td>True</td>
      <td>2a54cad766bc90828354db5c4199795aecfd0df1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/Mistral-7B-OpenOrca-lora-merged</td>
      <td>61.52</td>
      <td>61.77</td>
      <td>83.61</td>
      <td>64.34</td>
      <td>42.70</td>
      <td>78.53</td>
      <td>38.13</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>8a8e4763c3edd0a8e5bb02e4bc865c69a658b428</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-7b-v0.1.1</td>
      <td>61.49</td>
      <td>62.20</td>
      <td>84.28</td>
      <td>63.44</td>
      <td>44.30</td>
      <td>77.90</td>
      <td>36.85</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>349a2eb5c61e3e13c2b39d15c7b94f5c31ab6bd5</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/robin-65b-v2-fp16</td>
      <td>61.48</td>
      <td>61.95</td>
      <td>84.60</td>
      <td>62.51</td>
      <td>52.31</td>
      <td>80.51</td>
      <td>26.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>65.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>40edb31ba93045d673735361bc98f56125bbc77b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>walebadr/Mistral-7B-v0.1-DPO</td>
      <td>61.47</td>
      <td>61.26</td>
      <td>83.94</td>
      <td>63.76</td>
      <td>42.68</td>
      <td>78.77</td>
      <td>38.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Delta</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>bab460c2c68fca377bcc778031d51340104e2dc1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>teknium/OpenHermes-2.5-Mistral-7B</td>
      <td>61.45</td>
      <td>64.93</td>
      <td>84.30</td>
      <td>63.82</td>
      <td>52.31</td>
      <td>77.90</td>
      <td>25.47</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>543.0</td>
      <td>True</td>
      <td>2a54cad766bc90828354db5c4199795aecfd0df1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>osanseviero/mistral-instruct-moe-experimental</td>
      <td>61.39</td>
      <td>61.01</td>
      <td>81.55</td>
      <td>58.22</td>
      <td>60.40</td>
      <td>76.09</td>
      <td>31.08</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.88</td>
      <td>2.0</td>
      <td>True</td>
      <td>e926f4f97f89c54806547df1b65cb1e6f0c6b26e</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>smelborp/MixtralOrochi8x7B-Alt</td>
      <td>61.38</td>
      <td>67.92</td>
      <td>86.25</td>
      <td>70.06</td>
      <td>64.03</td>
      <td>80.03</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>2cbe1e99144674ff0570a6a38b75c4666ed16087</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-llama2-13b</td>
      <td>61.36</td>
      <td>62.03</td>
      <td>81.82</td>
      <td>58.69</td>
      <td>55.66</td>
      <td>76.01</td>
      <td>33.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>c6362c4fc0dc03420e3c08454b2e7689e4e32d3a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>tianlinliu0121/zephyr-7b-dpo-full-beta-0.2</td>
      <td>61.36</td>
      <td>61.86</td>
      <td>83.98</td>
      <td>61.85</td>
      <td>54.78</td>
      <td>76.95</td>
      <td>28.73</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>727b63fc1ca6a592072159a7185c22f74cd38480</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Metis-0.3-merged</td>
      <td>61.34</td>
      <td>62.20</td>
      <td>84.00</td>
      <td>62.65</td>
      <td>59.24</td>
      <td>78.14</td>
      <td>21.83</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>dbcf2c1f7cbea0bacd756f7d8251b5bb037e28d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Metis-0.4</td>
      <td>61.34</td>
      <td>62.20</td>
      <td>84.00</td>
      <td>62.65</td>
      <td>59.24</td>
      <td>78.14</td>
      <td>21.83</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>c2b149c7df2806add971b2c2ec27288abc18f312</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/alpaca-lora-65B-HF</td>
      <td>61.33</td>
      <td>64.85</td>
      <td>85.59</td>
      <td>63.11</td>
      <td>45.15</td>
      <td>81.22</td>
      <td>28.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>65.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>113b61b37a2862b950ada68620e57acafbcefe13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>microsoft/phi-2</td>
      <td>61.33</td>
      <td>61.09</td>
      <td>75.11</td>
      <td>58.11</td>
      <td>44.47</td>
      <td>74.35</td>
      <td>54.81</td>
      <td>pretrained</td>
      <td>PhiForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>2.78</td>
      <td>2360.0</td>
      <td>True</td>
      <td>d3186761bf5c4409f7679359284066c25ab668ee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>walebadr/Mistral-7B-v0.1-DPO</td>
      <td>61.30</td>
      <td>60.32</td>
      <td>83.69</td>
      <td>64.01</td>
      <td>43.53</td>
      <td>79.01</td>
      <td>37.23</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e1fa6fa7e272027d648c92873c06a42064b483ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Deacon-20B</td>
      <td>61.28</td>
      <td>60.75</td>
      <td>81.74</td>
      <td>60.70</td>
      <td>58.49</td>
      <td>76.80</td>
      <td>29.19</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>20.09</td>
      <td>0.0</td>
      <td>True</td>
      <td>dabbb1675c4bfe6fed3fd8fecc7f2d887e697fa7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Metis-0.4</td>
      <td>61.28</td>
      <td>62.29</td>
      <td>83.91</td>
      <td>62.70</td>
      <td>59.20</td>
      <td>77.35</td>
      <td>22.21</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>c2b149c7df2806add971b2c2ec27288abc18f312</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NickyNicky/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v3</td>
      <td>61.26</td>
      <td>60.58</td>
      <td>83.34</td>
      <td>61.53</td>
      <td>48.21</td>
      <td>77.74</td>
      <td>36.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>43abfcab8bf532a2601ed6e61e0c3614272b7df9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardLM-70B-V1.0</td>
      <td>61.25</td>
      <td>65.44</td>
      <td>84.41</td>
      <td>64.05</td>
      <td>54.81</td>
      <td>80.82</td>
      <td>17.97</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>199.0</td>
      <td>True</td>
      <td>6dae38060d70b82dcfe787a612d04aaf0adf0738</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_3.5</td>
      <td>61.24</td>
      <td>63.91</td>
      <td>84.79</td>
      <td>64.94</td>
      <td>46.38</td>
      <td>80.58</td>
      <td>26.84</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>1008.0</td>
      <td>True</td>
      <td>5b874a33a91d63023055e6cb2d5d86afe883b4ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Minirecord/Mini_DPO_test02</td>
      <td>61.23</td>
      <td>59.73</td>
      <td>83.89</td>
      <td>61.90</td>
      <td>48.47</td>
      <td>78.37</td>
      <td>35.03</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>cd417467644c4178100083e342bad88a3f968be6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_3.5</td>
      <td>61.22</td>
      <td>63.82</td>
      <td>84.80</td>
      <td>64.98</td>
      <td>46.39</td>
      <td>80.74</td>
      <td>26.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>1008.0</td>
      <td>True</td>
      <td>5b874a33a91d63023055e6cb2d5d86afe883b4ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-2.0</td>
      <td>61.20</td>
      <td>66.64</td>
      <td>86.66</td>
      <td>63.18</td>
      <td>49.11</td>
      <td>80.74</td>
      <td>20.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>huggingface/llama-65b</td>
      <td>61.19</td>
      <td>63.48</td>
      <td>86.09</td>
      <td>63.93</td>
      <td>43.43</td>
      <td>82.56</td>
      <td>27.67</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>65.29</td>
      <td>0.0</td>
      <td>False</td>
      <td>4ae2e56610e8b9b9a78472708390668e9096b4f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/mistral-7b-slimorcaboros</td>
      <td>61.18</td>
      <td>63.65</td>
      <td>83.70</td>
      <td>63.46</td>
      <td>55.81</td>
      <td>77.03</td>
      <td>23.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>c06e1a6b6c0fe764117f9ec7611ce31e796e602a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/jackalope-7b</td>
      <td>61.16</td>
      <td>63.40</td>
      <td>83.29</td>
      <td>63.50</td>
      <td>50.06</td>
      <td>78.06</td>
      <td>28.66</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>29.0</td>
      <td>True</td>
      <td>5ba23522319a51d0af23b336a6a83c72ae3780e7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mrm8488/mistral-7b-ft-h4-no_robots_instructions</td>
      <td>61.16</td>
      <td>60.92</td>
      <td>83.17</td>
      <td>63.37</td>
      <td>43.63</td>
      <td>78.85</td>
      <td>37.00</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>13.0</td>
      <td>True</td>
      <td>785446da9a53ceae48795069bf7ccaf46a91a5ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mrm8488/mistral-7b-ft-h4-no_robots_instructions</td>
      <td>61.16</td>
      <td>60.92</td>
      <td>83.24</td>
      <td>63.74</td>
      <td>43.64</td>
      <td>78.69</td>
      <td>36.69</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>13.0</td>
      <td>True</td>
      <td>785446da9a53ceae48795069bf7ccaf46a91a5ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/PlatYi-34B-Llama-Q-v3</td>
      <td>61.15</td>
      <td>64.33</td>
      <td>84.88</td>
      <td>74.98</td>
      <td>51.80</td>
      <td>84.21</td>
      <td>6.67</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>34.39</td>
      <td>2.0</td>
      <td>True</td>
      <td>2d04b9e3a6c86a718c33e0686c0b5f4e46feb364</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-2.0</td>
      <td>61.14</td>
      <td>66.81</td>
      <td>86.66</td>
      <td>63.41</td>
      <td>49.17</td>
      <td>80.27</td>
      <td>20.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/dolphin-2.1-mistral-7b</td>
      <td>61.12</td>
      <td>64.42</td>
      <td>84.92</td>
      <td>63.32</td>
      <td>55.56</td>
      <td>77.74</td>
      <td>20.77</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa5bd48c8b3040d1155a8fd59328df160aa63680</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Zardos/Kant-Test-0.1-Mistral-7B</td>
      <td>61.10</td>
      <td>61.77</td>
      <td>82.89</td>
      <td>62.86</td>
      <td>49.40</td>
      <td>78.53</td>
      <td>31.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5989100fa82aaab0db2f8ed3e37a446126050ef9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pinkyponky/Mistral-7B-Instruct-Sft-Tuned-V0.2</td>
      <td>61.08</td>
      <td>57.34</td>
      <td>78.95</td>
      <td>57.90</td>
      <td>50.66</td>
      <td>76.16</td>
      <td>45.49</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>826783eb0e7f2fc471ab9dfeea59acd112a6ecc3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Dans-DiscountModels/Dans-07YahooAnswers-7b</td>
      <td>61.07</td>
      <td>61.52</td>
      <td>83.69</td>
      <td>63.52</td>
      <td>41.84</td>
      <td>78.53</td>
      <td>37.30</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>a9d5e333dd7752b689b97bc7e0cfbd530536a06e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/dolphin-2.1-mistral-7b</td>
      <td>61.00</td>
      <td>63.99</td>
      <td>85.00</td>
      <td>63.44</td>
      <td>55.57</td>
      <td>77.90</td>
      <td>20.09</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa5bd48c8b3040d1155a8fd59328df160aa63680</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Cartinoe5930/Llama2_init_Mistral</td>
      <td>60.98</td>
      <td>60.07</td>
      <td>83.30</td>
      <td>64.09</td>
      <td>42.15</td>
      <td>78.37</td>
      <td>37.91</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e6d5223e089c417e29f56c5750a91e26e8fd5e01</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>mistralai/Mistral-7B-v0.1</td>
      <td>60.97</td>
      <td>59.98</td>
      <td>83.31</td>
      <td>64.16</td>
      <td>42.15</td>
      <td>78.37</td>
      <td>37.83</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2588.0</td>
      <td>True</td>
      <td>e836d8f71b5812f9fee65618453dc537c66bd82a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mihaiii/Pallas-0.5-frankenmerge</td>
      <td>60.95</td>
      <td>61.77</td>
      <td>80.36</td>
      <td>67.62</td>
      <td>54.07</td>
      <td>77.74</td>
      <td>24.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>36.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>b72731a305b62fd9fbcd7c1e99e18d6530600ca9</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-falcon-40b-v16.1-4k</td>
      <td>60.94</td>
      <td>60.58</td>
      <td>83.86</td>
      <td>56.05</td>
      <td>50.57</td>
      <td>77.82</td>
      <td>36.77</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>41.35</td>
      <td>1.0</td>
      <td>True</td>
      <td>4531abf8028eea1e94ad33697ff25cc53a6b10c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Reverb/Mistral-7B-LoreWeaver</td>
      <td>60.93</td>
      <td>59.98</td>
      <td>83.29</td>
      <td>64.12</td>
      <td>42.15</td>
      <td>78.37</td>
      <td>37.68</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>1e1796b7230cd5ba6146d748a90db15493465f22</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-mistral-moloras-7b</td>
      <td>60.93</td>
      <td>59.98</td>
      <td>83.29</td>
      <td>64.12</td>
      <td>42.15</td>
      <td>78.37</td>
      <td>37.68</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>7ef22bee2557aab8a29331653965b3fca22c9a97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CallComply/openchat-3.5-0106-11b</td>
      <td>60.91</td>
      <td>63.65</td>
      <td>78.64</td>
      <td>62.54</td>
      <td>48.07</td>
      <td>78.06</td>
      <td>34.50</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ea960b3343ec36e7f130d45d140fe192acf344b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>upstage/llama-30b-instruct-2048</td>
      <td>60.91</td>
      <td>64.93</td>
      <td>84.94</td>
      <td>61.90</td>
      <td>56.30</td>
      <td>79.56</td>
      <td>17.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>30.00</td>
      <td>103.0</td>
      <td>False</td>
      <td>be44a37814a20e790063086703f570732597887a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HiTZ/alpaca-lora-65b-en-pt-es-ca</td>
      <td>60.89</td>
      <td>65.02</td>
      <td>84.88</td>
      <td>62.19</td>
      <td>46.06</td>
      <td>80.51</td>
      <td>26.69</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa5bd88bd132925cf2dd5c44eceafdb5ed5e5be4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/SlimOpenOrca-Mistral-7B</td>
      <td>60.84</td>
      <td>62.97</td>
      <td>83.49</td>
      <td>62.30</td>
      <td>57.39</td>
      <td>77.43</td>
      <td>21.46</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>17.0</td>
      <td>True</td>
      <td>b0134a7512444dfbb60a2e2d81469a5bbbb18026</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>CalderaAI/30B-Epsilon</td>
      <td>60.80</td>
      <td>63.05</td>
      <td>83.59</td>
      <td>56.89</td>
      <td>59.03</td>
      <td>77.66</td>
      <td>24.56</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>30.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>6962638c2b0368ad496af6e20e46e3de97a7772b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b</td>
      <td>60.79</td>
      <td>64.33</td>
      <td>84.40</td>
      <td>63.72</td>
      <td>52.52</td>
      <td>78.37</td>
      <td>21.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>11.0</td>
      <td>True</td>
      <td>d4039b40e842df7f6b8de50532444c8944ea5791</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-m2.0</td>
      <td>60.79</td>
      <td>65.02</td>
      <td>86.35</td>
      <td>64.37</td>
      <td>46.66</td>
      <td>80.19</td>
      <td>22.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>fa081d52619b35d7016fb40ce855187d6a8e7e4c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16</td>
      <td>60.76</td>
      <td>62.37</td>
      <td>82.29</td>
      <td>58.18</td>
      <td>52.60</td>
      <td>77.51</td>
      <td>31.61</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.35</td>
      <td>0.0</td>
      <td>True</td>
      <td>85f7ad9d6ff016312262a47d45ffd07dee54aab0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-mistral-six-in-one-7b</td>
      <td>60.76</td>
      <td>62.97</td>
      <td>84.60</td>
      <td>63.29</td>
      <td>57.77</td>
      <td>77.51</td>
      <td>18.42</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>41e912e0f79094a80687f88ca5555f84aa9d307f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MetaIX/GPT4-X-Alpasta-30b</td>
      <td>60.76</td>
      <td>63.05</td>
      <td>83.56</td>
      <td>57.71</td>
      <td>51.52</td>
      <td>78.22</td>
      <td>30.48</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>30.00</td>
      <td>65.0</td>
      <td>False</td>
      <td>1a0d1d72a40946463fb4a9780207da19bfecc38b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf</td>
      <td>60.74</td>
      <td>61.35</td>
      <td>83.80</td>
      <td>57.89</td>
      <td>51.18</td>
      <td>78.77</td>
      <td>31.46</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>e04207847429af03c4780f5ac85c726536217981</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Venomia-m7</td>
      <td>60.74</td>
      <td>63.14</td>
      <td>84.00</td>
      <td>60.06</td>
      <td>49.08</td>
      <td>75.77</td>
      <td>32.37</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>46d997c522776af0236b254bd4c5f071b39a06a0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16</td>
      <td>60.71</td>
      <td>62.46</td>
      <td>82.30</td>
      <td>58.15</td>
      <td>52.57</td>
      <td>77.82</td>
      <td>30.93</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>32.35</td>
      <td>0.0</td>
      <td>True</td>
      <td>85f7ad9d6ff016312262a47d45ffd07dee54aab0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-m2.0</td>
      <td>60.68</td>
      <td>65.10</td>
      <td>86.34</td>
      <td>64.32</td>
      <td>46.63</td>
      <td>80.11</td>
      <td>21.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>fa081d52619b35d7016fb40ce855187d6a8e7e4c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>jondurbin/airoboros-65b-gpt4-1.4</td>
      <td>60.67</td>
      <td>65.78</td>
      <td>85.83</td>
      <td>62.27</td>
      <td>52.45</td>
      <td>79.64</td>
      <td>18.04</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>ae256799615c16443f9c423c653ed9f60577e99e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-1.4-peft</td>
      <td>60.67</td>
      <td>65.78</td>
      <td>85.83</td>
      <td>62.27</td>
      <td>52.45</td>
      <td>79.64</td>
      <td>18.04</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>85ae3b595c6b8415df87000c22bc14ea18c174f5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TeeZee/2xbagel-dpo-34b-v0.2</td>
      <td>60.66</td>
      <td>65.27</td>
      <td>79.35</td>
      <td>73.64</td>
      <td>67.15</td>
      <td>76.40</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>56.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>9d7e28d41f1f3221d5fefc48ed495eb921ad4be6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Zephyrus-L1-33B</td>
      <td>60.61</td>
      <td>64.51</td>
      <td>84.15</td>
      <td>57.37</td>
      <td>53.87</td>
      <td>80.19</td>
      <td>23.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>32.53</td>
      <td>4.0</td>
      <td>True</td>
      <td>679aae34440d576456b283070371b2a15dbb948b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-1.4</td>
      <td>60.59</td>
      <td>65.53</td>
      <td>85.77</td>
      <td>61.95</td>
      <td>52.43</td>
      <td>79.79</td>
      <td>18.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>ae256799615c16443f9c423c653ed9f60577e99e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Walmart-the-bag/Influxient-4x13B</td>
      <td>60.57</td>
      <td>61.26</td>
      <td>83.42</td>
      <td>57.25</td>
      <td>54.10</td>
      <td>74.35</td>
      <td>33.06</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-3.0</td>
      <td>38.50</td>
      <td>0.0</td>
      <td>True</td>
      <td>a06acd48979617eb1af25ede71b937767889218b</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Synatra-7B-v0.3-dpo</td>
      <td>60.55</td>
      <td>62.80</td>
      <td>82.58</td>
      <td>61.46</td>
      <td>56.46</td>
      <td>76.24</td>
      <td>23.73</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>405a4f1e6513cd1b8de5eb4e003bb49cc86d1f8a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/dolphin-2.2.1-mistral-7b</td>
      <td>60.54</td>
      <td>63.48</td>
      <td>83.86</td>
      <td>63.28</td>
      <td>53.17</td>
      <td>78.37</td>
      <td>21.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>001b48e9aebffb395c698af47b6b48364cc3cbe8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/kalomaze-stuff</td>
      <td>60.53</td>
      <td>59.64</td>
      <td>83.55</td>
      <td>63.41</td>
      <td>41.64</td>
      <td>78.61</td>
      <td>36.32</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7752f615d76e515aa956335ba8d2705c2cbc297b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Mistral-11B-TestBench9</td>
      <td>60.52</td>
      <td>64.08</td>
      <td>84.24</td>
      <td>64.00</td>
      <td>56.19</td>
      <td>78.45</td>
      <td>16.15</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>4ff48527af8c3907129c06160c7f7b7b786a5a79</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/WizardLM-70B-V1.0-GPTQ</td>
      <td>60.50</td>
      <td>63.82</td>
      <td>83.85</td>
      <td>63.68</td>
      <td>54.54</td>
      <td>78.61</td>
      <td>18.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>llama2</td>
      <td>72.82</td>
      <td>35.0</td>
      <td>True</td>
      <td>c234d7c9c0fd26efb55757fdbfb604d549539fe0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Dolphin2.1-OpenOrca-7B</td>
      <td>60.47</td>
      <td>63.91</td>
      <td>84.26</td>
      <td>62.66</td>
      <td>53.84</td>
      <td>78.22</td>
      <td>19.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>076c0f7de93307e8fb3ad3bd820fb5f73325ca70</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit</td>
      <td>60.45</td>
      <td>61.52</td>
      <td>83.50</td>
      <td>57.43</td>
      <td>50.70</td>
      <td>79.08</td>
      <td>30.48</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>33.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>62f92ddab8b37eaeda15cf5ecb5605141a0525eb</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/Instruct_Mistral-7B-v0.1_Dolly15K</td>
      <td>60.45</td>
      <td>59.39</td>
      <td>82.62</td>
      <td>62.71</td>
      <td>43.56</td>
      <td>79.32</td>
      <td>35.10</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c1d04418a3f404a9500c8292ec912e2b00694f45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aeala/GPT4-x-AlpacaDente-30b</td>
      <td>60.43</td>
      <td>62.12</td>
      <td>82.78</td>
      <td>56.19</td>
      <td>52.68</td>
      <td>78.69</td>
      <td>30.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>ee76c821f861f0ab0276f9f429dd06565f1f2051</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardMath-70B-V1.0</td>
      <td>60.42</td>
      <td>68.17</td>
      <td>86.49</td>
      <td>68.89</td>
      <td>52.69</td>
      <td>82.32</td>
      <td>3.94</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>109.0</td>
      <td>True</td>
      <td>e85b43e53c5379e35393b970c66d76c2d1060381</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardMath-70B-V1.0</td>
      <td>60.41</td>
      <td>67.92</td>
      <td>86.46</td>
      <td>68.92</td>
      <td>52.77</td>
      <td>82.32</td>
      <td>4.09</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>109.0</td>
      <td>True</td>
      <td>e85b43e53c5379e35393b970c66d76c2d1060381</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/SlimOrca-13B</td>
      <td>60.39</td>
      <td>60.15</td>
      <td>81.40</td>
      <td>57.04</td>
      <td>49.37</td>
      <td>74.43</td>
      <td>39.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>13.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>75427e93dc99a5e1d8b9aefa106ad36fc750b744</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>speechlessai/speechless-mistral-7b-dare-0.85</td>
      <td>60.39</td>
      <td>63.31</td>
      <td>84.93</td>
      <td>64.22</td>
      <td>50.68</td>
      <td>79.32</td>
      <td>19.86</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5eefd1b560cd65aec2f689880476f909b46d306c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/Dolphin2.1-OpenOrca-7B</td>
      <td>60.38</td>
      <td>64.16</td>
      <td>84.25</td>
      <td>62.70</td>
      <td>53.83</td>
      <td>77.66</td>
      <td>19.71</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>076c0f7de93307e8fb3ad3bd820fb5f73325ca70</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/Mistral-7B-SlimOrca</td>
      <td>60.37</td>
      <td>62.54</td>
      <td>83.86</td>
      <td>62.77</td>
      <td>54.23</td>
      <td>77.43</td>
      <td>21.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>25.0</td>
      <td>True</td>
      <td>a9744d8cf9ce4230678a891bcf8bba7cbc0aaece</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>EmbeddedLLM/Mistral-7B-Merge-14-v0.3-ft-step-9984</td>
      <td>60.37</td>
      <td>62.54</td>
      <td>82.18</td>
      <td>62.92</td>
      <td>53.70</td>
      <td>75.61</td>
      <td>25.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>4bb10bcc0f7dfc5039658eb5e6b36c8555d94e66</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BlouseJury/Mistral-7B-Discord-0.1</td>
      <td>60.28</td>
      <td>60.24</td>
      <td>83.13</td>
      <td>62.82</td>
      <td>44.10</td>
      <td>78.93</td>
      <td>32.45</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>090a440c18ac262ecc045b798b72f99ba9a22c9c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>diffnamehard/Psyfighter2-Noromaid-ties-Capybara-13B</td>
      <td>60.27</td>
      <td>62.29</td>
      <td>83.87</td>
      <td>56.59</td>
      <td>51.44</td>
      <td>77.03</td>
      <td>30.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>a7fa1f27d0a9123ce9dc415a5573b9e0525c69f8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_3.5</td>
      <td>60.26</td>
      <td>62.46</td>
      <td>83.96</td>
      <td>62.89</td>
      <td>45.43</td>
      <td>81.06</td>
      <td>25.78</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>1008.0</td>
      <td>True</td>
      <td>5b874a33a91d63023055e6cb2d5d86afe883b4ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/SlimOpenOrca-Mistral-7B-v2</td>
      <td>60.25</td>
      <td>62.88</td>
      <td>83.41</td>
      <td>62.05</td>
      <td>56.65</td>
      <td>77.58</td>
      <td>18.95</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7cd030ccdb169c2685fe028bb4380b91ad74920f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Mistral-11B-TestBench11</td>
      <td>60.25</td>
      <td>64.42</td>
      <td>83.93</td>
      <td>63.82</td>
      <td>56.68</td>
      <td>77.74</td>
      <td>14.94</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>9aae2b156b24557bb98e515f3a90c7865529d2e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MisterRid/wendigo-14b-alpha4</td>
      <td>60.25</td>
      <td>59.30</td>
      <td>79.65</td>
      <td>59.85</td>
      <td>54.98</td>
      <td>74.74</td>
      <td>32.98</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea3ecf4418cf3655cf5093a8feb045b47b92c331</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vihangd/smartyplats-7b-v2</td>
      <td>60.24</td>
      <td>57.94</td>
      <td>80.76</td>
      <td>58.16</td>
      <td>50.26</td>
      <td>75.53</td>
      <td>38.82</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>99049eb184b9b3ef074043d6e626fe3db09f5a19</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lilloukas/GPlatty-30B</td>
      <td>60.23</td>
      <td>65.78</td>
      <td>84.79</td>
      <td>63.49</td>
      <td>52.45</td>
      <td>80.98</td>
      <td>13.87</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>836cf4dcd60ebe2ff09415c72f809d94639e8d35</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>argilla/notus-7b-v1</td>
      <td>60.22</td>
      <td>64.59</td>
      <td>84.78</td>
      <td>63.03</td>
      <td>54.37</td>
      <td>79.40</td>
      <td>15.16</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>95.0</td>
      <td>True</td>
      <td>89f594b32aea9bf5de0abe3877f20ff302549934</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SicariusSicariiStuff/Tenebra_30B_Alpha01_FP16</td>
      <td>60.18</td>
      <td>64.51</td>
      <td>84.79</td>
      <td>54.29</td>
      <td>54.22</td>
      <td>78.61</td>
      <td>24.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>32.53</td>
      <td>0.0</td>
      <td>True</td>
      <td>ad31f850f8c061d79a05aaa2419ec0f0baf62034</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/Mistral-7B-OpenOrca</td>
      <td>60.17</td>
      <td>64.08</td>
      <td>83.99</td>
      <td>62.24</td>
      <td>53.05</td>
      <td>77.74</td>
      <td>19.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>570.0</td>
      <td>True</td>
      <td>7233ac83317946d05c474b71cc1379f49eb74c14</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CallComply/SOLAR-10.7B-Instruct-v1.0-128k</td>
      <td>60.16</td>
      <td>65.96</td>
      <td>84.35</td>
      <td>57.63</td>
      <td>65.42</td>
      <td>80.51</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>False</td>
      <td>bf951ef22381c0dbeb69959fb3c06e772adc2426</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-65b-gpt4-1.3</td>
      <td>60.15</td>
      <td>66.13</td>
      <td>85.99</td>
      <td>63.89</td>
      <td>51.32</td>
      <td>79.95</td>
      <td>13.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>4373e66135c6fb4a6063777c4270a34509e7e932</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-zephyr-6x7b-lora</td>
      <td>60.13</td>
      <td>61.01</td>
      <td>82.80</td>
      <td>60.09</td>
      <td>48.84</td>
      <td>77.03</td>
      <td>31.01</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ebf239f263dc1bfb7cf2030c96f0e967683e5946</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MisterRid/wendigo-14b-alpha3</td>
      <td>60.10</td>
      <td>59.39</td>
      <td>79.51</td>
      <td>59.72</td>
      <td>55.12</td>
      <td>74.74</td>
      <td>32.15</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>01c9ec549ddc830eaa6639e7e89b6337c51586e3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MexIvanov/zephyr-python-ru-merged</td>
      <td>60.10</td>
      <td>56.06</td>
      <td>82.06</td>
      <td>60.20</td>
      <td>52.81</td>
      <td>76.95</td>
      <td>32.52</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>046d180301dd6b764fc5def83f39c8b4aa62782f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>teknium/CollectiveCognition-v1-Mistral-7B</td>
      <td>60.10</td>
      <td>62.37</td>
      <td>85.50</td>
      <td>62.76</td>
      <td>54.48</td>
      <td>77.58</td>
      <td>17.89</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>58777f0563610fa770c4fa252c0350de71d4ab9d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>MexIvanov/zephyr-python-ru</td>
      <td>60.08</td>
      <td>56.14</td>
      <td>82.03</td>
      <td>60.18</td>
      <td>52.80</td>
      <td>76.80</td>
      <td>32.52</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>64a1984f1cba96880047c8f93a83fde9f5b1df35</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abhishek/ccy0-2g7e-wqsa-0</td>
      <td>60.07</td>
      <td>58.19</td>
      <td>82.19</td>
      <td>59.59</td>
      <td>49.99</td>
      <td>78.22</td>
      <td>32.22</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1cd1158f3104fa8ed8469e2b09d674b997e229b4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-zephyr-6x7b</td>
      <td>60.06</td>
      <td>60.75</td>
      <td>82.80</td>
      <td>60.03</td>
      <td>48.84</td>
      <td>77.03</td>
      <td>30.93</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>35.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>8d7ffe152c8dd278fbd8f29a80dfa13b024f3e52</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/llama-30b-2048-instruct-PL-lora_unload</td>
      <td>60.03</td>
      <td>63.82</td>
      <td>84.70</td>
      <td>61.49</td>
      <td>52.49</td>
      <td>79.79</td>
      <td>17.89</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b15f4310ea37fef99e4f16372a4b1f2342e27613</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Mihaiii/Metis-0.1</td>
      <td>60.02</td>
      <td>60.15</td>
      <td>82.85</td>
      <td>61.42</td>
      <td>45.24</td>
      <td>77.27</td>
      <td>33.21</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>ead51068b4208b37c37733109570b445d086551e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Delcos/Velara</td>
      <td>60.01</td>
      <td>58.96</td>
      <td>82.83</td>
      <td>59.45</td>
      <td>44.70</td>
      <td>73.80</td>
      <td>40.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>11.39</td>
      <td>9.0</td>
      <td>True</td>
      <td>0fad8e711563d3a5a4631500d6a1d6b87f10d396</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>ehartford/WizardLM-33B-V1.0-Uncensored</td>
      <td>59.99</td>
      <td>63.65</td>
      <td>83.84</td>
      <td>59.36</td>
      <td>56.80</td>
      <td>77.66</td>
      <td>18.65</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>3eca9fdee0ce28d6a4a635a6f19d9a413caee3e7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jebcarter/psyonic-cetacean-20B</td>
      <td>59.97</td>
      <td>63.57</td>
      <td>86.20</td>
      <td>59.66</td>
      <td>57.55</td>
      <td>78.14</td>
      <td>14.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>19.99</td>
      <td>12.0</td>
      <td>True</td>
      <td>298d2086a949d53af06096d229f64f4719261698</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ericpolewski/AIRIC-The-Mistral</td>
      <td>59.95</td>
      <td>59.98</td>
      <td>82.98</td>
      <td>60.67</td>
      <td>48.24</td>
      <td>76.95</td>
      <td>30.86</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>b491a2e09079cfd8d388a5a65e2c44910b10aad4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CallComply/Starling-LM-11B-alpha</td>
      <td>59.92</td>
      <td>61.26</td>
      <td>81.99</td>
      <td>61.50</td>
      <td>41.53</td>
      <td>78.06</td>
      <td>35.18</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>5.0</td>
      <td>True</td>
      <td>db8cffdb7d63b88239c3b27b5afe1b433400e72f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3-1</td>
      <td>59.90</td>
      <td>64.25</td>
      <td>82.49</td>
      <td>60.79</td>
      <td>56.40</td>
      <td>77.35</td>
      <td>18.12</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>505.0</td>
      <td>True</td>
      <td>3995e9a13d54ce95f0ad55de2eaa92e2dc580174</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Walmart-the-bag/Yi-6B-Infinity-Chat</td>
      <td>59.83</td>
      <td>56.57</td>
      <td>77.66</td>
      <td>64.05</td>
      <td>50.75</td>
      <td>73.95</td>
      <td>36.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>7a441a69e1ebd192fbf52b904589130c3875aacc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/samantha-1.2-mistral-7b</td>
      <td>59.83</td>
      <td>64.08</td>
      <td>85.08</td>
      <td>63.91</td>
      <td>50.40</td>
      <td>78.53</td>
      <td>16.98</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>5574a021f55a446a756dcbc776f1765aefc280a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/llama-30b-instruct-2048-PL-lora</td>
      <td>59.82</td>
      <td>63.31</td>
      <td>84.66</td>
      <td>61.66</td>
      <td>53.35</td>
      <td>79.08</td>
      <td>16.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1a076bce564f03bd47951eecab628c541fb1a6ad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jilp00/Hermes-2-SOLAR-10.7B-Symbolic</td>
      <td>59.81</td>
      <td>61.69</td>
      <td>82.57</td>
      <td>65.06</td>
      <td>54.85</td>
      <td>80.74</td>
      <td>13.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>1.0</td>
      <td>True</td>
      <td>a5e2987baf03cab726e1135877ce3ae319ccd843</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardMath-70B-V1.0</td>
      <td>59.81</td>
      <td>67.49</td>
      <td>86.03</td>
      <td>68.44</td>
      <td>52.23</td>
      <td>81.77</td>
      <td>2.88</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>109.0</td>
      <td>True</td>
      <td>97e5913edd2c593c3eef12070024674e7ee4e16c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HuggingFaceH4/mistral-7b-sft-beta</td>
      <td>59.78</td>
      <td>57.42</td>
      <td>82.23</td>
      <td>61.42</td>
      <td>43.58</td>
      <td>77.58</td>
      <td>36.47</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>c985a04e76fb00d3c3f65214d0b02c5a751d2274</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aloobun/bun_mistral_7b_v2</td>
      <td>59.76</td>
      <td>59.90</td>
      <td>82.65</td>
      <td>61.77</td>
      <td>40.67</td>
      <td>78.30</td>
      <td>35.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>4b7c558e530a9e887ba38fc5f58caf7b41db608e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Llama-2-70B-chat-GPTQ</td>
      <td>59.75</td>
      <td>62.63</td>
      <td>84.81</td>
      <td>62.74</td>
      <td>50.98</td>
      <td>78.69</td>
      <td>18.65</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>?</td>
      <td>72.82</td>
      <td>0.0</td>
      <td>True</td>
      <td>054fbf6f65e7ab7691ec07ec9ad366acf2dd90bf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>crumb/apricot-wildflower-20</td>
      <td>59.74</td>
      <td>59.64</td>
      <td>81.76</td>
      <td>63.38</td>
      <td>41.76</td>
      <td>77.90</td>
      <td>33.97</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>27610b542c84b446c397dd92cc28d53c278b1ecb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>adamo1139/Mistral-7B-AEZAKMI-v2</td>
      <td>59.69</td>
      <td>58.11</td>
      <td>82.53</td>
      <td>59.89</td>
      <td>51.50</td>
      <td>73.64</td>
      <td>32.45</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a1bbf8066d2ff0effdf6ba311f295a1a5b88c65</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>joey00072/ToxicHermes-2.5-Mistral-7B</td>
      <td>59.69</td>
      <td>64.59</td>
      <td>83.75</td>
      <td>63.67</td>
      <td>50.84</td>
      <td>77.90</td>
      <td>17.36</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>15.0</td>
      <td>True</td>
      <td>b8355885ec4e429f8cf1c7f0c324a696ee7a2893</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NECOUDBFM/Jellyfish</td>
      <td>59.65</td>
      <td>63.31</td>
      <td>83.19</td>
      <td>58.60</td>
      <td>53.32</td>
      <td>75.85</td>
      <td>23.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>33e7aa13e855f0342d7e3173e78142bd5989c671</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Synatra-RP-Orca-2-7b-v0.1</td>
      <td>59.65</td>
      <td>57.68</td>
      <td>77.37</td>
      <td>56.10</td>
      <td>52.52</td>
      <td>74.59</td>
      <td>39.65</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>5.0</td>
      <td>True</td>
      <td>da80bc823c407c28c464cc0547a8ed9e0ca82f79</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Locutusque/Orca-2-13B-no_robots</td>
      <td>59.63</td>
      <td>59.13</td>
      <td>79.57</td>
      <td>60.28</td>
      <td>51.17</td>
      <td>80.35</td>
      <td>27.29</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>6f32722f7d24501036698cbca9c7a3e2336f071f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Yarn-Mistral-7b-64k</td>
      <td>59.63</td>
      <td>59.90</td>
      <td>82.51</td>
      <td>62.96</td>
      <td>41.86</td>
      <td>77.27</td>
      <td>33.28</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>38.0</td>
      <td>True</td>
      <td>0273c624561fcecc8e8f4030492a9307aa60f945</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sequelbox/DiamondForce</td>
      <td>59.63</td>
      <td>62.12</td>
      <td>83.43</td>
      <td>58.10</td>
      <td>46.46</td>
      <td>79.01</td>
      <td>28.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>e92bbb8e6373408235e30cebcf4a71cc319b0ae3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/SynthIA-7B-v1.5</td>
      <td>59.59</td>
      <td>62.71</td>
      <td>83.37</td>
      <td>63.48</td>
      <td>51.32</td>
      <td>79.24</td>
      <td>17.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>5a9912ef90a0efc1aaea327e5cf3e9554c8bd897</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ValiantLabs/ShiningValiantXS</td>
      <td>59.56</td>
      <td>58.96</td>
      <td>81.93</td>
      <td>56.75</td>
      <td>48.70</td>
      <td>76.95</td>
      <td>34.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>8.0</td>
      <td>True</td>
      <td>8c1f86bd2e646408eed2ed3a2634b38ea4e5c599</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>maywell/Synatra-RP-Orca-2-7b-v0.1</td>
      <td>59.55</td>
      <td>57.42</td>
      <td>77.31</td>
      <td>56.12</td>
      <td>52.55</td>
      <td>74.43</td>
      <td>39.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>5.0</td>
      <td>True</td>
      <td>da80bc823c407c28c464cc0547a8ed9e0ca82f79</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>internlm/internlm-20b</td>
      <td>59.55</td>
      <td>60.49</td>
      <td>82.13</td>
      <td>61.85</td>
      <td>52.61</td>
      <td>76.72</td>
      <td>23.50</td>
      <td>pretrained</td>
      <td>InternLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>69.0</td>
      <td>True</td>
      <td>b8825fe3394608fe84f0f5eb6471454384fb83aa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>perlthoughts/Chupacabra-v3</td>
      <td>59.52</td>
      <td>66.21</td>
      <td>81.29</td>
      <td>59.36</td>
      <td>57.85</td>
      <td>77.43</td>
      <td>15.01</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1dfa5e16d4be646b496d657d86554482ad48b3c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/WizardLM-30B-fp16</td>
      <td>59.51</td>
      <td>62.54</td>
      <td>83.28</td>
      <td>59.03</td>
      <td>52.49</td>
      <td>77.51</td>
      <td>22.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>30.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>465f87a243969963f25ae6cf8f8d2de6c0898bbe</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/gpt4-alpaca-lora-30b-HF</td>
      <td>59.51</td>
      <td>64.85</td>
      <td>85.72</td>
      <td>58.51</td>
      <td>52.24</td>
      <td>80.19</td>
      <td>15.54</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>30.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>3c8007467a081dc72ae09b9d358416b056b38920</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HuggingFaceH4/zephyr-7b-alpha</td>
      <td>59.50</td>
      <td>61.01</td>
      <td>84.04</td>
      <td>61.39</td>
      <td>57.90</td>
      <td>78.61</td>
      <td>14.03</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1020.0</td>
      <td>True</td>
      <td>2cd2cd16a6ab22585d643cf264fac73b18e7852a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/HelpSteer-filtered-7B</td>
      <td>59.49</td>
      <td>59.56</td>
      <td>83.32</td>
      <td>63.52</td>
      <td>41.11</td>
      <td>76.01</td>
      <td>33.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>0f14404caa1b4609bb2f50714df973223f443e40</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>diffnamehard/Psyfighter2-Noromaid-ties-13B</td>
      <td>59.47</td>
      <td>61.86</td>
      <td>84.58</td>
      <td>57.04</td>
      <td>50.66</td>
      <td>75.37</td>
      <td>27.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>191d13355682a875a24d2ebdd3322df55d6f9954</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>LLMs/WizardLM-30B-V1.0</td>
      <td>59.45</td>
      <td>62.54</td>
      <td>83.27</td>
      <td>59.05</td>
      <td>52.49</td>
      <td>77.51</td>
      <td>21.83</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>30.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>75318440dba949804d6263d368e1f29a94ea7c5f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Yarn-Mistral-7b-128k</td>
      <td>59.42</td>
      <td>59.64</td>
      <td>82.50</td>
      <td>63.02</td>
      <td>41.78</td>
      <td>76.95</td>
      <td>32.60</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>505.0</td>
      <td>True</td>
      <td>d09f1f8ed437d61c1aff94c1beabee554843dcdd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>hywu/Camelidae-8x13B</td>
      <td>59.40</td>
      <td>61.18</td>
      <td>82.73</td>
      <td>57.21</td>
      <td>43.37</td>
      <td>77.35</td>
      <td>34.57</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>857292e46549732062a27eb965f3c9869dc62794</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-XS-v1.1</td>
      <td>59.39</td>
      <td>63.91</td>
      <td>84.06</td>
      <td>63.07</td>
      <td>49.92</td>
      <td>79.16</td>
      <td>16.22</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>e8850e534a3a9f602f72201b09c7ef8f879c1c0b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>deepseek-ai/deepseek-llm-7b-chat</td>
      <td>59.38</td>
      <td>55.80</td>
      <td>79.38</td>
      <td>51.75</td>
      <td>47.98</td>
      <td>74.82</td>
      <td>46.55</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>48.0</td>
      <td>True</td>
      <td>afbda8b347ec881666061fa67447046fc5164ec8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CallComply/openchat-3.5-0106-128k</td>
      <td>59.38</td>
      <td>64.25</td>
      <td>77.31</td>
      <td>57.58</td>
      <td>46.50</td>
      <td>77.66</td>
      <td>32.98</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>35cf427cc9af94533baeea8afa1428a0eff78f3f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/OpenAssistant-SFT-7-Llama-30B-HF</td>
      <td>59.34</td>
      <td>60.58</td>
      <td>82.17</td>
      <td>57.93</td>
      <td>46.94</td>
      <td>78.61</td>
      <td>29.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>30.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>a7a2306b9a63de2c545f35b24735f4540baf5903</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/SynthIA-7B-v1.3</td>
      <td>59.34</td>
      <td>62.12</td>
      <td>83.45</td>
      <td>62.65</td>
      <td>51.37</td>
      <td>78.85</td>
      <td>17.59</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>137.0</td>
      <td>True</td>
      <td>8e6d0b18be876e0ebfff47d6c4f33d776f189971</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bavest/fin-llama-33b-merged</td>
      <td>59.33</td>
      <td>65.02</td>
      <td>86.20</td>
      <td>58.73</td>
      <td>49.75</td>
      <td>80.03</td>
      <td>16.22</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl</td>
      <td>33.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>17114520801da7b9599fe7a9fdf238915713a59b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>dhanushreddy29/BrokenKeyboardMerge</td>
      <td>59.33</td>
      <td>59.73</td>
      <td>81.25</td>
      <td>58.36</td>
      <td>52.00</td>
      <td>78.69</td>
      <td>25.93</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>79693860dd86978c3b3de3fefe3b0664c9183e07</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Walmart-the-bag/MysticFusion-13B</td>
      <td>59.31</td>
      <td>61.35</td>
      <td>84.43</td>
      <td>57.29</td>
      <td>51.98</td>
      <td>76.01</td>
      <td>24.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>02255943c6eff59ef6bd17e1a43a37ce3751ff5e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ariellee/SuperPlatty-30B</td>
      <td>59.30</td>
      <td>65.78</td>
      <td>83.95</td>
      <td>62.57</td>
      <td>53.52</td>
      <td>80.35</td>
      <td>9.63</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>017e1c32bca060107337dbf26db2044a7caa56f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SuperAGI/SAM</td>
      <td>59.30</td>
      <td>59.39</td>
      <td>82.31</td>
      <td>62.15</td>
      <td>52.64</td>
      <td>76.40</td>
      <td>22.90</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>24.0</td>
      <td>True</td>
      <td>ce1fb6a278121df73eee5d7d39dc0d30b214a1b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>deepseek-ai/deepseek-llm-7b-chat</td>
      <td>59.27</td>
      <td>55.72</td>
      <td>79.38</td>
      <td>51.77</td>
      <td>47.92</td>
      <td>74.90</td>
      <td>45.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>48.0</td>
      <td>True</td>
      <td>afbda8b347ec881666061fa67447046fc5164ec8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Norquinal/Mistral-7B-claude-instruct</td>
      <td>59.27</td>
      <td>63.23</td>
      <td>84.99</td>
      <td>63.84</td>
      <td>47.47</td>
      <td>78.14</td>
      <td>17.97</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>faff0de73681ad1f0500169ae18d7a5ff424eb7f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Venomia-1.1-m7</td>
      <td>59.27</td>
      <td>58.45</td>
      <td>83.04</td>
      <td>56.39</td>
      <td>47.21</td>
      <td>74.43</td>
      <td>36.09</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>00dd78ef6ee386c860f9136b9ef703a4c141e7f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mergedlm/zephyrnotus-11b-alpha</td>
      <td>59.26</td>
      <td>61.35</td>
      <td>82.80</td>
      <td>60.67</td>
      <td>57.22</td>
      <td>76.40</td>
      <td>17.13</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>False</td>
      <td>a6f74e800b6c77261a1d212bb3e6b2752cbedef9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Synatra-7B-v0.3-RP</td>
      <td>59.26</td>
      <td>62.20</td>
      <td>82.29</td>
      <td>60.80</td>
      <td>52.64</td>
      <td>76.48</td>
      <td>21.15</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>372f6e0ab2c20b93e0c42218f76a71a4f9bb282e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BlueNipples/TimeCrystal-l2-13B</td>
      <td>59.26</td>
      <td>61.18</td>
      <td>83.71</td>
      <td>56.46</td>
      <td>51.30</td>
      <td>75.37</td>
      <td>27.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>f0076c437e766880841dc1768693dc745d093b8b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sethuiyer/Dr_Samantha_7b_mistral</td>
      <td>59.25</td>
      <td>60.41</td>
      <td>83.65</td>
      <td>63.14</td>
      <td>41.37</td>
      <td>75.45</td>
      <td>31.46</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>e0201aa9423f082a4182cbf910d75ba438528ddb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gaodrew/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps</td>
      <td>59.22</td>
      <td>61.52</td>
      <td>84.06</td>
      <td>60.23</td>
      <td>51.05</td>
      <td>80.82</td>
      <td>17.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1114ff08ed15ef417502da58f0237d2f6650c9ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Qwen/Qwen-7B</td>
      <td>59.19</td>
      <td>51.37</td>
      <td>78.47</td>
      <td>59.84</td>
      <td>47.79</td>
      <td>72.69</td>
      <td>44.96</td>
      <td>pretrained</td>
      <td>QWenLMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.72</td>
      <td>321.0</td>
      <td>True</td>
      <td>c9bdb955021a80ae26fa6978891996dbe4951d8d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigostral-7b-chat</td>
      <td>59.18</td>
      <td>62.63</td>
      <td>84.34</td>
      <td>63.53</td>
      <td>49.24</td>
      <td>78.61</td>
      <td>16.76</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>969fbfc7a91f53c8562a2c48a3c24dd3745d5a97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>maywell/PiVoT-0.1-Evil-a</td>
      <td>59.16</td>
      <td>59.64</td>
      <td>81.48</td>
      <td>58.94</td>
      <td>39.23</td>
      <td>75.30</td>
      <td>40.41</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.24</td>
      <td>27.0</td>
      <td>True</td>
      <td>b6e20287ba4156f06b4288d4003acc677040527f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>FPHam/Karen_TheEditor_V2_STRICT_Mistral_7B</td>
      <td>59.13</td>
      <td>59.56</td>
      <td>81.79</td>
      <td>59.56</td>
      <td>49.36</td>
      <td>74.35</td>
      <td>30.17</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>9.0</td>
      <td>True</td>
      <td>0935960b2765aa23d7a63c49873361b09dd12f60</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PeanutJar/Mistral-v0.1-PeanutButter-v0.0.0-7B</td>
      <td>59.09</td>
      <td>62.20</td>
      <td>84.10</td>
      <td>64.14</td>
      <td>46.94</td>
      <td>78.69</td>
      <td>18.50</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9609a969ba6429b84e538d96afac55eb133a9983</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-7B-0.4-DPO</td>
      <td>59.08</td>
      <td>62.29</td>
      <td>84.32</td>
      <td>63.20</td>
      <td>42.28</td>
      <td>76.95</td>
      <td>25.47</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>47a417a2167064112038e71f2be30d7293eb485d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abdulrahman-nuzha/finetuned-Mistral-5000-v1.0</td>
      <td>59.08</td>
      <td>59.90</td>
      <td>82.37</td>
      <td>61.68</td>
      <td>41.17</td>
      <td>78.30</td>
      <td>31.08</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e8050c54d484d7e0a885b9d97a0781f0dd2e745e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HuggingFaceH4/zephyr-7b-beta</td>
      <td>59.08</td>
      <td>62.03</td>
      <td>84.53</td>
      <td>61.06</td>
      <td>57.44</td>
      <td>78.06</td>
      <td>11.37</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1187.0</td>
      <td>True</td>
      <td>8af01af3d4f9dc9b962447180d6d0f8c5315da86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>osanseviero/mistral-instruct-slerp</td>
      <td>59.08</td>
      <td>57.42</td>
      <td>78.34</td>
      <td>55.19</td>
      <td>57.61</td>
      <td>75.14</td>
      <td>30.78</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1994dd1daadcfd88c471531e6a264271d6e07b4d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NurtureAI/openchat_3.5-16k</td>
      <td>59.03</td>
      <td>63.31</td>
      <td>83.58</td>
      <td>61.90</td>
      <td>43.47</td>
      <td>80.11</td>
      <td>21.83</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>30.0</td>
      <td>True</td>
      <td>e8d66e7fb2ebb918f468137ea5fa3dc13ddc69da</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>garage-bAInd/Platypus-30B</td>
      <td>59.03</td>
      <td>64.59</td>
      <td>84.26</td>
      <td>64.23</td>
      <td>45.35</td>
      <td>81.37</td>
      <td>14.40</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>32.53</td>
      <td>18.0</td>
      <td>True</td>
      <td>c5d21054f8dd71099696bd7790df07ac54990f29</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lilloukas/Platypus-30B</td>
      <td>59.03</td>
      <td>64.59</td>
      <td>84.24</td>
      <td>64.19</td>
      <td>45.35</td>
      <td>81.37</td>
      <td>14.40</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>979ad39b58a8e4a9419b7bc7a0dc8419f3912e71</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/orca_mini_v3_13B-GPTQ</td>
      <td>59.01</td>
      <td>61.95</td>
      <td>81.56</td>
      <td>56.10</td>
      <td>49.22</td>
      <td>75.77</td>
      <td>29.49</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>other</td>
      <td>16.23</td>
      <td>11.0</td>
      <td>True</td>
      <td>7b7a2dcd946f393e26215268c4c7e0699be2bbd8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/zephyr-alpha-Nebula-v2-7B</td>
      <td>59.01</td>
      <td>58.62</td>
      <td>83.05</td>
      <td>56.68</td>
      <td>58.28</td>
      <td>73.56</td>
      <td>23.88</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>e8f1fd1acceda7fb662340f5afe312a7ef030374</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>unaidedelf87777/wizard-mistral-v0.1</td>
      <td>59.01</td>
      <td>61.77</td>
      <td>83.51</td>
      <td>63.99</td>
      <td>47.46</td>
      <td>78.30</td>
      <td>19.03</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>b66724f8195e7b76289f8f3f72a98392557c46ad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>ehartford/samantha-1.1-llama-33b</td>
      <td>58.98</td>
      <td>67.83</td>
      <td>85.55</td>
      <td>58.79</td>
      <td>61.19</td>
      <td>76.48</td>
      <td>4.02</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>ad8892a17be1372f611203a4cf71560cc337e458</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>osanseviero/mistral-instruct-frankenmerge</td>
      <td>58.96</td>
      <td>58.19</td>
      <td>83.26</td>
      <td>59.53</td>
      <td>66.48</td>
      <td>75.06</td>
      <td>11.22</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>1.0</td>
      <td>True</td>
      <td>af5cbc3a435aab75424e4ecc75f041f2eda133ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Dans-DiscountModels/Mistral-7b-FFT-Test3</td>
      <td>58.96</td>
      <td>60.24</td>
      <td>82.36</td>
      <td>62.20</td>
      <td>44.36</td>
      <td>77.82</td>
      <td>26.76</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>ff6ab8204162794d7d74297d60acb741c2ef8e3a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Tess-XS-v1.0</td>
      <td>58.95</td>
      <td>61.43</td>
      <td>83.82</td>
      <td>64.10</td>
      <td>47.12</td>
      <td>78.93</td>
      <td>18.27</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>a581ab1793366ff2d5f3c966ff0e7b8b1149d775</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-7B-0.4-DPO</td>
      <td>58.93</td>
      <td>62.20</td>
      <td>84.41</td>
      <td>63.14</td>
      <td>42.34</td>
      <td>76.95</td>
      <td>24.56</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>6.0</td>
      <td>True</td>
      <td>47a417a2167064112038e71f2be30d7293eb485d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Henk717/chronoboros-33B</td>
      <td>58.92</td>
      <td>63.91</td>
      <td>85.00</td>
      <td>59.44</td>
      <td>49.83</td>
      <td>80.35</td>
      <td>15.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>33.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>a4deca117c5fa48f2cdc49ed2e2596046201d688</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>akjindal53244/Mistral-7B-v0.1-Open-Platypus</td>
      <td>58.92</td>
      <td>62.37</td>
      <td>85.08</td>
      <td>63.79</td>
      <td>47.33</td>
      <td>77.66</td>
      <td>17.29</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>aa2c84e89c4c8a10e0569e45021b59e6d1c08bda</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mwitiderrick/SwahiliInstruct-v0.1</td>
      <td>58.92</td>
      <td>57.59</td>
      <td>80.92</td>
      <td>57.00</td>
      <td>58.08</td>
      <td>74.66</td>
      <td>25.25</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>06ae9044dac3c8f7cf67f4fd33986c5b79dbe69e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>upstage/llama-30b-instruct</td>
      <td>58.91</td>
      <td>62.46</td>
      <td>86.23</td>
      <td>59.37</td>
      <td>52.78</td>
      <td>80.51</td>
      <td>12.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>30.00</td>
      <td>22.0</td>
      <td>False</td>
      <td>fea4312379557e8a1e8073965f560798de369edd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mncai/Mistral-7B-OpenOrca-1k</td>
      <td>58.90</td>
      <td>62.97</td>
      <td>84.66</td>
      <td>62.20</td>
      <td>52.96</td>
      <td>78.61</td>
      <td>11.98</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>ae9e37811a54ffe45f41a572c7e68363aa11b062</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/manticore-30b-chat-pyg-alpha</td>
      <td>58.86</td>
      <td>64.16</td>
      <td>84.38</td>
      <td>57.49</td>
      <td>51.57</td>
      <td>79.48</td>
      <td>16.07</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>32.53</td>
      <td>13.0</td>
      <td>False</td>
      <td>0cff8e9718e57202171003d556d2e6630061879d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-code-mistral-7b-v1.0</td>
      <td>58.85</td>
      <td>60.58</td>
      <td>83.75</td>
      <td>62.98</td>
      <td>47.90</td>
      <td>78.69</td>
      <td>19.18</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>753852b8cb52dc5f0411568e98c0cb445a7835dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>qblocks/mistral_7b_norobots</td>
      <td>58.85</td>
      <td>58.96</td>
      <td>80.57</td>
      <td>57.66</td>
      <td>41.91</td>
      <td>75.61</td>
      <td>38.36</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>36dde2c5b08140d612042d1ae047dd7551b7e15b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Henk717/airochronos-33B</td>
      <td>58.84</td>
      <td>64.42</td>
      <td>85.21</td>
      <td>59.79</td>
      <td>50.59</td>
      <td>79.32</td>
      <td>13.72</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>32.53</td>
      <td>5.0</td>
      <td>True</td>
      <td>06843c6693cc265dabb464c818a3d3713239721a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Mistral-11B-SynthIAirOmniMix</td>
      <td>58.84</td>
      <td>62.46</td>
      <td>83.13</td>
      <td>63.47</td>
      <td>55.69</td>
      <td>76.40</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>19694dc88e74a018d54bac6070cf521dff6d4397</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/Nebula-v2-7B</td>
      <td>58.82</td>
      <td>58.70</td>
      <td>83.06</td>
      <td>57.61</td>
      <td>46.72</td>
      <td>75.14</td>
      <td>31.69</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d2a5611f7d7c37bfa2270d1823bceef01c0be383</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/scarlett-33b</td>
      <td>58.81</td>
      <td>67.75</td>
      <td>85.48</td>
      <td>58.98</td>
      <td>61.05</td>
      <td>76.80</td>
      <td>2.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>33.00</td>
      <td>23.0</td>
      <td>True</td>
      <td>305eea72fb9fe2ac5929a62483ea51f152bcc060</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Dans-DiscountModels/Mistral-7b-FFT-Test3</td>
      <td>58.79</td>
      <td>60.41</td>
      <td>82.31</td>
      <td>62.45</td>
      <td>44.33</td>
      <td>77.58</td>
      <td>25.63</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>ff6ab8204162794d7d74297d60acb741c2ef8e3a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-13b-v0.3</td>
      <td>58.77</td>
      <td>62.80</td>
      <td>84.42</td>
      <td>56.86</td>
      <td>50.73</td>
      <td>74.74</td>
      <td>23.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>15.0</td>
      <td>True</td>
      <td>1013d7e539e53c15e5285ed27902a713c8caad09</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama-30b</td>
      <td>58.77</td>
      <td>64.25</td>
      <td>83.64</td>
      <td>58.23</td>
      <td>53.20</td>
      <td>77.43</td>
      <td>15.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>7f035eabd1d0e7b38ace395847a623f475d90da8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Henk717/airochronos-33B</td>
      <td>58.75</td>
      <td>64.25</td>
      <td>85.20</td>
      <td>59.83</td>
      <td>50.56</td>
      <td>79.08</td>
      <td>13.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>32.53</td>
      <td>5.0</td>
      <td>True</td>
      <td>06843c6693cc265dabb464c818a3d3713239721a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-m-7b-3.1.2</td>
      <td>58.75</td>
      <td>61.86</td>
      <td>83.51</td>
      <td>61.91</td>
      <td>53.75</td>
      <td>77.58</td>
      <td>13.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>38.0</td>
      <td>True</td>
      <td>e9a7f0271fa442d65bf6be87feeb3f4de2f5760e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/dromedary-65b-lora-HF</td>
      <td>58.73</td>
      <td>61.60</td>
      <td>82.53</td>
      <td>63.08</td>
      <td>38.82</td>
      <td>78.93</td>
      <td>27.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>65.00</td>
      <td>20.0</td>
      <td>True</td>
      <td>3fa4546259d6bbd6b5d637484c325ab19181a73c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ausboss/llama-30b-supercot</td>
      <td>58.73</td>
      <td>64.85</td>
      <td>85.08</td>
      <td>56.56</td>
      <td>53.96</td>
      <td>80.03</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>126.0</td>
      <td>False</td>
      <td>dc9d81f454d286ea040c5cd45b058aecaa51c13e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openbmb/UltraLM-13b-v2.0</td>
      <td>58.72</td>
      <td>62.63</td>
      <td>81.49</td>
      <td>56.17</td>
      <td>49.48</td>
      <td>76.48</td>
      <td>26.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>False</td>
      <td>a452045c96ae62379a98ef0d85666616a66e78a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/CollectiveCognition-v1.1-Mistral-7B-dare-0.85</td>
      <td>58.72</td>
      <td>61.01</td>
      <td>84.31</td>
      <td>64.34</td>
      <td>44.87</td>
      <td>78.85</td>
      <td>18.95</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>7ecfa4c5b100565bf8cfdfa7442e9772d28a9a23</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aeala/GPT4-x-AlpacaDente2-30b</td>
      <td>58.71</td>
      <td>60.58</td>
      <td>81.81</td>
      <td>56.63</td>
      <td>48.38</td>
      <td>78.14</td>
      <td>26.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>32.0</td>
      <td>False</td>
      <td>9fe5a8dada738f44e7ee9293b2140ae0be021787</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>bhenrym14/mistral-7b-platypus-fp16</td>
      <td>58.71</td>
      <td>63.05</td>
      <td>84.15</td>
      <td>64.11</td>
      <td>45.07</td>
      <td>78.53</td>
      <td>17.36</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>False</td>
      <td>d836a261afa0871d3734a7dfd1a28dc23c173ea7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jilp00/Nous-Hermes-2-SOLAR-10.7B-v1.1</td>
      <td>58.69</td>
      <td>63.99</td>
      <td>82.72</td>
      <td>65.85</td>
      <td>56.97</td>
      <td>81.22</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>8e1cbfa67643f49be67a6021db933cdd941a6d2f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/Dolphin-Nebula-7B</td>
      <td>58.69</td>
      <td>55.20</td>
      <td>78.57</td>
      <td>53.44</td>
      <td>57.97</td>
      <td>73.88</td>
      <td>33.06</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c14b3545066e5ee5562c1724a037b41db95f1f0d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/72B-preview-canary-llamafied-qwen-llamafy-unbias-qkv</td>
      <td>58.67</td>
      <td>53.07</td>
      <td>63.13</td>
      <td>67.39</td>
      <td>57.62</td>
      <td>75.14</td>
      <td>35.63</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>72.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>dc66cf314292f3bfd5a2eed74018671effac6405</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PeanutJar/Mistral-v0.1-PeanutButter-v0.0.2-7B</td>
      <td>58.66</td>
      <td>61.77</td>
      <td>84.11</td>
      <td>64.38</td>
      <td>45.92</td>
      <td>78.37</td>
      <td>17.44</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f4d471d7a9447d0969a58d5b3146d50cfa3005b3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/Orca-2-13b</td>
      <td>58.64</td>
      <td>60.67</td>
      <td>79.81</td>
      <td>60.37</td>
      <td>56.41</td>
      <td>76.64</td>
      <td>17.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>597.0</td>
      <td>True</td>
      <td>2539ff53e6baa4cc603774ad5a2d646f4041ea4e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dfurman/falcon-40b-openassistant-peft</td>
      <td>58.63</td>
      <td>62.63</td>
      <td>85.59</td>
      <td>57.77</td>
      <td>51.02</td>
      <td>81.45</td>
      <td>13.34</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>40.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3d5084b6fbcb9f9f36493d9fd1e3795b0b9860f0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/SOLAR-Platypus-10.7B-v1</td>
      <td>58.62</td>
      <td>61.69</td>
      <td>84.23</td>
      <td>60.37</td>
      <td>51.58</td>
      <td>82.79</td>
      <td>11.07</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>e9314a1f1ca7f790491c177e7720fb14851ef603</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/QuantumLM-70B-hf</td>
      <td>58.61</td>
      <td>59.47</td>
      <td>83.02</td>
      <td>62.25</td>
      <td>53.39</td>
      <td>78.77</td>
      <td>14.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>2.0</td>
      <td>True</td>
      <td>e13dd23ae5e611e959b6c8d5bc47bf4fd37cd9d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CobraMamba/mamba-gpt-7b-v1</td>
      <td>58.61</td>
      <td>61.26</td>
      <td>84.10</td>
      <td>63.46</td>
      <td>46.34</td>
      <td>79.16</td>
      <td>17.36</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e64d658b397748e409d9633fd24fc5a6df429600</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/koOpenChat-sft</td>
      <td>58.61</td>
      <td>59.81</td>
      <td>78.73</td>
      <td>61.32</td>
      <td>51.24</td>
      <td>76.40</td>
      <td>24.18</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>0.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>47472b36e181694422564b130ee075ffa596537d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gradientputri/MegaMix-T1-13B</td>
      <td>58.61</td>
      <td>61.35</td>
      <td>83.44</td>
      <td>58.49</td>
      <td>48.19</td>
      <td>76.09</td>
      <td>24.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>55d31300f8972b56320855bb40efb5e3d1e1a6fc</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/dolphin-2.0-mistral-7b</td>
      <td>58.58</td>
      <td>59.22</td>
      <td>80.26</td>
      <td>56.90</td>
      <td>61.09</td>
      <td>75.37</td>
      <td>18.65</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>c673387016c622fd0a707426953c03957398bc37</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>S4sch/zephyr-neural-chat-frankenmerge11b</td>
      <td>58.57</td>
      <td>61.52</td>
      <td>84.09</td>
      <td>61.51</td>
      <td>60.63</td>
      <td>76.24</td>
      <td>7.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>11.39</td>
      <td>4.0</td>
      <td>True</td>
      <td>f915831e904e0dcda760873aa16a35daf5ac9e6d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>umd-zhou-lab/claude2-alpaca-13B</td>
      <td>58.57</td>
      <td>61.18</td>
      <td>84.21</td>
      <td>55.93</td>
      <td>45.02</td>
      <td>76.80</td>
      <td>28.28</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d670244f2f70ab35219c9bbf83eef4f5dc28730</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>lmsys/vicuna-33b-v1.3</td>
      <td>58.54</td>
      <td>62.12</td>
      <td>83.00</td>
      <td>59.22</td>
      <td>56.16</td>
      <td>77.03</td>
      <td>13.72</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>33.00</td>
      <td>266.0</td>
      <td>False</td>
      <td>ef8d6becf883fb3ce52e3706885f761819477ab4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/72B-preview-canary-llamafied-qwen-llamafy-unbias-qkv</td>
      <td>58.54</td>
      <td>52.56</td>
      <td>62.99</td>
      <td>67.45</td>
      <td>57.61</td>
      <td>75.14</td>
      <td>35.48</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>72.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>dc66cf314292f3bfd5a2eed74018671effac6405</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gradientputri/MegaMix-A1-13B</td>
      <td>58.52</td>
      <td>61.60</td>
      <td>83.49</td>
      <td>58.26</td>
      <td>47.48</td>
      <td>76.16</td>
      <td>24.11</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>14e0756c210bcf420fbf825e6b8087ee5c716e7f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NeverSleep/Noromaid-13b-v0.2</td>
      <td>58.51</td>
      <td>60.92</td>
      <td>84.04</td>
      <td>57.67</td>
      <td>52.58</td>
      <td>74.11</td>
      <td>21.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>19.0</td>
      <td>True</td>
      <td>dad2d749b01cf10b65951dea6e130da8cc53e2c0</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewd-ReMM-L2-Chat-20B</td>
      <td>58.49</td>
      <td>62.46</td>
      <td>85.62</td>
      <td>59.13</td>
      <td>55.63</td>
      <td>77.19</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>18.0</td>
      <td>True</td>
      <td>cda06630a1d8173541431e5ce8bc17dcfaa37e5e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>manishiitg/open-aditi-hi-v1</td>
      <td>58.49</td>
      <td>58.79</td>
      <td>81.38</td>
      <td>58.51</td>
      <td>42.34</td>
      <td>76.48</td>
      <td>33.43</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>1f6cbcdf01831830ff0f25f6f0e84ec4e9337e72</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ</td>
      <td>58.47</td>
      <td>61.09</td>
      <td>82.40</td>
      <td>56.46</td>
      <td>49.90</td>
      <td>77.66</td>
      <td>23.28</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>495.0</td>
      <td>True</td>
      <td>56a82ece7a9309189561a590e8f4d2fe0d4be92b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Intel/neural-chat-7b-v3</td>
      <td>58.46</td>
      <td>67.15</td>
      <td>83.29</td>
      <td>62.26</td>
      <td>58.77</td>
      <td>78.06</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>60.0</td>
      <td>True</td>
      <td>7a05c8a2151f7d32252d9ef5db10445c13ae1f20</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>posicube/Llama2-chat-AYB-13B</td>
      <td>58.45</td>
      <td>63.40</td>
      <td>84.79</td>
      <td>59.34</td>
      <td>55.62</td>
      <td>76.24</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>True</td>
      <td>cc7ca1b8f906b9f62ace094540f4ff4124dd581a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/trurl-2-13b-pl-instruct_unload</td>
      <td>58.44</td>
      <td>59.90</td>
      <td>79.99</td>
      <td>78.66</td>
      <td>45.56</td>
      <td>74.35</td>
      <td>12.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>17f57642165e30a4025d6817bd47dcd80d0c5c4d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/X-MythoChronos-13B</td>
      <td>58.43</td>
      <td>59.73</td>
      <td>83.39</td>
      <td>56.50</td>
      <td>53.55</td>
      <td>74.43</td>
      <td>22.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>True</td>
      <td>8d302741466512f0621a594fce6bf5b8125c8d4c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CalderaAI/30B-Lazarus</td>
      <td>58.40</td>
      <td>64.93</td>
      <td>84.27</td>
      <td>56.47</td>
      <td>58.65</td>
      <td>78.37</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>30.00</td>
      <td>117.0</td>
      <td>False</td>
      <td>24da9e88f2b2b7946bc6fe9412d6728b9adc2c3d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-PersonalityEngine-30b</td>
      <td>58.39</td>
      <td>63.48</td>
      <td>84.37</td>
      <td>58.99</td>
      <td>46.98</td>
      <td>80.98</td>
      <td>15.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>30.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>1990b46a2e2ac1f6282d961bce691ceceafed514</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/SynthIA-7B-v1.3-dare-0.85</td>
      <td>58.38</td>
      <td>61.01</td>
      <td>83.50</td>
      <td>64.49</td>
      <td>43.77</td>
      <td>78.93</td>
      <td>18.57</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>91381d0ac625dcde542428ed6cb35177b4260923</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Uncensored-Frank-33B</td>
      <td>58.38</td>
      <td>62.12</td>
      <td>83.30</td>
      <td>57.57</td>
      <td>54.03</td>
      <td>76.56</td>
      <td>16.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>33.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>1c1f4e9256ac2be145a9106863ee9f2e9d701e74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>posicube/Llama-chat-AY-13B</td>
      <td>58.34</td>
      <td>62.80</td>
      <td>83.23</td>
      <td>60.01</td>
      <td>55.95</td>
      <td>75.93</td>
      <td>12.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>66037b5ee553f7b878d796d2b2d5ada5734cc164</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/SynthIA-v1.3-Nebula-v2-7B</td>
      <td>58.33</td>
      <td>59.39</td>
      <td>82.77</td>
      <td>57.57</td>
      <td>50.62</td>
      <td>74.74</td>
      <td>24.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c6030620e9d4390d54ec221a18ff3e530f4dcd84</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CobraMamba/mamba-gpt-7b-v2</td>
      <td>58.31</td>
      <td>61.95</td>
      <td>83.83</td>
      <td>61.74</td>
      <td>46.63</td>
      <td>78.45</td>
      <td>17.29</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>6439444e2c0b61253d3e61ae04fe0436717acc2f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/alignment-handbook-zephyr-7b-sft-full-dpo-5e7-cont1</td>
      <td>58.29</td>
      <td>60.24</td>
      <td>82.28</td>
      <td>60.61</td>
      <td>40.55</td>
      <td>77.11</td>
      <td>28.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1ad23ee605793a7d2dd6a5030b293c68cc8f6f1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/30B-Lazarus-instruct-PL-lora_unload</td>
      <td>58.29</td>
      <td>62.80</td>
      <td>84.13</td>
      <td>56.87</td>
      <td>55.49</td>
      <td>79.08</td>
      <td>11.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>eeb29b35ceb6dd5c532f1e4e1235f1cdd3f51f23</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>concedo/Vicuzard-30B-Uncensored</td>
      <td>58.26</td>
      <td>62.97</td>
      <td>83.68</td>
      <td>58.16</td>
      <td>52.27</td>
      <td>77.11</td>
      <td>15.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>30.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>e2329c05a6e59660ba3cbcc01adf30a78f852594</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PeanutJar/Mistral-v0.1-PeanutButter-v0.0.5-SFT-7B-QLoRA</td>
      <td>58.24</td>
      <td>60.75</td>
      <td>84.24</td>
      <td>63.66</td>
      <td>44.94</td>
      <td>78.69</td>
      <td>17.13</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2609363766acf308877a71aba352e60d7c044b49</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TriadParty/deepmoney-34b-200k-base</td>
      <td>58.21</td>
      <td>63.99</td>
      <td>83.87</td>
      <td>74.04</td>
      <td>45.93</td>
      <td>81.45</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>34.00</td>
      <td>28.0</td>
      <td>True</td>
      <td>8ae3d155e57352d4b7fef1d60f74e8c8650a8ab7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-1.4</td>
      <td>58.20</td>
      <td>64.42</td>
      <td>85.13</td>
      <td>59.53</td>
      <td>50.47</td>
      <td>77.90</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>04e1e194247a95cc60ba3cd70d026bc94c1f1764</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/mistral-7b-platypus1k</td>
      <td>58.19</td>
      <td>61.60</td>
      <td>82.93</td>
      <td>63.16</td>
      <td>46.96</td>
      <td>78.14</td>
      <td>16.38</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c34c4a249ecf0cc391beba142a1f9cb23154fcd1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Riiid/sheep-duck-llama-2-13b</td>
      <td>58.19</td>
      <td>63.14</td>
      <td>84.52</td>
      <td>59.89</td>
      <td>55.48</td>
      <td>76.95</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>71edf22c49677d0239caf5f87d8139dd9cc79078</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Secbone/llama-33B-instructed</td>
      <td>58.18</td>
      <td>64.59</td>
      <td>86.17</td>
      <td>60.50</td>
      <td>44.12</td>
      <td>79.32</td>
      <td>14.40</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>33.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7c40caaea4fe3264fd469dac428b0f9450e574a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>42MARU/sitebunny-13b</td>
      <td>58.17</td>
      <td>63.14</td>
      <td>83.64</td>
      <td>59.91</td>
      <td>56.21</td>
      <td>76.72</td>
      <td>9.40</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>67107327d09c2f9bf3e4b316d97767c97f5a0804</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-TotSirocco-7b</td>
      <td>58.16</td>
      <td>62.20</td>
      <td>84.28</td>
      <td>63.80</td>
      <td>46.04</td>
      <td>79.48</td>
      <td>13.19</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>824e3a4738818142374721306ce85b83770de24b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>martyn/llama-megamerge-dare-13b</td>
      <td>58.15</td>
      <td>60.58</td>
      <td>83.00</td>
      <td>54.91</td>
      <td>45.76</td>
      <td>76.16</td>
      <td>28.51</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5529ddb255dbdabdd179bdc911f141c3f0d2fb3f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-TotSirocco-7b</td>
      <td>58.15</td>
      <td>62.03</td>
      <td>84.23</td>
      <td>64.19</td>
      <td>46.49</td>
      <td>78.69</td>
      <td>13.27</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>824e3a4738818142374721306ce85b83770de24b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/Mistral-7B-OpenOrca-lora</td>
      <td>58.14</td>
      <td>61.95</td>
      <td>83.62</td>
      <td>64.16</td>
      <td>42.74</td>
      <td>79.08</td>
      <td>17.29</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>605dc043063cb9589c06883d839122920ed1eca5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>caisarl76/mistral-guanaco1k-ep2</td>
      <td>58.13</td>
      <td>60.07</td>
      <td>82.76</td>
      <td>61.50</td>
      <td>54.40</td>
      <td>78.06</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>9c9f31f213b69da7797c2c0630c17cf8f785fc13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>caisarl76/Mistral-7B-guanaco1k-ep2</td>
      <td>58.13</td>
      <td>60.07</td>
      <td>82.76</td>
      <td>61.50</td>
      <td>54.40</td>
      <td>78.06</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>9c9f31f213b69da7797c2c0630c17cf8f785fc13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-1.8-L2-13B</td>
      <td>58.12</td>
      <td>63.48</td>
      <td>84.12</td>
      <td>58.57</td>
      <td>52.86</td>
      <td>76.40</td>
      <td>13.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>fe054ab749a69375285df40913a88bd40f1e2bf6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/2x-LoRA-Assemble-13B</td>
      <td>58.10</td>
      <td>63.65</td>
      <td>83.47</td>
      <td>59.82</td>
      <td>55.94</td>
      <td>76.48</td>
      <td>9.25</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1aca45d37eade21eb381aaefc9245b58ec3b7b26</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Cartinoe5930/SOLAR-DUS-implement</td>
      <td>58.10</td>
      <td>59.56</td>
      <td>81.18</td>
      <td>63.68</td>
      <td>40.72</td>
      <td>76.48</td>
      <td>26.99</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>bfce9b8f3e599767b3c3974b0a3cbbd1b7f2da6c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/Vicuzard-30B-Uncensored-instruct-PL-lora_unload</td>
      <td>58.09</td>
      <td>62.46</td>
      <td>83.66</td>
      <td>57.82</td>
      <td>50.94</td>
      <td>78.37</td>
      <td>15.31</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>652f03ac67b4293198d98b618e64285fb32a28e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-33b-instruct</td>
      <td>58.08</td>
      <td>63.05</td>
      <td>85.00</td>
      <td>58.32</td>
      <td>52.10</td>
      <td>78.85</td>
      <td>11.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>33.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>9c2b558b888e0ef8b4a72e0771db72a06a5c8474</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>tiiuae/falcon-40b</td>
      <td>58.07</td>
      <td>61.86</td>
      <td>85.28</td>
      <td>56.89</td>
      <td>41.65</td>
      <td>81.29</td>
      <td>21.46</td>
      <td>pretrained</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>40.00</td>
      <td>2363.0</td>
      <td>True</td>
      <td>3d7c5902f1dc9da830979a826cd96114b3ba4ec1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mncai/Mistral-7B-openplatypus-1k</td>
      <td>58.07</td>
      <td>60.15</td>
      <td>84.25</td>
      <td>59.84</td>
      <td>49.86</td>
      <td>76.87</td>
      <td>17.44</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>dad401175da3782475a122008720ddc3338e2632</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TencentARC/LLaMA-Pro-8B-Instruct</td>
      <td>58.06</td>
      <td>52.99</td>
      <td>76.98</td>
      <td>52.58</td>
      <td>49.43</td>
      <td>72.22</td>
      <td>44.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>8.36</td>
      <td>46.0</td>
      <td>True</td>
      <td>209760d8bffdc49afa18afdb038b0cf921b19fe4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>camel-ai/CAMEL-33B-Combined-Data</td>
      <td>58.06</td>
      <td>62.97</td>
      <td>83.83</td>
      <td>58.98</td>
      <td>50.21</td>
      <td>78.30</td>
      <td>14.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>33.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>62c74e7531625c1383bbbdc7c8346a996e9d1e21</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>scb10x/typhoon-7b</td>
      <td>58.05</td>
      <td>58.53</td>
      <td>81.55</td>
      <td>59.54</td>
      <td>40.52</td>
      <td>76.56</td>
      <td>31.61</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>35.0</td>
      <td>True</td>
      <td>35fb2f9cee5dbac35109effc816ca206962dad43</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Mistral-11B-v0.1</td>
      <td>58.05</td>
      <td>59.56</td>
      <td>81.17</td>
      <td>63.56</td>
      <td>40.67</td>
      <td>76.64</td>
      <td>26.69</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>10.0</td>
      <td>True</td>
      <td>e9698271ea1ab340bacfd5ebf0d77108a6f18a90</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/hippogriff-30b-chat</td>
      <td>58.05</td>
      <td>64.51</td>
      <td>85.20</td>
      <td>59.09</td>
      <td>48.42</td>
      <td>80.82</td>
      <td>10.24</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>30.00</td>
      <td>22.0</td>
      <td>False</td>
      <td>64c10edf5312cd13704925b07413882d9e94c7a0</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/airoboros-m-7b-3.1.2-dare-0.85</td>
      <td>58.03</td>
      <td>61.09</td>
      <td>83.57</td>
      <td>64.05</td>
      <td>43.64</td>
      <td>78.37</td>
      <td>17.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>b5bc02f4e1008bd3a72046a93ac2f4dd4bef02da</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/Luban-Marcoroni-13B</td>
      <td>57.98</td>
      <td>63.65</td>
      <td>82.92</td>
      <td>58.70</td>
      <td>55.55</td>
      <td>77.03</td>
      <td>10.01</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>bf152c36935acd67a9029c017f0c1ff2d7a92314</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/samantha-mistral-7b</td>
      <td>57.96</td>
      <td>63.40</td>
      <td>84.10</td>
      <td>61.36</td>
      <td>46.08</td>
      <td>76.80</td>
      <td>16.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>7f9e40543fdff8c3e58eca0390c8a631829c1206</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>martyn/llama2-megamerge-dare-13b-v2</td>
      <td>57.94</td>
      <td>59.39</td>
      <td>80.93</td>
      <td>55.26</td>
      <td>47.27</td>
      <td>75.53</td>
      <td>29.26</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>d0ff28a0cb4a70b15f55a416fbae6979f4ae5775</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/Luban-Marcoroni-13B-v3</td>
      <td>57.94</td>
      <td>63.74</td>
      <td>82.88</td>
      <td>58.64</td>
      <td>55.56</td>
      <td>76.87</td>
      <td>9.93</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>9b68680ed8351ef8ef6948169e69a888af40002e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Llamix2-Xwin-MoE-4x13B</td>
      <td>57.93</td>
      <td>60.41</td>
      <td>82.96</td>
      <td>56.24</td>
      <td>39.63</td>
      <td>75.14</td>
      <td>33.21</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>38.50</td>
      <td>0.0</td>
      <td>True</td>
      <td>220833f87c233684e8a4b0e03126ffcdffce5229</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/Luban-Marcoroni-13B-v2</td>
      <td>57.92</td>
      <td>63.48</td>
      <td>82.89</td>
      <td>58.72</td>
      <td>55.56</td>
      <td>76.95</td>
      <td>9.93</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>d7c704a08218dcc03963bc08e9113e281c056f53</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>caisarl76/Mistral-7B-OpenOrca-Guanaco-accu16</td>
      <td>57.91</td>
      <td>59.73</td>
      <td>83.08</td>
      <td>61.29</td>
      <td>50.81</td>
      <td>76.56</td>
      <td>16.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e83b8c1887c45473961a4ff36ae202ada1ca3d42</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>oh-yeontaek/llama-2-13B-LoRA-assemble</td>
      <td>57.91</td>
      <td>63.57</td>
      <td>83.51</td>
      <td>59.82</td>
      <td>55.96</td>
      <td>76.16</td>
      <td>8.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>8.0</td>
      <td>False</td>
      <td>85bb49d333dba4a08b051418663d16853ce30cee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aeala/Enterredaas-33b</td>
      <td>57.90</td>
      <td>60.92</td>
      <td>84.18</td>
      <td>58.30</td>
      <td>49.02</td>
      <td>78.77</td>
      <td>16.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>33.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d72dc1f05eaf1beb6373fd53fd22eb90f293a5c4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Wizard-Vicuna-30B-Uncensored-fp16</td>
      <td>57.89</td>
      <td>62.12</td>
      <td>83.45</td>
      <td>58.24</td>
      <td>50.81</td>
      <td>78.45</td>
      <td>14.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>c7b7cecb5a314fc66deebabcb67c230a3fbe84f7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/Wizard-Vicuna-30B-Uncensored</td>
      <td>57.89</td>
      <td>62.12</td>
      <td>83.45</td>
      <td>58.24</td>
      <td>50.81</td>
      <td>78.45</td>
      <td>14.25</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>6374baef4cedd41f85c111b8eec3eb38ee24c4b9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>posicube/Llama2-chat-AYT-13B</td>
      <td>57.88</td>
      <td>63.31</td>
      <td>83.53</td>
      <td>59.67</td>
      <td>55.80</td>
      <td>76.09</td>
      <td>8.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>17.0</td>
      <td>True</td>
      <td>dd12dced8076a959c03b8b5c4a4266f234d6639a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aeala/VicUnlocked-alpaca-30b</td>
      <td>57.86</td>
      <td>61.86</td>
      <td>83.79</td>
      <td>57.64</td>
      <td>51.03</td>
      <td>78.22</td>
      <td>14.63</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>7.0</td>
      <td>False</td>
      <td>c63d117d1ec5794766dd6dc5e1469769df8aba1d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/Chat-AYB-Nova-13B</td>
      <td>57.84</td>
      <td>62.97</td>
      <td>84.28</td>
      <td>58.58</td>
      <td>51.28</td>
      <td>77.58</td>
      <td>12.36</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>942af4d59533af09cf9ba13d1e369b8e871a0a4b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>souvik0306/mistral_7b_2epoch_norobots</td>
      <td>57.84</td>
      <td>61.01</td>
      <td>83.37</td>
      <td>63.96</td>
      <td>42.62</td>
      <td>79.08</td>
      <td>16.98</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>624be22cfde6797a100230ec9dc1421f52eb0aa2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>s3nh/Noromaid-Aeryth-7B</td>
      <td>57.82</td>
      <td>56.74</td>
      <td>78.62</td>
      <td>57.29</td>
      <td>65.66</td>
      <td>71.82</td>
      <td>16.76</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>e87dbfaf98d6d9422f3a16b10c8005801b28b139</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Sao10K/Stheno-v2-Delta-fp16</td>
      <td>57.81</td>
      <td>62.46</td>
      <td>83.45</td>
      <td>59.04</td>
      <td>55.25</td>
      <td>73.88</td>
      <td>12.81</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>3979769be8d92aa2dd0c7aebf385635863f16dd9</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-V2-Delta-fp16</td>
      <td>57.81</td>
      <td>62.46</td>
      <td>83.45</td>
      <td>59.04</td>
      <td>55.25</td>
      <td>73.88</td>
      <td>12.81</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>3979769be8d92aa2dd0c7aebf385635863f16dd9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/ChatAYT-Lora-Assamble-Marcoroni</td>
      <td>57.76</td>
      <td>62.46</td>
      <td>83.05</td>
      <td>58.72</td>
      <td>56.12</td>
      <td>77.35</td>
      <td>8.87</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>1.0</td>
      <td>False</td>
      <td>51c9b600023cd26c4eb3754b9a89c60dde959ccc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-llama2-13b-v8.1-fp16</td>
      <td>57.76</td>
      <td>55.97</td>
      <td>79.79</td>
      <td>54.95</td>
      <td>51.16</td>
      <td>74.35</td>
      <td>30.33</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>62.0</td>
      <td>False</td>
      <td>b51c6b29abdf7c420cb5e5f4f309ff83179c7bb8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/OpenOrcaxOpenChat-Preview2-13B</td>
      <td>57.76</td>
      <td>62.37</td>
      <td>82.96</td>
      <td>58.68</td>
      <td>51.23</td>
      <td>77.19</td>
      <td>14.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>101.0</td>
      <td>True</td>
      <td>26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewd-L2-Chat-13B</td>
      <td>57.75</td>
      <td>62.03</td>
      <td>84.19</td>
      <td>58.75</td>
      <td>52.84</td>
      <td>77.43</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>28.0</td>
      <td>True</td>
      <td>6c66622a99c1bc73498aa6a15a59da825d875310</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/Pwen-14B-Chat-20_30</td>
      <td>57.74</td>
      <td>56.14</td>
      <td>79.78</td>
      <td>60.01</td>
      <td>47.02</td>
      <td>76.48</td>
      <td>26.99</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>14.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e878e1f1f7b533c32beb8e06ebcf0cfa23f3fe9b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ai-business/Luban-13B</td>
      <td>57.73</td>
      <td>63.05</td>
      <td>82.80</td>
      <td>58.73</td>
      <td>55.53</td>
      <td>76.56</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>01b0f2046083dd8d9d8f9e626d78d83eaa1d57dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-1.2</td>
      <td>57.69</td>
      <td>64.42</td>
      <td>84.93</td>
      <td>60.35</td>
      <td>49.18</td>
      <td>77.51</td>
      <td>9.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>b3254a827fb1dfe0d4e428bf5ab1c3a2bac82d68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Alpacino30b</td>
      <td>57.67</td>
      <td>62.71</td>
      <td>85.04</td>
      <td>58.48</td>
      <td>44.23</td>
      <td>79.79</td>
      <td>15.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>67.0</td>
      <td>True</td>
      <td>300bc5f3dc129a3d17adf059394e381eff7fbd55</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qblocks/mistral_7b_DolphinCoder</td>
      <td>57.67</td>
      <td>59.73</td>
      <td>81.64</td>
      <td>59.87</td>
      <td>43.95</td>
      <td>74.59</td>
      <td>26.23</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7c05d338e0210072e13eb82b023e7747d5354c6e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Zangs3011/mistral_7b_DolphinCoder</td>
      <td>57.67</td>
      <td>59.73</td>
      <td>81.64</td>
      <td>59.87</td>
      <td>43.95</td>
      <td>74.59</td>
      <td>26.23</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>faac4b13e74395ea4b366156fd8bed15498c667c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>boomerchan/magpie-13b</td>
      <td>57.64</td>
      <td>63.31</td>
      <td>84.25</td>
      <td>58.15</td>
      <td>49.15</td>
      <td>76.48</td>
      <td>14.48</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a58124cdc9f39ccd59d4290a8bdfda93ff3690dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>l3utterfly/mistral-7b-v0.1-layla-v2</td>
      <td>57.60</td>
      <td>56.31</td>
      <td>79.76</td>
      <td>50.81</td>
      <td>51.57</td>
      <td>75.77</td>
      <td>31.39</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e0dc1ba4a198773c2d47d0c8142aef1649f8c33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Orca-2-13B-GPTQ</td>
      <td>57.60</td>
      <td>59.81</td>
      <td>79.12</td>
      <td>59.35</td>
      <td>55.14</td>
      <td>76.64</td>
      <td>15.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>other</td>
      <td>16.24</td>
      <td>23.0</td>
      <td>True</td>
      <td>2fc627e11b197c7d563eeea9c4338c2adc8e2c93</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>steve-cse/MelloGPT</td>
      <td>57.59</td>
      <td>53.84</td>
      <td>76.12</td>
      <td>55.99</td>
      <td>55.61</td>
      <td>73.88</td>
      <td>30.10</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>aedecb296e2cdcb3da95a345a794ea26f071c419</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>l3utterfly/mistral-7b-v0.1-layla-v1</td>
      <td>57.56</td>
      <td>60.15</td>
      <td>83.25</td>
      <td>60.31</td>
      <td>48.90</td>
      <td>75.93</td>
      <td>16.83</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5f06add6aa1d51d78288dbdcbd1abfd5f0ed0c84</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alignment-handbook/zephyr-7b-sft-full</td>
      <td>57.56</td>
      <td>57.68</td>
      <td>80.82</td>
      <td>60.31</td>
      <td>41.71</td>
      <td>76.09</td>
      <td>28.73</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>10.0</td>
      <td>True</td>
      <td>92f9fac4529acacb2c33a35c46917393690c6311</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/tulu-30B-fp16</td>
      <td>57.53</td>
      <td>59.98</td>
      <td>83.40</td>
      <td>56.10</td>
      <td>45.14</td>
      <td>80.82</td>
      <td>19.71</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>30.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>37c3655676c37662f60c68dacfce3f0e861be846</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>timpal0l/Mistral-7B-v0.1-flashback-v2</td>
      <td>57.53</td>
      <td>57.17</td>
      <td>80.74</td>
      <td>59.98</td>
      <td>40.66</td>
      <td>77.19</td>
      <td>29.42</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>2711647da9d8da18d746406d60ad8d806b7f1fd7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b</td>
      <td>57.52</td>
      <td>59.64</td>
      <td>82.70</td>
      <td>58.30</td>
      <td>56.00</td>
      <td>75.37</td>
      <td>13.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>31.0</td>
      <td>False</td>
      <td>4410d8a20871927e9fe981c01bc8314b451b2fcd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alignment-handbook/zephyr-7b-sft-full</td>
      <td>57.52</td>
      <td>58.11</td>
      <td>80.83</td>
      <td>60.20</td>
      <td>41.74</td>
      <td>76.24</td>
      <td>27.98</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>10.0</td>
      <td>True</td>
      <td>92f9fac4529acacb2c33a35c46917393690c6311</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jilp00/SOLAR-10.7B-tutored</td>
      <td>57.49</td>
      <td>62.29</td>
      <td>82.24</td>
      <td>65.09</td>
      <td>55.13</td>
      <td>80.19</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>40e46542b4ec136c76f61008a942000ff030cddc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-1.3</td>
      <td>57.49</td>
      <td>63.82</td>
      <td>85.09</td>
      <td>58.94</td>
      <td>45.33</td>
      <td>79.01</td>
      <td>12.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>f94e5249d2b998933466d42e08fa9551e3238205</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xriminact/TarsChattyBasev0.2</td>
      <td>57.47</td>
      <td>52.22</td>
      <td>77.78</td>
      <td>47.99</td>
      <td>43.79</td>
      <td>69.46</td>
      <td>53.60</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>91d90f5feb9c01d8279ed891c72e225356a4ca97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Unholy-v1-12L-13B</td>
      <td>57.47</td>
      <td>63.57</td>
      <td>83.75</td>
      <td>58.08</td>
      <td>51.09</td>
      <td>77.27</td>
      <td>11.07</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>34.0</td>
      <td>True</td>
      <td>ee25c078f08b0812d82597afa3f5e877c19a5c83</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-AdventurousWinds-7b</td>
      <td>57.46</td>
      <td>61.01</td>
      <td>83.47</td>
      <td>63.69</td>
      <td>42.65</td>
      <td>78.22</td>
      <td>15.69</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>8.0</td>
      <td>True</td>
      <td>ddc7e4fcbbb5c666a3fe1bbe4a47b4477151b699</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-33b-gpt4-1.3</td>
      <td>57.43</td>
      <td>63.91</td>
      <td>85.04</td>
      <td>58.53</td>
      <td>45.36</td>
      <td>78.69</td>
      <td>13.04</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>f94e5249d2b998933466d42e08fa9551e3238205</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MXLewd-L2-20B</td>
      <td>57.43</td>
      <td>63.23</td>
      <td>85.33</td>
      <td>57.36</td>
      <td>51.65</td>
      <td>76.09</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>14.0</td>
      <td>True</td>
      <td>ac279478abd9ddb8d1f5adcc548be0287b963adf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-llama2-luban-orca-platypus-13b</td>
      <td>57.42</td>
      <td>62.54</td>
      <td>82.76</td>
      <td>59.23</td>
      <td>54.66</td>
      <td>77.11</td>
      <td>8.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>False</td>
      <td>908cfb670611875b52045c4bab81cff53f0279a7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>hfl/chinese-alpaca-2-13b</td>
      <td>57.41</td>
      <td>58.70</td>
      <td>79.76</td>
      <td>55.12</td>
      <td>50.22</td>
      <td>75.61</td>
      <td>25.02</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>77.0</td>
      <td>True</td>
      <td>3b2e3895ff83c8892ab20fb8f98754d947879186</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SkunkworksAI/Mistralic-7B-1</td>
      <td>57.40</td>
      <td>60.84</td>
      <td>82.29</td>
      <td>60.80</td>
      <td>52.38</td>
      <td>77.03</td>
      <td>11.07</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>ebf138de4fb7a57f0d187ad0ab43abd6b35bfb62</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chargoddard/llama-polyglot-13b</td>
      <td>57.36</td>
      <td>59.81</td>
      <td>81.27</td>
      <td>55.04</td>
      <td>48.71</td>
      <td>76.72</td>
      <td>22.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>7a08a96118aa86e0405a5f980d7e40dadf86e1be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>allenai/digital-socrates-13b</td>
      <td>57.34</td>
      <td>58.36</td>
      <td>80.14</td>
      <td>57.01</td>
      <td>44.47</td>
      <td>74.59</td>
      <td>29.49</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>c738ee4bb61e67eebb9d196c440dcb2d99e5f906</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/VicUnlocked-30B-LoRA-HF</td>
      <td>57.33</td>
      <td>59.73</td>
      <td>84.02</td>
      <td>57.81</td>
      <td>48.54</td>
      <td>79.48</td>
      <td>14.40</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>30.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>3259cb3c2a10cfb429fb51c4a76fffa049f4c44d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>jondurbin/airoboros-33b-gpt4</td>
      <td>57.32</td>
      <td>63.74</td>
      <td>84.87</td>
      <td>58.54</td>
      <td>47.06</td>
      <td>77.03</td>
      <td>12.66</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>5b6bd680b1c008e52521dc8c663dbc87820da3d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/BrainDerp2</td>
      <td>57.32</td>
      <td>60.92</td>
      <td>81.94</td>
      <td>58.90</td>
      <td>57.19</td>
      <td>75.93</td>
      <td>9.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>948ee7af94a8b092807df4becfc0a8c1cd042878</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>57.31</td>
      <td>62.37</td>
      <td>82.99</td>
      <td>59.38</td>
      <td>52.20</td>
      <td>75.77</td>
      <td>11.14</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>39ae03b77b4f1d453b02468ce6bb4ddeb6526b77</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>adonlee/LLaMA_2_13B_SFT_v0</td>
      <td>57.31</td>
      <td>62.03</td>
      <td>83.80</td>
      <td>58.39</td>
      <td>49.92</td>
      <td>77.27</td>
      <td>12.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6790d83337578f38d2bcd51038a779eaa8d0fac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>codellama/CodeLlama-34b-Instruct-hf</td>
      <td>57.29</td>
      <td>54.27</td>
      <td>76.92</td>
      <td>55.54</td>
      <td>44.44</td>
      <td>74.59</td>
      <td>37.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>238.0</td>
      <td>True</td>
      <td>bf5e5060fa30f33149efe84bbcc682001a00ab94</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/OpenOrca-Platypus2-13B</td>
      <td>57.28</td>
      <td>62.80</td>
      <td>83.15</td>
      <td>59.39</td>
      <td>53.08</td>
      <td>76.24</td>
      <td>9.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>222.0</td>
      <td>True</td>
      <td>e7a40134f7eb687c6ab66d445dc7251257f8d391</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/2x-LoRA-Assemble-Nova-13B</td>
      <td>57.26</td>
      <td>62.63</td>
      <td>83.24</td>
      <td>58.64</td>
      <td>51.88</td>
      <td>76.95</td>
      <td>10.24</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2a344b91b28ce4d0bd48b9b5a6cc87b71123eab5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewd-ReMM-L2-Chat-20B-Inverted</td>
      <td>57.25</td>
      <td>61.69</td>
      <td>85.32</td>
      <td>58.00</td>
      <td>53.77</td>
      <td>75.61</td>
      <td>9.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>3.0</td>
      <td>True</td>
      <td>b5b501b4d23ec7ab24b827f79e48b2c67e548ddb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>abacusai/Giraffe-13b-32k-v3</td>
      <td>57.24</td>
      <td>59.04</td>
      <td>79.59</td>
      <td>55.01</td>
      <td>46.68</td>
      <td>76.95</td>
      <td>26.16</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>bbc483fc0a3b88740fd6e04a7fd0c7d98b85cd1d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_v3_13b</td>
      <td>57.24</td>
      <td>63.14</td>
      <td>82.35</td>
      <td>56.52</td>
      <td>51.81</td>
      <td>76.48</td>
      <td>13.12</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>99904e4119575f2c1606ca1e31d288f38a9f20b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pankajmathur/orca_mini_v3_13b</td>
      <td>57.24</td>
      <td>63.14</td>
      <td>82.35</td>
      <td>56.52</td>
      <td>51.81</td>
      <td>76.48</td>
      <td>13.12</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>31.0</td>
      <td>True</td>
      <td>72eec98f68d240a71d3da8a266917b6e754ae831</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewd-Chat-v2-13B</td>
      <td>57.23</td>
      <td>61.86</td>
      <td>83.81</td>
      <td>57.00</td>
      <td>54.51</td>
      <td>75.77</td>
      <td>10.46</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>f6181961a6a2f9ca534e1a8907b4a4459be6b6bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>IkariDev/Athena-v4</td>
      <td>57.23</td>
      <td>62.54</td>
      <td>84.19</td>
      <td>57.33</td>
      <td>50.87</td>
      <td>76.48</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>14.0</td>
      <td>True</td>
      <td>dde640538a44a08f6f456a2b7634e31a5d7a1245</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/LLaMA2-13B-Estopia</td>
      <td>57.21</td>
      <td>62.29</td>
      <td>82.51</td>
      <td>55.12</td>
      <td>54.14</td>
      <td>75.77</td>
      <td>13.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>cfbf7f1372454aefb45d27504b11431828ad14f8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-llama2-hermes-orca-platypus-13b</td>
      <td>57.17</td>
      <td>60.92</td>
      <td>83.50</td>
      <td>59.39</td>
      <td>54.29</td>
      <td>75.22</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>False</td>
      <td>f227ad33b16726b099e35e5dc47f4db1f22665a7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-33b-2.1</td>
      <td>57.16</td>
      <td>63.65</td>
      <td>84.97</td>
      <td>57.37</td>
      <td>52.17</td>
      <td>78.22</td>
      <td>6.60</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>12ccd0e6c9ef12c7d3c2eab8266cd32c0b2f7683</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-m2.0</td>
      <td>57.16</td>
      <td>64.68</td>
      <td>84.95</td>
      <td>57.77</td>
      <td>47.44</td>
      <td>77.74</td>
      <td>10.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>57bd88e24d603dc4bbe4016ed0871db7c0e529d5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/BrainDerp3</td>
      <td>57.13</td>
      <td>60.92</td>
      <td>82.10</td>
      <td>58.91</td>
      <td>57.18</td>
      <td>75.61</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>0b575b9245406cca92942ce2ababb5b868109bed</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Doctor-Shotgun/CalliopeDS-v2-L2-13B</td>
      <td>57.12</td>
      <td>62.80</td>
      <td>84.14</td>
      <td>56.14</td>
      <td>51.06</td>
      <td>76.01</td>
      <td>12.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>e63d24870c840d47e82b029e7f405baa10ad9ea4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gradientputri/MegaMix-S1-13B</td>
      <td>57.12</td>
      <td>62.46</td>
      <td>83.65</td>
      <td>57.88</td>
      <td>44.52</td>
      <td>75.85</td>
      <td>18.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>afca2c9488cf8738faec4db6721f6a4c755a5d81</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/BrainDerp</td>
      <td>57.11</td>
      <td>60.75</td>
      <td>82.10</td>
      <td>58.81</td>
      <td>56.90</td>
      <td>75.85</td>
      <td>8.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba21a7ed5458b3fa2b05ce6aab431acd1f857516</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-v2.2-L2-13B</td>
      <td>57.10</td>
      <td>61.26</td>
      <td>84.16</td>
      <td>56.22</td>
      <td>51.35</td>
      <td>75.61</td>
      <td>14.03</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>d55031fbcd41d749bc0c0ffbcd85636718d373b6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>IkariDev/Athena-v3</td>
      <td>57.09</td>
      <td>61.69</td>
      <td>84.34</td>
      <td>57.87</td>
      <td>51.26</td>
      <td>75.77</td>
      <td>11.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>5e4024b6694bb13f1a81ce4277ac9141f0b226df</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/vicuna-33b-coder</td>
      <td>57.07</td>
      <td>60.41</td>
      <td>83.27</td>
      <td>57.17</td>
      <td>51.79</td>
      <td>76.87</td>
      <td>12.89</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>33.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>67f6e669d7a15c1104a1478057f3752a503e83c0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Emerhyst-20B</td>
      <td>57.07</td>
      <td>61.69</td>
      <td>84.98</td>
      <td>56.98</td>
      <td>54.16</td>
      <td>76.09</td>
      <td>8.49</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>29.0</td>
      <td>True</td>
      <td>e4c23af4f5dd88cb27d245e2bfc3b81db652632c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stabilityai/StableBeluga-13B</td>
      <td>57.05</td>
      <td>62.03</td>
      <td>82.27</td>
      <td>57.71</td>
      <td>49.61</td>
      <td>76.87</td>
      <td>13.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>113.0</td>
      <td>False</td>
      <td>1d6eef4cc2b73f39600a568803ad8183f2da4514</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>circulus/Llama-2-13b-orca-v1</td>
      <td>57.05</td>
      <td>62.03</td>
      <td>82.27</td>
      <td>57.71</td>
      <td>49.61</td>
      <td>76.87</td>
      <td>13.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>e77ec90f432bdffa210a0e4310d117e5d1c662df</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-m2.0</td>
      <td>57.03</td>
      <td>63.40</td>
      <td>85.19</td>
      <td>57.46</td>
      <td>48.15</td>
      <td>78.37</td>
      <td>9.63</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>84a89dee5bf3447079f115a3ef4d58ef8f924798</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/zephyr-beta-Nebula-v2-7B</td>
      <td>57.03</td>
      <td>56.57</td>
      <td>82.53</td>
      <td>56.40</td>
      <td>58.68</td>
      <td>70.48</td>
      <td>17.51</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>226caedb50a12730232c1f8fe9c96b6dcf818ba7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-2.0</td>
      <td>57.02</td>
      <td>63.91</td>
      <td>85.67</td>
      <td>57.95</td>
      <td>45.54</td>
      <td>77.98</td>
      <td>11.07</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>a4e1b721add286900c5a6f529c3d7a3e0049b2e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MisterRid/wendigo-14b-alpha2</td>
      <td>57.02</td>
      <td>56.66</td>
      <td>77.19</td>
      <td>58.00</td>
      <td>53.71</td>
      <td>73.64</td>
      <td>22.90</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>f8332eddcb7f8ab2b5195486d4b508c4628992f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WebraftAI/synapsellm-7b-mistral-v0.3-preview</td>
      <td>57.01</td>
      <td>53.84</td>
      <td>74.86</td>
      <td>54.81</td>
      <td>55.03</td>
      <td>74.59</td>
      <td>28.96</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>4e509275c5e51bee6e82c2c15082a6cc50d87b5b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MisterRid/wendigo-14b-alpha1</td>
      <td>57.01</td>
      <td>56.48</td>
      <td>77.20</td>
      <td>57.83</td>
      <td>53.76</td>
      <td>73.01</td>
      <td>23.81</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>0024ee75d8ed5d9373ff42df72c21f3217ba9d2e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-2.0</td>
      <td>57.01</td>
      <td>63.82</td>
      <td>85.65</td>
      <td>58.44</td>
      <td>45.57</td>
      <td>77.90</td>
      <td>10.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>ddc598f492f5098a8e308f51a82834f98f29a4ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yulan-team/YuLan-Chat-2-13b-fp16</td>
      <td>57.01</td>
      <td>59.04</td>
      <td>80.66</td>
      <td>56.72</td>
      <td>52.18</td>
      <td>79.64</td>
      <td>13.80</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>2d439187efd6edd91a0c0146f08dff52d92aa7bc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-v2-L2-13B</td>
      <td>56.99</td>
      <td>61.95</td>
      <td>84.00</td>
      <td>56.14</td>
      <td>50.81</td>
      <td>75.85</td>
      <td>13.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>bc42c77f88482c37c72c85c66135e99972bbca1b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/OpenOrca-Platypus2-13B-GPTQ</td>
      <td>56.98</td>
      <td>62.54</td>
      <td>82.67</td>
      <td>58.56</td>
      <td>51.93</td>
      <td>76.80</td>
      <td>9.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>16.24</td>
      <td>48.0</td>
      <td>True</td>
      <td>0fa9a56066656fbc94e3ec088bc900fd1d4d38e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/GenAI-Nova-13B</td>
      <td>56.98</td>
      <td>62.29</td>
      <td>83.27</td>
      <td>59.47</td>
      <td>51.79</td>
      <td>77.35</td>
      <td>7.73</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ce62a64ca53cd5feb18f523a96dd3be86e6513d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-33b-gpt4-m2.0</td>
      <td>56.97</td>
      <td>63.14</td>
      <td>85.19</td>
      <td>57.28</td>
      <td>48.07</td>
      <td>78.45</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>33.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>96af3dc6c9f2248d964cf14cef6e5f2e5894583a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>huggyllama/llama-30b</td>
      <td>56.96</td>
      <td>61.43</td>
      <td>84.73</td>
      <td>58.45</td>
      <td>42.27</td>
      <td>80.03</td>
      <td>14.86</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>32.53</td>
      <td>41.0</td>
      <td>True</td>
      <td>2b1edcdb3c7ced7bce6c1aa75c94545777c3118b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>huggingface/llama-30b</td>
      <td>56.94</td>
      <td>61.26</td>
      <td>84.73</td>
      <td>58.47</td>
      <td>42.27</td>
      <td>80.03</td>
      <td>14.86</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>32.53</td>
      <td>0.0</td>
      <td>False</td>
      <td>13c77caa472bfa79d4f3f0ec82cbdc9dd88e5d22</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yhyu13/llama-30B-hf-openassitant</td>
      <td>56.94</td>
      <td>61.26</td>
      <td>84.73</td>
      <td>58.47</td>
      <td>42.27</td>
      <td>80.03</td>
      <td>14.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>30.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>fba493af11a73cf5a2ee7857dd7aecb98c659dc4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/UndiMix-v4-13B</td>
      <td>56.93</td>
      <td>61.95</td>
      <td>83.88</td>
      <td>56.90</td>
      <td>48.96</td>
      <td>76.16</td>
      <td>13.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>6dd97c74cfe1d22432d5c993814e230f333ba401</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>andreaskoepf/llama2-13b-megacode2_min100</td>
      <td>56.92</td>
      <td>60.58</td>
      <td>81.26</td>
      <td>57.92</td>
      <td>48.89</td>
      <td>76.95</td>
      <td>15.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>b38d1b53c358a0313c69bcceebe97628327ada82</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>rombodawg/LosslessMegaCoder-llama2-13b-mini</td>
      <td>56.92</td>
      <td>60.58</td>
      <td>81.26</td>
      <td>57.92</td>
      <td>48.89</td>
      <td>76.95</td>
      <td>15.92</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>1f5609ffd40bc3af2dcbc5c88e9312d47a73c4b4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>circulus/Llama-2-13b-orca-v1</td>
      <td>56.91</td>
      <td>62.20</td>
      <td>82.32</td>
      <td>57.67</td>
      <td>49.60</td>
      <td>76.80</td>
      <td>12.89</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>e77ec90f432bdffa210a0e4310d117e5d1c662df</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Emerald-13B</td>
      <td>56.89</td>
      <td>62.29</td>
      <td>83.69</td>
      <td>55.70</td>
      <td>50.94</td>
      <td>75.93</td>
      <td>12.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>f7696299463d8ec402a4e1eb001f3a447f1c5552</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-Mistral-13B</td>
      <td>56.89</td>
      <td>62.20</td>
      <td>83.82</td>
      <td>55.43</td>
      <td>53.32</td>
      <td>74.51</td>
      <td>12.05</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5ef9385d9430a81778183d71b58eb2b869d6a7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/Instruct_Yi-6B_Dolly15K</td>
      <td>56.85</td>
      <td>54.86</td>
      <td>75.87</td>
      <td>63.37</td>
      <td>42.84</td>
      <td>74.90</td>
      <td>29.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>2c0644cf206bdc94f5e6db2aca63129af0fa4a45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ</td>
      <td>56.84</td>
      <td>61.26</td>
      <td>82.14</td>
      <td>57.85</td>
      <td>50.22</td>
      <td>77.11</td>
      <td>12.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>llama2</td>
      <td>16.24</td>
      <td>20.0</td>
      <td>True</td>
      <td>ec9eb4f471b5bb6a7e5e505369628586c0c72252</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dsvv-cair/alpaca-cleaned-llama-30b-bf16</td>
      <td>56.82</td>
      <td>61.77</td>
      <td>85.06</td>
      <td>57.52</td>
      <td>51.49</td>
      <td>77.35</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>30.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>2424b6346e9e8fd749b9a6734f5d7125b5926daf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TIGER-Lab/TIGERScore-13B</td>
      <td>56.79</td>
      <td>59.04</td>
      <td>82.79</td>
      <td>55.07</td>
      <td>40.38</td>
      <td>74.74</td>
      <td>28.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.02</td>
      <td>8.0</td>
      <td>True</td>
      <td>4a71ce15f9af6fd25b0cde1612e56a7ee589c3e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Locutusque/Orca-2-13b-SFT_v5</td>
      <td>56.77</td>
      <td>59.22</td>
      <td>80.09</td>
      <td>60.19</td>
      <td>51.84</td>
      <td>80.90</td>
      <td>8.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>3c1b86e1a4e89119e373198ff018838988cc74d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>01-ai/Yi-6B-200K</td>
      <td>56.76</td>
      <td>53.75</td>
      <td>75.57</td>
      <td>64.65</td>
      <td>41.56</td>
      <td>73.64</td>
      <td>31.39</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>146.0</td>
      <td>True</td>
      <td>6cb672ed8441c35d043dd3cda448466daa3b38b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3-v3</td>
      <td>56.74</td>
      <td>62.54</td>
      <td>82.10</td>
      <td>58.67</td>
      <td>46.96</td>
      <td>77.82</td>
      <td>12.36</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>17493c1f2e4620a44d7947edad0386d338e805ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Orca-Nova-13B</td>
      <td>56.72</td>
      <td>62.37</td>
      <td>82.47</td>
      <td>57.44</td>
      <td>45.97</td>
      <td>77.58</td>
      <td>14.48</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a6c3686749ecb76971a915403da8c07a98078a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-v2.1-L2-13B</td>
      <td>56.71</td>
      <td>61.43</td>
      <td>83.92</td>
      <td>55.95</td>
      <td>50.30</td>
      <td>75.93</td>
      <td>12.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>e6b5ac97f74355cb281a621261debe5720fb4da2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/OpenOrcaxOpenChat-Preview2-13B</td>
      <td>56.70</td>
      <td>62.71</td>
      <td>81.99</td>
      <td>57.51</td>
      <td>47.45</td>
      <td>76.80</td>
      <td>13.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>101.0</td>
      <td>True</td>
      <td>26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>01-ai/Yi-6B-200K</td>
      <td>56.69</td>
      <td>53.58</td>
      <td>75.58</td>
      <td>64.65</td>
      <td>41.74</td>
      <td>74.27</td>
      <td>30.33</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>146.0</td>
      <td>True</td>
      <td>6cb672ed8441c35d043dd3cda448466daa3b38b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vicgalle/SOLAR-13B-Instruct-v1.0</td>
      <td>56.65</td>
      <td>57.25</td>
      <td>78.03</td>
      <td>55.75</td>
      <td>61.99</td>
      <td>70.24</td>
      <td>16.60</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>12.48</td>
      <td>1.0</td>
      <td>True</td>
      <td>9608d346324d603a67e7cb52a9ebe8cb1ed9e42f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3</td>
      <td>56.65</td>
      <td>62.12</td>
      <td>82.10</td>
      <td>58.84</td>
      <td>47.88</td>
      <td>77.11</td>
      <td>11.83</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>5ca46029dd22c007d4dc1706f6284a32be4546c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/storytime-13b</td>
      <td>56.64</td>
      <td>62.03</td>
      <td>83.96</td>
      <td>57.48</td>
      <td>52.50</td>
      <td>75.53</td>
      <td>8.34</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>16.0</td>
      <td>True</td>
      <td>233568319a636b6a7b02a4def2c51d08a3e0fbfc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ZoidBB/unraveled-7b-a1</td>
      <td>56.63</td>
      <td>59.81</td>
      <td>82.80</td>
      <td>63.39</td>
      <td>42.23</td>
      <td>77.19</td>
      <td>14.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>fac05775fa8121b58cda8031b7001323bd43983d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/duplicitous-slurpbeast-13b</td>
      <td>56.62</td>
      <td>62.12</td>
      <td>83.92</td>
      <td>57.53</td>
      <td>52.33</td>
      <td>75.06</td>
      <td>8.79</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>88dc61b7afebf2220ca42898e1286c59961ed440</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Amethyst-13B</td>
      <td>56.62</td>
      <td>62.63</td>
      <td>83.17</td>
      <td>55.91</td>
      <td>52.43</td>
      <td>74.74</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>d4a85b1006f0b9439e64f0e7400533a7b867c24d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Amethyst-13B-Mistral</td>
      <td>56.62</td>
      <td>62.63</td>
      <td>83.17</td>
      <td>55.91</td>
      <td>52.43</td>
      <td>74.74</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>4328809e568f01e3f0a05764e3bb58e901310415</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BELLE-2/BELLE-Llama2-13B-chat-0.4M</td>
      <td>56.62</td>
      <td>60.67</td>
      <td>82.31</td>
      <td>55.94</td>
      <td>50.85</td>
      <td>75.53</td>
      <td>14.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>29.0</td>
      <td>True</td>
      <td>1776feacbf1052cff02eb3d7531a854555d3f6dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Clover3-17B</td>
      <td>56.61</td>
      <td>59.90</td>
      <td>81.18</td>
      <td>60.47</td>
      <td>40.72</td>
      <td>78.61</td>
      <td>18.80</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>16.84</td>
      <td>10.0</td>
      <td>True</td>
      <td>428f6f58869426baae2c49442b207a15bc2da3cc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elinas/chronos-33b</td>
      <td>56.59</td>
      <td>62.20</td>
      <td>83.48</td>
      <td>55.87</td>
      <td>46.67</td>
      <td>78.30</td>
      <td>13.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>33.00</td>
      <td>23.0</td>
      <td>True</td>
      <td>3c11f81d9180618f13777276b1eb0eb70ab99cf0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/LlongOrca-13B-16k</td>
      <td>56.59</td>
      <td>62.46</td>
      <td>82.75</td>
      <td>55.54</td>
      <td>50.11</td>
      <td>76.40</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>8ea1fb205553cadbc90069d80a7e58281b6281c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/llama2-13b-megacode2-oasst</td>
      <td>56.59</td>
      <td>60.67</td>
      <td>81.93</td>
      <td>57.38</td>
      <td>47.85</td>
      <td>76.16</td>
      <td>15.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>2c45ecf161da2ff2aa984900f2e4d2b7a7311ab8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Brouz/Slerpeno</td>
      <td>56.59</td>
      <td>61.69</td>
      <td>84.10</td>
      <td>56.77</td>
      <td>48.05</td>
      <td>76.40</td>
      <td>12.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>False</td>
      <td>7ff32abd17851a769a031659e91e660f219be363</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/NyakuraV2.1-m7</td>
      <td>56.57</td>
      <td>58.62</td>
      <td>81.89</td>
      <td>58.46</td>
      <td>45.01</td>
      <td>72.77</td>
      <td>22.67</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>0a1cd69beed347cd80a290ce5b568c03264ec595</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/duplicitous-mammal-13b</td>
      <td>56.57</td>
      <td>61.69</td>
      <td>83.79</td>
      <td>57.50</td>
      <td>52.27</td>
      <td>75.06</td>
      <td>9.10</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>a05d0562b8da2ac2e76aa65984e8063249bc85c8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/OpenRP-13B</td>
      <td>56.57</td>
      <td>62.12</td>
      <td>82.60</td>
      <td>57.50</td>
      <td>48.29</td>
      <td>76.01</td>
      <td>12.89</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>d11815287c51ef51485fb003f8f72773cf6f19a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MM-ReMM-L2-20B</td>
      <td>56.55</td>
      <td>60.84</td>
      <td>85.18</td>
      <td>56.45</td>
      <td>53.33</td>
      <td>75.77</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>2.0</td>
      <td>True</td>
      <td>37869800c15fb37d017ea83bb50fec6d6141f6ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sauce1337/BerrySauce-L2-13b</td>
      <td>56.55</td>
      <td>62.29</td>
      <td>83.78</td>
      <td>57.10</td>
      <td>48.30</td>
      <td>76.09</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>c8788874b78c84bc5593586d16fbd8ae7b5b2991</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bn22/DolphinMini-Mistral-7B</td>
      <td>56.53</td>
      <td>61.18</td>
      <td>84.25</td>
      <td>61.94</td>
      <td>52.34</td>
      <td>79.32</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>16ddf12ee58e71664f7e76551294ba54794c7903</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewdBoros-L2-13B</td>
      <td>56.51</td>
      <td>62.54</td>
      <td>83.90</td>
      <td>56.57</td>
      <td>48.14</td>
      <td>76.95</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>18.0</td>
      <td>True</td>
      <td>a3033ac5825662f1c66418d7543648dc76980185</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3-v4</td>
      <td>56.49</td>
      <td>61.43</td>
      <td>81.84</td>
      <td>59.02</td>
      <td>48.64</td>
      <td>77.19</td>
      <td>10.84</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>3aa9abe9cb2e5c699f80935e04fbb351cdfbf21b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/EnsembleV5-Nova-13B</td>
      <td>56.49</td>
      <td>62.71</td>
      <td>82.55</td>
      <td>56.79</td>
      <td>49.86</td>
      <td>76.24</td>
      <td>10.77</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>3e25556187ba576082a85c270d2d4b4ea6ea9f6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/EnsembleV5-Nova-13B</td>
      <td>56.49</td>
      <td>62.71</td>
      <td>82.55</td>
      <td>56.79</td>
      <td>49.86</td>
      <td>76.24</td>
      <td>10.77</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>7ba38d309709d35149b4a18f94096875885035ae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/mythalion-13b</td>
      <td>56.48</td>
      <td>61.26</td>
      <td>83.81</td>
      <td>56.53</td>
      <td>46.56</td>
      <td>77.43</td>
      <td>13.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>106.0</td>
      <td>True</td>
      <td>24916f62b8243a7e4646ea53eeb45d890cbd308f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jingyeom/SOLAR_KO_1.3_deup</td>
      <td>56.47</td>
      <td>55.97</td>
      <td>79.97</td>
      <td>55.88</td>
      <td>47.55</td>
      <td>76.87</td>
      <td>22.59</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>3218e4304fe55ec950347c96018f14f60baca25d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-code-mistral-7b-v2.0</td>
      <td>56.47</td>
      <td>52.47</td>
      <td>75.61</td>
      <td>51.31</td>
      <td>52.05</td>
      <td>71.43</td>
      <td>35.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>8371b49e786758da62de015daa006c0e58b7ce82</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/WizardLM-30B-Uncensored</td>
      <td>56.46</td>
      <td>60.24</td>
      <td>82.93</td>
      <td>56.80</td>
      <td>51.57</td>
      <td>74.35</td>
      <td>12.89</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>761783745fcb97831ad8035d3cbd5de484aca3ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SciPhi/SciPhi-Self-RAG-Mistral-7B-32k</td>
      <td>56.46</td>
      <td>57.34</td>
      <td>80.44</td>
      <td>60.81</td>
      <td>45.63</td>
      <td>74.82</td>
      <td>19.71</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>64.0</td>
      <td>True</td>
      <td>640192e2ba5898f87c407a9f771fc270f7628dee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>royallab/Pygmalion-2-13b-SuperCOT</td>
      <td>56.46</td>
      <td>63.23</td>
      <td>83.68</td>
      <td>54.90</td>
      <td>53.14</td>
      <td>77.51</td>
      <td>6.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>7.0</td>
      <td>True</td>
      <td>763b3fd5afc3e7fb6c7c8768d40f06901c8d5913</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jefferylovely/AiMaven-Orca2</td>
      <td>56.45</td>
      <td>54.69</td>
      <td>79.00</td>
      <td>54.61</td>
      <td>53.43</td>
      <td>74.35</td>
      <td>22.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fbba65dad747e1461c2b024fe6cc690a3b20db24</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-Inverted-L2-13B</td>
      <td>56.44</td>
      <td>59.30</td>
      <td>82.90</td>
      <td>56.45</td>
      <td>52.04</td>
      <td>74.74</td>
      <td>13.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>efaf592c95ae8e769e0d56d36ba4ed23e3bf4059</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Nova-13B</td>
      <td>56.44</td>
      <td>62.71</td>
      <td>82.57</td>
      <td>57.98</td>
      <td>51.34</td>
      <td>77.27</td>
      <td>6.75</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ae1145f9fa846ab8d39d8b7da888287ef917efb5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-L2-13B</td>
      <td>56.43</td>
      <td>61.01</td>
      <td>83.95</td>
      <td>56.33</td>
      <td>50.18</td>
      <td>75.14</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>c4e7b771e30fdbfd6bd2e66a6928024bd5692bbd</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Mythical-Destroyer-L2-13B</td>
      <td>56.39</td>
      <td>58.70</td>
      <td>82.00</td>
      <td>57.66</td>
      <td>56.35</td>
      <td>74.66</td>
      <td>8.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>7c87376b201b1c30c4e12c0b7bc2f28f017ce7bc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yeen214/llama2_7b_merge_orcafamily</td>
      <td>56.38</td>
      <td>56.91</td>
      <td>81.17</td>
      <td>51.49</td>
      <td>49.68</td>
      <td>75.93</td>
      <td>23.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>fb65f697de632f2f3fef57fc3cd12fb5e4913a89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-AdventurousWinds-Mk2-7b</td>
      <td>56.38</td>
      <td>58.19</td>
      <td>83.48</td>
      <td>61.80</td>
      <td>43.56</td>
      <td>76.32</td>
      <td>14.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>10.0</td>
      <td>True</td>
      <td>cfcc969a7e97275b2298253f1eabf4575e5a3768</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewd-v2.4-13B</td>
      <td>56.37</td>
      <td>61.69</td>
      <td>83.83</td>
      <td>55.10</td>
      <td>53.34</td>
      <td>74.51</td>
      <td>9.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>6f6ec6024ee054020e49fd96f149919692848f0b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-code-mistral-7b-v2.0</td>
      <td>56.37</td>
      <td>52.30</td>
      <td>75.61</td>
      <td>51.28</td>
      <td>52.05</td>
      <td>71.35</td>
      <td>35.63</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>8371b49e786758da62de015daa006c0e58b7ce82</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-l2-13b-2.2.1</td>
      <td>56.36</td>
      <td>60.92</td>
      <td>83.77</td>
      <td>56.47</td>
      <td>49.42</td>
      <td>76.01</td>
      <td>11.60</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>9b2dbc1f6f17a162228799df6e9449c903ddf04d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v3.1</td>
      <td>56.36</td>
      <td>59.81</td>
      <td>82.80</td>
      <td>56.76</td>
      <td>44.45</td>
      <td>76.24</td>
      <td>18.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>a95be7130d32da99bcd484f6f436b2dd49341110</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Doctor-Shotgun/CalliopeDS-L2-13B</td>
      <td>56.34</td>
      <td>60.49</td>
      <td>83.38</td>
      <td>55.80</td>
      <td>51.32</td>
      <td>77.03</td>
      <td>10.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>agpl-3.0</td>
      <td>13.02</td>
      <td>7.0</td>
      <td>True</td>
      <td>b373eda586a6527e62382eda5480204652a82499</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>iGenius-AI-Team/LLAMA-13B-test-finetuning</td>
      <td>56.34</td>
      <td>58.02</td>
      <td>82.36</td>
      <td>54.27</td>
      <td>44.14</td>
      <td>76.72</td>
      <td>22.52</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5bd0eb026b12c59fd198f307c0c17188af69744c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Gryphe/MythoMix-L2-13b</td>
      <td>56.31</td>
      <td>61.09</td>
      <td>83.86</td>
      <td>55.42</td>
      <td>52.08</td>
      <td>75.45</td>
      <td>9.93</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>eca790fb9394c9c61be27ef709080b3b92783a45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/mistral-7b_open_platypus</td>
      <td>56.29</td>
      <td>55.80</td>
      <td>82.13</td>
      <td>59.76</td>
      <td>48.87</td>
      <td>78.61</td>
      <td>12.59</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b9a60b9ad0fe06bd314ffe99d543f1df6ecd10da</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3-v2.1</td>
      <td>56.29</td>
      <td>62.29</td>
      <td>82.09</td>
      <td>57.91</td>
      <td>47.03</td>
      <td>77.43</td>
      <td>10.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>31e1e3235515717a151915131bc970be188d964e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Uncensored-Jordan-13B</td>
      <td>56.27</td>
      <td>57.42</td>
      <td>82.70</td>
      <td>55.75</td>
      <td>50.51</td>
      <td>76.16</td>
      <td>15.09</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>c56a396342133bbd75ab3f79622c85cb55be49a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-code-mistral-orca-7b-v1.0</td>
      <td>56.24</td>
      <td>59.64</td>
      <td>82.25</td>
      <td>61.33</td>
      <td>48.45</td>
      <td>77.51</td>
      <td>8.26</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>f7db67fe6c82657b35d0ffcf8b7ff1568d979482</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/StableBeluga-13B-instruct-PL-lora_unload</td>
      <td>56.24</td>
      <td>60.92</td>
      <td>82.13</td>
      <td>56.99</td>
      <td>48.64</td>
      <td>76.56</td>
      <td>12.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>6e1a6e1f91f6ac97b643be1bd24be6096e2e7dd3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>adamo1139/Yi-6B-200K-AEZAKMI-v2-rawrr1-DPO</td>
      <td>56.20</td>
      <td>52.47</td>
      <td>77.04</td>
      <td>62.57</td>
      <td>47.15</td>
      <td>71.03</td>
      <td>26.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>False</td>
      <td>9271df80f5221362cb5ffd71f463f8f8d08c31dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Gryphe/MythoLogic-L2-13b</td>
      <td>56.19</td>
      <td>61.01</td>
      <td>83.93</td>
      <td>55.70</td>
      <td>48.64</td>
      <td>76.09</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>665948fc79acc2bcce3e9e7d2b0689ca43ae62d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Synatra-11B-Testbench</td>
      <td>56.17</td>
      <td>57.34</td>
      <td>78.66</td>
      <td>55.56</td>
      <td>51.97</td>
      <td>75.77</td>
      <td>17.74</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>11.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>9399ea6c2a1d955e31d6b4d68b2b86115aea0e59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-1.2-L2-13B</td>
      <td>56.15</td>
      <td>60.75</td>
      <td>83.67</td>
      <td>56.27</td>
      <td>50.32</td>
      <td>74.98</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e76f35fe771ef142d6629092bd4a93301fd6cd4a</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Locutusque/Orca-2-13b-SFT-v6</td>
      <td>56.15</td>
      <td>60.41</td>
      <td>80.46</td>
      <td>59.51</td>
      <td>54.01</td>
      <td>77.43</td>
      <td>5.08</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>c31bf6f2d18f8fa4f6a25444ace549c4394b2b5a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-13b</td>
      <td>56.14</td>
      <td>57.00</td>
      <td>80.89</td>
      <td>54.38</td>
      <td>40.43</td>
      <td>76.87</td>
      <td>27.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>24be61d31af8ac3e8c57d924c749ca3cf5f681ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/SpeechlessV1-Nova-13B</td>
      <td>56.14</td>
      <td>61.77</td>
      <td>82.68</td>
      <td>57.75</td>
      <td>51.44</td>
      <td>77.43</td>
      <td>5.76</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fbe6f0e32b5ecf9d75510d0b11a286466f46d79e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HenryJJ/Instruct_Yi-6B_Dolly_CodeAlpaca</td>
      <td>56.11</td>
      <td>53.16</td>
      <td>75.30</td>
      <td>63.06</td>
      <td>41.42</td>
      <td>75.37</td>
      <td>28.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>97c31498b579cf4808195dd21a858a258d40b2dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WhoTookMyAmogusNickname/NewHope_HF_not_official</td>
      <td>56.11</td>
      <td>61.09</td>
      <td>84.03</td>
      <td>55.73</td>
      <td>44.96</td>
      <td>74.98</td>
      <td>15.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>f587f4a31de6818f4200d9cdc7f116ca8ba1cdc2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Austism/chronos-hermes-13b-v2</td>
      <td>56.10</td>
      <td>60.32</td>
      <td>83.21</td>
      <td>55.05</td>
      <td>50.91</td>
      <td>75.37</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>2f0e2cb734685a6ce0736a9f3e909a795d7592cc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/Nebula-7B</td>
      <td>56.10</td>
      <td>59.30</td>
      <td>83.46</td>
      <td>57.00</td>
      <td>45.56</td>
      <td>76.40</td>
      <td>14.86</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>569f848698a468fb03d37033c67f3734bbaec127</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kaist-ai/prometheus-13b-v1.0</td>
      <td>56.09</td>
      <td>53.24</td>
      <td>80.75</td>
      <td>51.49</td>
      <td>45.66</td>
      <td>73.72</td>
      <td>31.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>80.0</td>
      <td>True</td>
      <td>9088377314f91af4b48940e09a0c76d0878f5020</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augtoma/qCammel-13</td>
      <td>56.05</td>
      <td>60.84</td>
      <td>83.66</td>
      <td>56.73</td>
      <td>47.54</td>
      <td>76.16</td>
      <td>11.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>af473e64f6a4fa02a7e24ee7679eea9505eb179d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-SLERP-L2-13B</td>
      <td>56.03</td>
      <td>60.92</td>
      <td>83.56</td>
      <td>55.33</td>
      <td>51.97</td>
      <td>75.22</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>27baccf242bc1dc34fc39661a40bbf867cbea8b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>The-Face-Of-Goonery/Huginn-13b-v1.2</td>
      <td>56.03</td>
      <td>60.92</td>
      <td>83.56</td>
      <td>55.33</td>
      <td>51.97</td>
      <td>75.22</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>12.0</td>
      <td>False</td>
      <td>cb3562e7aae05a95fe61610b7b8f4957d3529ce7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/carl-33b</td>
      <td>56.03</td>
      <td>64.59</td>
      <td>85.27</td>
      <td>58.38</td>
      <td>45.32</td>
      <td>76.24</td>
      <td>6.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>33.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>5f80b372b493d901cab4490b4f23c71499023615</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WebraftAI/synapsellm-7b-mistral-v0.5-preview</td>
      <td>56.03</td>
      <td>52.73</td>
      <td>76.51</td>
      <td>54.67</td>
      <td>55.16</td>
      <td>74.35</td>
      <td>22.74</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d1b4d9a4657d145ce7cda431ed46076c1518af55</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/neural-chat-7b-v3-1-Nebula-v2-7B</td>
      <td>56.01</td>
      <td>61.77</td>
      <td>80.21</td>
      <td>59.07</td>
      <td>58.56</td>
      <td>71.82</td>
      <td>4.62</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>0b98e4ca35764da09cabcaaebbdac1f827629219</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Gryphe/MythoMax-L2-13b</td>
      <td>56.00</td>
      <td>60.92</td>
      <td>83.56</td>
      <td>55.33</td>
      <td>51.97</td>
      <td>75.22</td>
      <td>9.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>187.0</td>
      <td>True</td>
      <td>faa4ef8c87dbb00d447904ceb048d49b6a463d07</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>The-Face-Of-Goonery/huginnv1.2</td>
      <td>55.98</td>
      <td>62.37</td>
      <td>84.28</td>
      <td>57.02</td>
      <td>47.81</td>
      <td>75.22</td>
      <td>9.17</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aed4ddc951c657993939fa5b87a4088550569a3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Hermes-Llama2-13b</td>
      <td>55.97</td>
      <td>61.52</td>
      <td>83.29</td>
      <td>55.11</td>
      <td>50.38</td>
      <td>75.45</td>
      <td>10.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>13.00</td>
      <td>261.0</td>
      <td>True</td>
      <td>8f95aa9cd207db7b24179fc779c2b8973e71bee2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/Samantha-1.11-13b</td>
      <td>55.97</td>
      <td>60.84</td>
      <td>82.99</td>
      <td>55.96</td>
      <td>47.72</td>
      <td>76.01</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e355ead3a939f471fe2586201156fb972fad0f4b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/LongQLoRA-Vicuna-13b-8k</td>
      <td>55.96</td>
      <td>56.40</td>
      <td>81.05</td>
      <td>53.68</td>
      <td>47.07</td>
      <td>74.51</td>
      <td>23.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>470c90e30f9e49e948e066373c3ea6878ee5f171</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Walter-SOLAR-11B</td>
      <td>55.95</td>
      <td>60.41</td>
      <td>84.86</td>
      <td>64.99</td>
      <td>44.88</td>
      <td>79.56</td>
      <td>0.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>e7bbf8ba7572aced748c7fc7368dc024e2df7df0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Nous-Hermes-13B-Code</td>
      <td>55.93</td>
      <td>61.18</td>
      <td>83.21</td>
      <td>55.13</td>
      <td>50.56</td>
      <td>75.14</td>
      <td>10.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>5a45cb2a6442581ce32cc19c561c49cec1db4ebb</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/Chat-AYB-Platypus2-13B</td>
      <td>55.93</td>
      <td>60.49</td>
      <td>84.03</td>
      <td>57.83</td>
      <td>54.52</td>
      <td>75.77</td>
      <td>2.96</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a54eb9d5a66df4720ec52422f5627ccd94d5fd6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WebraftAI/synapsellm-7b-mistral-v0.4-preview2</td>
      <td>55.93</td>
      <td>52.99</td>
      <td>74.54</td>
      <td>54.60</td>
      <td>53.79</td>
      <td>73.95</td>
      <td>25.70</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>59e4ad04a24b656401fab0e8f20de387aaa95512</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WebraftAI/synapsellm-7b-mistral-v0.5-preview2</td>
      <td>55.93</td>
      <td>52.22</td>
      <td>75.54</td>
      <td>51.64</td>
      <td>55.47</td>
      <td>73.09</td>
      <td>27.60</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b6378fa3b7d39f946d3ce1e0b854622c2866cf7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sauce1337/AppleSauce-L2-13b</td>
      <td>55.91</td>
      <td>61.01</td>
      <td>83.61</td>
      <td>57.07</td>
      <td>47.81</td>
      <td>75.93</td>
      <td>10.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>ba253c52eb85e24987c81e5d36b5a9a00e276ce7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-13B-v1.2</td>
      <td>55.90</td>
      <td>61.26</td>
      <td>82.93</td>
      <td>56.47</td>
      <td>47.27</td>
      <td>76.48</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>60d4937ac3c4dcb84c40bbf7265c5cc7f5f3d4f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-codellama2-34b-v11.1-bf16</td>
      <td>55.88</td>
      <td>50.00</td>
      <td>71.19</td>
      <td>55.71</td>
      <td>53.01</td>
      <td>70.80</td>
      <td>34.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>34.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>1b361b3634bf59913b47c9dad1b138e99833472b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openBuddy/openbuddy-llama2-34b-v11.1-bf16</td>
      <td>55.88</td>
      <td>50.00</td>
      <td>71.19</td>
      <td>55.71</td>
      <td>53.01</td>
      <td>70.80</td>
      <td>34.57</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>33.53</td>
      <td>0.0</td>
      <td>True</td>
      <td>21ac0d26c0097e5ac5b4a757493574b156da7731</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>namirocks/tutor-model-13b-ep3</td>
      <td>55.88</td>
      <td>57.34</td>
      <td>81.51</td>
      <td>57.02</td>
      <td>52.99</td>
      <td>74.35</td>
      <td>12.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>714f04010ca1c3d72bbeead4a14695576ad36a88</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-tutor-13b-ep3</td>
      <td>55.88</td>
      <td>57.34</td>
      <td>81.51</td>
      <td>57.02</td>
      <td>52.99</td>
      <td>74.35</td>
      <td>12.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2cf2424169d31299caff38cd7ac68e69974d6535</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama2-13b-v1.2</td>
      <td>55.87</td>
      <td>60.67</td>
      <td>80.46</td>
      <td>56.51</td>
      <td>51.03</td>
      <td>74.82</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>97279d20a8c7e2d0576c9ff4b2e15a421c40d58a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/Synatra-V0.1-7B-Instruct</td>
      <td>55.86</td>
      <td>55.29</td>
      <td>76.63</td>
      <td>55.29</td>
      <td>55.76</td>
      <td>72.77</td>
      <td>19.41</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>7ee3416f31a3c7e8d5ab4295ac1b641075f36345</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>maywell/Synatra-V0.1-7B</td>
      <td>55.86</td>
      <td>55.29</td>
      <td>76.63</td>
      <td>55.29</td>
      <td>55.76</td>
      <td>72.77</td>
      <td>19.41</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>7ee3416f31a3c7e8d5ab4295ac1b641075f36345</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/7B-DPO-alpha</td>
      <td>55.84</td>
      <td>50.85</td>
      <td>73.00</td>
      <td>63.39</td>
      <td>57.58</td>
      <td>67.56</td>
      <td>22.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>wtfpl</td>
      <td>7.00</td>
      <td>54.0</td>
      <td>False</td>
      <td>36501a519950fb80c2e7df77e12c9110dca580f4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>janhq/Mistral-7B-Instruct-v0.2-DARE</td>
      <td>55.84</td>
      <td>61.95</td>
      <td>75.62</td>
      <td>49.99</td>
      <td>54.36</td>
      <td>74.98</td>
      <td>18.12</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>98731ddd2dd52fd1b2c69c4cb95bbb1ac03ce496</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Danielbrdz/Barcenas-13b</td>
      <td>55.83</td>
      <td>61.26</td>
      <td>82.13</td>
      <td>56.25</td>
      <td>46.67</td>
      <td>76.32</td>
      <td>12.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>fa988ba73f67ad0c8e7fa8f408106ea040070258</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>feidfoe/Metamath-reproduce-7b</td>
      <td>55.81</td>
      <td>47.18</td>
      <td>73.65</td>
      <td>42.94</td>
      <td>41.58</td>
      <td>71.35</td>
      <td>58.15</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>9955b88b535863a36ee9d9a255260bbc2cdab47b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-OpenOrca_5w</td>
      <td>55.80</td>
      <td>61.01</td>
      <td>82.82</td>
      <td>56.09</td>
      <td>44.87</td>
      <td>77.74</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ddd810c9150492d7318656acac44849651edbf2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Hermes-Llama2-13b</td>
      <td>55.75</td>
      <td>61.26</td>
      <td>83.26</td>
      <td>55.04</td>
      <td>50.41</td>
      <td>75.37</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>13.00</td>
      <td>261.0</td>
      <td>True</td>
      <td>8f95aa9cd207db7b24179fc779c2b8973e71bee2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>garage-bAInd/Stable-Platypus2-13B</td>
      <td>55.75</td>
      <td>62.71</td>
      <td>82.29</td>
      <td>58.30</td>
      <td>52.52</td>
      <td>76.87</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>13.02</td>
      <td>19.0</td>
      <td>True</td>
      <td>0e54aa49c24617e30a23a20c0c5da61419b9fe68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lu-vae/llama2-13B-sharegpt4-orca-openplatypus-8w</td>
      <td>55.75</td>
      <td>62.80</td>
      <td>84.04</td>
      <td>55.13</td>
      <td>45.66</td>
      <td>75.14</td>
      <td>11.75</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ad086aacf0176911133b6cccfb34364afce9de5a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PulsarAI/CollectiveCognition-v1.1-Nebula-7B</td>
      <td>55.72</td>
      <td>58.11</td>
      <td>82.39</td>
      <td>57.03</td>
      <td>53.53</td>
      <td>73.72</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c41d373a2d49b79236d6c4d0dfc4086e709c07eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v3.1</td>
      <td>55.71</td>
      <td>60.15</td>
      <td>82.84</td>
      <td>56.84</td>
      <td>44.38</td>
      <td>76.24</td>
      <td>13.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>cc708183e430234b8718c08d9f90474569eabeac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-1.1-L2-13B</td>
      <td>55.71</td>
      <td>60.75</td>
      <td>83.64</td>
      <td>56.39</td>
      <td>50.30</td>
      <td>75.22</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0f45a9f834dd216ce25ffa606b3b1ef2c99e7acd</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lu-vae/llama2-13b-sharegpt4-test</td>
      <td>55.69</td>
      <td>58.02</td>
      <td>82.65</td>
      <td>55.99</td>
      <td>48.27</td>
      <td>76.09</td>
      <td>13.12</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2be36a2dab4ed0f97727a1508367f53d59950818</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>meta-llama/Llama-2-13b-hf</td>
      <td>55.69</td>
      <td>59.39</td>
      <td>82.13</td>
      <td>55.77</td>
      <td>37.38</td>
      <td>76.64</td>
      <td>22.82</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>472.0</td>
      <td>False</td>
      <td>7da18fb10421c3ae2a1eb92815bad75e84816e35</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v3.2</td>
      <td>55.68</td>
      <td>59.64</td>
      <td>82.68</td>
      <td>56.68</td>
      <td>44.49</td>
      <td>76.95</td>
      <td>13.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>41.0</td>
      <td>True</td>
      <td>65320bf6dbe0cb4682d45a9e55dbc876502f8b66</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama2-13b</td>
      <td>55.68</td>
      <td>59.13</td>
      <td>81.99</td>
      <td>55.49</td>
      <td>51.57</td>
      <td>74.66</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>19.0</td>
      <td>False</td>
      <td>6e918dc8beb1e764def5938fdb8e3f64ba40a456</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-13b-fast</td>
      <td>55.67</td>
      <td>55.89</td>
      <td>80.73</td>
      <td>54.40</td>
      <td>40.31</td>
      <td>77.19</td>
      <td>25.47</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>f2d798d1a7dc6c254575b7a4fe24f4c76652e6d8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-hermes-coig-lite-13b</td>
      <td>55.65</td>
      <td>59.47</td>
      <td>82.28</td>
      <td>55.18</td>
      <td>47.60</td>
      <td>78.61</td>
      <td>10.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ee11d9c7acaefb723796227e2ad099b165f0dd9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/U-Amethyst-20B</td>
      <td>55.65</td>
      <td>62.20</td>
      <td>83.11</td>
      <td>55.88</td>
      <td>53.20</td>
      <td>74.19</td>
      <td>5.31</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>22.0</td>
      <td>True</td>
      <td>c0cbe0b3c88041bb6beef27dbe85146af8dddec9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Enno-Ai/ennodata-13b-8bit-raw-15epoch</td>
      <td>55.65</td>
      <td>61.60</td>
      <td>82.20</td>
      <td>57.55</td>
      <td>53.58</td>
      <td>77.51</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>ee2ceaae9cb806bc30df84ba4d598fdf32e53b17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Uncensored-Frank-13B</td>
      <td>55.64</td>
      <td>61.60</td>
      <td>82.62</td>
      <td>54.55</td>
      <td>48.34</td>
      <td>74.74</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>73a27445e5e5a72857626e551c70542ec607f60c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TFLai/Nova-13B-50-step</td>
      <td>55.61</td>
      <td>61.60</td>
      <td>82.31</td>
      <td>57.27</td>
      <td>51.53</td>
      <td>76.56</td>
      <td>4.40</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1a827ccb7f00157b3cc9ce538d61a6ba8d5a65db</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Severian/ANIMA-Phi-Neptune-Mistral-7B-v4</td>
      <td>55.61</td>
      <td>55.46</td>
      <td>77.63</td>
      <td>53.12</td>
      <td>59.01</td>
      <td>73.48</td>
      <td>14.94</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>a8e18f970f7ca994740177d6c228adee9e17aba9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>defog/sqlcoder-34b-alpha</td>
      <td>55.59</td>
      <td>54.18</td>
      <td>75.93</td>
      <td>54.42</td>
      <td>40.63</td>
      <td>73.48</td>
      <td>34.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>34.00</td>
      <td>131.0</td>
      <td>True</td>
      <td>6712da4d486caec81d6b1b650d0596517052cffe</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Stable-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>55.56</td>
      <td>62.29</td>
      <td>82.46</td>
      <td>57.09</td>
      <td>51.41</td>
      <td>76.56</td>
      <td>3.56</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>0c15b8540335b3e21a976a5fc5c33b47927fea6c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Severian/ANIMA-Phi-Neptune-Mistral-7B</td>
      <td>55.54</td>
      <td>55.97</td>
      <td>76.22</td>
      <td>52.89</td>
      <td>59.76</td>
      <td>73.48</td>
      <td>14.94</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>artistic-2.0</td>
      <td>7.00</td>
      <td>25.0</td>
      <td>True</td>
      <td>e8e9a4804c842b84def9e9aaae38236d4754f277</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>internlm/internlm-20b-chat</td>
      <td>55.53</td>
      <td>55.38</td>
      <td>78.58</td>
      <td>58.53</td>
      <td>43.22</td>
      <td>78.77</td>
      <td>18.73</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>20.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>79946225fa7a215e0ebcf4440a9cce88e475deaa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-dolphin_5w</td>
      <td>55.53</td>
      <td>60.67</td>
      <td>82.69</td>
      <td>56.23</td>
      <td>44.41</td>
      <td>77.35</td>
      <td>11.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ec406128968b41a9b7a5f18c358f7638d696b56</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-hermes-coig-lite-13b</td>
      <td>55.51</td>
      <td>59.56</td>
      <td>82.26</td>
      <td>55.30</td>
      <td>47.56</td>
      <td>78.53</td>
      <td>9.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ee11d9c7acaefb723796227e2ad099b165f0dd9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augmxnt/shisa-gamma-7b-v1</td>
      <td>55.50</td>
      <td>53.16</td>
      <td>77.30</td>
      <td>55.23</td>
      <td>50.73</td>
      <td>73.88</td>
      <td>22.74</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>5.0</td>
      <td>True</td>
      <td>49bf4a58453d191845668b8ff17e4b8f0e9ccae6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-Inverted-1.2-L2-13B</td>
      <td>55.50</td>
      <td>59.39</td>
      <td>83.01</td>
      <td>55.77</td>
      <td>51.22</td>
      <td>74.66</td>
      <td>8.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>8d2e9087093eef1c9173e167beb40b9d034a4655</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kingbri/airolima-chronos-grad-l2-13B</td>
      <td>55.50</td>
      <td>59.56</td>
      <td>83.50</td>
      <td>55.78</td>
      <td>44.67</td>
      <td>75.85</td>
      <td>13.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>d2ad57b2b50361485b2b04e59a989161599cb08b</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/UndiMix-v1-13b</td>
      <td>55.50</td>
      <td>59.47</td>
      <td>82.45</td>
      <td>55.83</td>
      <td>49.78</td>
      <td>75.45</td>
      <td>10.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fd311f52648825d6988d2f945918468ceb32289f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kingbri/chronolima-airo-grad-l2-13B</td>
      <td>55.50</td>
      <td>59.56</td>
      <td>83.47</td>
      <td>55.80</td>
      <td>44.58</td>
      <td>75.61</td>
      <td>13.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>agpl-3.0</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>9195bd6ea775daf347a275e190665e10bf1fb54b</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v3.2</td>
      <td>55.49</td>
      <td>59.47</td>
      <td>82.60</td>
      <td>56.82</td>
      <td>44.51</td>
      <td>76.09</td>
      <td>13.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>41.0</td>
      <td>True</td>
      <td>bc771c901529dedbf04864d0b81452f62301f882</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Suprit/Zhongjing-LLaMA-base</td>
      <td>55.47</td>
      <td>55.12</td>
      <td>79.72</td>
      <td>48.23</td>
      <td>48.88</td>
      <td>74.82</td>
      <td>26.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1b53d10b830b864d88032ae467016f8a1d7ba239</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-13b-v1.5</td>
      <td>55.41</td>
      <td>57.08</td>
      <td>81.24</td>
      <td>56.67</td>
      <td>51.51</td>
      <td>74.66</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>145.0</td>
      <td>True</td>
      <td>3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/model_007_13b_v2</td>
      <td>55.41</td>
      <td>61.95</td>
      <td>82.48</td>
      <td>57.32</td>
      <td>53.50</td>
      <td>75.85</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>1c959d4b5d5b8683b051f07475bb5c1ab24c8bb0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Expert68/llama2_13b_instructed_version2</td>
      <td>55.41</td>
      <td>60.07</td>
      <td>84.05</td>
      <td>55.61</td>
      <td>46.12</td>
      <td>75.61</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea321257d81e0f41c985f5155297b7fbd6ac375a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-13B</td>
      <td>55.41</td>
      <td>59.98</td>
      <td>81.86</td>
      <td>56.11</td>
      <td>47.41</td>
      <td>76.09</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>fbb23bc41438b016f1df1e9180c6c350a03557ea</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Enno-Ai/ennodata-raw-pankajmathur-13b-peft</td>
      <td>55.40</td>
      <td>61.95</td>
      <td>82.21</td>
      <td>57.44</td>
      <td>53.57</td>
      <td>75.93</td>
      <td>1.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>1.0</td>
      <td>False</td>
      <td>206553873db96a6730d36477837335dbbcc906fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple</td>
      <td>55.40</td>
      <td>59.13</td>
      <td>80.64</td>
      <td>56.12</td>
      <td>51.29</td>
      <td>74.66</td>
      <td>10.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>848ef91ab46a72260542283918a971347c6bfa93</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-orca-platypus-coig-lite-2k-0.6e-13b</td>
      <td>55.40</td>
      <td>59.90</td>
      <td>80.76</td>
      <td>58.34</td>
      <td>47.97</td>
      <td>77.90</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>65214c9923d55795ecd6e7f9e0fcee5ba5f26929</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-30b-chat</td>
      <td>55.38</td>
      <td>58.70</td>
      <td>82.54</td>
      <td>51.16</td>
      <td>52.42</td>
      <td>75.30</td>
      <td>12.13</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>30.00</td>
      <td>193.0</td>
      <td>True</td>
      <td>54f33278a04aa4e612bca482b82f801ab658e890</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>llm-agents/tora-13b-v1.0</td>
      <td>55.37</td>
      <td>58.96</td>
      <td>82.31</td>
      <td>54.59</td>
      <td>40.22</td>
      <td>75.37</td>
      <td>20.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>0636c1f582c979a5a292cc5f3dc293800b1494e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/minotaur-llama2-13b-qlora</td>
      <td>55.37</td>
      <td>60.07</td>
      <td>82.42</td>
      <td>55.87</td>
      <td>45.57</td>
      <td>76.24</td>
      <td>12.05</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>22c83f7d68e547fb0b59acfa01c60b108c59fe55</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SicariusSicariiStuff/Tinybra_13B</td>
      <td>55.36</td>
      <td>55.72</td>
      <td>80.99</td>
      <td>54.37</td>
      <td>49.14</td>
      <td>73.80</td>
      <td>18.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fa81ddf8b87ec339b2519044c4271bc59c4b65aa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gaodrew/gaodrew-gorgonzola-13b</td>
      <td>55.35</td>
      <td>53.84</td>
      <td>78.86</td>
      <td>71.54</td>
      <td>42.58</td>
      <td>75.30</td>
      <td>10.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>a53fbe358d4cb546916847d861ccfaf7c724a103</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Luban-Platypus2-13B-QLora-0.80-epoch</td>
      <td>55.34</td>
      <td>60.24</td>
      <td>82.22</td>
      <td>58.03</td>
      <td>55.26</td>
      <td>75.37</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>15a99bc147cf9b744cbab7a7c8c5f232cd0c8d10</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/SthenoWriter-L2-13B</td>
      <td>55.33</td>
      <td>62.29</td>
      <td>83.28</td>
      <td>56.14</td>
      <td>44.72</td>
      <td>74.35</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6d9e26ab765eb170cc0aa428ee5e25b08524657</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>minlik/chinese-alpaca-33b-merged</td>
      <td>55.33</td>
      <td>59.30</td>
      <td>78.43</td>
      <td>57.69</td>
      <td>52.45</td>
      <td>76.09</td>
      <td>8.04</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>33.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>fc2535104c0b48afc42575f9fe10bbcbb7612ec3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PulsarAI/2x-LoRA-Assemble-Platypus2-13B</td>
      <td>55.33</td>
      <td>60.58</td>
      <td>82.56</td>
      <td>58.25</td>
      <td>54.77</td>
      <td>74.90</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>f147bf8428c174d1dc0332da626d4b039690ceab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/llama-2-13b-Guanaco-QLoRA</td>
      <td>55.31</td>
      <td>61.09</td>
      <td>82.99</td>
      <td>55.47</td>
      <td>44.12</td>
      <td>77.19</td>
      <td>10.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>67e68284234538d3851d5c0c334383daffec57a2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/mistral-se-inst-ppo</td>
      <td>55.30</td>
      <td>56.31</td>
      <td>79.49</td>
      <td>60.91</td>
      <td>51.34</td>
      <td>78.14</td>
      <td>5.61</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f9d25d717f3972f80336fd15450329e2d8ee3ed4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Xwin-LM/Xwin-LM-13B-V0.1</td>
      <td>55.29</td>
      <td>62.54</td>
      <td>82.80</td>
      <td>56.53</td>
      <td>45.96</td>
      <td>74.27</td>
      <td>9.63</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>58.0</td>
      <td>True</td>
      <td>32938856dc3d713dcba706aded7c82791b6ff647</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>codellama/CodeLlama-34b-hf</td>
      <td>55.28</td>
      <td>54.18</td>
      <td>75.82</td>
      <td>54.92</td>
      <td>39.11</td>
      <td>73.32</td>
      <td>34.34</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>148.0</td>
      <td>True</td>
      <td>c778b02fdecd4663d2b0a42bfb340fd29969533b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-OpenOrca_20w</td>
      <td>55.28</td>
      <td>59.90</td>
      <td>82.51</td>
      <td>56.30</td>
      <td>43.14</td>
      <td>77.19</td>
      <td>12.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f01882672e89b164f76093cf3bd26cfc6ecf72ed</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-llama2-13b-v11.1-bf16</td>
      <td>55.28</td>
      <td>51.79</td>
      <td>76.23</td>
      <td>56.13</td>
      <td>49.70</td>
      <td>73.48</td>
      <td>24.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>19.0</td>
      <td>False</td>
      <td>76fb7d00836eb2f1d9c9605d8881d73b782cf324</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elinas/chronos-13b-v2</td>
      <td>55.25</td>
      <td>58.70</td>
      <td>82.52</td>
      <td>53.39</td>
      <td>50.55</td>
      <td>75.06</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>e5d411138e72370c5613dfea0f66ded99f6e62f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kyujinpy/SOLAR-Platypus-10.7B-v2</td>
      <td>55.25</td>
      <td>59.39</td>
      <td>83.57</td>
      <td>59.93</td>
      <td>43.15</td>
      <td>81.45</td>
      <td>4.02</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>10.73</td>
      <td>3.0</td>
      <td>True</td>
      <td>39a8673aa6d98a994661200e87cbd4069b8b6aa8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/CreativityEngine</td>
      <td>55.25</td>
      <td>59.30</td>
      <td>82.42</td>
      <td>53.55</td>
      <td>52.46</td>
      <td>74.19</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7870cc50b82b5cbebfa9935b6d73a9d20170299a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beaugogh/Llama2-13b-sharegpt4</td>
      <td>55.25</td>
      <td>61.77</td>
      <td>84.53</td>
      <td>55.21</td>
      <td>45.94</td>
      <td>75.22</td>
      <td>8.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>294c40349bf0c5377f71d92e7539bf5de3176a74</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>teknium/OpenHermes-13B</td>
      <td>55.24</td>
      <td>59.81</td>
      <td>82.24</td>
      <td>56.35</td>
      <td>46.01</td>
      <td>75.45</td>
      <td>11.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>41.0</td>
      <td>True</td>
      <td>f09d0fe655ad57cce9179b7b40ea6f81e07db18c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/vicuna-13b-v1.5-PL-lora_unload</td>
      <td>55.24</td>
      <td>56.91</td>
      <td>81.22</td>
      <td>56.06</td>
      <td>49.76</td>
      <td>75.22</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>5c8aeb722e11d1c7258abd45f9f2840f57976c28</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shareAI/llama2-13b-Chinese-chat</td>
      <td>55.22</td>
      <td>60.58</td>
      <td>82.19</td>
      <td>55.45</td>
      <td>45.11</td>
      <td>76.64</td>
      <td>11.37</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>31103acf93479d5c3865fb9b51dcb38e10d8b801</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>55.22</td>
      <td>60.84</td>
      <td>82.56</td>
      <td>56.42</td>
      <td>53.32</td>
      <td>75.93</td>
      <td>2.27</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1f81c0439f60d848e3cbc7f06fcd58b5161a8557</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/Chronorctypus-Limarobormes-13b</td>
      <td>55.22</td>
      <td>59.90</td>
      <td>82.75</td>
      <td>58.45</td>
      <td>51.90</td>
      <td>74.43</td>
      <td>3.87</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>False</td>
      <td>75c1bf5f4b40cf61873ff6487ccd3efc4f684330</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-13b-3.0</td>
      <td>55.21</td>
      <td>59.81</td>
      <td>83.71</td>
      <td>54.86</td>
      <td>47.79</td>
      <td>76.16</td>
      <td>8.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>7.0</td>
      <td>True</td>
      <td>2fcef275782b2c1061cf671d889aea652d13236c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Mythical-Destroyer-V2-L2-13B</td>
      <td>55.20</td>
      <td>59.30</td>
      <td>82.66</td>
      <td>57.39</td>
      <td>57.09</td>
      <td>74.74</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>11.0</td>
      <td>True</td>
      <td>cbc8b2e4a3beafc311b9e61f8fa9f7526a77c360</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/minotaur-13b-fixed</td>
      <td>55.19</td>
      <td>59.04</td>
      <td>81.66</td>
      <td>50.10</td>
      <td>50.36</td>
      <td>76.87</td>
      <td>13.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>5dac6f7559dba1c6fb59fee18c3e713cc3c83db7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decem/Dionysus-Mistral-n1-v1</td>
      <td>55.18</td>
      <td>60.24</td>
      <td>81.60</td>
      <td>59.32</td>
      <td>47.94</td>
      <td>71.35</td>
      <td>10.61</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d60ffacb4671aa412dde58d6c58173296cb0d566</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>qblocks/zephyr_7b_norobots</td>
      <td>55.16</td>
      <td>56.48</td>
      <td>79.64</td>
      <td>55.52</td>
      <td>44.60</td>
      <td>74.11</td>
      <td>20.62</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>312485e3c11a5cace45ad04dcf87a89df6e69571</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-LoRa</td>
      <td>55.15</td>
      <td>60.75</td>
      <td>82.09</td>
      <td>58.77</td>
      <td>45.15</td>
      <td>77.03</td>
      <td>7.13</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>8b2f5d65c03d415b7c43530def622e133e1ef014</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-c34b-2.2.1</td>
      <td>55.15</td>
      <td>54.69</td>
      <td>76.84</td>
      <td>55.43</td>
      <td>51.36</td>
      <td>72.53</td>
      <td>20.02</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>79d9761af231fecbfaf6066d6d405a0f8c04f4ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dfurman/Llama-2-13B-Instruct-v0.2</td>
      <td>55.14</td>
      <td>60.58</td>
      <td>81.96</td>
      <td>55.46</td>
      <td>45.71</td>
      <td>77.82</td>
      <td>9.33</td>
      <td>instruction-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>ac4b0962df8430f0b31c76a3d97a61134114c87e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/WizardLM-1.0-Uncensored-Llama2-13b</td>
      <td>55.14</td>
      <td>55.72</td>
      <td>80.34</td>
      <td>55.40</td>
      <td>51.44</td>
      <td>74.66</td>
      <td>13.27</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>134cea14627fd875f6f277cad92f988024855478</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-2-13b-instruct</td>
      <td>55.14</td>
      <td>61.18</td>
      <td>83.25</td>
      <td>55.92</td>
      <td>51.08</td>
      <td>77.35</td>
      <td>2.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>14.0</td>
      <td>False</td>
      <td>ac1f326ea75a28197c4b8e7c015071e8eef64485</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CalderaAI/13B-Legerdemain-L2</td>
      <td>55.13</td>
      <td>61.26</td>
      <td>83.26</td>
      <td>56.00</td>
      <td>41.99</td>
      <td>75.22</td>
      <td>13.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>d6624ce1bcc6b50c86b86e879a8c9822218b84d2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/pygmalion-2-13b</td>
      <td>55.12</td>
      <td>60.32</td>
      <td>82.37</td>
      <td>56.02</td>
      <td>42.22</td>
      <td>78.06</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>53.0</td>
      <td>True</td>
      <td>3cdc103995ccd5fc7fd2cb5f51f71b510466f5fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>totally-not-an-llm/PuddleJumper-13b</td>
      <td>55.11</td>
      <td>58.70</td>
      <td>81.18</td>
      <td>58.25</td>
      <td>56.44</td>
      <td>72.77</td>
      <td>3.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>f3a8a475ff0c6ae37ac8ae0690980be11cac731a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/WizardLM-1.0-Uncensored-Llama2-13b</td>
      <td>55.10</td>
      <td>55.80</td>
      <td>80.41</td>
      <td>55.59</td>
      <td>51.42</td>
      <td>74.11</td>
      <td>13.27</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>134cea14627fd875f6f277cad92f988024855478</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/Llama-2-13b-FINETUNE4</td>
      <td>55.09</td>
      <td>58.70</td>
      <td>81.93</td>
      <td>57.21</td>
      <td>43.26</td>
      <td>76.95</td>
      <td>12.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>939d06081210fa943c60210a47583f43b60901ad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/llama2-13b-orca-8k-3319</td>
      <td>55.09</td>
      <td>60.75</td>
      <td>81.91</td>
      <td>57.06</td>
      <td>42.64</td>
      <td>77.19</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>124.0</td>
      <td>True</td>
      <td>160f58ec85ef25ad935eb583f14c7e8c7f7e7839</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>speechlessai/speechless-llama2-dolphin-orca-platypus-13b</td>
      <td>55.09</td>
      <td>59.64</td>
      <td>82.65</td>
      <td>57.90</td>
      <td>43.44</td>
      <td>77.19</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>fd23b7d052eb7c18ecd2acc1be77c66b7b8d6dad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FlagAlpha/Llama2-Chinese-13b-Chat</td>
      <td>55.07</td>
      <td>55.97</td>
      <td>82.05</td>
      <td>54.74</td>
      <td>48.90</td>
      <td>76.16</td>
      <td>12.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>251.0</td>
      <td>True</td>
      <td>cb69cda10a72bc9736b1c10181ac41f28b69ff9b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jphme/Llama-2-13b-chat-german</td>
      <td>55.07</td>
      <td>57.85</td>
      <td>81.66</td>
      <td>54.45</td>
      <td>46.32</td>
      <td>76.48</td>
      <td>13.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>54.0</td>
      <td>False</td>
      <td>d72667bd92fd6f76835466d302563d213e0b1ee1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>royallab/PsyOrca2-13b-DARE</td>
      <td>55.07</td>
      <td>60.58</td>
      <td>83.83</td>
      <td>55.69</td>
      <td>53.27</td>
      <td>74.90</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>4d3b1d7a4a5e243d3b8882abaa4b4a13d0ecbce4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-dolphin_20w</td>
      <td>55.06</td>
      <td>59.56</td>
      <td>82.55</td>
      <td>55.89</td>
      <td>42.67</td>
      <td>77.27</td>
      <td>12.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c75073d7545a4d222f40dc519021c55a81850d75</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Python-Code-33B</td>
      <td>55.06</td>
      <td>56.31</td>
      <td>81.01</td>
      <td>54.22</td>
      <td>44.39</td>
      <td>75.22</td>
      <td>19.18</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>33.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>cf9a561b57145748455fd3e193d2b0e4ae0a0fce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MayaPH/GodziLLa-30B</td>
      <td>55.05</td>
      <td>61.52</td>
      <td>82.13</td>
      <td>54.21</td>
      <td>55.91</td>
      <td>76.16</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>30.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>aa9912a2ac60abeac28b4566731cd903dcc582ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WizardLM/WizardLM-13B-V1.1</td>
      <td>55.05</td>
      <td>60.24</td>
      <td>81.39</td>
      <td>50.92</td>
      <td>54.56</td>
      <td>75.06</td>
      <td>8.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>71.0</td>
      <td>False</td>
      <td>badd80f8a6f46fb15310fedf6d4db54959854897</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/llama-2-16b-nastychat</td>
      <td>55.04</td>
      <td>57.42</td>
      <td>80.59</td>
      <td>55.99</td>
      <td>53.45</td>
      <td>74.66</td>
      <td>8.11</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>16.19</td>
      <td>4.0</td>
      <td>False</td>
      <td>6fb7f82d486b3eee53d750f83cc7eae434349809</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>royallab/PsyOrca2-13b-DARE</td>
      <td>55.03</td>
      <td>60.32</td>
      <td>83.85</td>
      <td>55.62</td>
      <td>53.33</td>
      <td>74.59</td>
      <td>2.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>4d3b1d7a4a5e243d3b8882abaa4b4a13d0ecbce4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>augmxnt/shisa-7b-v1</td>
      <td>55.01</td>
      <td>56.14</td>
      <td>78.63</td>
      <td>23.12</td>
      <td>52.49</td>
      <td>78.06</td>
      <td>41.62</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.96</td>
      <td>21.0</td>
      <td>True</td>
      <td>131c2f3bf4955d1e2b6762380132bdd8688c0646</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>duliadotio/dulia-13b-8k-alpha</td>
      <td>55.00</td>
      <td>60.67</td>
      <td>82.00</td>
      <td>56.87</td>
      <td>42.59</td>
      <td>77.19</td>
      <td>10.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>c3bcafd7f6133a7e7c069f8765a99fe84989d926</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Aspik101/Redmond-Puffin-13B-instruct-PL-lora_unload</td>
      <td>55.00</td>
      <td>60.92</td>
      <td>82.43</td>
      <td>55.61</td>
      <td>44.26</td>
      <td>75.69</td>
      <td>11.07</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b933009635299bca32c694336aa2007d756a2dda</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v3.2_super</td>
      <td>54.99</td>
      <td>59.81</td>
      <td>82.50</td>
      <td>55.90</td>
      <td>42.30</td>
      <td>75.93</td>
      <td>13.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>33.0</td>
      <td>True</td>
      <td>aab7ce4d48b31a295a0116b61569d8e87a09bb7a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ziqingyang/chinese-alpaca-2-13b</td>
      <td>54.99</td>
      <td>58.70</td>
      <td>79.74</td>
      <td>55.10</td>
      <td>50.22</td>
      <td>75.69</td>
      <td>10.46</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.97</td>
      <td>0.0</td>
      <td>True</td>
      <td>576094cbf4988baf88b3bb66678be1db70bd720a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>prithivida/Asimov-7B-v1</td>
      <td>54.98</td>
      <td>59.04</td>
      <td>80.04</td>
      <td>56.35</td>
      <td>51.15</td>
      <td>73.95</td>
      <td>9.33</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>0b33ad0a6dde60156ee6008ff47f7cfa6cd27937</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-13b-v1.5-16k</td>
      <td>54.97</td>
      <td>56.74</td>
      <td>80.37</td>
      <td>55.28</td>
      <td>51.96</td>
      <td>72.38</td>
      <td>13.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>205.0</td>
      <td>True</td>
      <td>277697af19d4b267626ebc9f4e078d19a9a0fddf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LLMs/WizardLM-13B-V1.0</td>
      <td>54.97</td>
      <td>57.25</td>
      <td>80.88</td>
      <td>52.92</td>
      <td>50.55</td>
      <td>74.11</td>
      <td>14.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>f802ea7c01e2da27b0f7091c70d3ecfd8fc042b9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/Medusa-7B-bf16</td>
      <td>54.96</td>
      <td>60.58</td>
      <td>79.98</td>
      <td>57.71</td>
      <td>55.74</td>
      <td>73.95</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>dfe9982247761c6a54b76803483fe0d412e182f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/Hippolyta-7B-bf16</td>
      <td>54.96</td>
      <td>60.58</td>
      <td>79.98</td>
      <td>57.71</td>
      <td>55.74</td>
      <td>73.95</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>dfe9982247761c6a54b76803483fe0d412e182f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mistralai/Mistral-7B-Instruct-v0.1</td>
      <td>54.96</td>
      <td>54.52</td>
      <td>75.63</td>
      <td>55.38</td>
      <td>56.28</td>
      <td>73.72</td>
      <td>14.25</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1274.0</td>
      <td>True</td>
      <td>7961f5aa9b736bf8e364b2e6f201190f97a27931</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WebraftAI/synapsellm-7b-mistral-v0.4-preview3</td>
      <td>54.94</td>
      <td>51.28</td>
      <td>74.83</td>
      <td>52.93</td>
      <td>52.35</td>
      <td>73.48</td>
      <td>24.79</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>67c0d0fe71c620f0be410a06f58b928f89218639</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>FPHam/Sydney_Overthinker_13b_HF</td>
      <td>54.94</td>
      <td>58.96</td>
      <td>80.85</td>
      <td>51.28</td>
      <td>45.70</td>
      <td>73.95</td>
      <td>18.88</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>13.0</td>
      <td>True</td>
      <td>c4d2617fb452a55ac3a39c64128a98874595adb1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/wizardLM-13B-1.0-fp16</td>
      <td>54.93</td>
      <td>57.25</td>
      <td>80.88</td>
      <td>52.90</td>
      <td>50.55</td>
      <td>74.11</td>
      <td>13.87</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b79733805e98e668ff9a459975c259881b1b8014</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>adamo1139/Yi-6B-200K-AEZAKMI-v2</td>
      <td>54.93</td>
      <td>52.99</td>
      <td>71.20</td>
      <td>63.00</td>
      <td>46.79</td>
      <td>70.48</td>
      <td>25.09</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>1.0</td>
      <td>True</td>
      <td>0c4dd0e7119bbef9fa5b28b5a581b60822cebaf5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>adamo1139/Mistral-7B-AEZAKMI-v1</td>
      <td>54.92</td>
      <td>58.87</td>
      <td>82.01</td>
      <td>58.72</td>
      <td>53.54</td>
      <td>75.69</td>
      <td>0.68</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>fec4e695e5af743bb49d1976de83fa695be5f105</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>digitous/13B-Chimera</td>
      <td>54.92</td>
      <td>57.59</td>
      <td>81.50</td>
      <td>49.86</td>
      <td>52.59</td>
      <td>77.27</td>
      <td>10.69</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>85cfe8e6db2bee804873cfdb48955696cc5b0689</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>meta-llama/Llama-2-13b-chat-hf</td>
      <td>54.91</td>
      <td>59.04</td>
      <td>81.94</td>
      <td>54.64</td>
      <td>44.12</td>
      <td>74.51</td>
      <td>15.24</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>787.0</td>
      <td>False</td>
      <td>f848cf15ab9a51ae5735ab28120a9a0773eeb541</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NewstaR/Morningstar-13b-hf</td>
      <td>54.91</td>
      <td>59.04</td>
      <td>81.93</td>
      <td>54.63</td>
      <td>44.12</td>
      <td>74.51</td>
      <td>15.24</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2605b5b3b0ecba906ac26d39aab40f33c2ec81c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>deepse/CodeUp-Llama-2-13b-chat-hf</td>
      <td>54.91</td>
      <td>59.04</td>
      <td>81.93</td>
      <td>54.63</td>
      <td>44.12</td>
      <td>74.51</td>
      <td>15.24</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail++</td>
      <td>13.00</td>
      <td>29.0</td>
      <td>True</td>
      <td>d4af0b233a5b6a214e96582e103396e99dcf5f95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Kimiko-v2-13B-fp16</td>
      <td>54.91</td>
      <td>61.01</td>
      <td>83.32</td>
      <td>55.17</td>
      <td>40.65</td>
      <td>76.80</td>
      <td>12.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>0fed305667508e50330e71a2d43e9cee5ea73783</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>The-Face-Of-Goonery/Huginn-13b-FP16</td>
      <td>54.89</td>
      <td>60.58</td>
      <td>82.53</td>
      <td>53.71</td>
      <td>54.46</td>
      <td>73.72</td>
      <td>4.32</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>14.0</td>
      <td>False</td>
      <td>69615d9a8e1547f2407afd3380868a99f780e008</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>garage-bAInd/Platypus2-13B</td>
      <td>54.89</td>
      <td>61.26</td>
      <td>82.56</td>
      <td>56.70</td>
      <td>44.86</td>
      <td>76.87</td>
      <td>7.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>13.02</td>
      <td>17.0</td>
      <td>True</td>
      <td>b5e926e3d6c03e83c7983e87eb71098b5e80a62e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down</td>
      <td>54.88</td>
      <td>58.53</td>
      <td>82.27</td>
      <td>55.90</td>
      <td>40.26</td>
      <td>76.95</td>
      <td>15.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>fdc145fe1b47cdda483535c018e35a5ab249a552</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/LewdEngine</td>
      <td>54.88</td>
      <td>60.49</td>
      <td>83.08</td>
      <td>54.84</td>
      <td>43.63</td>
      <td>74.90</td>
      <td>12.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>6e918ff9f563552af4ad66f4308f6d040e24af4b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikael110/llama-2-13b-guanaco-fp16</td>
      <td>54.86</td>
      <td>60.92</td>
      <td>83.18</td>
      <td>54.58</td>
      <td>44.00</td>
      <td>74.90</td>
      <td>11.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>feb7ef47ceca6aec9548264a39622b63fdcb853c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/manticore-13b</td>
      <td>54.86</td>
      <td>58.70</td>
      <td>81.63</td>
      <td>50.84</td>
      <td>49.17</td>
      <td>76.64</td>
      <td>12.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>113.0</td>
      <td>False</td>
      <td>aed786b0200251c9962ac200c50f7e367f264b46</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Redmond-Puffin-13B</td>
      <td>54.86</td>
      <td>60.41</td>
      <td>83.20</td>
      <td>55.36</td>
      <td>42.12</td>
      <td>76.64</td>
      <td>11.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>13.00</td>
      <td>109.0</td>
      <td>True</td>
      <td>12af25fa7ea02c4fc636952ea8b9dc9cf48e35be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch</td>
      <td>54.86</td>
      <td>59.81</td>
      <td>82.69</td>
      <td>56.96</td>
      <td>52.92</td>
      <td>74.43</td>
      <td>2.35</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5427ceec420f943a0b011a4d96f3efc292306933</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CausalLM/7B</td>
      <td>54.86</td>
      <td>50.00</td>
      <td>74.58</td>
      <td>61.79</td>
      <td>50.13</td>
      <td>69.69</td>
      <td>22.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>wtfpl</td>
      <td>7.00</td>
      <td>116.0</td>
      <td>False</td>
      <td>3f4f76e2d94308ea6b0edc3de83f18c213a8fde5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Code-13B</td>
      <td>54.81</td>
      <td>57.34</td>
      <td>83.28</td>
      <td>53.17</td>
      <td>42.46</td>
      <td>73.56</td>
      <td>19.03</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>91f5a6d5cdf93aeb86dd8965e195d51522957fc6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/Samantha-1.11-CodeLlama-34b</td>
      <td>54.80</td>
      <td>56.57</td>
      <td>75.47</td>
      <td>53.51</td>
      <td>50.46</td>
      <td>73.48</td>
      <td>19.33</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>33.48</td>
      <td>0.0</td>
      <td>True</td>
      <td>3fd110de9282e52f56f999bf1da1a76425f00e29</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/llama-13b-FINETUNE3</td>
      <td>54.79</td>
      <td>59.30</td>
      <td>81.53</td>
      <td>57.46</td>
      <td>41.63</td>
      <td>76.72</td>
      <td>12.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>bacd035db122dafaf86bf52bb9ca8c613070cc58</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Ensemble5-Platypus2-13B-QLora-0.80-epoch</td>
      <td>54.76</td>
      <td>59.73</td>
      <td>82.66</td>
      <td>56.94</td>
      <td>52.92</td>
      <td>74.43</td>
      <td>1.90</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2af03c3287c60c4ba2fb6afa86c26cf722ab001d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WizardLM/WizardLM-13B-V1.2</td>
      <td>54.76</td>
      <td>59.04</td>
      <td>82.21</td>
      <td>54.64</td>
      <td>47.27</td>
      <td>71.90</td>
      <td>13.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>206.0</td>
      <td>True</td>
      <td>6760d0c07ffdc2405295ed7a29437cf4dc414bac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Redmond-Puffin-13B</td>
      <td>54.74</td>
      <td>60.49</td>
      <td>83.21</td>
      <td>54.95</td>
      <td>42.08</td>
      <td>76.48</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>13.00</td>
      <td>109.0</td>
      <td>True</td>
      <td>12af25fa7ea02c4fc636952ea8b9dc9cf48e35be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>The-Face-Of-Goonery/Chronos-Beluga-v2-13bfp16</td>
      <td>54.74</td>
      <td>60.75</td>
      <td>81.94</td>
      <td>54.08</td>
      <td>53.23</td>
      <td>73.80</td>
      <td>4.62</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>6d50e6681bc26c9bc0c8377c26c438e295ee0c2f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/TekniumAiroboros-Nebula-7B</td>
      <td>54.74</td>
      <td>57.17</td>
      <td>81.72</td>
      <td>55.25</td>
      <td>51.64</td>
      <td>73.24</td>
      <td>9.40</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>ef964d514cc25a600b0de78fc469d1acbec34591</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/MythoMix-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>54.74</td>
      <td>60.32</td>
      <td>83.72</td>
      <td>55.74</td>
      <td>52.18</td>
      <td>75.53</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>3d91f63d82abd598d5b80d24d74feb6b00b7d80f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>CalderaAI/13B-Thorns-l2</td>
      <td>54.72</td>
      <td>62.88</td>
      <td>83.57</td>
      <td>56.95</td>
      <td>49.52</td>
      <td>74.51</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>adc5e7befcc3d0a26f46198fdda4a098a2742fe6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Medusa-13b</td>
      <td>54.72</td>
      <td>58.19</td>
      <td>81.35</td>
      <td>57.39</td>
      <td>51.24</td>
      <td>73.32</td>
      <td>6.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>be755c9eef8233ca59e0178db75de878f5859222</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-13b-instruct</td>
      <td>54.72</td>
      <td>58.36</td>
      <td>82.20</td>
      <td>55.65</td>
      <td>42.40</td>
      <td>75.22</td>
      <td>14.48</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>23.0</td>
      <td>True</td>
      <td>ed15089024f3ecad9a8c4ce1db302cc01aa9f4ee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>abacusai/Giraffe-beta-13b-32k</td>
      <td>54.69</td>
      <td>55.63</td>
      <td>80.42</td>
      <td>53.61</td>
      <td>42.58</td>
      <td>74.59</td>
      <td>21.30</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>259f3fe9ebbff7532498f44286f253d56699da6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/LLaMA2-13B-Psyfighter2</td>
      <td>54.66</td>
      <td>60.07</td>
      <td>84.02</td>
      <td>55.07</td>
      <td>53.00</td>
      <td>74.35</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>14.0</td>
      <td>True</td>
      <td>cc51a4e64b0821feda101dc04737486b4ff60735</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-34b-v1.9</td>
      <td>54.64</td>
      <td>54.27</td>
      <td>75.20</td>
      <td>56.12</td>
      <td>43.92</td>
      <td>73.56</td>
      <td>24.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>68aad9f8452b2abf7d5415d48c09bd55d5b7ca05</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o</td>
      <td>54.64</td>
      <td>57.25</td>
      <td>81.73</td>
      <td>55.72</td>
      <td>41.53</td>
      <td>77.58</td>
      <td>14.03</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>209da26cff560ab34064f277190ab63f8c970b93</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Secbone/llama-2-13B-instructed</td>
      <td>54.63</td>
      <td>59.39</td>
      <td>83.88</td>
      <td>55.57</td>
      <td>46.89</td>
      <td>74.03</td>
      <td>8.04</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e676fbd9015beacfba5d71426beace7605200477</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r16-q_k_v_o</td>
      <td>54.63</td>
      <td>58.70</td>
      <td>81.66</td>
      <td>53.87</td>
      <td>43.02</td>
      <td>76.72</td>
      <td>13.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>33fd8a46a711ab8c45698dae9601678dfd7b3d33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/UltraLM-13B-fp16</td>
      <td>54.62</td>
      <td>57.59</td>
      <td>80.20</td>
      <td>51.85</td>
      <td>51.56</td>
      <td>75.85</td>
      <td>10.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>734f5641f6c548474517d1536c46024517f120e0</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Chat-Stheno-L2-13B</td>
      <td>54.61</td>
      <td>58.45</td>
      <td>80.96</td>
      <td>54.80</td>
      <td>43.31</td>
      <td>75.37</td>
      <td>14.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>20419fdd5b4bdcbbf075223c33b396958c48a6cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16</td>
      <td>54.61</td>
      <td>60.41</td>
      <td>82.58</td>
      <td>55.86</td>
      <td>43.61</td>
      <td>76.72</td>
      <td>8.49</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>891be2d8f205baa04c8a92f6ab1225f0d0c3e5bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>54.60</td>
      <td>59.90</td>
      <td>83.29</td>
      <td>56.69</td>
      <td>51.08</td>
      <td>75.22</td>
      <td>1.44</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>6e49d3d205e7f2e15c01ace0901da8931bbaab3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/Samantha-Nebula-7B</td>
      <td>54.58</td>
      <td>57.00</td>
      <td>82.25</td>
      <td>54.21</td>
      <td>49.58</td>
      <td>73.09</td>
      <td>11.37</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>a7d4b8a1683e33dd3c60064d7dd9d5c35691323f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BAAI/Aquila2-34B</td>
      <td>54.57</td>
      <td>52.47</td>
      <td>81.90</td>
      <td>76.03</td>
      <td>40.85</td>
      <td>75.53</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>356733caf6221e9dd898cde8ff189a98175526ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gaodrew/OpenOrca-Platypus2-13B-thera-1250</td>
      <td>54.56</td>
      <td>59.22</td>
      <td>81.02</td>
      <td>57.04</td>
      <td>48.43</td>
      <td>73.09</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b1c2ebcda387211732e87911e39edca503502a33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/Orca-2-7b</td>
      <td>54.55</td>
      <td>54.10</td>
      <td>76.19</td>
      <td>56.37</td>
      <td>52.45</td>
      <td>73.48</td>
      <td>14.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>184.0</td>
      <td>True</td>
      <td>60e31e6bdcf582ad103b807cb74b73ee1d2c4b17</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KoboldAI/LLaMA2-13B-Holomax</td>
      <td>54.52</td>
      <td>60.49</td>
      <td>82.86</td>
      <td>54.67</td>
      <td>42.97</td>
      <td>74.66</td>
      <td>11.45</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.02</td>
      <td>15.0</td>
      <td>True</td>
      <td>2c4fddeb097636d6462b7628a8e053ad3ff4678c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-shishya-all-hal-13b-ep3</td>
      <td>54.51</td>
      <td>48.63</td>
      <td>80.28</td>
      <td>56.40</td>
      <td>42.75</td>
      <td>73.16</td>
      <td>25.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>d773c696778d4f6fe63282d206ed042003346ed1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/LLaMA2-13B-Tiefighter</td>
      <td>54.51</td>
      <td>59.90</td>
      <td>84.00</td>
      <td>54.98</td>
      <td>53.02</td>
      <td>74.51</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>49.0</td>
      <td>True</td>
      <td>0d193a4562d6836724485cb7df6e58ca846bbfeb</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chansung/gpt4-alpaca-lora-13b-decapoda-1024</td>
      <td>54.51</td>
      <td>59.39</td>
      <td>81.87</td>
      <td>47.75</td>
      <td>52.59</td>
      <td>77.35</td>
      <td>8.11</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7aedafea409de07a997d70a84e30242c7b86877c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down</td>
      <td>54.50</td>
      <td>59.22</td>
      <td>81.52</td>
      <td>54.94</td>
      <td>42.83</td>
      <td>76.87</td>
      <td>11.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a759c4fae8dc5fcd264bf58b89b9fd13d06784ae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BAAI/Aquila2-34B</td>
      <td>54.50</td>
      <td>52.65</td>
      <td>81.99</td>
      <td>76.02</td>
      <td>40.80</td>
      <td>75.06</td>
      <td>0.45</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>356733caf6221e9dd898cde8ff189a98175526ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2-13B-LoRa</td>
      <td>54.48</td>
      <td>60.67</td>
      <td>82.50</td>
      <td>56.34</td>
      <td>43.91</td>
      <td>75.93</td>
      <td>7.51</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1450c541cf9e378e81862fabeb234b8e0a2bdf5a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>hywu/Camelidae-8x7B</td>
      <td>54.47</td>
      <td>55.63</td>
      <td>79.18</td>
      <td>50.10</td>
      <td>42.86</td>
      <td>76.24</td>
      <td>22.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>c12485aa7b31943113d992076cc2d79dce2a73a4</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Limarp-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>54.46</td>
      <td>60.49</td>
      <td>82.76</td>
      <td>56.52</td>
      <td>44.14</td>
      <td>76.80</td>
      <td>6.07</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>0a8560232ff73ca3c3f8e217b4517fa6c4f55558</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_TEST2</td>
      <td>54.46</td>
      <td>58.45</td>
      <td>81.70</td>
      <td>56.61</td>
      <td>40.19</td>
      <td>76.64</td>
      <td>13.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e312c4c59cab9d130c33288c92aad7c0cb5331d5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CallComply/zephyr-7b-beta-128k</td>
      <td>54.45</td>
      <td>58.28</td>
      <td>81.00</td>
      <td>53.57</td>
      <td>46.10</td>
      <td>74.74</td>
      <td>13.04</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>fc4c02de7b878edf07999d35efa91b62b6bfa35c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bhenrym14/airophin-13b-pntk-16k-fp16</td>
      <td>54.44</td>
      <td>61.18</td>
      <td>82.86</td>
      <td>55.19</td>
      <td>43.20</td>
      <td>76.16</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>False</td>
      <td>6b5418b69e8270df659eacb192f469e7c3af70b3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Technoculture/Medorca-2x7b</td>
      <td>54.43</td>
      <td>54.10</td>
      <td>76.04</td>
      <td>53.30</td>
      <td>48.04</td>
      <td>74.51</td>
      <td>20.62</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>11.07</td>
      <td>1.0</td>
      <td>True</td>
      <td>597c5b2e36b7b5375f0c05c05acc2699ec2a26cd</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NekoPunchBBB/Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu</td>
      <td>54.40</td>
      <td>57.51</td>
      <td>82.49</td>
      <td>54.83</td>
      <td>43.81</td>
      <td>77.27</td>
      <td>10.46</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f65029ea8f030731ace568e40bab33a7097a13de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r16</td>
      <td>54.37</td>
      <td>57.25</td>
      <td>82.27</td>
      <td>56.16</td>
      <td>39.75</td>
      <td>77.43</td>
      <td>13.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5da5c92f3cf85a62c1be90a0bb2ae8dffce64a7d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj</td>
      <td>54.35</td>
      <td>59.73</td>
      <td>81.06</td>
      <td>54.53</td>
      <td>38.64</td>
      <td>78.14</td>
      <td>14.03</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aeeded8db9eea97e2e6a2e19a006ce1acd110a82</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/orca_mini_v3_7B-GPTQ</td>
      <td>54.35</td>
      <td>54.52</td>
      <td>78.53</td>
      <td>51.85</td>
      <td>51.20</td>
      <td>74.66</td>
      <td>15.31</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>other</td>
      <td>9.05</td>
      <td>10.0</td>
      <td>True</td>
      <td>4f06a6151128861d5bb256275620f7eadcab3238</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lajonbot/Llama-2-13b-hf-instruct-pl-lora_unload</td>
      <td>54.34</td>
      <td>59.47</td>
      <td>82.16</td>
      <td>54.83</td>
      <td>41.45</td>
      <td>76.24</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>4ef2c736641c2983996c4662bf481782a9de5055</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-13b-instruct</td>
      <td>54.34</td>
      <td>57.94</td>
      <td>81.32</td>
      <td>47.62</td>
      <td>50.23</td>
      <td>77.11</td>
      <td>11.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>13.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>a13e08a36c355d64fae59f28162e5fa542a8d235</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>anhnv125/llama-op-v4</td>
      <td>54.34</td>
      <td>61.52</td>
      <td>79.21</td>
      <td>57.01</td>
      <td>42.72</td>
      <td>75.93</td>
      <td>9.63</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>6cd644049de2b944beaefcc6aa34965c00e08529</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down</td>
      <td>54.32</td>
      <td>58.70</td>
      <td>81.89</td>
      <td>56.08</td>
      <td>38.95</td>
      <td>77.35</td>
      <td>12.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4c3a4cb54c0487666bd58589b50f90c22de80969</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>garage-bAInd/Camel-Platypus2-13B</td>
      <td>54.32</td>
      <td>60.75</td>
      <td>83.61</td>
      <td>56.51</td>
      <td>49.60</td>
      <td>75.37</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>False</td>
      <td>0480a52799cb8e8de73bb41994df8b6b793937c7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-Open-Platypus_2.5w</td>
      <td>54.32</td>
      <td>59.56</td>
      <td>82.46</td>
      <td>56.06</td>
      <td>42.45</td>
      <td>76.80</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>bc55678af8226e1323305f743a4882da31994e0c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>StudentLLM/Alpagasus-2-13b-QLoRA-merged</td>
      <td>54.31</td>
      <td>61.09</td>
      <td>82.46</td>
      <td>55.27</td>
      <td>38.53</td>
      <td>77.35</td>
      <td>11.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>dacbafa40716a2d87e593240cc5c1dc883b5066a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/WizardLM-13B-V1.1-GPTQ</td>
      <td>54.28</td>
      <td>58.53</td>
      <td>80.66</td>
      <td>49.59</td>
      <td>54.35</td>
      <td>74.43</td>
      <td>8.11</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>27.0</td>
      <td>True</td>
      <td>9df807ac64034bc6e7387326689d6e39656ce5e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/wizard-mega-13b</td>
      <td>54.27</td>
      <td>57.34</td>
      <td>81.09</td>
      <td>50.59</td>
      <td>50.22</td>
      <td>76.32</td>
      <td>10.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>103.0</td>
      <td>False</td>
      <td>76e90314541be6cfa2b55208831c99f1351c1a33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-13b-v1.3</td>
      <td>54.27</td>
      <td>54.61</td>
      <td>80.41</td>
      <td>52.88</td>
      <td>52.14</td>
      <td>74.82</td>
      <td>10.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>178.0</td>
      <td>False</td>
      <td>7900eeb715a49affee9e6390f824e62eea3f3fb1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mwitiderrick/SwahiliInstruct-v0.2</td>
      <td>54.25</td>
      <td>55.20</td>
      <td>78.22</td>
      <td>50.30</td>
      <td>57.08</td>
      <td>73.24</td>
      <td>11.45</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>1b822c08b1065d5843cc48bf3a841ac5cd9d3b40</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>layoric/llama-2-13b-code-alpaca</td>
      <td>54.25</td>
      <td>60.84</td>
      <td>82.14</td>
      <td>55.93</td>
      <td>38.27</td>
      <td>76.40</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa1d543fe3391fe9f0e6143ef785fffe9c871225</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/13B-HyperMantis</td>
      <td>54.25</td>
      <td>58.53</td>
      <td>82.20</td>
      <td>50.61</td>
      <td>47.50</td>
      <td>76.24</td>
      <td>10.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>aa828ef92c363a5577ffd7d29e678277b9d2eb3c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>totally-not-an-llm/EverythingLM-13b-V3-peft</td>
      <td>54.24</td>
      <td>58.36</td>
      <td>81.03</td>
      <td>54.70</td>
      <td>52.98</td>
      <td>72.85</td>
      <td>5.53</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>7a2eed5038addcf4fa3b8dd358b45eb96134e749</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2-13B-IA3</td>
      <td>54.23</td>
      <td>61.09</td>
      <td>82.65</td>
      <td>56.32</td>
      <td>38.35</td>
      <td>75.69</td>
      <td>11.30</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b738c64d536df02f5c137a94bc7a32a4c486012b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/llama-2-13b-hf-platypus</td>
      <td>54.22</td>
      <td>58.87</td>
      <td>82.14</td>
      <td>54.98</td>
      <td>42.84</td>
      <td>77.11</td>
      <td>9.40</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>39e07f6213a64d79cf31e9c0773dea6224f7f021</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NekoPunchBBB/Llama-2-13b-hf_Open-Platypus</td>
      <td>54.22</td>
      <td>58.87</td>
      <td>82.14</td>
      <td>54.98</td>
      <td>42.84</td>
      <td>77.11</td>
      <td>9.40</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c318a24121bd69509f395e17a9636093213ece21</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FPHam/Free_Sydney_13b_HF</td>
      <td>54.22</td>
      <td>59.39</td>
      <td>81.40</td>
      <td>53.73</td>
      <td>45.63</td>
      <td>76.01</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>5474ecbccd1f2a2cda9f77a157993f55c97377ed</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>budecosystem/genz-13b-v2</td>
      <td>54.20</td>
      <td>55.97</td>
      <td>79.98</td>
      <td>54.30</td>
      <td>48.09</td>
      <td>74.59</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>98e0e2086df11b9f80e1571110540a657e52c2e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>StudentLLM/Alpagasus-2-13b-QLoRA-merged</td>
      <td>54.20</td>
      <td>60.84</td>
      <td>82.43</td>
      <td>55.55</td>
      <td>38.65</td>
      <td>76.87</td>
      <td>10.84</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>e324e828c8d68aa8510f50dfab133388a44fd821</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/llama2-13b-math1.2</td>
      <td>54.19</td>
      <td>56.91</td>
      <td>80.71</td>
      <td>53.21</td>
      <td>48.25</td>
      <td>74.74</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b05b4c22893e950e8e33acb67087a9acc8f0ab97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>totally-not-an-llm/PuddleJumper-13b-V2</td>
      <td>54.19</td>
      <td>57.00</td>
      <td>81.06</td>
      <td>58.30</td>
      <td>52.66</td>
      <td>72.45</td>
      <td>3.64</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>1fe9494e334a32ba73dc2926f58246450850c534</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/llama2-13b-math1.1</td>
      <td>54.18</td>
      <td>57.25</td>
      <td>80.74</td>
      <td>53.56</td>
      <td>48.43</td>
      <td>74.43</td>
      <td>10.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>3c4d83d3525e54a493ff510443fdcca44bf63b59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r4</td>
      <td>54.18</td>
      <td>56.74</td>
      <td>82.27</td>
      <td>56.18</td>
      <td>39.65</td>
      <td>77.03</td>
      <td>13.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7e0046627fabb0f23ace4b71f279d459ec4a0ff1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</td>
      <td>54.16</td>
      <td>59.13</td>
      <td>82.13</td>
      <td>54.98</td>
      <td>44.23</td>
      <td>76.40</td>
      <td>8.11</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>aad13bce3b243721e52e9cda479f1102dda99f12</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/Metharme-13b-Merged</td>
      <td>54.15</td>
      <td>59.90</td>
      <td>81.12</td>
      <td>47.18</td>
      <td>51.18</td>
      <td>76.80</td>
      <td>8.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>90c02cc338afcdd890a948af06432674743363ad</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/llama2-13b-math1.1</td>
      <td>54.14</td>
      <td>56.83</td>
      <td>80.69</td>
      <td>53.43</td>
      <td>48.48</td>
      <td>74.74</td>
      <td>10.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>3c4d83d3525e54a493ff510443fdcca44bf63b59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/Wizard-Vicuna-13B-Uncensored</td>
      <td>54.14</td>
      <td>58.96</td>
      <td>81.95</td>
      <td>47.92</td>
      <td>51.69</td>
      <td>75.69</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>95bfd1640a54e76b3e857c2462fd3a77eca0b275</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Wizard-Vicuna-13B-Uncensored-HF</td>
      <td>54.14</td>
      <td>58.96</td>
      <td>81.95</td>
      <td>47.92</td>
      <td>51.69</td>
      <td>75.69</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>201.0</td>
      <td>True</td>
      <td>fff9ac7f0e2e7b340f2301f5f089d989fc03be67</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16</td>
      <td>54.14</td>
      <td>59.98</td>
      <td>82.43</td>
      <td>55.41</td>
      <td>39.90</td>
      <td>76.56</td>
      <td>10.54</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6a0a2b6672c7b36c714a66c4a836e0b50c6cb5e6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-orca-platypus-coig-lite-4k-0.5e-13b</td>
      <td>54.13</td>
      <td>58.02</td>
      <td>80.15</td>
      <td>57.26</td>
      <td>48.04</td>
      <td>75.45</td>
      <td>5.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>081d1da5cfa2f6ad43abdf4fb5e41f8ec5846224</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/manticore-13b-chat-pyg</td>
      <td>54.13</td>
      <td>58.53</td>
      <td>81.96</td>
      <td>48.76</td>
      <td>48.76</td>
      <td>77.19</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>28.0</td>
      <td>False</td>
      <td>f9ef65a3cf50e3c09ccb443f99225148e08517aa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CalderaAI/13B-BlueMethod</td>
      <td>54.12</td>
      <td>59.64</td>
      <td>82.07</td>
      <td>50.34</td>
      <td>47.74</td>
      <td>77.11</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>False</td>
      <td>315aa0924dd42840b8cced581c9db1240f9bae1d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>hfl/chinese-alpaca-2-13b-16k</td>
      <td>54.12</td>
      <td>55.03</td>
      <td>77.41</td>
      <td>51.28</td>
      <td>46.50</td>
      <td>73.40</td>
      <td>21.08</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>ba4536aed022c49bda60e1b56a0dbefc2ea6a30a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj</td>
      <td>54.12</td>
      <td>57.17</td>
      <td>82.26</td>
      <td>55.89</td>
      <td>39.93</td>
      <td>76.56</td>
      <td>12.89</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c1a5ad1b5e490ed860eeb1b449a02e14da10717f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>IkariDev/Athena-v1</td>
      <td>54.11</td>
      <td>60.07</td>
      <td>82.64</td>
      <td>55.61</td>
      <td>46.58</td>
      <td>74.82</td>
      <td>4.93</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>11.0</td>
      <td>False</td>
      <td>8f96e561c8c795e383ca0faeb1696fa1e33e87de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chickencaesar/llama2-platypus-llama2-chat-13B-hf</td>
      <td>54.11</td>
      <td>62.97</td>
      <td>82.75</td>
      <td>56.86</td>
      <td>42.93</td>
      <td>76.32</td>
      <td>2.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e35bb473156d74c8b5ad23a5e9df815891e8139a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/Guanaco-13B-Uncensored</td>
      <td>54.10</td>
      <td>59.56</td>
      <td>82.70</td>
      <td>53.65</td>
      <td>43.26</td>
      <td>76.32</td>
      <td>9.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>cf315234979f5924ad73399bcdcdf51b05a1fc98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/llama-2-13b-Beluga-QLoRA</td>
      <td>54.09</td>
      <td>59.22</td>
      <td>81.92</td>
      <td>56.67</td>
      <td>48.23</td>
      <td>77.19</td>
      <td>1.29</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c0d3c0a5d4e9001ea933c6b71ca3adc99d1f71a2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>01-ai/Yi-6B</td>
      <td>54.08</td>
      <td>55.55</td>
      <td>76.57</td>
      <td>64.11</td>
      <td>41.96</td>
      <td>74.19</td>
      <td>12.13</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>331.0</td>
      <td>True</td>
      <td>e00f7cbde45745a22625ac85c6ad5d5b9f27098d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o</td>
      <td>54.08</td>
      <td>59.30</td>
      <td>81.20</td>
      <td>55.58</td>
      <td>38.13</td>
      <td>76.80</td>
      <td>13.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>71224344025dbfada6821c6a89cade1d8358dad1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stabilityai/StableBeluga1-Delta</td>
      <td>54.08</td>
      <td>68.17</td>
      <td>85.88</td>
      <td>64.83</td>
      <td>55.81</td>
      <td>49.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>65.29</td>
      <td>56.0</td>
      <td>True</td>
      <td>40a78d91d43ad9aef6663ff15ddc15be9922bce5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bhenrym14/airophin-v2-13b-PI-8k-fp16</td>
      <td>54.07</td>
      <td>60.58</td>
      <td>82.96</td>
      <td>56.75</td>
      <td>40.14</td>
      <td>76.64</td>
      <td>7.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>False</td>
      <td>26b7edfd282af223d86d5e539451357bb114247b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o</td>
      <td>54.06</td>
      <td>57.68</td>
      <td>81.91</td>
      <td>54.95</td>
      <td>41.31</td>
      <td>76.48</td>
      <td>12.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f76f93dad8408523e69c59abbb96ce6b1b9b9f69</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-L2-13B-PIPPA</td>
      <td>54.06</td>
      <td>59.73</td>
      <td>83.12</td>
      <td>54.10</td>
      <td>49.94</td>
      <td>74.51</td>
      <td>2.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>79e711178c6881496ae1f5635b08bc193f370709</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/ReMM-L2-13B</td>
      <td>54.06</td>
      <td>59.73</td>
      <td>83.10</td>
      <td>54.11</td>
      <td>49.94</td>
      <td>74.51</td>
      <td>2.96</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>c4710577003a23ca8e9040d16dfb8f3e9bc5d636</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/llama2-13b-math1.2</td>
      <td>54.05</td>
      <td>57.08</td>
      <td>80.61</td>
      <td>53.05</td>
      <td>48.30</td>
      <td>74.27</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b05b4c22893e950e8e33acb67087a9acc8f0ab97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_compare8k2</td>
      <td>54.05</td>
      <td>58.28</td>
      <td>81.39</td>
      <td>56.87</td>
      <td>39.86</td>
      <td>76.01</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>fe1b604097aad9408ce63fa7ffc9c320cdd06e4f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/airoboros-13B-HF</td>
      <td>54.05</td>
      <td>58.28</td>
      <td>81.05</td>
      <td>50.03</td>
      <td>51.57</td>
      <td>76.24</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>9219b61a0e8bc880e4cd0f8bebc48a97ee0950c7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Hermes-13b</td>
      <td>54.04</td>
      <td>56.57</td>
      <td>82.11</td>
      <td>50.44</td>
      <td>51.50</td>
      <td>75.30</td>
      <td>8.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl</td>
      <td>13.00</td>
      <td>407.0</td>
      <td>True</td>
      <td>24e8c03148ffd1f3e469744dfc24ad2ad82848f8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>The-Face-Of-Goonery/Huginn-13b-v4.5</td>
      <td>54.04</td>
      <td>60.67</td>
      <td>82.34</td>
      <td>52.32</td>
      <td>50.62</td>
      <td>73.64</td>
      <td>4.62</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>f3be56d8bf71a8d3905974b1e5fcba7336b02159</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>The-Face-Of-Goonery/Huginn-v3-13b</td>
      <td>54.04</td>
      <td>60.67</td>
      <td>82.34</td>
      <td>52.32</td>
      <td>50.62</td>
      <td>73.64</td>
      <td>4.62</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>6c2faf828c5380d28c51fcb4d3d0f1a420fb9a9a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>The-Face-Of-Goonery/Huginn-13b-V4</td>
      <td>54.04</td>
      <td>60.67</td>
      <td>82.34</td>
      <td>52.32</td>
      <td>50.62</td>
      <td>73.64</td>
      <td>4.62</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6186feee849e0c2b7e62d4cbdc4cdc48260ac684</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b</td>
      <td>54.02</td>
      <td>58.28</td>
      <td>81.05</td>
      <td>50.03</td>
      <td>51.57</td>
      <td>76.24</td>
      <td>6.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>102.0</td>
      <td>True</td>
      <td>44830f9e1559f318f5dad875bab40d1d1beddbfc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>01-ai/Yi-6B</td>
      <td>54.02</td>
      <td>55.55</td>
      <td>76.42</td>
      <td>63.85</td>
      <td>41.86</td>
      <td>73.80</td>
      <td>12.66</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>331.0</td>
      <td>True</td>
      <td>d8029c814d8faa68e1aef2e488f668a3af5d1a8a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/llama-13b-pretrained</td>
      <td>54.02</td>
      <td>56.31</td>
      <td>79.32</td>
      <td>47.03</td>
      <td>48.42</td>
      <td>76.95</td>
      <td>16.07</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c28cc0cf5a1a1bf4de96b23d06b02129dca85eb9</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-gate_up_down</td>
      <td>54.02</td>
      <td>57.17</td>
      <td>82.15</td>
      <td>54.88</td>
      <td>40.23</td>
      <td>76.32</td>
      <td>13.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>86adab5c098c9338e098a8e5b0188b0aa39b2478</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch</td>
      <td>54.01</td>
      <td>57.34</td>
      <td>81.24</td>
      <td>55.64</td>
      <td>55.98</td>
      <td>73.88</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ada55b32fe8ed55b7691d997ad2e86f232c91aad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/based-30b</td>
      <td>54.00</td>
      <td>63.91</td>
      <td>85.67</td>
      <td>58.28</td>
      <td>35.70</td>
      <td>80.11</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>5818a6344f48dc5a324589b57cb288a9d54c0b79</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-orca-platypus-coig-lite-4k-0.6e-13b</td>
      <td>53.99</td>
      <td>58.79</td>
      <td>79.93</td>
      <td>56.77</td>
      <td>48.29</td>
      <td>75.93</td>
      <td>4.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6bf4cf6211489bdbea70585a4a5c0f39deefb4e5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o</td>
      <td>53.99</td>
      <td>56.06</td>
      <td>81.89</td>
      <td>55.04</td>
      <td>40.12</td>
      <td>76.56</td>
      <td>14.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f907fffbb08698040325b3f2e47200a1b48b3ed9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/gpt4-alpaca-lora-13B-HF</td>
      <td>53.98</td>
      <td>59.56</td>
      <td>82.09</td>
      <td>47.48</td>
      <td>48.96</td>
      <td>76.72</td>
      <td>9.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>49678a2dd15fb4e1f1b99616ccc1ffd269912833</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/webMistral-7B</td>
      <td>53.97</td>
      <td>59.04</td>
      <td>80.89</td>
      <td>59.00</td>
      <td>39.71</td>
      <td>76.32</td>
      <td>8.87</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>0b221c617df3d2f883cfd925f646ebd93de23037</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardMath-13B-V1.0</td>
      <td>53.97</td>
      <td>60.07</td>
      <td>82.01</td>
      <td>54.80</td>
      <td>42.70</td>
      <td>71.90</td>
      <td>12.36</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>209316bea6eab73d8b18fca2a730b1dff3dcf999</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openaccess-ai-collective/minotaur-13b</td>
      <td>53.97</td>
      <td>56.40</td>
      <td>79.13</td>
      <td>49.61</td>
      <td>49.62</td>
      <td>76.56</td>
      <td>12.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>b5ae4519d4c8f4559a0aa80b6efe2008413ece01</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v2_w</td>
      <td>53.96</td>
      <td>57.34</td>
      <td>81.23</td>
      <td>50.17</td>
      <td>50.70</td>
      <td>75.93</td>
      <td>8.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>30.0</td>
      <td>True</td>
      <td>0eb53946b8fac30606dc72541f2fc073cb6a0e12</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_v2</td>
      <td>53.96</td>
      <td>57.17</td>
      <td>81.14</td>
      <td>50.58</td>
      <td>49.54</td>
      <td>76.24</td>
      <td>9.10</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>bd2a0968964c0f2dfae8f5a8950b43e35142f830</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_TEST3</td>
      <td>53.95</td>
      <td>59.04</td>
      <td>81.65</td>
      <td>56.37</td>
      <td>39.98</td>
      <td>75.45</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e81b5d4550224711929fdea4effdd990cc0c7404</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Weyaxi/Platypus-Nebula-v2-7B</td>
      <td>53.95</td>
      <td>55.38</td>
      <td>83.02</td>
      <td>56.07</td>
      <td>46.94</td>
      <td>72.22</td>
      <td>10.08</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>2d95180bae03c0b268dff44a1f9806fc295adc09</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj</td>
      <td>53.95</td>
      <td>57.42</td>
      <td>82.42</td>
      <td>55.57</td>
      <td>39.19</td>
      <td>77.03</td>
      <td>12.05</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>469c6674ad2190b639d6f5ce6bfecc1463825dfb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down</td>
      <td>53.94</td>
      <td>58.36</td>
      <td>82.33</td>
      <td>56.14</td>
      <td>39.51</td>
      <td>76.40</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d824054153586d58139b7c3527ba211f33a81382</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Voicelab/trurl-2-13b-academic</td>
      <td>53.94</td>
      <td>57.94</td>
      <td>79.55</td>
      <td>55.20</td>
      <td>43.46</td>
      <td>76.56</td>
      <td>10.92</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>2e95049edf02368bbd4b4f6ffb50bc8821e919bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/Pwen-7B-Chat-20_30</td>
      <td>53.93</td>
      <td>51.45</td>
      <td>73.99</td>
      <td>62.08</td>
      <td>47.01</td>
      <td>68.43</td>
      <td>20.62</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e6c38a7d2f4ba7b867fff421c08c02ba1908224e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>euclaise/Ferret-7B</td>
      <td>53.93</td>
      <td>62.29</td>
      <td>81.31</td>
      <td>60.27</td>
      <td>40.01</td>
      <td>77.66</td>
      <td>2.05</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1ef5adff5ceb06d2d9808bccf5e06705f9e19dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/llama-2-13b-chat-platypus</td>
      <td>53.92</td>
      <td>53.84</td>
      <td>80.67</td>
      <td>54.44</td>
      <td>46.23</td>
      <td>76.01</td>
      <td>12.36</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>828aa1020fc7d394fe8ee2c596e3211df7656eac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
      <td>53.92</td>
      <td>60.58</td>
      <td>82.97</td>
      <td>52.10</td>
      <td>46.10</td>
      <td>73.64</td>
      <td>8.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>32.53</td>
      <td>4.0</td>
      <td>False</td>
      <td>24ebae726954e4c1f24a8b2cbe0ca863012a7338</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w</td>
      <td>53.91</td>
      <td>59.47</td>
      <td>81.00</td>
      <td>54.31</td>
      <td>38.17</td>
      <td>77.27</td>
      <td>13.27</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa5b161b39900c5e80d5bb39d098f6333ad964f7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Gryphe/MythoBoros-13b</td>
      <td>53.90</td>
      <td>58.19</td>
      <td>81.75</td>
      <td>50.13</td>
      <td>48.93</td>
      <td>75.77</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>67695d15e6610bc8055fbcde82f298e48ad2d374</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/llama-2-13b-QLoRA</td>
      <td>53.87</td>
      <td>58.02</td>
      <td>82.33</td>
      <td>55.80</td>
      <td>46.23</td>
      <td>77.58</td>
      <td>3.26</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d1a41d83c6bcc14378ee4859d65ef77a261d39d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b-gpt4-1.4</td>
      <td>53.87</td>
      <td>59.64</td>
      <td>83.22</td>
      <td>47.56</td>
      <td>48.82</td>
      <td>76.24</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>d0d2687ed2b4a63a644ed6c5b3f6401844718659</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b-gpt4-1.4-fp16</td>
      <td>53.87</td>
      <td>59.64</td>
      <td>83.22</td>
      <td>47.56</td>
      <td>48.82</td>
      <td>76.24</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>037e369be06a8a0eef87f2cddfd3469670483f29</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>euclaise/Ferret_7B</td>
      <td>53.87</td>
      <td>62.29</td>
      <td>81.33</td>
      <td>60.09</td>
      <td>39.94</td>
      <td>77.51</td>
      <td>2.05</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>c1e1e2743ffa7b9369aebac751b04f7e8740f80d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>euclaise/Ferret-7B</td>
      <td>53.87</td>
      <td>62.29</td>
      <td>81.33</td>
      <td>60.09</td>
      <td>39.94</td>
      <td>77.51</td>
      <td>2.05</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e96b5245ef97999f143a2c9f9739e5cf52ec0d64</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>zyh3826/llama2-13b-ft-openllm-leaderboard-v1</td>
      <td>53.86</td>
      <td>59.64</td>
      <td>83.14</td>
      <td>60.93</td>
      <td>40.72</td>
      <td>77.35</td>
      <td>1.36</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>70404059013c74b0641ed69d293b3d1ad708cd1e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-gate_up_down</td>
      <td>53.86</td>
      <td>55.38</td>
      <td>81.92</td>
      <td>55.28</td>
      <td>40.76</td>
      <td>76.09</td>
      <td>13.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2ca747d779feaa99c475b8015c9b4a50aea41cd2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lajonbot/WizardLM-13B-V1.2-PL-lora_unload</td>
      <td>53.86</td>
      <td>58.53</td>
      <td>81.10</td>
      <td>55.15</td>
      <td>46.18</td>
      <td>71.03</td>
      <td>11.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>5f14e6f5ea67fd2840791c46b3e00846cbdb32cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Gryphe/MythoLogic-13b</td>
      <td>53.85</td>
      <td>58.45</td>
      <td>81.56</td>
      <td>49.36</td>
      <td>49.47</td>
      <td>75.61</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>d89d925ad1eeaee465c4de3e5c74240a5a40b585</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/platypus-2-22b-relora</td>
      <td>53.83</td>
      <td>57.68</td>
      <td>82.44</td>
      <td>55.33</td>
      <td>43.61</td>
      <td>77.35</td>
      <td>6.60</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>21.83</td>
      <td>0.0</td>
      <td>False</td>
      <td>15bca3e9b25cc2f280fec21686ef3bc445217503</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Envoid/Libra-19B</td>
      <td>53.83</td>
      <td>60.58</td>
      <td>82.04</td>
      <td>55.57</td>
      <td>48.41</td>
      <td>76.32</td>
      <td>0.08</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>19.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>a4e1f8f62740d676c25eedb4f29f4e776dcc0c22</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch</td>
      <td>53.80</td>
      <td>58.62</td>
      <td>82.56</td>
      <td>55.84</td>
      <td>42.09</td>
      <td>76.64</td>
      <td>7.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>001a5f96daea57b5f256c2df270b35653b439f6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Weyaxi/test-help-steer-filtered-orig</td>
      <td>53.77</td>
      <td>57.59</td>
      <td>80.42</td>
      <td>57.24</td>
      <td>41.10</td>
      <td>76.64</td>
      <td>9.63</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>bda6d45ddb3ef73df4d198d95416c66872429927</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NekoPunchBBB/Llama-2-13b-hf_Open-Platypus-8bit-att</td>
      <td>53.75</td>
      <td>57.51</td>
      <td>82.14</td>
      <td>54.56</td>
      <td>42.21</td>
      <td>76.56</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>83a8e51d0a72dcfbe5de13dc7ee10dc20e91602e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Kimiko-13B-fp16</td>
      <td>53.75</td>
      <td>59.22</td>
      <td>82.35</td>
      <td>55.85</td>
      <td>39.55</td>
      <td>76.72</td>
      <td>8.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>27868769e2d6b1af46337f0997c71b0577952a3d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>NobodyExistsOnTheInternet/GiftedConvo13bLoraNoEconsE4</td>
      <td>53.74</td>
      <td>59.90</td>
      <td>84.11</td>
      <td>54.67</td>
      <td>41.94</td>
      <td>74.03</td>
      <td>7.81</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f3d421aadb29830345bf392f793ce3c33e7d68c5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj</td>
      <td>53.74</td>
      <td>58.53</td>
      <td>82.47</td>
      <td>53.90</td>
      <td>37.92</td>
      <td>76.80</td>
      <td>12.81</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d74752b931bfddaa063a292e7ea85dfb1d7a4998</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2-13B-QLoRa</td>
      <td>53.74</td>
      <td>57.51</td>
      <td>82.55</td>
      <td>57.34</td>
      <td>43.38</td>
      <td>76.64</td>
      <td>5.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e62a8fafce0d64ac03d465a4e915bc1f50776a08</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zarafusionex-1.2-l2-7b</td>
      <td>53.73</td>
      <td>56.66</td>
      <td>79.16</td>
      <td>51.94</td>
      <td>51.29</td>
      <td>74.74</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>68ca01427848528ab21263fd06720a081b09d063</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down</td>
      <td>53.71</td>
      <td>57.25</td>
      <td>81.49</td>
      <td>55.90</td>
      <td>39.79</td>
      <td>75.77</td>
      <td>12.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a12fb5937e6904977e8123b0d5ef21283b6895d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down</td>
      <td>53.71</td>
      <td>57.25</td>
      <td>81.79</td>
      <td>53.96</td>
      <td>39.66</td>
      <td>77.82</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>8a75b17d4b60f820159bb0100f26f438727bb199</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kz919/mistral-7b-sft-open-orca-flan-50k</td>
      <td>53.70</td>
      <td>58.79</td>
      <td>81.92</td>
      <td>55.72</td>
      <td>37.49</td>
      <td>77.98</td>
      <td>10.31</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>54129b5d7a3824af7d457e007742750029cb3904</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lajonbot/vicuna-13b-v1.3-PL-lora_unload</td>
      <td>53.70</td>
      <td>54.86</td>
      <td>80.41</td>
      <td>52.20</td>
      <td>49.62</td>
      <td>76.09</td>
      <td>9.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>5582369752583b02df3cba4bd2a733d12265cddb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>gaodrew/gaodrew-gorgonzola-13b</td>
      <td>53.70</td>
      <td>50.94</td>
      <td>77.65</td>
      <td>68.93</td>
      <td>40.63</td>
      <td>75.45</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>a53fbe358d4cb546916847d861ccfaf7c724a103</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down</td>
      <td>53.69</td>
      <td>55.72</td>
      <td>81.55</td>
      <td>53.90</td>
      <td>41.89</td>
      <td>77.19</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>eb934db4644738a74143b381445213979c8858ed</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w</td>
      <td>53.69</td>
      <td>58.62</td>
      <td>82.32</td>
      <td>54.25</td>
      <td>38.17</td>
      <td>76.80</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>08bc7112a775dd4223d441355f3d619694013789</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mixtral-8x7b-v16.2-32k</td>
      <td>53.69</td>
      <td>34.39</td>
      <td>81.72</td>
      <td>71.33</td>
      <td>56.65</td>
      <td>77.82</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.74</td>
      <td>5.0</td>
      <td>True</td>
      <td>eae1e422ac65e856c03a9da0a840114267d24b68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>BramVanroy/Llama-2-13b-chat-dutch</td>
      <td>53.69</td>
      <td>59.30</td>
      <td>81.45</td>
      <td>55.82</td>
      <td>38.23</td>
      <td>76.64</td>
      <td>10.69</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>13.02</td>
      <td>16.0</td>
      <td>True</td>
      <td>428508a0cf288c0f5b7891c9b2f758ddf4d62c26</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b-gpt4-1.1</td>
      <td>53.68</td>
      <td>59.04</td>
      <td>83.05</td>
      <td>49.41</td>
      <td>46.62</td>
      <td>75.77</td>
      <td>8.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>19c7060adcb34d42e742fe51dd36b8657ac069b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o</td>
      <td>53.68</td>
      <td>56.23</td>
      <td>81.98</td>
      <td>55.87</td>
      <td>39.76</td>
      <td>76.72</td>
      <td>11.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>cc3c5e5a874cf4ff4f94ea919e819f8a914c8acb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>shareAI/bimoGPT-llama2-13b</td>
      <td>53.68</td>
      <td>58.79</td>
      <td>82.08</td>
      <td>55.60</td>
      <td>37.82</td>
      <td>76.48</td>
      <td>11.30</td>
      <td></td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c29b67965ea55da3e2ac678eef7ffdf36f8ef5ab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TaylorAI/Flash-Llama-13B</td>
      <td>53.67</td>
      <td>59.30</td>
      <td>82.15</td>
      <td>55.67</td>
      <td>37.39</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>81b40096471a8980e3e1a8998f358bd363033783</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NewstaR/Starlight-13B</td>
      <td>53.67</td>
      <td>59.30</td>
      <td>82.15</td>
      <td>55.67</td>
      <td>37.39</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>cb9fced568b1abd881133c642c427aaa488f00cc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TheBloke/Llama-2-13B-fp16</td>
      <td>53.67</td>
      <td>59.30</td>
      <td>82.15</td>
      <td>55.67</td>
      <td>37.39</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>55.0</td>
      <td>False</td>
      <td>b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1</td>
      <td>53.66</td>
      <td>55.80</td>
      <td>82.27</td>
      <td>55.63</td>
      <td>38.15</td>
      <td>77.43</td>
      <td>12.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>48b8ceeb62e5ca897f284bbc0923201689af7c89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chargoddard/llama2-22b-blocktriangular</td>
      <td>53.65</td>
      <td>58.28</td>
      <td>82.69</td>
      <td>54.53</td>
      <td>39.23</td>
      <td>75.93</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>22.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>7adbaa5b8e122bb93bf510d8655ec4132d7b4a8a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b-gpt4</td>
      <td>53.64</td>
      <td>59.39</td>
      <td>83.29</td>
      <td>47.89</td>
      <td>47.65</td>
      <td>75.77</td>
      <td>7.88</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>c0eef6e6f63d4b11953539308717cea0079b44f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>chargoddard/llama2-22b</td>
      <td>53.64</td>
      <td>58.53</td>
      <td>82.55</td>
      <td>54.68</td>
      <td>39.84</td>
      <td>76.32</td>
      <td>9.93</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>22.00</td>
      <td>42.0</td>
      <td>False</td>
      <td>2bece0787009b4b584f49d0e0d1b49ecf4a52da9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/platypus2-22b-relora</td>
      <td>53.64</td>
      <td>57.51</td>
      <td>82.36</td>
      <td>54.94</td>
      <td>43.62</td>
      <td>77.11</td>
      <td>6.29</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>21.83</td>
      <td>0.0</td>
      <td>True</td>
      <td>15bca3e9b25cc2f280fec21686ef3bc445217503</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NobodyExistsOnTheInternet/PuffedLIMA13bQLORA</td>
      <td>53.63</td>
      <td>59.90</td>
      <td>84.39</td>
      <td>53.68</td>
      <td>39.90</td>
      <td>75.22</td>
      <td>8.72</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7da6d235d625e16c850ccd0b947dee40071b1f89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/deacon-13b</td>
      <td>53.63</td>
      <td>57.85</td>
      <td>82.63</td>
      <td>55.25</td>
      <td>39.33</td>
      <td>76.32</td>
      <td>10.39</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>6c3a002f6c9e8a481a7375d91856d603bf6dd040</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o</td>
      <td>53.62</td>
      <td>59.04</td>
      <td>81.15</td>
      <td>53.00</td>
      <td>40.16</td>
      <td>76.48</td>
      <td>11.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ac40ecf48cf5f7168e8c3929632c654bc834c3d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/llama2-13b-FINETUNE3_TEST2</td>
      <td>53.62</td>
      <td>54.69</td>
      <td>81.48</td>
      <td>56.80</td>
      <td>39.93</td>
      <td>76.24</td>
      <td>12.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9e6431061bd13852a7435f5fe7a6eb0bbd148e14</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-agents/tora-13b-v1.0</td>
      <td>53.62</td>
      <td>58.96</td>
      <td>82.31</td>
      <td>54.73</td>
      <td>40.25</td>
      <td>75.61</td>
      <td>9.86</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>0636c1f582c979a5a292cc5f3dc293800b1494e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/MistralInstructLongish</td>
      <td>53.62</td>
      <td>60.75</td>
      <td>81.86</td>
      <td>60.49</td>
      <td>40.55</td>
      <td>76.56</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>813c4707970cb5bf3e2a49f7f350af59e7032c24</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NobodyExistsOnTheInternet/PuffedConvo13bLoraE4</td>
      <td>53.62</td>
      <td>59.81</td>
      <td>84.39</td>
      <td>53.62</td>
      <td>39.87</td>
      <td>75.22</td>
      <td>8.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>40e4fce0c25bd23f6011b424748ee2b5374b98d5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_TEST</td>
      <td>53.62</td>
      <td>54.78</td>
      <td>81.52</td>
      <td>56.03</td>
      <td>39.14</td>
      <td>77.03</td>
      <td>13.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>0ed198a814192b06e60715112d2a4b6bfd630806</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/Nous-Hermes-13b-pl-lora_unload</td>
      <td>53.61</td>
      <td>57.08</td>
      <td>81.49</td>
      <td>49.17</td>
      <td>48.30</td>
      <td>76.40</td>
      <td>9.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d0ef3991a11c4dc2ea2f832d4082c89c3c5e810c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Python-Code-13B</td>
      <td>53.61</td>
      <td>58.79</td>
      <td>81.66</td>
      <td>54.78</td>
      <td>42.83</td>
      <td>74.03</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>981454b6a2275f787592589609df7f2bf558706d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BramVanroy/llama2-13b-ft-mc4_nl_cleaned_tiny</td>
      <td>53.60</td>
      <td>59.30</td>
      <td>82.04</td>
      <td>54.67</td>
      <td>38.03</td>
      <td>77.27</td>
      <td>10.31</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>b23fe7d174653b87dc08507d9b83504a8dddbc45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/WizardLM-1.0-Uncensored-CodeLlama-34b</td>
      <td>53.59</td>
      <td>56.40</td>
      <td>75.45</td>
      <td>54.51</td>
      <td>43.06</td>
      <td>72.45</td>
      <td>19.64</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>33.48</td>
      <td>0.0</td>
      <td>True</td>
      <td>3e8df2cf4a4ee1c0b2d079cb7be70024d425ea8c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down</td>
      <td>53.58</td>
      <td>54.35</td>
      <td>82.13</td>
      <td>55.33</td>
      <td>39.60</td>
      <td>77.19</td>
      <td>12.89</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1646a2b77ddeaf0f848c96ed68726556c7539729</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/tulu-13B-fp16</td>
      <td>53.58</td>
      <td>53.92</td>
      <td>80.66</td>
      <td>53.19</td>
      <td>43.84</td>
      <td>75.61</td>
      <td>14.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>532aeb363b0ceee155b3cf9479ef635b797cee7c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</td>
      <td>53.57</td>
      <td>58.96</td>
      <td>81.94</td>
      <td>55.00</td>
      <td>40.26</td>
      <td>76.56</td>
      <td>8.72</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>30edbe648df2661dd779cd19ef613e6914dcc8e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stabilityai/StableBeluga-7B</td>
      <td>53.56</td>
      <td>56.31</td>
      <td>79.14</td>
      <td>52.71</td>
      <td>50.19</td>
      <td>75.22</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>127.0</td>
      <td>False</td>
      <td>329adcfc39f48dce183eb0b155b732dbe03c6304</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>circulus/Llama-2-7b-orca-v1</td>
      <td>53.56</td>
      <td>56.31</td>
      <td>79.14</td>
      <td>52.71</td>
      <td>50.19</td>
      <td>75.22</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>e501f231277671710384ba0397da2c4486865958</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/dolphin-llama-13b</td>
      <td>53.56</td>
      <td>55.55</td>
      <td>77.11</td>
      <td>52.16</td>
      <td>52.23</td>
      <td>69.93</td>
      <td>14.40</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b6d16c3e1cffef5e914863f41fd96152dafddd6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/oasst-llama-13b-2-epochs</td>
      <td>53.55</td>
      <td>57.94</td>
      <td>82.40</td>
      <td>48.56</td>
      <td>47.27</td>
      <td>76.87</td>
      <td>8.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>8.0</td>
      <td>False</td>
      <td>0e3796192f7edf43968541b9454ea35da4a2b1c5</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/guanaco-13B-HF</td>
      <td>53.54</td>
      <td>57.85</td>
      <td>83.84</td>
      <td>48.28</td>
      <td>46.73</td>
      <td>75.85</td>
      <td>8.72</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>bd59c700815124df616a17f5b49a0bc51590b231</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lajonbot/tableBeluga-7B-instruct-pl-lora_unload</td>
      <td>53.54</td>
      <td>56.23</td>
      <td>79.12</td>
      <td>52.70</td>
      <td>50.19</td>
      <td>75.22</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>eeb22ca9481a5ed7e131a329324494f234300a45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chargoddard/llama2-22b-blocktriangular</td>
      <td>53.53</td>
      <td>58.53</td>
      <td>82.59</td>
      <td>54.64</td>
      <td>39.30</td>
      <td>76.32</td>
      <td>9.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>22.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>40a51343ae776b5cb39f2b4343ae8f9b676ffd58</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama-13b</td>
      <td>53.53</td>
      <td>58.96</td>
      <td>79.71</td>
      <td>49.10</td>
      <td>49.59</td>
      <td>75.61</td>
      <td>8.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>dd326f89ce885844d714d9ab33603e0d17f56cc5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w</td>
      <td>53.52</td>
      <td>58.96</td>
      <td>82.51</td>
      <td>56.12</td>
      <td>40.07</td>
      <td>76.64</td>
      <td>6.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2929bfa1049db46df94f5710755178d18a981665</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down</td>
      <td>53.52</td>
      <td>55.03</td>
      <td>81.97</td>
      <td>56.64</td>
      <td>38.07</td>
      <td>77.19</td>
      <td>12.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>555486843f613276b6edb480f6d37b9203daa226</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>budecosystem/code-millenials-34b</td>
      <td>53.51</td>
      <td>49.83</td>
      <td>75.09</td>
      <td>49.28</td>
      <td>45.37</td>
      <td>69.06</td>
      <td>32.45</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>2.0</td>
      <td>True</td>
      <td>fdb4dc33b18c884e51f9d8258f192b4ed0f93dc3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>IGeniusDev/llama13B-quant8-testv1-openorca-customdataset</td>
      <td>53.50</td>
      <td>60.49</td>
      <td>82.97</td>
      <td>54.44</td>
      <td>37.34</td>
      <td>75.69</td>
      <td>10.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>f364d000bedac80e72aa103c08b77aee1b61b7da</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-13b-chat</td>
      <td>53.50</td>
      <td>58.62</td>
      <td>80.85</td>
      <td>47.76</td>
      <td>48.73</td>
      <td>76.72</td>
      <td>8.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>27002e974774c3599e6a4d731dd44e68b9e41f92</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mistral-7b-v13</td>
      <td>53.50</td>
      <td>52.30</td>
      <td>75.09</td>
      <td>56.34</td>
      <td>50.81</td>
      <td>71.74</td>
      <td>14.71</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>e6c4cc00e1bb2aa2082c2b8fd93c949aa36ce300</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama-13b-v1.2</td>
      <td>53.49</td>
      <td>56.74</td>
      <td>80.34</td>
      <td>48.90</td>
      <td>51.00</td>
      <td>75.93</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>c0a56d9f5a15bea07493191b5a6295f6797a9b2c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down</td>
      <td>53.48</td>
      <td>55.80</td>
      <td>81.74</td>
      <td>55.09</td>
      <td>39.12</td>
      <td>76.32</td>
      <td>12.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>aefc3a122cb054b070a212d1127600775aded4be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pankajmathur/orca_mini_v3_7b</td>
      <td>53.47</td>
      <td>56.91</td>
      <td>79.64</td>
      <td>52.37</td>
      <td>50.51</td>
      <td>74.27</td>
      <td>7.13</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>40.0</td>
      <td>True</td>
      <td>f9849ea6bf0f6ebb78dca1cea1c7a3ef8f7d715c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_v3_7b</td>
      <td>53.47</td>
      <td>56.91</td>
      <td>79.64</td>
      <td>52.37</td>
      <td>50.51</td>
      <td>74.27</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1583d2f02041fb37df28eeae4da644d8dff33eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama2-13b-chat</td>
      <td>53.46</td>
      <td>57.51</td>
      <td>77.94</td>
      <td>52.56</td>
      <td>48.18</td>
      <td>74.74</td>
      <td>9.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>9497e3bd12e19e1300bc7b1980fbe232420134b9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r16-gate_up_down</td>
      <td>53.44</td>
      <td>55.80</td>
      <td>82.10</td>
      <td>55.33</td>
      <td>39.82</td>
      <td>76.24</td>
      <td>11.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>86f255afabc8986c73376cafd98628a068649022</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down</td>
      <td>53.43</td>
      <td>57.94</td>
      <td>81.19</td>
      <td>53.43</td>
      <td>40.48</td>
      <td>76.72</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>15f1b122d60631091419cb8e668a28737b92a0e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TigerResearch/tigerbot-13b-base</td>
      <td>53.42</td>
      <td>53.84</td>
      <td>77.05</td>
      <td>53.57</td>
      <td>44.06</td>
      <td>74.98</td>
      <td>17.06</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>2df5ed76be7eff0962f2d816a64eca1e78e1cbf3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zarafusionex-1.1-l2-7b</td>
      <td>53.41</td>
      <td>56.14</td>
      <td>79.34</td>
      <td>52.10</td>
      <td>50.66</td>
      <td>74.43</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>3268ff5291934a14f3f5e7013bbb408f33adb542</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>quantumaikr/QuantumLM</td>
      <td>53.41</td>
      <td>55.80</td>
      <td>79.74</td>
      <td>54.17</td>
      <td>46.71</td>
      <td>74.19</td>
      <td>9.86</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>9058130b416355b37f5f78777748aa56d98a4da0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/samantha-mistral-instruct-7b</td>
      <td>53.40</td>
      <td>53.50</td>
      <td>75.14</td>
      <td>51.72</td>
      <td>58.81</td>
      <td>70.40</td>
      <td>10.84</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>3a33eea0858d411617c472c3c0ae39f17d2b3f5d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-30b-instruct</td>
      <td>53.40</td>
      <td>58.45</td>
      <td>84.31</td>
      <td>49.15</td>
      <td>38.05</td>
      <td>75.14</td>
      <td>15.31</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>30.00</td>
      <td>95.0</td>
      <td>True</td>
      <td>2abf1163dd8c9b11f07d805c06e6ec90a1f2037e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aeala/GPT4-x-Alpasta-13b</td>
      <td>53.38</td>
      <td>58.53</td>
      <td>79.92</td>
      <td>46.03</td>
      <td>53.06</td>
      <td>73.95</td>
      <td>8.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>50af05b015446110a2dc52a1b4b341142c98e62b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down</td>
      <td>53.38</td>
      <td>55.89</td>
      <td>81.38</td>
      <td>53.77</td>
      <td>40.25</td>
      <td>76.72</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a8b15badead658df6ec5b884b813962b9fd29cfb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>NobodyExistsOnTheInternet/GiftedConvo13bLoraNoEcons</td>
      <td>53.35</td>
      <td>59.39</td>
      <td>83.19</td>
      <td>55.15</td>
      <td>40.56</td>
      <td>74.03</td>
      <td>7.81</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>9d7031e7d956dd2d25c61d85f594d115ce65b172</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down</td>
      <td>53.35</td>
      <td>56.40</td>
      <td>81.93</td>
      <td>53.63</td>
      <td>39.23</td>
      <td>76.95</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>dd61a482fa2f71efe6f22aae6949355ca4b06ccc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-13b-2.1</td>
      <td>53.34</td>
      <td>59.47</td>
      <td>82.47</td>
      <td>54.83</td>
      <td>44.65</td>
      <td>75.06</td>
      <td>3.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>172e30e56e939f73d7d00a165c2d49cbd284481f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o</td>
      <td>53.32</td>
      <td>58.36</td>
      <td>81.10</td>
      <td>54.53</td>
      <td>37.02</td>
      <td>76.64</td>
      <td>12.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5cbcd9c0a6b9a19f0d099e653cde18e11bf95303</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/vicuna-13b-v1.3.0-GPTQ</td>
      <td>53.29</td>
      <td>54.35</td>
      <td>79.47</td>
      <td>51.97</td>
      <td>50.88</td>
      <td>74.66</td>
      <td>8.42</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>19.0</td>
      <td>True</td>
      <td>6ef1f8d8638ea2d6681a8e3da73be57c501d847b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>pillowtalks-ai/delta13b</td>
      <td>53.29</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>83fa0860990df1db35550f973ba4306449e35412</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>eachadea/vicuna-13b-1.1</td>
      <td>53.29</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>136.0</td>
      <td>True</td>
      <td>bfcc6ca66694310be6c85ba0638597f4256c4143</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kevinpro/Vicuna-13B-CoT</td>
      <td>53.29</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>346e3c46959cf9f1e03feffa761afe020c0fb6a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/vicuna-13B-1.1-HF</td>
      <td>53.29</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>8c71dbe9221e83d2ec72e4dc08beccfc78b563c0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-13b-delta-v1.1</td>
      <td>53.28</td>
      <td>52.73</td>
      <td>80.14</td>
      <td>51.90</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>409.0</td>
      <td>False</td>
      <td>ffed4c7cf1b9814812078efbe29ec3f610ea39e7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-13b-v1.1</td>
      <td>53.28</td>
      <td>52.73</td>
      <td>80.14</td>
      <td>51.90</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>96.0</td>
      <td>False</td>
      <td>8c71dbe9221e83d2ec72e4dc08beccfc78b563c0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Vicuna-13B-CoT-fp16</td>
      <td>53.28</td>
      <td>52.73</td>
      <td>80.14</td>
      <td>51.90</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>fe74a0ece9089828b301bd0f067ae5f257516179</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Llama-2-13B-GPTQ</td>
      <td>53.26</td>
      <td>59.13</td>
      <td>81.48</td>
      <td>54.45</td>
      <td>37.07</td>
      <td>76.16</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>llama2</td>
      <td>16.23</td>
      <td>111.0</td>
      <td>True</td>
      <td>b7db471d1789802a3a8e3b93cdd66a9f046f17c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AdaptLLM/finance-chat</td>
      <td>53.26</td>
      <td>53.75</td>
      <td>76.60</td>
      <td>50.16</td>
      <td>44.54</td>
      <td>75.69</td>
      <td>18.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>42d449dc4f42960a52130893843136ab3fed1256</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down</td>
      <td>53.23</td>
      <td>56.31</td>
      <td>81.43</td>
      <td>55.30</td>
      <td>39.11</td>
      <td>76.80</td>
      <td>10.46</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>0d8d502e4e5ef89592dd0d3bc7223eaf7f77f78b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/airoboros-2.1-llama-2-13B-QLoRa</td>
      <td>53.23</td>
      <td>59.73</td>
      <td>82.91</td>
      <td>54.77</td>
      <td>45.14</td>
      <td>74.03</td>
      <td>2.81</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ebf991c8d34314caab6ccc6b078c681d20bac39a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Technoculture/Medtulu-2x7b</td>
      <td>53.21</td>
      <td>54.61</td>
      <td>75.68</td>
      <td>49.12</td>
      <td>50.04</td>
      <td>72.85</td>
      <td>16.98</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>11.07</td>
      <td>1.0</td>
      <td>True</td>
      <td>76a032af4d8eec7cd9b621c887cdfaa5d99b4cd9</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE2_TEST_2.2w</td>
      <td>53.20</td>
      <td>56.23</td>
      <td>82.70</td>
      <td>55.35</td>
      <td>39.55</td>
      <td>76.72</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3be177b35f1b44d147751ab38ca6d8a008eb6b7f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>meta-math/MetaMath-Llemma-7B</td>
      <td>53.19</td>
      <td>46.50</td>
      <td>61.69</td>
      <td>47.66</td>
      <td>39.61</td>
      <td>62.75</td>
      <td>60.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>e31ec61dccd8fa24f44f0592a518491ef76a2235</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o</td>
      <td>53.18</td>
      <td>54.78</td>
      <td>81.40</td>
      <td>54.73</td>
      <td>41.02</td>
      <td>76.64</td>
      <td>10.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>8702b433008a62e9f8bf15e70ba15fa7100e991c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zarafusionix-l2-7b</td>
      <td>53.18</td>
      <td>55.55</td>
      <td>79.40</td>
      <td>51.21</td>
      <td>51.05</td>
      <td>74.66</td>
      <td>7.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>13d0e2498a4b5f53f6dc2464f20e093b07a4bd4b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-fp16</td>
      <td>53.16</td>
      <td>58.62</td>
      <td>81.07</td>
      <td>48.32</td>
      <td>54.19</td>
      <td>76.01</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>83905656ca3e63877b8d9f3a74118da0c9bc6939</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Athena-Platypus2-13B-QLora-0.80-epoch</td>
      <td>53.16</td>
      <td>56.66</td>
      <td>80.56</td>
      <td>55.43</td>
      <td>53.62</td>
      <td>72.61</td>
      <td>0.08</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7b6c11b4df16079dfdd1e8dd8c489a8835c7cc4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Airboros2.1-Platypus2-13B-QLora-0.80-epoch</td>
      <td>53.15</td>
      <td>58.96</td>
      <td>82.46</td>
      <td>54.62</td>
      <td>47.71</td>
      <td>75.14</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>45bd1e47218ba2e075e03f6407980eb839e67eb3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Enno-Ai/vigogne2-enno-13b-sft-lora-4bit</td>
      <td>53.15</td>
      <td>62.03</td>
      <td>82.65</td>
      <td>54.11</td>
      <td>42.98</td>
      <td>76.95</td>
      <td>0.15</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2a1b03977395eee44742abda63a4787ea5371d06</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Airoboros-L2-13B-2.1-GPTQ</td>
      <td>53.14</td>
      <td>58.96</td>
      <td>81.72</td>
      <td>53.16</td>
      <td>44.68</td>
      <td>74.35</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>llama2</td>
      <td>16.23</td>
      <td>9.0</td>
      <td>True</td>
      <td>d90d96e40b9359cb5c35e6b6c8f0eb24896e827b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16</td>
      <td>53.14</td>
      <td>59.04</td>
      <td>82.33</td>
      <td>55.36</td>
      <td>35.75</td>
      <td>76.32</td>
      <td>10.01</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a3ed7416156963f49bf4dc056188e006c0c214d2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/llama-13b-pretrained-sft-do2</td>
      <td>53.12</td>
      <td>58.96</td>
      <td>80.32</td>
      <td>47.25</td>
      <td>47.41</td>
      <td>75.53</td>
      <td>9.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>6cb016f5bfcbc24ee08312b52f08ef5e8f860871</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/MLewd-L2-13B</td>
      <td>53.12</td>
      <td>58.28</td>
      <td>82.32</td>
      <td>54.67</td>
      <td>48.66</td>
      <td>73.48</td>
      <td>1.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>feb1fa71e0b24261d3ca428b4aed881dd31f166e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jjaaaww/posi_13b</td>
      <td>53.12</td>
      <td>59.64</td>
      <td>82.52</td>
      <td>56.56</td>
      <td>42.14</td>
      <td>76.24</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ff4eeb0f876c41553c302020041a0e78a15f9aa7</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/llama-13b-pretrained-sft-epoch-1</td>
      <td>53.11</td>
      <td>57.25</td>
      <td>79.99</td>
      <td>45.52</td>
      <td>44.45</td>
      <td>77.58</td>
      <td>13.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1f839c019153789c15bbc45ecbb512d0f5015881</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/manticore-13b-chat-pyg-GPTQ</td>
      <td>53.11</td>
      <td>57.85</td>
      <td>81.07</td>
      <td>47.56</td>
      <td>47.77</td>
      <td>75.93</td>
      <td>8.49</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>31.0</td>
      <td>True</td>
      <td>923f27245d13058c9c1b3ab0eab6c6c93ffc162e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>itsliupeng/llama2_7b_mmlu</td>
      <td>53.10</td>
      <td>56.14</td>
      <td>79.13</td>
      <td>60.04</td>
      <td>40.95</td>
      <td>74.43</td>
      <td>7.88</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>553178f8d5d69eb1dfa5b9503d2ce0c1e481e5b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/llama2-13b-FINETUNE3_TEST</td>
      <td>53.09</td>
      <td>53.67</td>
      <td>79.66</td>
      <td>54.48</td>
      <td>40.22</td>
      <td>75.93</td>
      <td>14.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>22cea7bf138eb0d6c962812df2b2235290acbee2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down</td>
      <td>53.06</td>
      <td>57.76</td>
      <td>80.78</td>
      <td>54.32</td>
      <td>40.80</td>
      <td>76.72</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ebe1b75fa315a9b55f686368070a0bcd0245ee39</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/FINETUNE3_TEST4</td>
      <td>53.02</td>
      <td>55.63</td>
      <td>81.31</td>
      <td>52.13</td>
      <td>41.14</td>
      <td>76.72</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>5195e87bb34317c5aaf201faa476aae78ecc9f1b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Open-Orca/LlongOrca-7B-16k</td>
      <td>53.02</td>
      <td>57.51</td>
      <td>79.44</td>
      <td>49.35</td>
      <td>49.84</td>
      <td>74.51</td>
      <td>7.51</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>42.0</td>
      <td>True</td>
      <td>1370c7c595e6c8394e6332bc535ae25e21def85b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-13b-gpt4-1.4.1</td>
      <td>53.02</td>
      <td>59.13</td>
      <td>82.78</td>
      <td>55.62</td>
      <td>40.27</td>
      <td>73.32</td>
      <td>6.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>35ff51ebe5668269dfd33a9ed94412d88f1f4b65</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Walter-Mistral-7B</td>
      <td>53.00</td>
      <td>58.87</td>
      <td>83.43</td>
      <td>58.65</td>
      <td>39.93</td>
      <td>77.03</td>
      <td>0.08</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>d7ccd4f0360c397765578521efaed394fe00dbf5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/llama-13b-pretrained-dropout</td>
      <td>52.99</td>
      <td>56.40</td>
      <td>79.34</td>
      <td>46.59</td>
      <td>48.60</td>
      <td>75.22</td>
      <td>11.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>045c84727d495bfb4b612a2482ce0d807c067b46</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AdaptLLM/medicine-chat</td>
      <td>52.99</td>
      <td>53.75</td>
      <td>76.11</td>
      <td>49.98</td>
      <td>43.46</td>
      <td>75.69</td>
      <td>18.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>32824ba93e88ccfe8464f6d267a5d67024c7722b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>The-Face-Of-Goonery/Huginn-19b-prototype</td>
      <td>52.99</td>
      <td>59.22</td>
      <td>81.03</td>
      <td>55.73</td>
      <td>41.15</td>
      <td>76.40</td>
      <td>4.40</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>19.36</td>
      <td>2.0</td>
      <td>False</td>
      <td>d2c8cc15c57da217ff29ebaaae4bc4f57d6b21b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>heegyu/LIMA2-13b-hf</td>
      <td>52.98</td>
      <td>60.24</td>
      <td>83.69</td>
      <td>53.17</td>
      <td>41.81</td>
      <td>73.24</td>
      <td>5.76</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ed3535921eb24e0737f9a6cda70b1a3fd71532cd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xzuyn/Alpacino-SuperCOT-13B</td>
      <td>52.97</td>
      <td>58.36</td>
      <td>81.69</td>
      <td>47.89</td>
      <td>45.42</td>
      <td>76.95</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>3a82b04684fe99d59556421c3f96a187049a3cec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Code-290k-13B</td>
      <td>52.96</td>
      <td>56.06</td>
      <td>81.55</td>
      <td>51.99</td>
      <td>37.65</td>
      <td>72.69</td>
      <td>17.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>e2595df2aedc1decaf73d167ce0114e7a9cb2126</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>allenai/digital-socrates-7b</td>
      <td>52.95</td>
      <td>54.44</td>
      <td>75.99</td>
      <td>51.41</td>
      <td>44.88</td>
      <td>73.09</td>
      <td>17.89</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>5d26db18b95778c31dc8425871052f495b267563</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zaraxe-l2-7b</td>
      <td>52.95</td>
      <td>57.17</td>
      <td>79.34</td>
      <td>51.00</td>
      <td>49.11</td>
      <td>73.48</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0875bf202aedeef7a58d7382fd6f55f5bca12968</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sethuiyer/Dr_Samantha-7b</td>
      <td>52.95</td>
      <td>53.84</td>
      <td>77.95</td>
      <td>47.94</td>
      <td>45.58</td>
      <td>73.56</td>
      <td>18.80</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>2.0</td>
      <td>True</td>
      <td>b1a643e32e467d8dd722186d6c36d16ea4281003</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</td>
      <td>52.94</td>
      <td>58.45</td>
      <td>81.97</td>
      <td>55.02</td>
      <td>35.85</td>
      <td>75.69</td>
      <td>10.69</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>5a89844b1aea3f0573e696143ec66727df4b5d79</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>project-baize/baize-v2-13b</td>
      <td>52.94</td>
      <td>56.91</td>
      <td>79.29</td>
      <td>49.72</td>
      <td>47.88</td>
      <td>74.90</td>
      <td>8.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>24.0</td>
      <td>True</td>
      <td>a3c4bbccca8b650700a49a225582c17bb49b446b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-llama2-13b-v11-bf16</td>
      <td>52.93</td>
      <td>52.99</td>
      <td>75.38</td>
      <td>51.36</td>
      <td>47.94</td>
      <td>71.03</td>
      <td>18.88</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>4d4e72c553e9d60fdc208663b0a1c0364caa2f30</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-100step-flan-v2</td>
      <td>52.92</td>
      <td>53.24</td>
      <td>78.43</td>
      <td>48.43</td>
      <td>45.66</td>
      <td>72.30</td>
      <td>19.48</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0f1873b505a5f32ca429c164a229bab663eaf617</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nkpz/llama2-22b-chat-wizard-uncensored</td>
      <td>52.90</td>
      <td>56.23</td>
      <td>80.39</td>
      <td>53.62</td>
      <td>45.76</td>
      <td>70.24</td>
      <td>11.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>21.83</td>
      <td>3.0</td>
      <td>True</td>
      <td>90cffebc8f530161505b84740ff6c8f646299d6c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pe-nlp/llama-2-13b-platypus-vicuna-wizard</td>
      <td>52.90</td>
      <td>61.26</td>
      <td>82.31</td>
      <td>55.21</td>
      <td>41.91</td>
      <td>75.77</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>71aa919fc15fa9d9def9185791b15a3f76e7bd8d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>clibrain/Llama-2-13b-ft-instruct-es</td>
      <td>52.89</td>
      <td>59.39</td>
      <td>81.51</td>
      <td>54.31</td>
      <td>37.81</td>
      <td>75.77</td>
      <td>8.57</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>772b53f64f484fa0d651d453bcefc35a0f52f251</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AdaptLLM/law-chat</td>
      <td>52.88</td>
      <td>53.41</td>
      <td>76.16</td>
      <td>50.24</td>
      <td>43.53</td>
      <td>75.45</td>
      <td>18.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>0.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>0bf36fdc22bf30632cced8044667d3d46061d619</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wei123602/llama2-13b-fintune2-4E</td>
      <td>52.88</td>
      <td>55.89</td>
      <td>80.95</td>
      <td>53.73</td>
      <td>42.72</td>
      <td>73.09</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>645ede9d6ec60d8fa051bc7ad32ab5f7bfdc066d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-100step-flan</td>
      <td>52.88</td>
      <td>52.90</td>
      <td>78.44</td>
      <td>48.40</td>
      <td>45.67</td>
      <td>72.38</td>
      <td>19.48</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d502ae9a15c38118baa5ae55e048a080cb05c89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down</td>
      <td>52.88</td>
      <td>55.97</td>
      <td>81.53</td>
      <td>54.42</td>
      <td>40.72</td>
      <td>75.06</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>905fc0b26dcb9e1fc5be99e73596e0884f9b71df</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>ContextualAI/archangel_sft-kto_llama13b</td>
      <td>52.87</td>
      <td>56.14</td>
      <td>80.80</td>
      <td>47.84</td>
      <td>39.42</td>
      <td>76.16</td>
      <td>16.83</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>d596fb0060168006360610d673c2c35edcbbf110</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Yhyu13/chimera-inst-chat-13b-hf</td>
      <td>52.86</td>
      <td>55.38</td>
      <td>78.93</td>
      <td>50.60</td>
      <td>50.12</td>
      <td>73.95</td>
      <td>8.19</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>a6943d2d30d0af904b3321559157d589e60f9e0f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>stabilityai/japanese-stablelm-instruct-gamma-7b</td>
      <td>52.82</td>
      <td>50.68</td>
      <td>78.68</td>
      <td>54.82</td>
      <td>39.77</td>
      <td>73.72</td>
      <td>19.26</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>38.0</td>
      <td>True</td>
      <td>044918151c5b3910d12f2e489fb7c60752048e1e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>mosaicml/mpt-30b</td>
      <td>52.77</td>
      <td>55.97</td>
      <td>82.42</td>
      <td>48.00</td>
      <td>38.42</td>
      <td>74.90</td>
      <td>16.91</td>
      <td>pretrained</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>30.00</td>
      <td>331.0</td>
      <td>True</td>
      <td>0261af71d7177453889f868d26607dec8d5aaa2e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Llama2-13B-no_robots-alpaca-lora</td>
      <td>52.77</td>
      <td>58.87</td>
      <td>82.43</td>
      <td>53.11</td>
      <td>40.46</td>
      <td>75.30</td>
      <td>6.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>581aba329e607533c299746bb9eb4154a7aab139</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chargoddard/ypotryll-22b-epoch2-qlora</td>
      <td>52.75</td>
      <td>59.22</td>
      <td>80.66</td>
      <td>54.52</td>
      <td>40.42</td>
      <td>76.32</td>
      <td>5.38</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>22.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>26fdd8fa420d72ed835c7d17086f0441db0985d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/wizard-vicuna-13B-HF</td>
      <td>52.75</td>
      <td>54.69</td>
      <td>79.18</td>
      <td>48.88</td>
      <td>49.62</td>
      <td>74.82</td>
      <td>9.33</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>48.0</td>
      <td>False</td>
      <td>12dc8aacb474522ae2a83c18cb0fdf0907987f8f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_v2_13b</td>
      <td>52.75</td>
      <td>55.12</td>
      <td>79.69</td>
      <td>50.07</td>
      <td>52.56</td>
      <td>72.69</td>
      <td>6.37</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>1058709314f7ca090937d0a2b7b37b0b3a8f12a3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-200step-flan-v2</td>
      <td>52.75</td>
      <td>52.65</td>
      <td>78.04</td>
      <td>48.51</td>
      <td>45.42</td>
      <td>72.93</td>
      <td>18.95</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>35e4747656b719af659625092174f188584934c1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>totally-not-an-llm/EverythingLM-13b-V2-16k</td>
      <td>52.75</td>
      <td>58.70</td>
      <td>80.88</td>
      <td>49.69</td>
      <td>47.37</td>
      <td>73.01</td>
      <td>6.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>31.0</td>
      <td>True</td>
      <td>943f932ae1ae462389e6d2db5273158530749fff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>junelee/wizard-vicuna-13b</td>
      <td>52.73</td>
      <td>54.69</td>
      <td>79.18</td>
      <td>48.88</td>
      <td>49.62</td>
      <td>74.82</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>73.0</td>
      <td>False</td>
      <td>419dc5acc391de54a60d0b041e94e767d1ef2032</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-13b-fast-instruct</td>
      <td>52.72</td>
      <td>57.51</td>
      <td>81.82</td>
      <td>54.52</td>
      <td>43.82</td>
      <td>75.93</td>
      <td>2.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>2a0b52cd72a30d26ef0391c171b64900106a90a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openchat/openchat_8192</td>
      <td>52.72</td>
      <td>59.56</td>
      <td>81.44</td>
      <td>46.26</td>
      <td>46.70</td>
      <td>74.98</td>
      <td>7.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>220.0</td>
      <td>False</td>
      <td>f661da5af278fbda8a43b19ff0250e4efc103e3e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-7B-v1.2</td>
      <td>52.71</td>
      <td>54.35</td>
      <td>79.29</td>
      <td>49.33</td>
      <td>48.92</td>
      <td>73.56</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>85ea4f4818478084eedd01e958ac5cc7cf64b3bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>PocketDoc/Dans-PersonalityEngine-13b</td>
      <td>52.71</td>
      <td>58.45</td>
      <td>82.30</td>
      <td>47.58</td>
      <td>41.12</td>
      <td>77.51</td>
      <td>9.33</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>3b37c31e04419adcc91eddb57f24fd6f9ac91938</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>meta-math/MetaMath-13B-V1.0</td>
      <td>52.71</td>
      <td>49.49</td>
      <td>76.48</td>
      <td>47.74</td>
      <td>41.58</td>
      <td>72.45</td>
      <td>28.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>0b448f6f64808f8bca94dc871e96a3eae7e95621</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Yehoon/yehoon_llama2</td>
      <td>52.71</td>
      <td>54.78</td>
      <td>78.98</td>
      <td>51.29</td>
      <td>49.17</td>
      <td>74.74</td>
      <td>7.28</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>443cb81ce988ea6c0b1e20132c170463d559367e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/mcq-hal-vicuna-13b-v1.5</td>
      <td>52.70</td>
      <td>55.97</td>
      <td>80.72</td>
      <td>52.85</td>
      <td>45.03</td>
      <td>72.77</td>
      <td>8.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>bb3029bce8347b09c2fd6908475b195bcabe53e3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Capybara-7B</td>
      <td>52.70</td>
      <td>55.29</td>
      <td>80.73</td>
      <td>48.72</td>
      <td>51.13</td>
      <td>73.32</td>
      <td>6.97</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>42dfc6f7d735670e2f3e30b0919708a81f9a0df9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HyperbeeAI/Tulpar-7b-v0</td>
      <td>52.69</td>
      <td>56.31</td>
      <td>79.01</td>
      <td>52.55</td>
      <td>51.68</td>
      <td>73.88</td>
      <td>2.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>d7c2bc52a3ae13571357f51273ae948caf84400e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Capybara-7B</td>
      <td>52.69</td>
      <td>55.20</td>
      <td>80.76</td>
      <td>48.80</td>
      <td>51.07</td>
      <td>73.40</td>
      <td>6.90</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>42dfc6f7d735670e2f3e30b0919708a81f9a0df9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/CodeEngine</td>
      <td>52.68</td>
      <td>58.36</td>
      <td>82.27</td>
      <td>54.18</td>
      <td>45.18</td>
      <td>74.59</td>
      <td>1.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f57879831c39f2dcb656cb2c9e9ce5878e92bb44</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/mcq-vicuna-13b-v1.5</td>
      <td>52.68</td>
      <td>56.66</td>
      <td>81.09</td>
      <td>53.30</td>
      <td>43.99</td>
      <td>73.01</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f769a92cfeffe8ee07beee8814ce7eca7cd62805</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mixtral-8x7b-v16.1-32k</td>
      <td>52.68</td>
      <td>29.10</td>
      <td>82.27</td>
      <td>71.37</td>
      <td>55.97</td>
      <td>77.35</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.74</td>
      <td>3.0</td>
      <td>True</td>
      <td>51086693792910d6bc89398200c5eca8b6930f6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>teknium/Mistral-Trismegistus-7B</td>
      <td>52.66</td>
      <td>54.10</td>
      <td>77.91</td>
      <td>54.49</td>
      <td>49.36</td>
      <td>70.17</td>
      <td>9.93</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>156.0</td>
      <td>True</td>
      <td>0a5752d096ebab21759dbe203f6b7c7f6092faf2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-13b-gpt4-m2.0</td>
      <td>52.66</td>
      <td>59.22</td>
      <td>81.02</td>
      <td>53.73</td>
      <td>39.70</td>
      <td>73.64</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>27.0</td>
      <td>True</td>
      <td>a852b77f7d0777092c76898bc83f8e657ca2af3e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LeoLM/leo-hessianai-13b</td>
      <td>52.65</td>
      <td>57.25</td>
      <td>81.94</td>
      <td>53.65</td>
      <td>38.03</td>
      <td>76.09</td>
      <td>8.95</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>27.0</td>
      <td>False</td>
      <td>a947965cb07ca12a38ff981fe65b618d7dea28d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-200step-flan</td>
      <td>52.62</td>
      <td>52.47</td>
      <td>78.02</td>
      <td>48.42</td>
      <td>45.47</td>
      <td>72.69</td>
      <td>18.65</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>03550d05aac147dde6d70b7b63f4a1661ecf5cb3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mistral-7b-v13.1</td>
      <td>52.62</td>
      <td>52.56</td>
      <td>75.73</td>
      <td>56.68</td>
      <td>50.44</td>
      <td>71.59</td>
      <td>8.72</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>b64386bde3d7850a01df763f5c777c74888d34fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>heegyu/LIMA-13b-hf</td>
      <td>52.61</td>
      <td>57.42</td>
      <td>81.68</td>
      <td>48.72</td>
      <td>41.76</td>
      <td>77.19</td>
      <td>8.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>98faa74a9b41cbd9033904cd58420705936849eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LinkSoul/Chinese-Llama-2-7b</td>
      <td>52.59</td>
      <td>52.99</td>
      <td>75.64</td>
      <td>50.74</td>
      <td>48.94</td>
      <td>72.77</td>
      <td>14.48</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>7.00</td>
      <td>292.0</td>
      <td>True</td>
      <td>72efd71d7f89d9c46008b7a574faf90300ed9ba8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>stabilityai/japanese-stablelm-base-gamma-7b</td>
      <td>52.59</td>
      <td>50.34</td>
      <td>77.47</td>
      <td>54.75</td>
      <td>41.20</td>
      <td>73.95</td>
      <td>17.82</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>13.0</td>
      <td>True</td>
      <td>e1c3840c716485077b688296fefa8e5641249843</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Locutusque/Mistral-7B-SFT</td>
      <td>52.58</td>
      <td>46.50</td>
      <td>75.69</td>
      <td>51.04</td>
      <td>52.02</td>
      <td>72.77</td>
      <td>17.44</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>db1c291a7cbab162ebfb9512f9d27a95b42c6548</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>frank098/Wizard-Vicuna-13B-juniper</td>
      <td>52.55</td>
      <td>55.89</td>
      <td>79.75</td>
      <td>44.99</td>
      <td>54.72</td>
      <td>72.69</td>
      <td>7.28</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>24f58beb9ed4cf635fc962853ed71d0f4b1909ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wahaha1987/llama_13b_sharegpt94k_fastchat</td>
      <td>52.55</td>
      <td>53.75</td>
      <td>79.47</td>
      <td>51.50</td>
      <td>49.54</td>
      <td>72.61</td>
      <td>8.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>388bc2f82a1ee8b963c7f94f9c7b6743f7214306</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/mcq-vicuna-13b-v1.5</td>
      <td>52.55</td>
      <td>56.23</td>
      <td>81.15</td>
      <td>53.38</td>
      <td>44.08</td>
      <td>72.93</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f769a92cfeffe8ee07beee8814ce7eca7cd62805</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-dolphin-orca-platypus-34b</td>
      <td>52.53</td>
      <td>52.47</td>
      <td>74.13</td>
      <td>53.47</td>
      <td>47.14</td>
      <td>73.24</td>
      <td>14.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>57e18e617b4fd7ab61bd7da8ee9516513ad76842</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>speechlessai/speechless-codellama-34b-v1.0</td>
      <td>52.53</td>
      <td>52.47</td>
      <td>74.13</td>
      <td>53.47</td>
      <td>47.14</td>
      <td>73.24</td>
      <td>14.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d64d871cd56da3031e19bc267ef8bd0b85b9936</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-34b-v2.0</td>
      <td>52.51</td>
      <td>54.35</td>
      <td>75.65</td>
      <td>54.67</td>
      <td>45.21</td>
      <td>73.56</td>
      <td>11.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>cb81174d72dbe06f8db1c406ef97981532de6f09</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lvkaokao/llama2-7b-hf-chat-lora-v2</td>
      <td>52.50</td>
      <td>55.03</td>
      <td>78.81</td>
      <td>51.35</td>
      <td>44.05</td>
      <td>74.90</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>0b8e61d3325cddbad207cbf885c2b5db6a83a059</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-13b-gpt4-2.0</td>
      <td>52.49</td>
      <td>59.04</td>
      <td>82.82</td>
      <td>54.71</td>
      <td>36.47</td>
      <td>74.19</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>ec556571acc6783fea4414e4ca72d291c563b6dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>liuda1/Mistral-7B-golden</td>
      <td>52.49</td>
      <td>60.75</td>
      <td>44.42</td>
      <td>59.29</td>
      <td>53.51</td>
      <td>76.64</td>
      <td>20.32</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>unknown</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>bc4624485fef5a2e3fcde465eaf2191cb1df1877</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-13b-longlora-32k-ft</td>
      <td>52.49</td>
      <td>59.47</td>
      <td>82.61</td>
      <td>52.13</td>
      <td>37.44</td>
      <td>75.53</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>6d17c854025b0bd54ce572ac803f1bb052875dbf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lvkaokao/llama2-7b-hf-chat-lora-v3</td>
      <td>52.48</td>
      <td>57.25</td>
      <td>78.62</td>
      <td>50.57</td>
      <td>50.62</td>
      <td>76.32</td>
      <td>1.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>79047f667253c878ad3143b016e3dcb3df707572</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>wang7776/Llama-2-7b-chat-hf-10-sparsity</td>
      <td>52.48</td>
      <td>53.16</td>
      <td>78.26</td>
      <td>48.18</td>
      <td>45.29</td>
      <td>71.59</td>
      <td>18.42</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>9dda6f163ab399b0ae0fd19d6fe8ec37d9ff97be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beaugogh/Llama2-7b-openorca-mc-v2</td>
      <td>52.47</td>
      <td>55.55</td>
      <td>81.26</td>
      <td>48.30</td>
      <td>51.49</td>
      <td>72.85</td>
      <td>5.38</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e74a9cca843cdeb8591d4e4f4320dc1870adf1b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lizhuang144/llama_mirror_13b_v1.0</td>
      <td>52.46</td>
      <td>57.59</td>
      <td>80.53</td>
      <td>48.00</td>
      <td>44.54</td>
      <td>76.64</td>
      <td>7.43</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>379cb8f080110f3418155029f534f67a79e25db4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-2-7b-chat</td>
      <td>52.45</td>
      <td>55.63</td>
      <td>78.71</td>
      <td>50.98</td>
      <td>47.21</td>
      <td>74.43</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>7a1b76feabe3e0ed007ea83ee93f7644156d3b23</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ausboss/llama-13b-supercot</td>
      <td>52.44</td>
      <td>56.06</td>
      <td>81.71</td>
      <td>45.36</td>
      <td>48.55</td>
      <td>75.77</td>
      <td>7.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>f6953fa162b487a3d4c6bdc7b7951e09576c2ae5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>camel-ai/CAMEL-13B-Combined-Data</td>
      <td>52.44</td>
      <td>55.63</td>
      <td>79.25</td>
      <td>49.74</td>
      <td>47.42</td>
      <td>75.45</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>6d98f2801f13d89de7978ee9f348a52ea46a24ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-PileOfSets-Mk1-llama-13b-merged</td>
      <td>52.43</td>
      <td>58.79</td>
      <td>81.79</td>
      <td>48.12</td>
      <td>41.24</td>
      <td>76.16</td>
      <td>8.49</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a7e5484df8aceae7800ae9301a3954cf74b527e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-300step-flan-v2</td>
      <td>52.41</td>
      <td>52.56</td>
      <td>77.76</td>
      <td>48.51</td>
      <td>45.14</td>
      <td>72.53</td>
      <td>17.97</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a2191bd90b04396016b7420dd14675916056f44a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>52.41</td>
      <td>54.52</td>
      <td>79.36</td>
      <td>55.15</td>
      <td>54.32</td>
      <td>71.11</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>4b5aabc51907e4cba49f373c6dc09a2634f2fb8a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zararp-l2-7b</td>
      <td>52.39</td>
      <td>56.31</td>
      <td>79.19</td>
      <td>51.36</td>
      <td>51.26</td>
      <td>74.51</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>6032c5106970f98d59925959fbd330ae4b1d1a7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Unbabel/TowerInstruct-7B-v0.1</td>
      <td>52.39</td>
      <td>55.46</td>
      <td>79.00</td>
      <td>46.88</td>
      <td>42.59</td>
      <td>73.95</td>
      <td>16.45</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>6.74</td>
      <td>19.0</td>
      <td>True</td>
      <td>d97a456da8a218425b5171a906a7d9a0c5cd7b2f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Alpacino13b</td>
      <td>52.39</td>
      <td>58.53</td>
      <td>81.31</td>
      <td>47.92</td>
      <td>41.66</td>
      <td>76.95</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>29.0</td>
      <td>True</td>
      <td>7092a5c8dec649694dd66ff8cfe5452ce52e6a40</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mncai/Llama2-7B-guanaco-dolphin-500</td>
      <td>52.38</td>
      <td>56.74</td>
      <td>81.63</td>
      <td>48.69</td>
      <td>46.94</td>
      <td>74.27</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>afe00170f084f773e401ba7d738d692533cca6b4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>The-Face-Of-Goonery/Huginn-22b-Prototype</td>
      <td>52.36</td>
      <td>57.68</td>
      <td>80.69</td>
      <td>49.81</td>
      <td>52.11</td>
      <td>71.59</td>
      <td>2.27</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>21.83</td>
      <td>2.0</td>
      <td>False</td>
      <td>29222b05794abb862ad0aaaf3020696c9f599810</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>totally-not-an-llm/EverythingLM-13b-16k</td>
      <td>52.33</td>
      <td>56.57</td>
      <td>80.58</td>
      <td>50.18</td>
      <td>47.46</td>
      <td>72.77</td>
      <td>6.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>31.0</td>
      <td>True</td>
      <td>8456a856a8b115b05e76a7d0d945853b10ac71e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>beaugogh/Llama2-7b-openorca-mc-v2-dpo</td>
      <td>52.32</td>
      <td>54.78</td>
      <td>81.48</td>
      <td>47.20</td>
      <td>53.13</td>
      <td>72.85</td>
      <td>4.47</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>734a6f0c69e1e53b988c107926bc17cb0536f851</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-atom-13b-v9-bf16</td>
      <td>52.31</td>
      <td>51.19</td>
      <td>75.99</td>
      <td>49.33</td>
      <td>48.66</td>
      <td>73.32</td>
      <td>15.39</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>35bb2c73953f6ea40be6f0c8c6b2dfa7ecbaa0df</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b-gpt4-1.2</td>
      <td>52.31</td>
      <td>58.36</td>
      <td>81.61</td>
      <td>48.84</td>
      <td>47.54</td>
      <td>73.64</td>
      <td>3.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>482bd38b65e73fde13f5d03fed2bee7acda8fadd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>eachadea/vicuna-13b</td>
      <td>52.30</td>
      <td>51.71</td>
      <td>79.94</td>
      <td>50.84</td>
      <td>52.68</td>
      <td>71.03</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ac4218770a58baaaaf25201076fe082abb6ffd13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>prithivida/Asimov-7B-v2</td>
      <td>52.29</td>
      <td>54.27</td>
      <td>78.72</td>
      <td>52.59</td>
      <td>45.44</td>
      <td>71.82</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0aeea2284ac78cac081bee88e5a98a19bb987227</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mncai/Llama2-7B-guanaco-1k</td>
      <td>52.28</td>
      <td>55.12</td>
      <td>80.53</td>
      <td>47.93</td>
      <td>47.69</td>
      <td>74.82</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>5f3194b779897bbc4c4218a9dddc44a9b5faea15</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-441step-flan-v2</td>
      <td>52.28</td>
      <td>52.13</td>
      <td>77.63</td>
      <td>48.52</td>
      <td>45.02</td>
      <td>72.53</td>
      <td>17.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>daede60607179be05b5d6e90b4c6777806b10fb8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TFLai/Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>52.27</td>
      <td>57.76</td>
      <td>81.63</td>
      <td>55.63</td>
      <td>39.70</td>
      <td>75.93</td>
      <td>2.96</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>114eb8efd2de1c9eae85d92de490b95c854dfae9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>oh-yeontaek/llama-2-7B-LoRA-assemble</td>
      <td>52.26</td>
      <td>57.34</td>
      <td>78.81</td>
      <td>50.75</td>
      <td>53.18</td>
      <td>73.48</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>72e866a96a2e9afc6527c8d757c69088c3a069c8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-200step-merged</td>
      <td>52.26</td>
      <td>52.05</td>
      <td>77.38</td>
      <td>48.65</td>
      <td>44.60</td>
      <td>71.90</td>
      <td>18.95</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>858de1c14854e55d5141b8d1b3954b335044669e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>beaugogh/Llama2-7b-openorca-mc-v1</td>
      <td>52.24</td>
      <td>55.63</td>
      <td>80.17</td>
      <td>48.44</td>
      <td>51.62</td>
      <td>73.48</td>
      <td>4.09</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2c4096fa2129665fb127f1c2a1302f30565a5265</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zararp-1.1-l2-7b</td>
      <td>52.22</td>
      <td>56.48</td>
      <td>78.85</td>
      <td>51.49</td>
      <td>51.99</td>
      <td>73.40</td>
      <td>1.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>31fa6527a3285d5fd320219d7c2dadde07b83718</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>LTC-AI-Labs/L2-7b-Hermes-Synthia</td>
      <td>52.21</td>
      <td>51.02</td>
      <td>79.12</td>
      <td>47.88</td>
      <td>46.77</td>
      <td>74.51</td>
      <td>13.95</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>6f9bd33be62c4b5dbbb8d76ad30d61c3ceb01641</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Nous-Hermes-13B-SuperHOT-8K-fp16</td>
      <td>52.18</td>
      <td>55.29</td>
      <td>81.87</td>
      <td>48.23</td>
      <td>51.19</td>
      <td>75.30</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>b407c1ece029ad5693d38e6e0931e9482962ed15</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/oasst-llama-13b-1000-steps</td>
      <td>52.18</td>
      <td>58.11</td>
      <td>81.52</td>
      <td>48.65</td>
      <td>35.99</td>
      <td>77.51</td>
      <td>11.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d2cd599cc40db3370009f45d6caa7e486cb6d31f</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HyperbeeAI/Tulpar-7b-v1</td>
      <td>52.16</td>
      <td>57.00</td>
      <td>79.69</td>
      <td>51.33</td>
      <td>51.83</td>
      <td>72.45</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>719d8e1eb4a820f01e0a92ef6220d041964bb472</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ</td>
      <td>52.15</td>
      <td>57.00</td>
      <td>80.32</td>
      <td>47.08</td>
      <td>53.46</td>
      <td>74.35</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>46.0</td>
      <td>True</td>
      <td>085eb5cd394f30d72bf5efcf83a580e87264b3e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-1.3-L2-13B</td>
      <td>52.15</td>
      <td>56.83</td>
      <td>81.70</td>
      <td>52.79</td>
      <td>50.23</td>
      <td>71.11</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>45ba2f603769aa6b97639962f522b8d7398c2393</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>xxyyy123/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2</td>
      <td>52.13</td>
      <td>57.17</td>
      <td>79.57</td>
      <td>50.24</td>
      <td>52.51</td>
      <td>72.93</td>
      <td>0.38</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>9c4a7444d6fb12931e50f111053e016531fe60b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>StudentLLM/Alpagasus-2-13B-QLoRA-pipeline</td>
      <td>52.13</td>
      <td>58.28</td>
      <td>80.98</td>
      <td>54.14</td>
      <td>34.21</td>
      <td>75.93</td>
      <td>9.25</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>86329885e029c1f4fb6ff6b6f3409007158499e7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k</td>
      <td>52.13</td>
      <td>26.45</td>
      <td>80.83</td>
      <td>71.99</td>
      <td>56.39</td>
      <td>77.11</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.74</td>
      <td>14.0</td>
      <td>True</td>
      <td>133279baf54f2b8fe414203318272e7d3619ace4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Biomimicry-AI/ANIMA-Nectar-v2</td>
      <td>52.13</td>
      <td>53.24</td>
      <td>76.63</td>
      <td>54.21</td>
      <td>49.04</td>
      <td>74.11</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>304e41b614d1ac9debccfa266887640b508c9823</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Xwin-LM/Xwin-LM-7B-V0.1</td>
      <td>52.08</td>
      <td>56.57</td>
      <td>79.40</td>
      <td>49.98</td>
      <td>47.89</td>
      <td>73.32</td>
      <td>5.31</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>75.0</td>
      <td>True</td>
      <td>470e680120a7249d6e8a875345015ddba1711100</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-7b-v1.5</td>
      <td>52.06</td>
      <td>53.24</td>
      <td>77.39</td>
      <td>51.04</td>
      <td>50.34</td>
      <td>72.14</td>
      <td>8.19</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>152.0</td>
      <td>True</td>
      <td>de56c35b1763eaae20f4d60efd64af0a9091ebe5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>l3utterfly/llama2-7b-layla</td>
      <td>52.05</td>
      <td>54.18</td>
      <td>79.34</td>
      <td>49.70</td>
      <td>46.50</td>
      <td>74.11</td>
      <td>8.49</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>733016abcd2abee63eb45ed63d2bba14b91da217</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LTC-AI-Labs/L2-7b-Beluga-WVG-Test</td>
      <td>52.04</td>
      <td>53.75</td>
      <td>78.38</td>
      <td>51.57</td>
      <td>45.76</td>
      <td>74.90</td>
      <td>7.88</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b90c207e248c0ad541274c2eb5ef76da1181802f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lvkaokao/llama2-7b-hf-chat-lora</td>
      <td>52.03</td>
      <td>55.72</td>
      <td>78.75</td>
      <td>47.99</td>
      <td>43.11</td>
      <td>75.85</td>
      <td>10.77</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e92a1439ac8d2edb5e311b8a42e13ed7c5e70db5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-2-7b-instruct</td>
      <td>52.02</td>
      <td>56.23</td>
      <td>79.97</td>
      <td>47.17</td>
      <td>49.51</td>
      <td>75.45</td>
      <td>3.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>22.0</td>
      <td>False</td>
      <td>8f4dd9c870f748322989168af5c109e16b01c63d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>wang7776/Llama-2-7b-chat-hf-20-sparsity</td>
      <td>52.01</td>
      <td>52.47</td>
      <td>77.91</td>
      <td>47.27</td>
      <td>45.88</td>
      <td>70.72</td>
      <td>17.82</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7b44f4902cde1b21b48c87c0379c7aab819436ef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>haonan-li/bactrian-x-llama-13b-merged</td>
      <td>52.00</td>
      <td>56.40</td>
      <td>79.33</td>
      <td>48.40</td>
      <td>48.38</td>
      <td>73.95</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>cc5ee2231066c147423f89e9df40f7364c3275a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-7b-v1.5</td>
      <td>51.99</td>
      <td>53.24</td>
      <td>77.39</td>
      <td>50.82</td>
      <td>50.33</td>
      <td>72.06</td>
      <td>8.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>152.0</td>
      <td>True</td>
      <td>de56c35b1763eaae20f4d60efd64af0a9091ebe5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/Qwen-LLaMAfied-7B-Chat</td>
      <td>51.99</td>
      <td>50.94</td>
      <td>83.47</td>
      <td>53.52</td>
      <td>46.09</td>
      <td>73.16</td>
      <td>4.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>7.00</td>
      <td>100.0</td>
      <td>True</td>
      <td>4d70cf0047a7a5cd2c864bc2606e81f0830e4405</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-mistral-7b-v13-base</td>
      <td>51.99</td>
      <td>52.90</td>
      <td>76.12</td>
      <td>57.54</td>
      <td>52.82</td>
      <td>71.35</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>8ff18d61b1c8295ecd73153b8e0b63934187a50e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/spicyboros-7b-2.2</td>
      <td>51.95</td>
      <td>56.57</td>
      <td>80.09</td>
      <td>48.47</td>
      <td>47.22</td>
      <td>74.51</td>
      <td>4.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>fdf075081555f3ed84c037e8dd3fe85c3b3609d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xxyyy123/10k_v1_lora_qkvo_rank28_v2</td>
      <td>51.95</td>
      <td>55.38</td>
      <td>79.21</td>
      <td>50.50</td>
      <td>52.75</td>
      <td>73.24</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>70e38a7424544193f0ad6a93ae26a5bfd15e4e90</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>pe-nlp/llama-2-13b-vicuna-wizard</td>
      <td>51.94</td>
      <td>57.76</td>
      <td>82.16</td>
      <td>54.68</td>
      <td>41.11</td>
      <td>74.98</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b51bf8c4e132308751cc8b9d9c1131539f79f07f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chinoll/Yi-6b-200k-dpo</td>
      <td>51.93</td>
      <td>43.09</td>
      <td>74.53</td>
      <td>64.00</td>
      <td>45.51</td>
      <td>73.09</td>
      <td>11.37</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>925c5fbaeccb321ba8edbde79c3d994adc460a41</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>chinoll/Yi-7b-dpo</td>
      <td>51.93</td>
      <td>43.09</td>
      <td>74.53</td>
      <td>64.00</td>
      <td>45.51</td>
      <td>73.09</td>
      <td>11.37</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>925c5fbaeccb321ba8edbde79c3d994adc460a41</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/Nous-Hermes-llama-2-7b</td>
      <td>51.87</td>
      <td>55.12</td>
      <td>78.94</td>
      <td>48.34</td>
      <td>49.01</td>
      <td>74.03</td>
      <td>5.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[mit]</td>
      <td>6.74</td>
      <td>58.0</td>
      <td>True</td>
      <td>60e58acecdc1552e1b1752a38d1d91d942d1c3f0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-zephyr-7b-v14.1</td>
      <td>51.86</td>
      <td>52.13</td>
      <td>75.02</td>
      <td>56.21</td>
      <td>49.84</td>
      <td>73.24</td>
      <td>4.70</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>47.0</td>
      <td>True</td>
      <td>208b6fb841239a36fb0ea675179a231e0ad9d287</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ashercn97/manatee-7b</td>
      <td>51.84</td>
      <td>54.52</td>
      <td>78.95</td>
      <td>49.26</td>
      <td>46.77</td>
      <td>74.51</td>
      <td>7.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>e66094c43ffe6c5b3f4164cd4ba048d3bc422fd0</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>migtissera/Synthia-7B</td>
      <td>51.83</td>
      <td>56.14</td>
      <td>78.60</td>
      <td>50.35</td>
      <td>45.03</td>
      <td>74.27</td>
      <td>6.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>4f9e95665d95b4c692910190ff77257216e476f1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Medusa-1.1-L2-7B</td>
      <td>51.80</td>
      <td>56.48</td>
      <td>78.57</td>
      <td>51.56</td>
      <td>47.70</td>
      <td>75.06</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>df23c3d22bc546dbce0267415e94bdb482446c06</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elliotthwang/Elliott-Chinese-LLaMa-GPTQ</td>
      <td>51.79</td>
      <td>51.02</td>
      <td>75.23</td>
      <td>49.58</td>
      <td>45.09</td>
      <td>72.61</td>
      <td>17.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>53.90</td>
      <td>0.0</td>
      <td>False</td>
      <td>bbbca62bb340b4ae0a19ba93dae38fc9f9787c16</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Sao10K/Stheno-Mix-L2-20B</td>
      <td>51.79</td>
      <td>57.76</td>
      <td>79.63</td>
      <td>52.51</td>
      <td>51.80</td>
      <td>68.98</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>20.63</td>
      <td>0.0</td>
      <td>True</td>
      <td>6f9dcdaae6ef9071effe63d2107abe8b9712345b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>umd-zhou-lab/recycled-wizardlm-7b-v2.0</td>
      <td>51.79</td>
      <td>54.95</td>
      <td>77.85</td>
      <td>45.79</td>
      <td>48.29</td>
      <td>71.51</td>
      <td>12.36</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>4a770caf3509b3fdda5ed54735dc40a8f0442c61</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-13b-gpt4-1.3</td>
      <td>51.76</td>
      <td>58.53</td>
      <td>81.60</td>
      <td>46.96</td>
      <td>45.29</td>
      <td>75.85</td>
      <td>2.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>32a474742c2a235ca12c96afaea57dcb6b46ef56</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/llama-2-7b-miniguanaco</td>
      <td>51.74</td>
      <td>50.00</td>
      <td>76.96</td>
      <td>48.05</td>
      <td>42.84</td>
      <td>73.48</td>
      <td>19.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>ef3fa61b50387f5a982aa2578933dfc20afb7237</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lazycuber/L2-7b-Orca-WVG-Test</td>
      <td>51.72</td>
      <td>54.86</td>
      <td>78.25</td>
      <td>51.13</td>
      <td>43.68</td>
      <td>74.35</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>6073a87872eb36149404bfb7d60e0108074ee1c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Azure99/blossom-v2-llama2-7b</td>
      <td>51.71</td>
      <td>54.10</td>
      <td>78.57</td>
      <td>51.66</td>
      <td>46.84</td>
      <td>74.35</td>
      <td>4.78</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>8c71cdb481ce6bbda3b2042e5526a232ab23825c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jphme/em_german_leo_mistral</td>
      <td>51.69</td>
      <td>52.82</td>
      <td>78.03</td>
      <td>50.03</td>
      <td>50.19</td>
      <td>73.48</td>
      <td>5.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>46.0</td>
      <td>True</td>
      <td>aa63a32154923034fb89b1408d3d7ffa994d3327</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>haoranxu/ALMA-13B-Pretrain</td>
      <td>51.68</td>
      <td>56.91</td>
      <td>80.15</td>
      <td>50.31</td>
      <td>37.44</td>
      <td>76.40</td>
      <td>8.87</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>69e9e12d8bab66dffdcb15fa534fc3f0dc34acec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-ziya-13b</td>
      <td>51.67</td>
      <td>55.38</td>
      <td>78.47</td>
      <td>45.18</td>
      <td>49.29</td>
      <td>74.82</td>
      <td>6.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>7.0</td>
      <td>False</td>
      <td>9a21051ae490d2f8ab8b1181c1b45e0412d71a90</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TencentARC/LLaMA-Pro-8B</td>
      <td>51.67</td>
      <td>53.75</td>
      <td>77.91</td>
      <td>47.49</td>
      <td>38.86</td>
      <td>74.19</td>
      <td>17.82</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>8.36</td>
      <td>137.0</td>
      <td>True</td>
      <td>7a2b46875f68ca276562a44ea99b713d86ddb9f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LTC-AI-Labs/L2-7b-Base-test-WVG</td>
      <td>51.66</td>
      <td>54.27</td>
      <td>77.81</td>
      <td>51.07</td>
      <td>46.28</td>
      <td>73.56</td>
      <td>6.97</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>2491546f1219c3e9bb1a8cf37fbecf0b299c2177</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>rombodawg/LosslessMegaCoder-llama2-7b-mini</td>
      <td>51.66</td>
      <td>53.50</td>
      <td>77.38</td>
      <td>49.72</td>
      <td>45.77</td>
      <td>74.03</td>
      <td>9.55</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>186b105d61054611d0b921a55c220d41c6aefe43</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>augmxnt/shisa-base-7b-v1</td>
      <td>51.64</td>
      <td>52.30</td>
      <td>77.63</td>
      <td>23.12</td>
      <td>42.40</td>
      <td>78.53</td>
      <td>35.86</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.96</td>
      <td>10.0</td>
      <td>True</td>
      <td>5aa465caca707816a4bb36b4980aef5d102d76fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elliotthwang/Elliott-Chinese-LLaMa-GPTQ-V1.0</td>
      <td>51.64</td>
      <td>50.68</td>
      <td>75.36</td>
      <td>49.33</td>
      <td>44.70</td>
      <td>72.38</td>
      <td>17.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>53.90</td>
      <td>0.0</td>
      <td>False</td>
      <td>01305dc473ba231519fe71e7f4b2d1e3f6aa9bc8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>TheBloke/stable-vicuna-13B-HF</td>
      <td>51.64</td>
      <td>53.33</td>
      <td>78.50</td>
      <td>50.29</td>
      <td>48.38</td>
      <td>75.22</td>
      <td>4.09</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>13.00</td>
      <td>95.0</td>
      <td>True</td>
      <td>2b099b2be0dafb2606ae9808c0f6183fe4bff7bc</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-LoRa-v2</td>
      <td>51.61</td>
      <td>58.62</td>
      <td>81.17</td>
      <td>50.23</td>
      <td>43.43</td>
      <td>76.16</td>
      <td>0.08</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>568ac6a5f1a9f5eb6bc09efb2188740d771ed0e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rufjdk5480/llama-7b-ludwig-alpaca</td>
      <td>51.60</td>
      <td>54.01</td>
      <td>78.73</td>
      <td>45.80</td>
      <td>41.91</td>
      <td>74.27</td>
      <td>14.86</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7928584c0329c3ed88915a823033908be90ba657</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>abhinand/tamil-llama-13b-instruct-v0.1</td>
      <td>51.59</td>
      <td>54.52</td>
      <td>79.35</td>
      <td>50.37</td>
      <td>41.22</td>
      <td>76.56</td>
      <td>7.51</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>7d6d6f23f69d1d8806ac21eec7ef8feba63c0e67</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-7b-v1.5-16k</td>
      <td>51.58</td>
      <td>54.69</td>
      <td>77.32</td>
      <td>49.51</td>
      <td>50.41</td>
      <td>71.11</td>
      <td>6.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>78.0</td>
      <td>True</td>
      <td>9a93d7d11fac7f3f9074510b80092b53bc1a5bec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Envoid/Yousei-22B</td>
      <td>51.56</td>
      <td>55.89</td>
      <td>78.55</td>
      <td>52.31</td>
      <td>50.68</td>
      <td>71.51</td>
      <td>0.45</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>21.83</td>
      <td>2.0</td>
      <td>False</td>
      <td>ae8f93963266d31000433f1a52d43435e1473e2b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-MysteryModel-13b</td>
      <td>51.54</td>
      <td>57.00</td>
      <td>80.35</td>
      <td>52.06</td>
      <td>45.00</td>
      <td>74.82</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>c38a9df20162455b53eb35d38a9b67fb824559e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lvkaokao/llama2-7b-hf-instruction-lora</td>
      <td>51.54</td>
      <td>55.38</td>
      <td>78.57</td>
      <td>49.39</td>
      <td>41.83</td>
      <td>74.19</td>
      <td>9.86</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f660a40323b29040e78097acca320517ed242512</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-c34b-2.1</td>
      <td>51.52</td>
      <td>54.69</td>
      <td>76.45</td>
      <td>55.08</td>
      <td>46.15</td>
      <td>68.43</td>
      <td>8.34</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>2caa8ce3aab012bf34c7c531827f6befc7cc1c98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GeneZC/MiniChat-2-3B</td>
      <td>51.49</td>
      <td>44.88</td>
      <td>67.69</td>
      <td>47.59</td>
      <td>49.64</td>
      <td>66.46</td>
      <td>32.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>f9c59fdc14c42d1a84539e4195335da0a10af955</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elliotthwang/Elliott-Chinese-LLaMa-GPTQ-V2.0</td>
      <td>51.47</td>
      <td>50.77</td>
      <td>75.36</td>
      <td>49.41</td>
      <td>44.70</td>
      <td>72.61</td>
      <td>16.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ebffe57ba6cc70b60ff5295889abc62d91eeb4dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lajonbot/vicuna-7b-v1.5-PL-lora_unload</td>
      <td>51.46</td>
      <td>53.50</td>
      <td>76.74</td>
      <td>49.69</td>
      <td>49.68</td>
      <td>71.98</td>
      <td>7.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>92bf763ce7ae0bfe155bfd60190eed64582e5080</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-tutor-7b-ep3</td>
      <td>51.45</td>
      <td>52.13</td>
      <td>78.07</td>
      <td>51.32</td>
      <td>52.30</td>
      <td>71.19</td>
      <td>3.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>724cf8becd6dbb0b67070c34711ef6d60ad5f216</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/Pwen-VL-Chat-20_30</td>
      <td>51.45</td>
      <td>50.17</td>
      <td>72.21</td>
      <td>56.34</td>
      <td>42.52</td>
      <td>68.35</td>
      <td>19.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>64a9b89fb18140fc1af1f11471dc9fe34ebc7446</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>amazon/MistralLite</td>
      <td>51.45</td>
      <td>59.56</td>
      <td>81.84</td>
      <td>50.93</td>
      <td>37.87</td>
      <td>77.43</td>
      <td>1.06</td>
      <td></td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>379.0</td>
      <td>True</td>
      <td>23486089ab7ba741b34adc69ab7555885f8abe71</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>frank098/WizardLM_13B_juniper</td>
      <td>51.45</td>
      <td>55.38</td>
      <td>77.20</td>
      <td>45.46</td>
      <td>51.50</td>
      <td>71.11</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2204970fc0d96b071e2b1b003fbc5c87cfc46840</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>itsliupeng/llama2_7b_zh</td>
      <td>51.44</td>
      <td>52.05</td>
      <td>74.88</td>
      <td>60.69</td>
      <td>42.86</td>
      <td>71.74</td>
      <td>6.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>410711781d2e24226c0d62959e4990d1de851c3c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>tlphams/zoyllm-7b-slimorca</td>
      <td>51.44</td>
      <td>50.60</td>
      <td>72.12</td>
      <td>48.78</td>
      <td>49.13</td>
      <td>67.32</td>
      <td>20.70</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>4b49caa2c42b3e8757f986624b047dab485ee26f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>camel-ai/CAMEL-13B-Role-Playing-Data</td>
      <td>51.42</td>
      <td>54.95</td>
      <td>79.25</td>
      <td>46.61</td>
      <td>46.35</td>
      <td>74.03</td>
      <td>7.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>17.0</td>
      <td>False</td>
      <td>762ecb0d85572c8f8bcbca06d27f7f64a4d74615</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-7b-v1.5-16k</td>
      <td>51.42</td>
      <td>54.18</td>
      <td>77.31</td>
      <td>49.30</td>
      <td>50.35</td>
      <td>71.03</td>
      <td>6.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>78.0</td>
      <td>True</td>
      <td>9a93d7d11fac7f3f9074510b80092b53bc1a5bec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hiyouga/Baichuan2-7B-Chat-LLaMAfied</td>
      <td>51.42</td>
      <td>52.47</td>
      <td>74.04</td>
      <td>53.88</td>
      <td>48.04</td>
      <td>69.14</td>
      <td>10.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>da2cd76e2d61bf0247bd67a4f2835319c54a7d62</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nkpz/llama2-22b-daydreamer-v3</td>
      <td>51.39</td>
      <td>56.06</td>
      <td>80.07</td>
      <td>52.49</td>
      <td>42.43</td>
      <td>73.48</td>
      <td>3.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>22.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>e6c74222958328e50712aa00294dc818c24075b2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hpcai-tech/Colossal-LLaMA-2-7b-base</td>
      <td>51.39</td>
      <td>53.50</td>
      <td>70.50</td>
      <td>54.40</td>
      <td>50.19</td>
      <td>70.01</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>70.0</td>
      <td>True</td>
      <td>1f30e4f2037e1e30122667639b8ef37138e85057</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Open-Orca/OpenOrca-Preview1-13B</td>
      <td>51.38</td>
      <td>54.95</td>
      <td>78.19</td>
      <td>50.12</td>
      <td>49.05</td>
      <td>71.03</td>
      <td>4.93</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>147.0</td>
      <td>True</td>
      <td>d120381b03051b60a7c77ec3fb1be6c3c1546466</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/kuchiki-1.1-l2-7b</td>
      <td>51.36</td>
      <td>54.18</td>
      <td>78.00</td>
      <td>48.14</td>
      <td>49.96</td>
      <td>73.16</td>
      <td>4.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>10fe70fec0df5c4dcbdfd2e9ec74830c41b3cfd2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>huggingface/llama-13b</td>
      <td>51.36</td>
      <td>56.23</td>
      <td>80.93</td>
      <td>47.67</td>
      <td>39.48</td>
      <td>76.24</td>
      <td>7.58</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>4022c52fcc7473ce7364bb5ac166195903ea1efb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LTC-AI-Labs/L2-7b-Hermes-WVG-Test</td>
      <td>51.35</td>
      <td>54.95</td>
      <td>78.48</td>
      <td>48.36</td>
      <td>45.72</td>
      <td>74.74</td>
      <td>5.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>eb5b1d65fdf916ca71f89a46eb91175c1c630a57</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>huggyllama/llama-13b</td>
      <td>51.33</td>
      <td>56.14</td>
      <td>80.92</td>
      <td>47.61</td>
      <td>39.48</td>
      <td>76.24</td>
      <td>7.58</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.02</td>
      <td>125.0</td>
      <td>True</td>
      <td>bf57045473f207bb1de1ed035ace226f4d9f9bba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/kuchiki-l2-7b</td>
      <td>51.33</td>
      <td>54.35</td>
      <td>78.44</td>
      <td>47.74</td>
      <td>49.88</td>
      <td>73.09</td>
      <td>4.47</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>745c34e70aa92056e8cd79c1d16e8fcfe1797645</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jordiclive/gpt4all-alpaca-oa-codealpaca-lora-13b</td>
      <td>51.33</td>
      <td>56.14</td>
      <td>80.93</td>
      <td>47.66</td>
      <td>39.48</td>
      <td>76.16</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>13443d633eaa5b7e1a90ac9cdb4a4d51b1c8d0d1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Tap-M/Luna-AI-Llama2-Uncensored</td>
      <td>51.29</td>
      <td>54.35</td>
      <td>78.60</td>
      <td>46.70</td>
      <td>45.50</td>
      <td>72.77</td>
      <td>9.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>0.00</td>
      <td>119.0</td>
      <td>True</td>
      <td>6b5e1067e412cc5750aec7415a065671df3618be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zarablend-l2-7b</td>
      <td>51.29</td>
      <td>54.44</td>
      <td>78.62</td>
      <td>47.61</td>
      <td>49.38</td>
      <td>73.32</td>
      <td>4.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>8b14e71ae3f52c409a25e1ac98dd05e0bb91eaff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abdulrahman-nuzha/finetuned-llama2-chat-5000-v2.0</td>
      <td>51.28</td>
      <td>52.05</td>
      <td>76.13</td>
      <td>46.33</td>
      <td>45.18</td>
      <td>72.30</td>
      <td>15.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3e740254650b5f41e77d04c66806e6a0d3145195</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Locutusque/Rhino-Mistral-7B</td>
      <td>51.27</td>
      <td>48.12</td>
      <td>71.42</td>
      <td>48.95</td>
      <td>45.90</td>
      <td>71.11</td>
      <td>22.14</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>9d23ebfc46951058a44d99c3ee45abf0c55d08ef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hyunseoki/ko-en-llama2-13b</td>
      <td>51.27</td>
      <td>58.19</td>
      <td>81.89</td>
      <td>52.02</td>
      <td>39.96</td>
      <td>74.82</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>24.0</td>
      <td>False</td>
      <td>2768cf6f955b65868ccbb20658e2cc444b2f3be9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>teknium/OpenHermes-7B</td>
      <td>51.26</td>
      <td>56.14</td>
      <td>78.32</td>
      <td>48.62</td>
      <td>45.00</td>
      <td>74.51</td>
      <td>5.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>74edb1ad58d3d517ef46c4e2a31081084ecbc473</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Norquinal/llama-2-7b-claude-chat-rp</td>
      <td>51.25</td>
      <td>54.95</td>
      <td>80.05</td>
      <td>47.03</td>
      <td>43.47</td>
      <td>74.74</td>
      <td>7.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>4309eedebe8ba5709e0cc7cf186cb783f3bc8060</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zarablend-1.1-l2-7b</td>
      <td>51.25</td>
      <td>54.86</td>
      <td>78.58</td>
      <td>47.89</td>
      <td>49.00</td>
      <td>72.61</td>
      <td>4.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>e46bfa43829cbea7608192a6d07bcc147387fdb7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LTC-AI-Labs/L2-7b-Synthia-WVG-Test</td>
      <td>51.25</td>
      <td>55.97</td>
      <td>77.89</td>
      <td>49.48</td>
      <td>44.11</td>
      <td>74.11</td>
      <td>5.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>23ae02efba01c37abe3cff0fedc7d2d9644fe98e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airoboros-l2-7b-2.2.1</td>
      <td>51.22</td>
      <td>55.03</td>
      <td>80.06</td>
      <td>47.64</td>
      <td>44.65</td>
      <td>73.80</td>
      <td>6.14</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>eafbba6fec094a17ca7bce6d9605cac97b90a483</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Harshvir/Llama-2-7B-physics</td>
      <td>51.22</td>
      <td>52.90</td>
      <td>77.71</td>
      <td>48.83</td>
      <td>48.93</td>
      <td>71.90</td>
      <td>7.05</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>5e66b59c145586266b2351a63f0cf1b4f62f5454</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ncsgobubble/Llama-7B-rollercoaster_v2</td>
      <td>51.20</td>
      <td>52.82</td>
      <td>78.22</td>
      <td>49.80</td>
      <td>43.62</td>
      <td>73.16</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>b099c0725af7e984a8dd9d4ba2af2230613aa367</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>klyang/MentaLLaMA-chat-7B</td>
      <td>51.17</td>
      <td>52.82</td>
      <td>76.10</td>
      <td>47.51</td>
      <td>44.02</td>
      <td>70.40</td>
      <td>16.15</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>eb0b119279aada6404042c69763aaadb5be5000d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/koala-13B-HF</td>
      <td>51.16</td>
      <td>52.99</td>
      <td>77.59</td>
      <td>45.32</td>
      <td>50.23</td>
      <td>74.03</td>
      <td>6.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>41.0</td>
      <td>True</td>
      <td>b20f96a0171ce4c0fa27d6048215ebe710521587</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>64bits/LexPodLM-13B</td>
      <td>51.14</td>
      <td>57.76</td>
      <td>81.04</td>
      <td>48.38</td>
      <td>43.48</td>
      <td>76.16</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>64.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>3553d84037addc97678f99a3464be4c866a0c268</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FlagAlpha/Llama2-Chinese-7b-Chat</td>
      <td>51.13</td>
      <td>52.39</td>
      <td>77.52</td>
      <td>47.72</td>
      <td>46.87</td>
      <td>74.27</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>162.0</td>
      <td>True</td>
      <td>4c3bc725f71898c6a1acd4ea98a2f8d74d1b1b6b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>chargoddard/llama-2-26b-trenchcoat-stack</td>
      <td>51.13</td>
      <td>55.03</td>
      <td>79.90</td>
      <td>53.73</td>
      <td>40.48</td>
      <td>74.74</td>
      <td>2.88</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>25.70</td>
      <td>2.0</td>
      <td>True</td>
      <td>075d67c3223f4b379ab7f997c3787cd0630d80f7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>totally-not-an-llm/EverythingLM-13b-V3-16k</td>
      <td>51.11</td>
      <td>58.19</td>
      <td>80.12</td>
      <td>50.48</td>
      <td>45.18</td>
      <td>70.72</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>1de9244bfadb947f80872727f76790cbc76e7142</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/pygmalion-2-7b</td>
      <td>51.11</td>
      <td>54.01</td>
      <td>78.23</td>
      <td>49.11</td>
      <td>43.78</td>
      <td>75.14</td>
      <td>6.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>45.0</td>
      <td>True</td>
      <td>983f8ad5c156f4a0e4d2b7b5f1146981ad2e8a8b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abdulrahman-nuzha/finetuned-llama2-chat-5000-v1.0-squad</td>
      <td>51.09</td>
      <td>50.94</td>
      <td>76.61</td>
      <td>46.43</td>
      <td>44.45</td>
      <td>71.98</td>
      <td>16.15</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>67c1301cb8a9ea7eb6e2b2c1829719ef746465d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>umd-zhou-lab/recycled-alpaca-7b-v2.0</td>
      <td>51.09</td>
      <td>54.18</td>
      <td>77.98</td>
      <td>46.79</td>
      <td>45.40</td>
      <td>71.35</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>12ab9aed495d8129856fdc469ce3ec672c94e6a3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abhishek/llama2guanacotest</td>
      <td>51.08</td>
      <td>51.62</td>
      <td>77.55</td>
      <td>48.49</td>
      <td>43.88</td>
      <td>73.16</td>
      <td>11.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>679d17809939a0bf9b79bbb027898cbea64045b2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>ehartford/Samantha-1.11-7b</td>
      <td>51.07</td>
      <td>55.03</td>
      <td>79.12</td>
      <td>40.51</td>
      <td>50.37</td>
      <td>74.19</td>
      <td>7.20</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>730cbd8f3077f3d24001aab714def991f1e4e7e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>deepseek-ai/deepseek-moe-16b-base</td>
      <td>51.07</td>
      <td>53.24</td>
      <td>79.77</td>
      <td>46.31</td>
      <td>36.08</td>
      <td>73.72</td>
      <td>17.29</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>16.38</td>
      <td>0.0</td>
      <td>True</td>
      <td>521d2bc4fb69a3f3ae565310fcc3b65f97af2580</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HWERI/Llama2-7b-sharegpt4</td>
      <td>51.05</td>
      <td>55.72</td>
      <td>80.94</td>
      <td>47.47</td>
      <td>48.34</td>
      <td>71.19</td>
      <td>2.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>8ecaba5dd0e9929f5858cfe9f5f8cd8ba285c9e5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beaugogh/Llama2-7b-sharegpt4</td>
      <td>51.05</td>
      <td>55.72</td>
      <td>80.94</td>
      <td>47.47</td>
      <td>48.34</td>
      <td>71.19</td>
      <td>2.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>922d1d963ad1b042c30b774a818d9f6180c28075</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>heegyu/WizardVicuna2-13b-hf</td>
      <td>51.05</td>
      <td>55.38</td>
      <td>79.14</td>
      <td>48.46</td>
      <td>42.43</td>
      <td>73.48</td>
      <td>7.43</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6cfd95e2dcdb6996afa9eb5c63273a1a3524c6c6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikael110/llama-2-7b-guanaco-fp16</td>
      <td>51.04</td>
      <td>54.86</td>
      <td>79.65</td>
      <td>46.38</td>
      <td>43.83</td>
      <td>75.22</td>
      <td>6.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>f769fed10874af73ad12115efd044cb4a64506b0</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ziqingyang/chinese-llama-2-13b</td>
      <td>51.04</td>
      <td>55.80</td>
      <td>79.53</td>
      <td>53.01</td>
      <td>38.24</td>
      <td>75.69</td>
      <td>3.94</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.97</td>
      <td>0.0</td>
      <td>True</td>
      <td>484c8a18b02f95eb2b6f6302105cf9a329e76ec8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>wang7776/Llama-2-7b-chat-hf-30-sparsity</td>
      <td>51.02</td>
      <td>52.47</td>
      <td>76.58</td>
      <td>45.57</td>
      <td>44.82</td>
      <td>69.61</td>
      <td>17.06</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>c3d07c4f8b6a509334d0f63e5057e9447f01b318</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lorinma/yi6B_Vicuna</td>
      <td>51.02</td>
      <td>46.16</td>
      <td>69.30</td>
      <td>58.43</td>
      <td>48.11</td>
      <td>65.67</td>
      <td>18.42</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>4ba7237cc904a14240f426154dc5233ef47db9e4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Delcos/Mistral-Pygmalion-7b</td>
      <td>51.02</td>
      <td>54.44</td>
      <td>78.48</td>
      <td>49.23</td>
      <td>41.82</td>
      <td>75.30</td>
      <td>6.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>4e5fa9ae7f572b4841b02c3f96d8a3c7a7e59521</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Norquinal/llama-2-7b-claude-chat</td>
      <td>50.98</td>
      <td>54.44</td>
      <td>80.66</td>
      <td>46.74</td>
      <td>41.39</td>
      <td>74.90</td>
      <td>7.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>e65d34ed31cdcd2637f6284aa0605f30ef5a9381</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AlekseyKorshuk/vic15-exp-syn-fight-cp3838</td>
      <td>50.97</td>
      <td>51.79</td>
      <td>75.79</td>
      <td>50.23</td>
      <td>49.61</td>
      <td>71.82</td>
      <td>6.60</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>91ce25dbdb67793ad1fcfdfd59f7603c2be65aea</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>meta-llama/Llama-2-7b-hf</td>
      <td>50.97</td>
      <td>53.07</td>
      <td>78.59</td>
      <td>46.87</td>
      <td>38.76</td>
      <td>74.03</td>
      <td>14.48</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>1011.0</td>
      <td>False</td>
      <td>e8f058fa738b6b308540024e9aa12e274e291f75</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>formulae/Dorflan</td>
      <td>50.96</td>
      <td>54.44</td>
      <td>75.78</td>
      <td>51.36</td>
      <td>51.17</td>
      <td>72.61</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>5d8e7e5764ace89e6ccd1deece33b0e8a4b4587b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dfurman/llama-2-7b-instruct-peft</td>
      <td>50.94</td>
      <td>51.19</td>
      <td>78.92</td>
      <td>46.63</td>
      <td>48.50</td>
      <td>74.43</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>0fc43413117187e0723cdac133068ab527c80fe2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PeanutJar/LLaMa-2-PeanutButter_v18_B-7B</td>
      <td>50.94</td>
      <td>54.61</td>
      <td>81.00</td>
      <td>47.07</td>
      <td>41.93</td>
      <td>74.51</td>
      <td>6.52</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>bc8c239cacf1e3211f05e27be67a74d84c12aea9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>davzoku/cria-llama2-7b-v1.3</td>
      <td>50.93</td>
      <td>52.73</td>
      <td>78.58</td>
      <td>48.30</td>
      <td>45.58</td>
      <td>71.90</td>
      <td>8.49</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>163a5bec7b6f5aaa4667aa6a95746deff50ceab1</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-100step-v2</td>
      <td>50.89</td>
      <td>52.65</td>
      <td>78.25</td>
      <td>48.47</td>
      <td>45.18</td>
      <td>72.30</td>
      <td>8.49</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>4ee3182f614473f9ea3b6e429b01872bc90e89f1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Danielbrdz/Barcenas-7b</td>
      <td>50.87</td>
      <td>55.12</td>
      <td>77.40</td>
      <td>49.27</td>
      <td>43.64</td>
      <td>73.64</td>
      <td>6.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>770fa73981a599e935c21a95b1817a553c726694</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-13b-2.1</td>
      <td>50.84</td>
      <td>55.12</td>
      <td>80.24</td>
      <td>50.89</td>
      <td>44.62</td>
      <td>71.90</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>172e30e56e939f73d7d00a165c2d49cbd284481f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LTC-AI-Labs/Guanaco-Vicuna-7B-L2</td>
      <td>50.83</td>
      <td>53.24</td>
      <td>78.89</td>
      <td>46.77</td>
      <td>42.75</td>
      <td>75.37</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba8e755feab0bbf90675dcb9f8875a42f92112a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama2-13b-pretrain</td>
      <td>50.77</td>
      <td>53.92</td>
      <td>79.10</td>
      <td>51.25</td>
      <td>36.24</td>
      <td>75.53</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.97</td>
      <td>0.0</td>
      <td>True</td>
      <td>f87d66f9c4541c575a6fad3c19a31b11568e0dfb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PeanutJar/LLaMa-2-PeanutButter_v10-7B</td>
      <td>50.75</td>
      <td>55.29</td>
      <td>81.69</td>
      <td>46.97</td>
      <td>43.78</td>
      <td>70.88</td>
      <td>5.91</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f98bb987216448aa3aa89e575a7494fae8b68066</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>meta-llama/Llama-2-7b-chat-hf</td>
      <td>50.74</td>
      <td>52.90</td>
      <td>78.55</td>
      <td>48.32</td>
      <td>45.57</td>
      <td>71.74</td>
      <td>7.35</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>2428.0</td>
      <td>False</td>
      <td>b7701a9e825e79a5ab18b5801be113c2160cc627</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-timedial-unit-080082</td>
      <td>50.74</td>
      <td>52.82</td>
      <td>76.07</td>
      <td>50.47</td>
      <td>43.54</td>
      <td>73.72</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>372c90543ebb2a317fb9b51ff3890cc270e5ce3a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-timedial-unit-080091</td>
      <td>50.71</td>
      <td>52.82</td>
      <td>76.10</td>
      <td>50.58</td>
      <td>43.40</td>
      <td>73.72</td>
      <td>7.66</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>ae7e0fb58f4201bb14fd4e641d0d6dcc22674e0e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>wang7776/Mistral-7B-Instruct-v0.2-sparsity-20</td>
      <td>50.70</td>
      <td>52.65</td>
      <td>76.71</td>
      <td>47.27</td>
      <td>47.22</td>
      <td>69.06</td>
      <td>11.30</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>afbc5381ebc40d33832702045c8b6cd567f6f1f8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>revolutionarybukhari/Llama-2-7b-chat-finetune-AUTOMATE</td>
      <td>50.68</td>
      <td>53.07</td>
      <td>75.59</td>
      <td>48.80</td>
      <td>44.73</td>
      <td>73.24</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>55862462a23ab43fb73d4c784f1518ab4645764c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>psyche/kollama2-7b-v2</td>
      <td>50.66</td>
      <td>53.33</td>
      <td>78.50</td>
      <td>43.61</td>
      <td>46.37</td>
      <td>75.61</td>
      <td>6.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>d5b6e9d5b882d4f6ba322396e027925ed915f848</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vonjack/Qwen-LLaMAfied-HFTok-7B-Chat</td>
      <td>50.64</td>
      <td>50.51</td>
      <td>83.65</td>
      <td>51.53</td>
      <td>44.23</td>
      <td>71.43</td>
      <td>2.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>22.0</td>
      <td>False</td>
      <td>b8d5c09c83b1ef23668cb9209dbc43c0df2de8ae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LTC-AI-Labs/L2-7b-Base-WVG-Uncensored</td>
      <td>50.63</td>
      <td>53.24</td>
      <td>79.13</td>
      <td>46.65</td>
      <td>42.59</td>
      <td>75.14</td>
      <td>7.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>67ede9be6ceffdf574294351cca937d88d7d448d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DopeorNope/LaOT</td>
      <td>50.62</td>
      <td>55.63</td>
      <td>78.96</td>
      <td>50.30</td>
      <td>44.72</td>
      <td>74.11</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>df3a2c77a63a370405c7711b323e7ffa550cdd9e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zarakiquemparte/zaraxls-l2-7b</td>
      <td>50.61</td>
      <td>54.44</td>
      <td>78.94</td>
      <td>50.39</td>
      <td>46.51</td>
      <td>73.16</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>cc1dad50689b3ebcc1c9c67f275da6b4bb63e2ce</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Severian/ANIMA-Nectar-v3</td>
      <td>50.58</td>
      <td>49.49</td>
      <td>75.99</td>
      <td>53.34</td>
      <td>46.16</td>
      <td>73.72</td>
      <td>4.78</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>8ff9dd66d8cb8fba5c745e5bdb9928c4fc9889e4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Voicelab/trurl-2-7b</td>
      <td>50.58</td>
      <td>53.41</td>
      <td>75.29</td>
      <td>50.00</td>
      <td>45.42</td>
      <td>72.22</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>e26ca5f157c60fc527170cc04db7fc0ea04ad26f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>guardrail/llama-2-7b-guanaco-instruct-sharded</td>
      <td>50.58</td>
      <td>53.75</td>
      <td>78.69</td>
      <td>46.65</td>
      <td>43.93</td>
      <td>72.61</td>
      <td>7.81</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>6.0</td>
      <td>True</td>
      <td>fc7a3abbc3b9a9b3e163ef3c4844307ac270fca7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>maximuslee07/llama-2-7b-rockwell-final</td>
      <td>50.55</td>
      <td>52.73</td>
      <td>79.10</td>
      <td>47.88</td>
      <td>47.21</td>
      <td>68.43</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>de4cfe99e9e3db62733b40f48b2b11faf9abe4bf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>decruz07/llama-2-7b-miniguanaco</td>
      <td>50.55</td>
      <td>49.06</td>
      <td>75.59</td>
      <td>46.14</td>
      <td>43.73</td>
      <td>72.61</td>
      <td>16.15</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>ef3fa61b50387f5a982aa2578933dfc20afb7237</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-mmlu-val-mcq-7b-ep2</td>
      <td>50.55</td>
      <td>53.33</td>
      <td>77.73</td>
      <td>46.85</td>
      <td>43.87</td>
      <td>71.27</td>
      <td>10.24</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6e6639ddaed9b2a8a549424f8c8a2d2bca241d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lazycuber/L2-7b-Guanaco-Uncensored</td>
      <td>50.55</td>
      <td>50.60</td>
      <td>76.99</td>
      <td>48.93</td>
      <td>43.42</td>
      <td>75.37</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>9d49378c69c00113cf7f6e66d1ddb9d9b003dddc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/trurl-2-7b-pl-instruct_unload</td>
      <td>50.52</td>
      <td>53.16</td>
      <td>74.64</td>
      <td>49.89</td>
      <td>45.74</td>
      <td>72.30</td>
      <td>7.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>768d800e4dbe3fc95334f30ca7cd02113d3e3fd3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>922-CA/monika-ddlc-7b-v1</td>
      <td>50.49</td>
      <td>54.95</td>
      <td>76.78</td>
      <td>45.61</td>
      <td>43.94</td>
      <td>72.85</td>
      <td>8.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>4157d696bb0015da3ba26a58c1d24925515e4125</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jilp00/OpenHermes-Symbolic-Mistral-7B</td>
      <td>50.46</td>
      <td>54.86</td>
      <td>72.55</td>
      <td>61.80</td>
      <td>45.35</td>
      <td>66.22</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>False</td>
      <td>d231c07fea44298a7fa33f84a0179fb1d683a94d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardCoder-Python-34B-V1.0</td>
      <td>50.46</td>
      <td>52.13</td>
      <td>74.78</td>
      <td>49.15</td>
      <td>48.85</td>
      <td>68.35</td>
      <td>9.48</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>708.0</td>
      <td>True</td>
      <td>5cdc34e4a81d202f1d4a3b5d60e028aab895dfeb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jilp00/OpenHermes-Symbolic-Mistral-7B</td>
      <td>50.45</td>
      <td>54.78</td>
      <td>72.56</td>
      <td>61.96</td>
      <td>45.28</td>
      <td>66.22</td>
      <td>1.90</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>14.22</td>
      <td>0.0</td>
      <td>False</td>
      <td>d231c07fea44298a7fa33f84a0179fb1d683a94d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lazycuber/L2-7b-Base-Guanaco-Uncensored</td>
      <td>50.45</td>
      <td>52.22</td>
      <td>79.08</td>
      <td>46.63</td>
      <td>42.97</td>
      <td>74.51</td>
      <td>7.28</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>dd51a3b26ad378e2953c947a1e4c2f8febe0cb52</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openthaigpt/openthaigpt-1.0.0-beta-13b-chat-hf</td>
      <td>50.45</td>
      <td>53.58</td>
      <td>79.09</td>
      <td>51.13</td>
      <td>44.16</td>
      <td>73.88</td>
      <td>0.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>e800c7aec39678c5c0f30b0af16cb43800a0d379</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NurtureAI/MistralLite-11B</td>
      <td>50.43</td>
      <td>57.68</td>
      <td>79.54</td>
      <td>50.09</td>
      <td>38.27</td>
      <td>76.64</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>2.0</td>
      <td>True</td>
      <td>1a327551e7b2b4fdfbe27fcdb03d1cf5cbffdab4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>chavinlo/gpt4-x-alpaca</td>
      <td>50.41</td>
      <td>52.82</td>
      <td>79.59</td>
      <td>48.19</td>
      <td>48.88</td>
      <td>70.17</td>
      <td>2.81</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>473.0</td>
      <td>False</td>
      <td>6a571f458cab9a23d14324ec63e0abd1744c8353</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</td>
      <td>50.40</td>
      <td>53.67</td>
      <td>78.21</td>
      <td>45.90</td>
      <td>46.13</td>
      <td>73.80</td>
      <td>4.70</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>1295069e9fef63aed87d36fe108d6c934cb34ded</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-dpo</td>
      <td>50.38</td>
      <td>53.67</td>
      <td>78.79</td>
      <td>46.78</td>
      <td>43.97</td>
      <td>71.74</td>
      <td>7.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ec98429034fc84a4555dd4e3db4d6af534a03832</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NewstaR/Koss-7B-chat</td>
      <td>50.37</td>
      <td>53.67</td>
      <td>78.79</td>
      <td>46.72</td>
      <td>43.97</td>
      <td>71.74</td>
      <td>7.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1ab836d9ebf7029fafa07949b51d3838501d537</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-7b-delta-v1.1</td>
      <td>50.37</td>
      <td>53.67</td>
      <td>77.50</td>
      <td>45.61</td>
      <td>48.95</td>
      <td>70.96</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>7.00</td>
      <td>200.0</td>
      <td>False</td>
      <td>24fb8e1e9cc78e0aa7ef154b026c4a83296e3fc4</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>eachadea/vicuna-7b-1.1</td>
      <td>50.37</td>
      <td>53.67</td>
      <td>77.46</td>
      <td>45.63</td>
      <td>48.94</td>
      <td>70.96</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>109.0</td>
      <td>True</td>
      <td>9d8eea215e00b388a22e8f050768ea8911d41f1d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Ejafa/vicuna_7B_vanilla_1.1</td>
      <td>50.37</td>
      <td>53.67</td>
      <td>77.46</td>
      <td>45.63</td>
      <td>48.94</td>
      <td>70.96</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>d971d788db19648ad16bf77ec3f1de35ebf9a8e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>joehuangx/spatial-vicuna-7b-v1.5-LoRA</td>
      <td>50.36</td>
      <td>50.77</td>
      <td>74.63</td>
      <td>48.13</td>
      <td>49.36</td>
      <td>72.38</td>
      <td>6.90</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>dc71924cfb214b91461d35178e6ea6fef7946f13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-timedial</td>
      <td>50.35</td>
      <td>52.90</td>
      <td>76.29</td>
      <td>50.47</td>
      <td>41.60</td>
      <td>73.56</td>
      <td>7.28</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e1709818cca48af4cd31c07c493f996854aa10f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>willnguyen/lacda-2-7B-chat-v0.1</td>
      <td>50.29</td>
      <td>53.07</td>
      <td>77.57</td>
      <td>46.03</td>
      <td>44.57</td>
      <td>74.19</td>
      <td>6.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>afca346816726b83e331bb4d93246ed5146e1675</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>beomi/Yi-Ko-6B</td>
      <td>50.27</td>
      <td>48.89</td>
      <td>74.48</td>
      <td>55.72</td>
      <td>37.09</td>
      <td>72.93</td>
      <td>12.51</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>6.18</td>
      <td>24.0</td>
      <td>True</td>
      <td>8f2f500574cd3c2972f05b7ae6e2807819cce051</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rishiraj/smol-3b</td>
      <td>50.27</td>
      <td>46.33</td>
      <td>68.23</td>
      <td>46.33</td>
      <td>50.73</td>
      <td>65.35</td>
      <td>24.64</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>3.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>21c18e02cbd8becf5cb48eaff485379b6d62a2cd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>starmpcc/Asclepius-Llama2-13B</td>
      <td>50.25</td>
      <td>55.89</td>
      <td>79.66</td>
      <td>52.38</td>
      <td>40.76</td>
      <td>72.69</td>
      <td>0.15</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>579271bebb894d89369205060d151120a217ce81</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/tulu-7B-fp16</td>
      <td>50.24</td>
      <td>50.17</td>
      <td>77.04</td>
      <td>47.63</td>
      <td>41.61</td>
      <td>73.80</td>
      <td>11.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>8a026683f79119643f4007da4e9155c7849792cc</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/Llama-2-7b-hf-instruct-pl-lora_unload</td>
      <td>50.23</td>
      <td>53.75</td>
      <td>78.34</td>
      <td>46.80</td>
      <td>42.34</td>
      <td>73.95</td>
      <td>6.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>3dfef350be9c8ce92c2d314dbe96a002bd6ca97d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GeneZC/MiniChat-1.5-3B</td>
      <td>50.23</td>
      <td>46.50</td>
      <td>68.28</td>
      <td>46.67</td>
      <td>50.71</td>
      <td>65.04</td>
      <td>24.18</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>28.0</td>
      <td>True</td>
      <td>886af9601d57d8675c09bab02144b68366cd4437</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>vishesht27/22-Neuro_Model</td>
      <td>50.23</td>
      <td>49.15</td>
      <td>62.31</td>
      <td>62.01</td>
      <td>60.23</td>
      <td>66.54</td>
      <td>1.14</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>807caffa6064420c088fadb9f2d34012da6b3236</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>conceptofmind/LLongMA-2-13b-16k</td>
      <td>50.22</td>
      <td>54.27</td>
      <td>79.63</td>
      <td>50.97</td>
      <td>37.71</td>
      <td>72.77</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c2defe28e2f3f10460baf8f778b00986a53aa7a2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kashif/stack-llama-2</td>
      <td>50.21</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>10.01</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-openrail-m</td>
      <td>0.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>28a206689c0097738177840a40e455a308db2d7d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Korabbit/Llama-2-7b-chat-hf-afr-200step-v2</td>
      <td>50.21</td>
      <td>51.79</td>
      <td>77.41</td>
      <td>48.55</td>
      <td>43.69</td>
      <td>71.90</td>
      <td>7.88</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a3575a542e1dc3db4a7794b8f36b104c93b39875</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elliotthwang/elliott_Llama-2-7b-hf</td>
      <td>50.20</td>
      <td>53.16</td>
      <td>78.33</td>
      <td>47.09</td>
      <td>42.11</td>
      <td>73.64</td>
      <td>6.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>ac5d22e14c2c7a400519da5d12d88e4fe683ccfa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>RatanRohith/SRBOSGPT-7B-slerp</td>
      <td>50.19</td>
      <td>49.15</td>
      <td>62.28</td>
      <td>61.95</td>
      <td>60.23</td>
      <td>66.54</td>
      <td>0.99</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>False</td>
      <td>037f68c68da7ff3f981534f6deec3c85e86a9a86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>edor/Platypus2-mini-7B</td>
      <td>50.18</td>
      <td>53.33</td>
      <td>78.81</td>
      <td>45.58</td>
      <td>42.00</td>
      <td>75.14</td>
      <td>6.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>4ede4a6f8a8d6cc3bfff8b98837116c74c280f63</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>haoranxu/ALMA-13B</td>
      <td>50.16</td>
      <td>56.83</td>
      <td>80.29</td>
      <td>49.92</td>
      <td>37.57</td>
      <td>76.32</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>6798d9501a71b203be0610e640ec92fc08ea8dc6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/llama-2-7b-hf-guanaco-1k</td>
      <td>50.13</td>
      <td>51.62</td>
      <td>76.73</td>
      <td>47.45</td>
      <td>44.79</td>
      <td>72.77</td>
      <td>7.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>bdb57c5c992872ced47f48cb2177a5fa159f926a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-hf-guanaco</td>
      <td>50.12</td>
      <td>52.47</td>
      <td>78.75</td>
      <td>45.33</td>
      <td>43.90</td>
      <td>74.19</td>
      <td>6.07</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>1.0</td>
      <td>False</td>
      <td>6c1fc95e67b11f1011a3b2fc1aa05c7b83251e40</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TinyPixel/elm-test</td>
      <td>50.09</td>
      <td>53.16</td>
      <td>78.98</td>
      <td>47.04</td>
      <td>39.51</td>
      <td>74.35</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>aa8f81624d897aa493474bcd96dc3feae9f7a535</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>conceptofmind/LLongMA-2-13b-16k</td>
      <td>50.09</td>
      <td>54.27</td>
      <td>79.66</td>
      <td>50.86</td>
      <td>37.68</td>
      <td>72.61</td>
      <td>5.46</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c2defe28e2f3f10460baf8f778b00986a53aa7a2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/Llama-2-7B-32K-Instruct</td>
      <td>50.02</td>
      <td>51.11</td>
      <td>78.51</td>
      <td>46.11</td>
      <td>44.86</td>
      <td>73.88</td>
      <td>5.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>149.0</td>
      <td>True</td>
      <td>35696b9a7ab330dcbe240ff76fb44ab1eccf45bf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-guanaco</td>
      <td>50.02</td>
      <td>50.51</td>
      <td>76.72</td>
      <td>48.03</td>
      <td>43.36</td>
      <td>72.93</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>5d33696ee324899d52fc43794b46009fea08a9af</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</td>
      <td>50.00</td>
      <td>53.75</td>
      <td>78.76</td>
      <td>46.02</td>
      <td>43.31</td>
      <td>73.48</td>
      <td>4.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>6ba5416f618ed3e11b409326e84c36fa542f0951</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>dhmeltzer/llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16</td>
      <td>49.98</td>
      <td>54.10</td>
      <td>78.74</td>
      <td>45.44</td>
      <td>43.40</td>
      <td>73.64</td>
      <td>4.55</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f1f3b9fdb1e2d8d8fa913d57a8fe15d7bdf72c20</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>garage-bAInd/Platypus2-7B</td>
      <td>49.97</td>
      <td>55.20</td>
      <td>78.84</td>
      <td>49.83</td>
      <td>40.64</td>
      <td>73.48</td>
      <td>1.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.74</td>
      <td>4.0</td>
      <td>True</td>
      <td>f784afa7887b0738d92ea470797582756f02e630</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>RoversX/llama-2-7b-hf-small-shards-Samantha-V1-SFT</td>
      <td>49.96</td>
      <td>53.16</td>
      <td>77.71</td>
      <td>43.47</td>
      <td>45.28</td>
      <td>73.80</td>
      <td>6.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>True</td>
      <td>None</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>c39cee3821269e7fdffa690c2d0836c74dfebd25</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CobraMamba/mamba-gpt-7b</td>
      <td>49.96</td>
      <td>51.19</td>
      <td>75.40</td>
      <td>47.47</td>
      <td>42.06</td>
      <td>71.67</td>
      <td>11.98</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>cb0b04b1bff7921614efbd87d5b87bac04c58d13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TinyPixel/lima-test</td>
      <td>49.96</td>
      <td>53.07</td>
      <td>78.88</td>
      <td>46.42</td>
      <td>39.40</td>
      <td>74.03</td>
      <td>7.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4d6a006c6341f29b11c02f19bf9535f51b4da1b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Uncensored-Jordan-7B</td>
      <td>49.95</td>
      <td>51.28</td>
      <td>77.37</td>
      <td>45.69</td>
      <td>47.50</td>
      <td>71.11</td>
      <td>6.75</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>96a9fbe5aaef8410a8d0dad25f3cc97b408c4efb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mrm8488/llama-2-coder-7b</td>
      <td>49.95</td>
      <td>54.01</td>
      <td>78.35</td>
      <td>46.25</td>
      <td>38.49</td>
      <td>75.45</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>41.0</td>
      <td>True</td>
      <td>f21c0d5e3f9f8c5addf093358e6885afa9602296</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>llm-agents/tora-code-34b-v1.0</td>
      <td>49.92</td>
      <td>50.26</td>
      <td>75.48</td>
      <td>46.65</td>
      <td>39.62</td>
      <td>67.72</td>
      <td>19.79</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>cbb33eea774cc03d4363c424d81e8c9d58332274</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-deepseekcoder-33b-v16.1-32k</td>
      <td>49.91</td>
      <td>45.05</td>
      <td>60.79</td>
      <td>43.24</td>
      <td>44.49</td>
      <td>62.19</td>
      <td>43.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>33.40</td>
      <td>0.0</td>
      <td>True</td>
      <td>afab8e521c80d127a2795539a48de4d93bd02e88</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lodrick-the-lafted/Winged-Lagomorph-2x13B</td>
      <td>49.90</td>
      <td>47.95</td>
      <td>69.39</td>
      <td>44.50</td>
      <td>44.54</td>
      <td>67.40</td>
      <td>25.63</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>llama2</td>
      <td>21.51</td>
      <td>0.0</td>
      <td>True</td>
      <td>f3959f69559f531fb9202798baf641b4af90c1bb</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PeanutJar/LLaMa-2-PeanutButter_v18_A-7B</td>
      <td>49.88</td>
      <td>53.16</td>
      <td>78.11</td>
      <td>45.54</td>
      <td>40.37</td>
      <td>74.90</td>
      <td>7.20</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>15b2fa81418792841014f589e61d1d9e30457040</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wenge-research/yayi-7b-llama2</td>
      <td>49.88</td>
      <td>54.78</td>
      <td>77.94</td>
      <td>41.35</td>
      <td>44.02</td>
      <td>74.51</td>
      <td>6.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>18a4ed38285c732efc583a4bd883b3a681f8d005</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TinyPixel/testmodel2</td>
      <td>49.88</td>
      <td>53.24</td>
      <td>78.78</td>
      <td>46.61</td>
      <td>39.17</td>
      <td>73.80</td>
      <td>7.66</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>cb1111653997cee2818ffcf13a1c37237ea2934d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lajonbot/Llama-2-7b-chat-hf-instruct-pl-lora_unload</td>
      <td>49.86</td>
      <td>52.99</td>
      <td>77.49</td>
      <td>47.12</td>
      <td>42.61</td>
      <td>72.06</td>
      <td>6.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f838fda8d2b97effae1e8af4dbb6217eab14fb7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psyche/kollama2-7b</td>
      <td>49.81</td>
      <td>53.24</td>
      <td>78.78</td>
      <td>42.31</td>
      <td>44.56</td>
      <td>73.95</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>48fca4ba1e2d31ff4fbe6856b9b93ad2d97da8b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kodonho/Solar-M-SakuraSolar-Mixed</td>
      <td>49.81</td>
      <td>45.90</td>
      <td>58.56</td>
      <td>64.51</td>
      <td>59.62</td>
      <td>70.24</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>True</td>
      <td>9d67378e58c9b6ec96d1712f5313a49b33028629</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TinyPixel/testmodel-3</td>
      <td>49.79</td>
      <td>53.24</td>
      <td>78.72</td>
      <td>46.57</td>
      <td>38.75</td>
      <td>73.88</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a1fbc4d8a2c1a3d211325bdff9e7f0539fa7a2b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WizardLM/WizardMath-7B-V1.0</td>
      <td>49.78</td>
      <td>54.10</td>
      <td>79.55</td>
      <td>45.97</td>
      <td>43.65</td>
      <td>72.69</td>
      <td>2.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>39.0</td>
      <td>True</td>
      <td>06dbd3e0da08255c575e585cb82e0554c1d2707a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b-instruct</td>
      <td>49.78</td>
      <td>53.16</td>
      <td>78.25</td>
      <td>47.07</td>
      <td>39.08</td>
      <td>73.24</td>
      <td>7.88</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>45.0</td>
      <td>True</td>
      <td>48fa08b3098a23d3671e09565499a4cfbaff1923</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/vicuna-7b-v1.3</td>
      <td>49.78</td>
      <td>50.43</td>
      <td>76.92</td>
      <td>48.14</td>
      <td>47.01</td>
      <td>70.48</td>
      <td>5.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>118.0</td>
      <td>False</td>
      <td>ac066c83424c4a7221aa10c0ebe074b24d3bcdb6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>undi95/llama2-to-mistral-diff</td>
      <td>49.78</td>
      <td>53.41</td>
      <td>78.56</td>
      <td>46.43</td>
      <td>38.71</td>
      <td>74.03</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>16c279c5e7d12b8a6ff7771881808ef253a406b9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v4</td>
      <td>49.78</td>
      <td>53.41</td>
      <td>78.56</td>
      <td>46.43</td>
      <td>38.71</td>
      <td>74.03</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>405c54ec7aea0735996ef5ff6ede6c35ab930381</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mixed-datasets-time-unit</td>
      <td>49.77</td>
      <td>51.79</td>
      <td>76.41</td>
      <td>49.58</td>
      <td>40.33</td>
      <td>73.40</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>26626ea669172be6bc8e6b2b0bc5f14aef8061aa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wenge-research/yayi-7b-llama2</td>
      <td>49.75</td>
      <td>55.03</td>
      <td>77.84</td>
      <td>40.92</td>
      <td>44.02</td>
      <td>73.72</td>
      <td>6.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>f1a9e8d91e5b636cde3ea7fcf752a9f0234bd92a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/LongQLoRA-Llama2-7b-8k</td>
      <td>49.75</td>
      <td>52.47</td>
      <td>78.11</td>
      <td>45.37</td>
      <td>38.94</td>
      <td>72.06</td>
      <td>11.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>d29069d302700fcbd9322c4b4189a0eac4bccaa7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>wang7776/Mistral-7B-Instruct-v0.2-sparsity-30</td>
      <td>49.74</td>
      <td>51.11</td>
      <td>75.72</td>
      <td>46.54</td>
      <td>45.53</td>
      <td>68.98</td>
      <td>10.54</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>8454c7220f153f57b84d789225a141e3cdc3ba00</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/llama-2-7b-hf_open-platypus</td>
      <td>49.73</td>
      <td>51.45</td>
      <td>78.63</td>
      <td>43.60</td>
      <td>43.71</td>
      <td>74.43</td>
      <td>6.60</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>c7e776f3f3afc0fa22cb7aff0d00522e571e9b29</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bongchoi/test-llama2-7b</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.86</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ebe2e68699cb7ab6bb22688f265c89be2ac0fa6d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yeen214/test_llama2_7b</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.86</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>69a4886f51ed752216cdd7f41a584d14240126f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v4</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>405c54ec7aea0735996ef5ff6ede6c35ab930381</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v2</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>1c97650d4b919e2c6a2829778caa3a109935a58c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ibranze/araproje-llama2-7b-hf</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>7fe54f507e762b0f62265813aef908765b1298c0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NewstaR/Starlight-7B</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1f7436c458ebc3d8d31b91091c1a7a48e942cd3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TaylorAI/Flash-Llama-7B</td>
      <td>49.73</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>27c84ef23d850582453e1cc2dcea13de48da090f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>davzoku/cria-llama2-7b-v1.3_peft</td>
      <td>49.72</td>
      <td>51.45</td>
      <td>77.35</td>
      <td>46.47</td>
      <td>45.52</td>
      <td>70.80</td>
      <td>6.75</td>
      <td>instruction-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>True</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6864fa8ee43fa4d6b4f3ae055bbf464a5dcca570</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ToolBench/ToolLLaMA-7b-LoRA</td>
      <td>49.72</td>
      <td>52.99</td>
      <td>78.62</td>
      <td>46.87</td>
      <td>38.67</td>
      <td>74.35</td>
      <td>6.82</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>67f2e8af850049a86fb9ee8ef581deb0f51e58e6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</td>
      <td>49.71</td>
      <td>53.67</td>
      <td>78.09</td>
      <td>45.63</td>
      <td>41.72</td>
      <td>73.56</td>
      <td>5.61</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>2af3d3acb0466fef466512bc17b9bf57024629e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-shishya-ac-hal-13b-ep3</td>
      <td>49.70</td>
      <td>48.46</td>
      <td>80.78</td>
      <td>56.17</td>
      <td>39.32</td>
      <td>73.48</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a8ccf2bf67a7ee21c3d3accc8a1c5b318677c25</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mixed-datasets</td>
      <td>49.70</td>
      <td>51.71</td>
      <td>76.44</td>
      <td>50.13</td>
      <td>39.57</td>
      <td>73.24</td>
      <td>7.13</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>9c74b9396ff6b33e7a7622e59aa1f46103d993fe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abdulrahman-nuzha/finetuned-llama-v2.0</td>
      <td>49.67</td>
      <td>53.16</td>
      <td>77.75</td>
      <td>43.69</td>
      <td>39.08</td>
      <td>74.43</td>
      <td>9.93</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9ffa847a1a0b229ea9c218e865bcf20f78556a8e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>georgesung/llama2_7b_chat_uncensored</td>
      <td>49.67</td>
      <td>53.58</td>
      <td>78.66</td>
      <td>44.49</td>
      <td>41.34</td>
      <td>74.11</td>
      <td>5.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>180.0</td>
      <td>True</td>
      <td>e9a972b12c6b59bfbcf30fe3779c2c933ce755bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shibing624/chinese-alpaca-plus-13b-hf</td>
      <td>49.66</td>
      <td>53.16</td>
      <td>73.51</td>
      <td>48.81</td>
      <td>45.32</td>
      <td>75.06</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>34.0</td>
      <td>True</td>
      <td>a118d2c35573b9a70c6f5b56fba4b657f74ce00c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HuggingFaceH4/starchat-beta</td>
      <td>49.66</td>
      <td>52.47</td>
      <td>80.59</td>
      <td>42.85</td>
      <td>47.22</td>
      <td>69.69</td>
      <td>5.16</td>
      <td>fine-tuned</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigcode-openrail-m</td>
      <td>15.52</td>
      <td>242.0</td>
      <td>True</td>
      <td>b1bcda690655777373f57ea6614eb095ec2c886f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>togethercomputer/Llama-2-7B-32K-Instruct</td>
      <td>49.65</td>
      <td>51.37</td>
      <td>78.47</td>
      <td>45.53</td>
      <td>45.01</td>
      <td>72.85</td>
      <td>4.70</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>149.0</td>
      <td>True</td>
      <td>b050a6f17d46e32c4b90a30492f14746589f74b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TaylorAI/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model</td>
      <td>49.64</td>
      <td>52.47</td>
      <td>79.08</td>
      <td>47.58</td>
      <td>37.14</td>
      <td>74.74</td>
      <td>6.82</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>819f3f384e37f8906a62a8048556c9e58e495c02</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-7b-2.1</td>
      <td>49.64</td>
      <td>54.44</td>
      <td>78.68</td>
      <td>44.45</td>
      <td>43.95</td>
      <td>74.11</td>
      <td>2.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>699491e2e73cc2936205db143f59c1a686b88f14</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/longchat-13b-16k</td>
      <td>49.64</td>
      <td>53.58</td>
      <td>77.67</td>
      <td>45.24</td>
      <td>47.07</td>
      <td>70.09</td>
      <td>4.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>129.0</td>
      <td>False</td>
      <td>70e2e38b82f1e25d8b90b50fbfc2361123bef45f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>clibrain/Llama-2-7b-ft-instruct-es</td>
      <td>49.63</td>
      <td>53.67</td>
      <td>77.83</td>
      <td>46.58</td>
      <td>38.82</td>
      <td>75.22</td>
      <td>5.69</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>b62f431c88b232204ea7046f9d906ae1daa68437</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jondurbin/airocoder-34b-2.1</td>
      <td>49.61</td>
      <td>54.18</td>
      <td>73.84</td>
      <td>50.67</td>
      <td>40.70</td>
      <td>69.93</td>
      <td>8.34</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>f66e783ac783837b3f59f274ecf55f18a9221cd0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>malhajar/meditron-7b-chat</td>
      <td>49.59</td>
      <td>50.77</td>
      <td>75.37</td>
      <td>40.49</td>
      <td>48.56</td>
      <td>73.16</td>
      <td>9.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>3.0</td>
      <td>True</td>
      <td>b2e32b581d1484c831654fb2c03d2d29e7f520d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>gywy/llama2-13b-chinese-v2</td>
      <td>49.58</td>
      <td>53.92</td>
      <td>74.64</td>
      <td>49.74</td>
      <td>45.43</td>
      <td>71.59</td>
      <td>2.20</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>8f6b11ca4344ac230d6b55defa4e04e60a39f9b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sia-ai/llama-2-7b-1-percent-open-orca-1000-steps-v0</td>
      <td>49.56</td>
      <td>51.28</td>
      <td>78.75</td>
      <td>44.68</td>
      <td>45.83</td>
      <td>74.11</td>
      <td>2.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a893ebef4b818de1968dd9e932da2f513d16386a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dotvignesh/perry-7b</td>
      <td>49.55</td>
      <td>51.79</td>
      <td>76.43</td>
      <td>46.18</td>
      <td>40.08</td>
      <td>72.53</td>
      <td>10.31</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f35ae37b436637cd3e14d086324ccdaccfd69045</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CalderaAI/13B-Ouroboros</td>
      <td>49.54</td>
      <td>57.42</td>
      <td>82.11</td>
      <td>51.43</td>
      <td>47.99</td>
      <td>57.85</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>8.0</td>
      <td>False</td>
      <td>97981254d4b0ac0d1472376f602c004670070fdd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-7b-gpt4-1.4.1</td>
      <td>49.54</td>
      <td>55.12</td>
      <td>79.60</td>
      <td>45.17</td>
      <td>40.29</td>
      <td>74.27</td>
      <td>2.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>77bdd1f049f27876c38b68782fc240518208f391</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jb723/llama2-ko-7B-model</td>
      <td>49.52</td>
      <td>56.31</td>
      <td>79.51</td>
      <td>45.71</td>
      <td>40.98</td>
      <td>72.06</td>
      <td>2.58</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>6.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>03d23910fa0f9b0542ce7634cbcd36983321f55a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>llama-anon/instruct-13b</td>
      <td>49.52</td>
      <td>56.14</td>
      <td>80.27</td>
      <td>47.89</td>
      <td>36.97</td>
      <td>73.56</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>142e198df473fd0cd4370b0d50be5f57e1da399b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>quantumaikr/QuantumLM-7B</td>
      <td>49.51</td>
      <td>50.26</td>
      <td>76.10</td>
      <td>45.27</td>
      <td>46.25</td>
      <td>71.51</td>
      <td>7.66</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>f44998432fb90d88094ddf42e57ec458877a197f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>abhinand/tamil-llama-13b-base-v0.1</td>
      <td>49.50</td>
      <td>52.82</td>
      <td>79.95</td>
      <td>52.05</td>
      <td>36.56</td>
      <td>75.61</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6cbdb6b6088910459cd104b1752177ab52e7f892</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ</td>
      <td>49.47</td>
      <td>52.82</td>
      <td>79.63</td>
      <td>39.83</td>
      <td>52.55</td>
      <td>71.82</td>
      <td>0.15</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>16.0</td>
      <td>True</td>
      <td>bd3c66e626c81de4977f197e1534bd3dfa2f569d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/Guanaco-7B-Uncensored</td>
      <td>49.35</td>
      <td>52.13</td>
      <td>78.77</td>
      <td>43.42</td>
      <td>44.45</td>
      <td>73.09</td>
      <td>4.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>db068e363e66e5d4b131e1d7a42a3a849e406a9b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rameshm/llama-2-13b-mathgpt-v4</td>
      <td>49.35</td>
      <td>50.94</td>
      <td>75.56</td>
      <td>43.78</td>
      <td>41.96</td>
      <td>69.14</td>
      <td>14.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c5072a762070c6b3756385c63805348c155004b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ceadar-ie/FinanceConnect-13B</td>
      <td>49.34</td>
      <td>55.12</td>
      <td>77.73</td>
      <td>52.08</td>
      <td>37.68</td>
      <td>71.82</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>True</td>
      <td>9ed6c7154cd14d1a5cdbec603a3ae8c8ce05cb29</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-openllama-13b-v7-fp16</td>
      <td>49.31</td>
      <td>47.61</td>
      <td>72.24</td>
      <td>47.74</td>
      <td>48.73</td>
      <td>69.69</td>
      <td>9.86</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>8690c065bccd3e897ccbf3d8aa24b0216a6f5dba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qualis2006/llama-2-7b-int4-python-code-18k</td>
      <td>49.30</td>
      <td>52.13</td>
      <td>78.55</td>
      <td>46.25</td>
      <td>37.69</td>
      <td>74.98</td>
      <td>6.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>aed968a4b3f3b716064eb8b50c5ae24b38007627</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>LeoLM/leo-hessianai-7b-chat</td>
      <td>49.29</td>
      <td>52.56</td>
      <td>77.61</td>
      <td>45.58</td>
      <td>44.89</td>
      <td>69.93</td>
      <td>5.16</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>7c343a501f5cd3b768d2f78d9941b760fd66815d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-7b-chat</td>
      <td>49.27</td>
      <td>52.47</td>
      <td>78.35</td>
      <td>39.51</td>
      <td>44.52</td>
      <td>73.16</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>9af636df9c8693ea857b62442bd1c6c73d657dc6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>heegyu/LIMA2-7b-hf</td>
      <td>49.27</td>
      <td>53.24</td>
      <td>80.60</td>
      <td>43.22</td>
      <td>44.74</td>
      <td>69.93</td>
      <td>3.87</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>6a1aa59cb7624f059728840ce68b20b1070ebdcb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TehVenom/Pygmalion-Vicuna-1.1-7b</td>
      <td>49.25</td>
      <td>52.82</td>
      <td>78.66</td>
      <td>43.61</td>
      <td>42.21</td>
      <td>71.98</td>
      <td>6.22</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.74</td>
      <td>26.0</td>
      <td>False</td>
      <td>bdac596568769d1ba4af8df9a611eee9723adf29</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</td>
      <td>49.22</td>
      <td>53.41</td>
      <td>77.90</td>
      <td>43.56</td>
      <td>40.81</td>
      <td>74.59</td>
      <td>5.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>6ca41503b383c654aee8d5496e70fbdfaa33db10</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>venkycs/llama-v2-7b-32kC-Security</td>
      <td>49.19</td>
      <td>49.83</td>
      <td>77.33</td>
      <td>44.41</td>
      <td>47.96</td>
      <td>71.74</td>
      <td>3.87</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ae2abdc539a79ad84b141f894d614adf3754882</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wahaha1987/llama_7b_sharegpt94k_fastchat</td>
      <td>49.19</td>
      <td>53.24</td>
      <td>76.94</td>
      <td>44.64</td>
      <td>45.34</td>
      <td>70.64</td>
      <td>4.32</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>2d82abff150b7a5ae484f9cd7c64c72fd4eaf7f5</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-RetroRodeo-13b</td>
      <td>49.15</td>
      <td>53.84</td>
      <td>79.63</td>
      <td>48.93</td>
      <td>38.73</td>
      <td>73.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>False</td>
      <td>102f9fdad903f5eaffe1ed8173ae56081072e429</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/vicuna-7B-physics</td>
      <td>49.15</td>
      <td>49.49</td>
      <td>75.88</td>
      <td>46.58</td>
      <td>49.31</td>
      <td>69.38</td>
      <td>4.25</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>2147983e9493347c3424c07403f65e7a81c0b19f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b-fast-instruct</td>
      <td>49.15</td>
      <td>53.75</td>
      <td>77.55</td>
      <td>46.85</td>
      <td>38.84</td>
      <td>71.59</td>
      <td>6.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>63.0</td>
      <td>True</td>
      <td>89de33d1ad568855853196802aeaecd799c6586f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Unbabel/TowerBase-7B-v0.1</td>
      <td>49.11</td>
      <td>51.02</td>
      <td>77.68</td>
      <td>43.48</td>
      <td>37.29</td>
      <td>72.06</td>
      <td>13.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>6.74</td>
      <td>9.0</td>
      <td>True</td>
      <td>227253877d67620f45c7b45ff22ead1dc6e03e4f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>whiterabbitneo/WhiteRabbitNeo-13B</td>
      <td>49.11</td>
      <td>48.55</td>
      <td>68.70</td>
      <td>43.04</td>
      <td>44.58</td>
      <td>67.40</td>
      <td>22.37</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>594b9222df90074334697d0ed36ffeb3b478e9ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/scarlett-7b</td>
      <td>49.09</td>
      <td>57.17</td>
      <td>80.27</td>
      <td>36.11</td>
      <td>48.52</td>
      <td>72.14</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>0715b738e750830ba7213f26fe32fa1cc1bb15b3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>itsliupeng/llama2_7b_code</td>
      <td>49.05</td>
      <td>52.13</td>
      <td>75.71</td>
      <td>48.05</td>
      <td>38.76</td>
      <td>71.51</td>
      <td>8.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>0e6d1edd87c8753b55d280179c8fb0e65ebf5fa2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hiyouga/Baichuan2-7B-Base-LLaMAfied</td>
      <td>48.99</td>
      <td>49.57</td>
      <td>73.45</td>
      <td>54.86</td>
      <td>37.54</td>
      <td>70.72</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>dc5bda435771212fc73a8c6556fbdf4fcd87f96d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-mmlu-val-only-correct-mcq-7b-ep2</td>
      <td>48.96</td>
      <td>52.99</td>
      <td>77.67</td>
      <td>47.92</td>
      <td>43.17</td>
      <td>71.90</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f0606bca9bea0afdd1dd8c26f0664b65f4dc5967</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-agents/tora-code-34b-v1.0</td>
      <td>48.95</td>
      <td>50.43</td>
      <td>75.54</td>
      <td>46.78</td>
      <td>39.66</td>
      <td>68.19</td>
      <td>13.12</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>cbb33eea774cc03d4363c424d81e8c9d58332274</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>martyn/mistral-megamerge-dare-7b</td>
      <td>48.93</td>
      <td>55.29</td>
      <td>70.48</td>
      <td>43.05</td>
      <td>51.08</td>
      <td>67.09</td>
      <td>6.60</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f116230ee12e55d1716b89e1b114dd2ee3d397bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PotatoOff/HamSter-0.2</td>
      <td>48.91</td>
      <td>50.09</td>
      <td>73.65</td>
      <td>50.39</td>
      <td>49.63</td>
      <td>69.69</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>85cd65a8a1ac1fc2814a06e11640da72db25935a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xzuyn/MedicWizard-7B</td>
      <td>48.88</td>
      <td>53.50</td>
      <td>78.39</td>
      <td>44.61</td>
      <td>41.32</td>
      <td>70.56</td>
      <td>4.93</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>0b3ef975fb5e8ac1eae775160ab54c98221889df</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ziqingyang/chinese-alpaca-2-7b</td>
      <td>48.85</td>
      <td>49.57</td>
      <td>72.62</td>
      <td>46.50</td>
      <td>48.63</td>
      <td>70.01</td>
      <td>5.76</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>ab2476bffedeed752daedd77e71900578e136e7c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>openchat/opencoderplus</td>
      <td>48.84</td>
      <td>50.60</td>
      <td>78.22</td>
      <td>42.73</td>
      <td>50.72</td>
      <td>66.14</td>
      <td>4.62</td>
      <td></td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>105.0</td>
      <td>False</td>
      <td>845e9e4452dd4440760b3d5f680400fc014e91b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</td>
      <td>48.82</td>
      <td>54.35</td>
      <td>78.06</td>
      <td>45.35</td>
      <td>37.11</td>
      <td>73.40</td>
      <td>4.62</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>684c4f4612fadae47c2c7db9fe9e9be4aaafc7e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v3</td>
      <td>48.81</td>
      <td>52.22</td>
      <td>76.78</td>
      <td>45.89</td>
      <td>38.38</td>
      <td>73.40</td>
      <td>6.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>a5269bc93a7f98e192e34553cec1302877ca4327</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-shishya-all-hal-7b-ep3</td>
      <td>48.75</td>
      <td>45.48</td>
      <td>77.21</td>
      <td>51.54</td>
      <td>44.83</td>
      <td>71.03</td>
      <td>2.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a1424eb777c8a3ce94ab31486510da8f617d17e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/vicuna-7b-v1.3-instruct-pl-lora_unload</td>
      <td>48.74</td>
      <td>48.04</td>
      <td>76.28</td>
      <td>47.42</td>
      <td>44.40</td>
      <td>70.09</td>
      <td>6.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e4b19d9d6168b32402da4ab2b5ec7ff27cf40d9b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>LeoLM/leo-hessianai-7b-chat-bilingual</td>
      <td>48.72</td>
      <td>51.02</td>
      <td>76.03</td>
      <td>44.68</td>
      <td>47.16</td>
      <td>70.72</td>
      <td>2.73</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>5ee98fd03b310e3081f0c9986c5153b27ec5dce6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GOAT-AI/GOAT-7B-Community</td>
      <td>48.71</td>
      <td>48.81</td>
      <td>74.63</td>
      <td>49.58</td>
      <td>42.48</td>
      <td>72.30</td>
      <td>4.47</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>35.0</td>
      <td>True</td>
      <td>a7073a0f5142ce04aaa1603b0812b358f62a8de8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b</td>
      <td>48.70</td>
      <td>52.22</td>
      <td>76.42</td>
      <td>44.60</td>
      <td>37.92</td>
      <td>72.69</td>
      <td>8.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>55.0</td>
      <td>True</td>
      <td>976887c5891284db204320860bb84b71d598063e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v3</td>
      <td>48.65</td>
      <td>51.96</td>
      <td>76.70</td>
      <td>45.36</td>
      <td>38.31</td>
      <td>73.56</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>a5269bc93a7f98e192e34553cec1302877ca4327</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>fblgit/una-llama-7b</td>
      <td>48.64</td>
      <td>53.67</td>
      <td>80.07</td>
      <td>37.37</td>
      <td>38.01</td>
      <td>72.93</td>
      <td>9.78</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>6321d1b950c6a3997a424b20273d66cb2b9395a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PocketDoc/Dans-CreepingSenseOfDoom</td>
      <td>48.58</td>
      <td>53.33</td>
      <td>78.90</td>
      <td>48.09</td>
      <td>37.84</td>
      <td>73.32</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>efc7cbc5d0461c137e8ea0c83e54bc5357188783</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b-gpt4-1.1</td>
      <td>48.57</td>
      <td>54.61</td>
      <td>80.15</td>
      <td>39.25</td>
      <td>41.22</td>
      <td>73.09</td>
      <td>3.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>5a45a16bac51ed9529a6dc2eab7355cc61eefb5b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mlabonne/GML-Mistral-merged-v1</td>
      <td>48.54</td>
      <td>43.77</td>
      <td>57.89</td>
      <td>64.13</td>
      <td>51.58</td>
      <td>73.88</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>1.0</td>
      <td>True</td>
      <td>3ec981e2e8c018f9e34a7553df2a2ed0d032dd37</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-shishya-13b-ep3</td>
      <td>48.52</td>
      <td>46.50</td>
      <td>80.36</td>
      <td>57.06</td>
      <td>35.00</td>
      <td>72.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>870fe04090a6a6cfe27d0bf4b06cc9f18dd4c67d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>namirocks/student-model-13b-ep3</td>
      <td>48.52</td>
      <td>46.50</td>
      <td>80.36</td>
      <td>57.06</td>
      <td>35.00</td>
      <td>72.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1f21e9d0506e908a10d5e611d5f1c022fdee6585</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>rinna/youri-7b-chat</td>
      <td>48.51</td>
      <td>51.19</td>
      <td>76.09</td>
      <td>46.06</td>
      <td>41.17</td>
      <td>75.06</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>14.0</td>
      <td>True</td>
      <td>96d1690c4a1fa192ab26c4be8f9c79e1faed8346</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-agents/tora-7b-v1.0</td>
      <td>48.50</td>
      <td>52.47</td>
      <td>78.68</td>
      <td>45.90</td>
      <td>37.90</td>
      <td>73.56</td>
      <td>2.50</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>717edbee98945192b1a396fc9c337c5b32d6c79c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yash21/Mistral-Quantum-dpo</td>
      <td>48.50</td>
      <td>43.43</td>
      <td>57.76</td>
      <td>64.29</td>
      <td>51.49</td>
      <td>74.03</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>8.99</td>
      <td>0.0</td>
      <td>True</td>
      <td>8aba300293cd8abbf71517e7d7c80fe26bf07baa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/Pygmalion-13b-Merged</td>
      <td>48.49</td>
      <td>56.48</td>
      <td>80.02</td>
      <td>42.93</td>
      <td>35.86</td>
      <td>75.53</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>25.0</td>
      <td>False</td>
      <td>f96308083033c84db47b6c093da3817c085c87c7</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TheBloke/Llama-2-7B-GPTQ</td>
      <td>48.48</td>
      <td>52.05</td>
      <td>77.59</td>
      <td>43.99</td>
      <td>39.32</td>
      <td>72.93</td>
      <td>5.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>llama2</td>
      <td>9.05</td>
      <td>70.0</td>
      <td>True</td>
      <td>ecd7ab9f6adc36ecbe0d751eeea0d90ae1863c3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/WizardLM-13B-Uncensored</td>
      <td>48.48</td>
      <td>50.94</td>
      <td>76.64</td>
      <td>43.96</td>
      <td>46.73</td>
      <td>70.56</td>
      <td>2.05</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>9025c5f96fef9525da9238369ad082961b0e9494</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cognitivecomputations/yayi2-30b-llama</td>
      <td>48.46</td>
      <td>35.67</td>
      <td>53.37</td>
      <td>70.60</td>
      <td>49.08</td>
      <td>63.14</td>
      <td>18.88</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.40</td>
      <td>20.0</td>
      <td>True</td>
      <td>01b331f04153b84a4ac049e71fd122d891424756</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>medalpaca/medalpaca-7b</td>
      <td>48.45</td>
      <td>54.10</td>
      <td>80.42</td>
      <td>41.47</td>
      <td>40.46</td>
      <td>71.19</td>
      <td>3.03</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc</td>
      <td>7.00</td>
      <td>48.0</td>
      <td>True</td>
      <td>b57b9f5ff34059e485b769973d023021fc66a8f7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/vicuna-7B-chemical</td>
      <td>48.42</td>
      <td>49.83</td>
      <td>74.42</td>
      <td>44.10</td>
      <td>51.70</td>
      <td>67.17</td>
      <td>3.34</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>fbf6476ebfa856ffe743e41f8d4413c15b2127c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b-gpt4-1.4</td>
      <td>48.40</td>
      <td>53.92</td>
      <td>80.33</td>
      <td>38.61</td>
      <td>41.05</td>
      <td>72.77</td>
      <td>3.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>cae1ab8991f66bbe66ae95ed23a87846e7343047</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-7b-gpt4-2.0</td>
      <td>48.38</td>
      <td>52.90</td>
      <td>78.53</td>
      <td>45.09</td>
      <td>39.45</td>
      <td>71.11</td>
      <td>3.18</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>8432fe95c426ca7709cf2d31a64eee612c4dea42</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>AlpinDale/pygmalion-instruct</td>
      <td>48.37</td>
      <td>52.56</td>
      <td>77.65</td>
      <td>35.94</td>
      <td>42.13</td>
      <td>72.06</td>
      <td>9.86</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>1665b271316dfee05b2a8daf8b9d6c22ed0aef60</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LLMs/AlpacaGPT4-7B-elina</td>
      <td>48.35</td>
      <td>55.03</td>
      <td>78.79</td>
      <td>37.50</td>
      <td>41.53</td>
      <td>72.69</td>
      <td>4.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>bbece5e3f8ee9be09c8defc536a95c6ef780c681</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Monero/WizardLM-13b-OpenAssistant-Uncensored</td>
      <td>48.32</td>
      <td>48.55</td>
      <td>76.03</td>
      <td>43.15</td>
      <td>49.40</td>
      <td>69.77</td>
      <td>3.03</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>ff8e15fd68119d36ae1f0cebaa87f16e2ad3c732</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Juniplayground/Mist_LLaMA-2-7B-1024_V3</td>
      <td>48.31</td>
      <td>51.37</td>
      <td>77.74</td>
      <td>41.34</td>
      <td>41.21</td>
      <td>73.32</td>
      <td>4.85</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>05ec8f4a568777e1e543acdf8a587e080fb18fba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Wizard-Vicuna-7B-Uncensored-HF</td>
      <td>48.27</td>
      <td>53.41</td>
      <td>78.85</td>
      <td>37.09</td>
      <td>43.48</td>
      <td>72.22</td>
      <td>4.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>20.0</td>
      <td>True</td>
      <td>b802f1b4401d0b2242137160c20cc11b9ffd3a4c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/Wizard-Vicuna-7B-Uncensored</td>
      <td>48.27</td>
      <td>53.41</td>
      <td>78.85</td>
      <td>37.09</td>
      <td>43.48</td>
      <td>72.22</td>
      <td>4.55</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1097285acd9c48a1d09bc0a9844d365384732111</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Nexusflow/NexusRaven-V2-13B</td>
      <td>48.21</td>
      <td>45.14</td>
      <td>67.40</td>
      <td>44.88</td>
      <td>44.54</td>
      <td>66.38</td>
      <td>20.92</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>326.0</td>
      <td>True</td>
      <td>3bec1dcc7cb6f1895a923e66d87438e903bebb57</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco</td>
      <td>48.02</td>
      <td>45.65</td>
      <td>75.65</td>
      <td>49.27</td>
      <td>43.12</td>
      <td>69.93</td>
      <td>4.47</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>883b0fa4158de8207d0a94f4b8cb188e6250aa9d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>titan087/OpenLlama13B-Guanaco</td>
      <td>47.99</td>
      <td>51.19</td>
      <td>75.24</td>
      <td>43.76</td>
      <td>38.40</td>
      <td>71.74</td>
      <td>7.58</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>42ed3023ae1afe861f533570be881a03b10fc860</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lmsys/longchat-7b-v1.5-32k</td>
      <td>47.95</td>
      <td>51.71</td>
      <td>74.97</td>
      <td>43.16</td>
      <td>44.42</td>
      <td>68.67</td>
      <td>4.78</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>50.0</td>
      <td>False</td>
      <td>16deb633ef4d6a18d5750239edc5a85ffeaf3918</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-l2-7b-gpt4-m2.0</td>
      <td>47.95</td>
      <td>50.51</td>
      <td>76.87</td>
      <td>45.35</td>
      <td>41.34</td>
      <td>69.53</td>
      <td>4.09</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>67729407add902e3d4d36bb105d7c011fb368ea5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>LLMs/Stable-Vicuna-13B</td>
      <td>47.95</td>
      <td>53.41</td>
      <td>78.57</td>
      <td>50.37</td>
      <td>48.36</td>
      <td>56.99</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>13.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>51f3d9eaa71de287c96195abd0ff954839857b19</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TigerResearch/tigerbot-7b-base</td>
      <td>47.93</td>
      <td>47.70</td>
      <td>72.08</td>
      <td>45.11</td>
      <td>42.27</td>
      <td>69.61</td>
      <td>10.84</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>300831494aa1eb16e59799310a09531f60dcc904</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>itsliupeng/openllama-7b-icl</td>
      <td>47.93</td>
      <td>47.95</td>
      <td>77.04</td>
      <td>44.37</td>
      <td>37.06</td>
      <td>70.17</td>
      <td>10.99</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>d6317fed3b190cc4d4c27b9f27ccf7c77f0b2e3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vibhorag101/llama-2-7b-chat-hf-phr_mental_health-2048</td>
      <td>47.92</td>
      <td>52.39</td>
      <td>75.39</td>
      <td>39.77</td>
      <td>42.89</td>
      <td>71.19</td>
      <td>5.91</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>81d424a431ab7fa4ff725925b6d0e4269d4563e4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DevaMalla/llama_7b_qlora_pds-eval</td>
      <td>47.90</td>
      <td>53.92</td>
      <td>78.13</td>
      <td>32.98</td>
      <td>45.60</td>
      <td>72.61</td>
      <td>4.17</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>d20419e1d9e9a6a59ced3edf5169e8e7b3e8394c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/Uncensored-Frank-7B</td>
      <td>47.90</td>
      <td>54.27</td>
      <td>76.52</td>
      <td>37.50</td>
      <td>43.86</td>
      <td>70.24</td>
      <td>5.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>65bbcb80158a6d2e133bba99a90142caf4e2e242</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-shishya-ac-hal-7b-ep3</td>
      <td>47.89</td>
      <td>44.62</td>
      <td>76.98</td>
      <td>50.96</td>
      <td>43.03</td>
      <td>71.74</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a76df6b71b959745a5f1804791071332ee6522ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>synapsoft/Llama-2-7b-chat-hf-flan2022-1.2M</td>
      <td>47.89</td>
      <td>49.57</td>
      <td>76.25</td>
      <td>45.99</td>
      <td>42.17</td>
      <td>71.82</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>False</td>
      <td>825506858e4603745a479215b8dea1524bfab6a0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>allbyai/ToRoLaMa-7b-v1.0</td>
      <td>47.87</td>
      <td>51.71</td>
      <td>73.82</td>
      <td>45.34</td>
      <td>44.89</td>
      <td>70.09</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>9dd9ebe69ae8b391722c4edbfe70bd6c59b3b14d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>webbigdata/ALMA-7B-Ja-V2</td>
      <td>47.85</td>
      <td>52.39</td>
      <td>77.92</td>
      <td>44.72</td>
      <td>38.66</td>
      <td>73.40</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>c2497586b28f419ad12c734600d08b2a5784ddc1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azazelle/Sina-Odin-7b-Merge</td>
      <td>47.82</td>
      <td>52.82</td>
      <td>68.86</td>
      <td>45.54</td>
      <td>39.20</td>
      <td>72.22</td>
      <td>8.26</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>cc-by-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c60ddc48eabbd4e7629afd26eb5a79efb4278084</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/goims</td>
      <td>47.80</td>
      <td>49.49</td>
      <td>72.67</td>
      <td>43.85</td>
      <td>44.80</td>
      <td>69.69</td>
      <td>6.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9ef1045ca31f670d9cbf820af904b33a097cd787</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-7b-8k-chat</td>
      <td>47.78</td>
      <td>48.04</td>
      <td>77.62</td>
      <td>41.88</td>
      <td>43.68</td>
      <td>71.03</td>
      <td>4.40</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>7.00</td>
      <td>34.0</td>
      <td>True</td>
      <td>ef97b878a279cd1765fbed7b8321fb3cff1aa5b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>h2m/mhm-7b-v1.3-DPO-1</td>
      <td>47.77</td>
      <td>49.57</td>
      <td>68.10</td>
      <td>45.76</td>
      <td>45.88</td>
      <td>62.04</td>
      <td>15.24</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ebd98fba486278e82be038bdc4b410c6bbd9c2d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bofenghuang/vigogne-7b-instruct</td>
      <td>47.76</td>
      <td>51.96</td>
      <td>78.11</td>
      <td>38.43</td>
      <td>42.47</td>
      <td>72.85</td>
      <td>2.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>7.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>c6e2f515a0b289478118b5b75ff74107002ad962</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>quantumaikr/KoreanLM-hf</td>
      <td>47.73</td>
      <td>51.45</td>
      <td>76.77</td>
      <td>40.61</td>
      <td>44.34</td>
      <td>69.77</td>
      <td>3.41</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>a7261e7ae6ee76c78e1ba1ac8c59bcc3e0868bf9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LeoLM/leo-hessianai-7b</td>
      <td>47.72</td>
      <td>51.96</td>
      <td>75.84</td>
      <td>42.85</td>
      <td>37.94</td>
      <td>72.14</td>
      <td>5.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>32.0</td>
      <td>False</td>
      <td>88c5ac07006ea8f1b5d10aa4f03f0d624dd27e56</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b-gpt4</td>
      <td>47.70</td>
      <td>53.07</td>
      <td>78.69</td>
      <td>38.90</td>
      <td>40.72</td>
      <td>73.09</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>d9bcb0ad365bfacdf95128bc1272b4106aff7be9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/airoboros-7b-gpt4-fp16</td>
      <td>47.70</td>
      <td>53.07</td>
      <td>78.67</td>
      <td>38.88</td>
      <td>40.73</td>
      <td>73.09</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>14aa50fba9f6418c0d5e2d24087eb802931040ef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>microsoft/phi-1_5</td>
      <td>47.69</td>
      <td>52.90</td>
      <td>63.79</td>
      <td>43.89</td>
      <td>40.89</td>
      <td>72.22</td>
      <td>12.43</td>
      <td>pretrained</td>
      <td>PhiForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>1197.0</td>
      <td>True</td>
      <td>ea95720a352172db6fcbcd89032bfb1cb8481797</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b-fast</td>
      <td>47.67</td>
      <td>51.88</td>
      <td>75.46</td>
      <td>44.34</td>
      <td>36.45</td>
      <td>71.59</td>
      <td>6.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>e326078aa122fb1c4973997952d7b8630071776a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jphme/orca_mini_v2_ger_7b</td>
      <td>47.65</td>
      <td>49.83</td>
      <td>75.50</td>
      <td>39.10</td>
      <td>45.74</td>
      <td>71.59</td>
      <td>4.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>7.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>175965f50907c6a8cd40f1a4b10d28342969c066</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>openthaigpt/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf</td>
      <td>47.65</td>
      <td>50.85</td>
      <td>74.89</td>
      <td>40.02</td>
      <td>47.23</td>
      <td>69.06</td>
      <td>3.87</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>cdffb3488c5cb1a9aa5039a6b3bc72af24827db0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>keyfan/vicuna-chinese-replication-v1.1</td>
      <td>47.65</td>
      <td>42.83</td>
      <td>71.47</td>
      <td>47.47</td>
      <td>47.24</td>
      <td>67.40</td>
      <td>9.48</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>259ab0967975012a546f2362d6cd03ab10768157</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>golaxy/gowizardlm</td>
      <td>47.64</td>
      <td>49.74</td>
      <td>71.90</td>
      <td>42.96</td>
      <td>47.66</td>
      <td>69.61</td>
      <td>3.94</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>385f2d164e7fe780e053276d95d36240f2368c21</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>teilomillet/MiniMerlin-3B</td>
      <td>47.63</td>
      <td>44.37</td>
      <td>66.56</td>
      <td>43.21</td>
      <td>47.07</td>
      <td>64.40</td>
      <td>20.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>7fefc3d23e77c699aadba55c40d9e364eb73baf0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>project-baize/baize-healthcare-lora-7B</td>
      <td>47.62</td>
      <td>54.10</td>
      <td>77.32</td>
      <td>37.09</td>
      <td>39.96</td>
      <td>72.85</td>
      <td>4.40</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e3eb8bb0d8840431afe24760d964f8ba57edd83e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bigcode/starcoderplus</td>
      <td>47.61</td>
      <td>48.72</td>
      <td>77.30</td>
      <td>43.72</td>
      <td>37.85</td>
      <td>70.01</td>
      <td>8.04</td>
      <td>fine-tuned</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>204.0</td>
      <td>False</td>
      <td>95be82087c33f14ee9941c812a154a9dd66efe72</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>s3nh/nsfw-noromaid-mistral-instruct</td>
      <td>47.49</td>
      <td>51.79</td>
      <td>75.39</td>
      <td>46.47</td>
      <td>33.49</td>
      <td>71.19</td>
      <td>6.60</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>a1f9d4f788c52967433396cbbb46e8bec4e0d891</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Neko-Institute-of-Science/metharme-7b</td>
      <td>47.48</td>
      <td>53.67</td>
      <td>78.62</td>
      <td>35.91</td>
      <td>39.16</td>
      <td>72.53</td>
      <td>5.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>12.0</td>
      <td>False</td>
      <td>62ca156891feead8db117be8f5f35687b6274e6e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DevaMalla/llama_7b_qlora_cds</td>
      <td>47.43</td>
      <td>52.47</td>
      <td>77.76</td>
      <td>32.38</td>
      <td>46.14</td>
      <td>71.74</td>
      <td>4.09</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b6b5c65c5c1cce34d24c8f790bb0cc011e0f0808</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aiplanet/effi-7b</td>
      <td>47.42</td>
      <td>55.12</td>
      <td>78.07</td>
      <td>35.91</td>
      <td>39.71</td>
      <td>72.53</td>
      <td>3.18</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>d58c62ee27cae60392bd0bd53e1fd05ea82e273b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b-gpt4-1.2</td>
      <td>47.42</td>
      <td>52.13</td>
      <td>78.14</td>
      <td>38.64</td>
      <td>41.79</td>
      <td>71.67</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>28.0</td>
      <td>True</td>
      <td>431fda60009d9b37a73211123ffb9c797764e182</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_v2_7b</td>
      <td>47.41</td>
      <td>50.77</td>
      <td>76.02</td>
      <td>39.50</td>
      <td>43.86</td>
      <td>71.43</td>
      <td>2.88</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>165850882991d7fa4eabab577a03ed84e0713bfa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b</td>
      <td>47.40</td>
      <td>53.07</td>
      <td>77.65</td>
      <td>37.23</td>
      <td>43.39</td>
      <td>70.96</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>7ea67f85ff3a7a8ec77f1819dec3e56779b764b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-shishya-7b-ep3-v1</td>
      <td>47.40</td>
      <td>45.90</td>
      <td>76.36</td>
      <td>50.04</td>
      <td>40.32</td>
      <td>71.74</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>082cf758aa3f6d8f956056003b5b3b6cde447d88</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jxhong/CAlign-alpaca-7b</td>
      <td>47.39</td>
      <td>50.94</td>
      <td>74.55</td>
      <td>38.56</td>
      <td>46.89</td>
      <td>72.06</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f5cc642a10160a014e2afeefcd57d4781994c51e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-7b-8k-instruct</td>
      <td>47.37</td>
      <td>45.90</td>
      <td>74.47</td>
      <td>41.97</td>
      <td>35.21</td>
      <td>65.98</td>
      <td>20.70</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>24.0</td>
      <td>True</td>
      <td>736f68aceeb61298a5de3cf5ae81d0bc2697edf4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/guanaco-7B-HF</td>
      <td>47.34</td>
      <td>52.99</td>
      <td>80.05</td>
      <td>35.32</td>
      <td>39.20</td>
      <td>71.43</td>
      <td>5.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>7.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>293c24105fa15afa127a2ec3905fdc2a0a3a6dac</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>h2m/mhm-7b-v1.3</td>
      <td>47.29</td>
      <td>47.53</td>
      <td>65.31</td>
      <td>45.74</td>
      <td>46.22</td>
      <td>62.27</td>
      <td>16.68</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>0e8363818fdbdc8bacb1467e019f49fa8a9f4329</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>openlm-research/open_llama_13b</td>
      <td>47.26</td>
      <td>51.19</td>
      <td>75.23</td>
      <td>43.75</td>
      <td>38.08</td>
      <td>72.06</td>
      <td>3.26</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>446.0</td>
      <td>True</td>
      <td>b6d7fde8392250730d24cc2fcfa3b7e5f9a03ce8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rufjdk5480/gov-qna-ko-merged</td>
      <td>47.24</td>
      <td>39.51</td>
      <td>39.06</td>
      <td>71.86</td>
      <td>48.61</td>
      <td>56.75</td>
      <td>27.67</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>810c90db1842f6c5f314f23b7549d58316e0db95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>rufjdk5480/mixtral-ko-qna-merged</td>
      <td>47.24</td>
      <td>39.51</td>
      <td>39.06</td>
      <td>71.86</td>
      <td>48.61</td>
      <td>56.75</td>
      <td>27.67</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>810c90db1842f6c5f314f23b7549d58316e0db95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>mosaicml/mpt-7b-8k</td>
      <td>47.24</td>
      <td>47.35</td>
      <td>77.40</td>
      <td>42.58</td>
      <td>36.65</td>
      <td>71.11</td>
      <td>8.34</td>
      <td>pretrained</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>23.0</td>
      <td>True</td>
      <td>c94f57239fed80eac0dc62507aee049681c799a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Undi95/Mixtral-8x7B-MoE-RP-Story</td>
      <td>47.23</td>
      <td>51.54</td>
      <td>70.00</td>
      <td>43.04</td>
      <td>41.53</td>
      <td>67.32</td>
      <td>9.93</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>46.70</td>
      <td>24.0</td>
      <td>True</td>
      <td>ce4a4e4ffec063a3e338b6ebc328365270b6c5f0</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FelixChao/CodeLlama13B-Finetune-v1</td>
      <td>47.19</td>
      <td>45.82</td>
      <td>69.36</td>
      <td>45.05</td>
      <td>44.97</td>
      <td>66.93</td>
      <td>10.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>40ff78ce37efcaf83718534c494829a573b9d719</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mosaicml/mpt-7b-8k-instruct</td>
      <td>47.18</td>
      <td>45.48</td>
      <td>74.41</td>
      <td>42.11</td>
      <td>35.06</td>
      <td>65.51</td>
      <td>20.55</td>
      <td>instruction-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>24.0</td>
      <td>True</td>
      <td>736f68aceeb61298a5de3cf5ae81d0bc2697edf4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>starmpcc/Asclepius-Llama2-7B</td>
      <td>47.15</td>
      <td>50.85</td>
      <td>76.53</td>
      <td>43.61</td>
      <td>43.31</td>
      <td>68.27</td>
      <td>0.30</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>2f15bd8250d7825307e59cc2c785074ebbec3395</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>rinna/youri-7b</td>
      <td>47.11</td>
      <td>49.06</td>
      <td>74.89</td>
      <td>42.22</td>
      <td>36.03</td>
      <td>71.82</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>2be40b8a7b669c4520bc04ce954bdbd7d4b0da7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>itsliupeng/openllama-7b-base</td>
      <td>47.09</td>
      <td>46.16</td>
      <td>76.40</td>
      <td>42.82</td>
      <td>36.65</td>
      <td>70.88</td>
      <td>9.63</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>24d98f339fabfa479e3c85404f5e4dda9e43dcd1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/LLaMA-2-7B-32K</td>
      <td>47.07</td>
      <td>47.53</td>
      <td>76.14</td>
      <td>43.33</td>
      <td>39.23</td>
      <td>71.90</td>
      <td>4.32</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>492.0</td>
      <td>True</td>
      <td>aef6d8946ae1015bdb65c478a2dd73b58daaef47</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-gpt-3.5-turbo-100k-7b</td>
      <td>47.05</td>
      <td>53.07</td>
      <td>76.16</td>
      <td>33.63</td>
      <td>45.07</td>
      <td>70.80</td>
      <td>3.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>53887996c0f17f7711d182537505a895fb404542</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>notstoic/PygmalionCoT-7b</td>
      <td>47.00</td>
      <td>51.45</td>
      <td>76.92</td>
      <td>33.35</td>
      <td>48.13</td>
      <td>68.90</td>
      <td>3.26</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>c03ac527360663d17bb142405251028eec843ed9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mncai/chatdoctor</td>
      <td>46.95</td>
      <td>53.75</td>
      <td>78.54</td>
      <td>35.95</td>
      <td>43.55</td>
      <td>69.93</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LLaMAForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>8fdcfdda6877d7f21173dfac48b2c14499ba8264</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>ausboss/llama7b-wizardlm-unfiltered</td>
      <td>46.94</td>
      <td>52.99</td>
      <td>77.89</td>
      <td>36.41</td>
      <td>37.75</td>
      <td>72.30</td>
      <td>4.32</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>2123beec77083c414b2ae51dd25b7a870b0b936c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/dolphin-llama2-7b</td>
      <td>46.94</td>
      <td>46.59</td>
      <td>67.52</td>
      <td>48.37</td>
      <td>49.72</td>
      <td>63.77</td>
      <td>5.69</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>85aa4f67191fd016ab7ea8c389fddb5d9e5a9a52</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WhiteRabbitNeo/WhiteRabbitNeo-33B-v1</td>
      <td>46.93</td>
      <td>44.37</td>
      <td>60.22</td>
      <td>40.56</td>
      <td>41.68</td>
      <td>61.01</td>
      <td>33.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>33.00</td>
      <td>57.0</td>
      <td>True</td>
      <td>e508c81aaf6b8bf8d1c7cbad5c9ddaed85fbb7dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b-gpt4-1.3</td>
      <td>46.91</td>
      <td>52.47</td>
      <td>77.98</td>
      <td>41.97</td>
      <td>35.73</td>
      <td>72.30</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7b5f77827636bbf3174c48ca16e774c89d71d7bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stanford-oval/Llama-2-7b-WikiChat-fused</td>
      <td>46.81</td>
      <td>50.68</td>
      <td>75.00</td>
      <td>39.69</td>
      <td>46.36</td>
      <td>69.06</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>3.0</td>
      <td>True</td>
      <td>47cc2d3e1719da0f0300d07111ea6a9b6e3aa2d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DevaMalla/llama_7b_lora</td>
      <td>46.77</td>
      <td>54.86</td>
      <td>79.10</td>
      <td>33.63</td>
      <td>34.74</td>
      <td>72.77</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7f4cbd810b4bef0d75c1fd3f551146b4ea97d9fd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>project-baize/baize-v2-7b</td>
      <td>46.72</td>
      <td>48.98</td>
      <td>75.06</td>
      <td>39.60</td>
      <td>41.39</td>
      <td>71.11</td>
      <td>4.17</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>e4731c2c2671e2d0b47b5eba08c753ca21671fab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-dolphin-orca-platypus-13b</td>
      <td>46.70</td>
      <td>44.80</td>
      <td>68.60</td>
      <td>44.03</td>
      <td>46.28</td>
      <td>66.93</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>0c41023f8f665946a2c46c3823afee431408bcbd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-platypus-13b</td>
      <td>46.68</td>
      <td>46.16</td>
      <td>68.88</td>
      <td>44.55</td>
      <td>44.98</td>
      <td>66.14</td>
      <td>9.40</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>7a771bd8899b9ef4ba9680e96f84dc85810a67d6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.2-yi-34b-200k</td>
      <td>46.67</td>
      <td>42.24</td>
      <td>68.22</td>
      <td>55.51</td>
      <td>45.94</td>
      <td>64.17</td>
      <td>3.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>28.0</td>
      <td>True</td>
      <td>33950ffa68b9f8cd5dc2f046c6c9a2d0f0bf7eff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ehartford/dolphin-2.2-yi-34b-200k</td>
      <td>46.67</td>
      <td>42.15</td>
      <td>68.18</td>
      <td>55.47</td>
      <td>45.93</td>
      <td>64.56</td>
      <td>3.71</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>34.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c4e02a3a5c7a9d51f8b0cad85952dfdfb34c9413</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/gogpt2-7b</td>
      <td>46.65</td>
      <td>46.76</td>
      <td>71.53</td>
      <td>42.85</td>
      <td>47.85</td>
      <td>68.67</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>ee60ed402dedf24b6154aef05df54512e02fc9e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>sarvamai/OpenHathi-7B-Hi-v0.1-Base</td>
      <td>46.64</td>
      <td>49.49</td>
      <td>74.34</td>
      <td>41.38</td>
      <td>37.46</td>
      <td>71.27</td>
      <td>5.91</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.87</td>
      <td>56.0</td>
      <td>True</td>
      <td>2cbb156ab4426113115bc3387b06d1940015119a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vikash06/llama-2-7b-small-model-new</td>
      <td>46.62</td>
      <td>45.22</td>
      <td>72.35</td>
      <td>46.23</td>
      <td>42.46</td>
      <td>63.93</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>eefcb721d116ff2e486c4b70cf506e6c0d00fb0f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DevaMalla/llama_7b_qlora</td>
      <td>46.61</td>
      <td>55.12</td>
      <td>78.26</td>
      <td>35.71</td>
      <td>33.98</td>
      <td>72.06</td>
      <td>4.55</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7f94b0be78193abc54722cf723541c3800426f7b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>stabilityai/stablelm-3b-4e1t</td>
      <td>46.58</td>
      <td>46.59</td>
      <td>75.94</td>
      <td>45.23</td>
      <td>37.20</td>
      <td>71.19</td>
      <td>3.34</td>
      <td>pretrained</td>
      <td>StableLMEpochForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>2.80</td>
      <td>282.0</td>
      <td>True</td>
      <td>a4750ace0db6f08d7bbba0aa52a585f231ea3cde</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>chavinlo/alpaca-native</td>
      <td>46.58</td>
      <td>52.30</td>
      <td>77.09</td>
      <td>41.60</td>
      <td>37.58</td>
      <td>69.46</td>
      <td>1.44</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>252.0</td>
      <td>False</td>
      <td>cc7773cac2478231807c56ef2f02292d98f85cf5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/gogpt2-13b</td>
      <td>46.55</td>
      <td>48.38</td>
      <td>71.78</td>
      <td>44.50</td>
      <td>44.73</td>
      <td>67.88</td>
      <td>2.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>16d4c4214fa8d5a962b9064a8b958076b7c79a17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DevaMalla/llama7b_alpaca_1gpu_bf16</td>
      <td>46.49</td>
      <td>52.73</td>
      <td>78.78</td>
      <td>36.26</td>
      <td>33.71</td>
      <td>72.93</td>
      <td>4.55</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>305683c1b95f6888b8668dbc6b56d9efa5d07fef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/Pygmalion_AlpacaLora-7b</td>
      <td>46.49</td>
      <td>53.24</td>
      <td>76.92</td>
      <td>35.92</td>
      <td>39.44</td>
      <td>72.22</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>7.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>1f61442e1238062095b31b4909c5e9ab26105794</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cognitivecomputations/dolphin-2.2-yi-34b-200k</td>
      <td>46.47</td>
      <td>42.06</td>
      <td>68.13</td>
      <td>55.35</td>
      <td>45.93</td>
      <td>64.25</td>
      <td>3.11</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>34.00</td>
      <td>28.0</td>
      <td>True</td>
      <td>33950ffa68b9f8cd5dc2f046c6c9a2d0f0bf7eff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>0x7194633/fialka-7B-v3</td>
      <td>46.40</td>
      <td>48.55</td>
      <td>71.05</td>
      <td>43.06</td>
      <td>44.79</td>
      <td>69.46</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>d0dae57538d9379526726e66d5156ec0042528be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jerryjalapeno/nart-100k-7b</td>
      <td>46.39</td>
      <td>54.10</td>
      <td>78.47</td>
      <td>34.98</td>
      <td>36.74</td>
      <td>70.48</td>
      <td>3.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.00</td>
      <td>16.0</td>
      <td>False</td>
      <td>50e61b8e6cc17cb3fbcb490fe3dc7e2c8b248378</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/gogpt-7b</td>
      <td>46.38</td>
      <td>48.81</td>
      <td>73.79</td>
      <td>43.03</td>
      <td>41.00</td>
      <td>69.77</td>
      <td>1.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>7eb70c0e330b7d3ff490047ddbb153bb96294882</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>huggyllama/llama-7b</td>
      <td>46.37</td>
      <td>50.94</td>
      <td>77.81</td>
      <td>35.69</td>
      <td>34.33</td>
      <td>71.43</td>
      <td>8.04</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>238.0</td>
      <td>True</td>
      <td>8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jondurbin/airoboros-7b-gpt4-1.4.1-qlora</td>
      <td>46.34</td>
      <td>52.73</td>
      <td>77.89</td>
      <td>38.77</td>
      <td>36.07</td>
      <td>70.32</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>91ffa900ed637cf5fd904d96e6985b6f7857ad64</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wenge-research/yayi-13b-llama2</td>
      <td>46.32</td>
      <td>48.55</td>
      <td>74.82</td>
      <td>38.68</td>
      <td>42.19</td>
      <td>69.69</td>
      <td>4.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>9fc1bc4409b9e71f54213245a91c2742fbf7b3d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>speechlessai/speechless-codellama-dolphin-orca-platypus-13b</td>
      <td>46.32</td>
      <td>45.82</td>
      <td>67.71</td>
      <td>45.88</td>
      <td>44.67</td>
      <td>65.35</td>
      <td>8.49</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>25e1c346c2a01588a728307d5c35fbeecd58b51b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-orca-13b</td>
      <td>46.28</td>
      <td>46.33</td>
      <td>67.71</td>
      <td>47.19</td>
      <td>46.66</td>
      <td>63.77</td>
      <td>5.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>a82467de3cb9438aa8f9e0ea8ea692f16a5724b2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yhyhy3/open_llama_7b_v2_med_instruct</td>
      <td>46.24</td>
      <td>46.50</td>
      <td>76.91</td>
      <td>42.32</td>
      <td>40.33</td>
      <td>69.30</td>
      <td>2.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>cabb47abd422a2d67161e2d038265ee23be45fb8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-llama2-7b-pretrain</td>
      <td>46.18</td>
      <td>48.63</td>
      <td>74.83</td>
      <td>41.04</td>
      <td>39.08</td>
      <td>70.24</td>
      <td>3.26</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>444c85ef809f8793d84b0813ab78bec50700cfcf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>stabilityai/stablelm-base-alpha-7b-v2</td>
      <td>46.18</td>
      <td>47.35</td>
      <td>77.08</td>
      <td>45.10</td>
      <td>36.46</td>
      <td>68.51</td>
      <td>2.58</td>
      <td>pretrained</td>
      <td>StableLMAlphaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>6.89</td>
      <td>46.0</td>
      <td>True</td>
      <td>eb3b56fee1ad4b1efe6625bbbc7a277df8ab5b96</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ajibawa-2023/carl-7b</td>
      <td>46.16</td>
      <td>53.50</td>
      <td>78.29</td>
      <td>33.96</td>
      <td>40.29</td>
      <td>68.59</td>
      <td>2.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>de4c7af9598bebc47dd43253c972be719f3195d6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/vicuna-class-shishya-7b-ep3</td>
      <td>46.14</td>
      <td>40.61</td>
      <td>76.72</td>
      <td>50.77</td>
      <td>36.87</td>
      <td>71.90</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c2bd682b9f3babbb3bc84f84856fabe69a3c21d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bigscience/bloom</td>
      <td>46.07</td>
      <td>50.43</td>
      <td>76.41</td>
      <td>30.85</td>
      <td>39.76</td>
      <td>72.06</td>
      <td>6.90</td>
      <td>pretrained</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>176.25</td>
      <td>4301.0</td>
      <td>True</td>
      <td>053d9cd9fbe814e091294f67fcfedb3397b954bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fireballoon/baichuan-vicuna-chinese-7b</td>
      <td>46.06</td>
      <td>43.52</td>
      <td>71.12</td>
      <td>46.87</td>
      <td>42.45</td>
      <td>66.85</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>62.0</td>
      <td>False</td>
      <td>6cdb9e75cd473e31e87067c2a0b646083247d9ab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>illuin/test-custom-llama</td>
      <td>46.05</td>
      <td>52.30</td>
      <td>77.49</td>
      <td>36.61</td>
      <td>33.81</td>
      <td>72.06</td>
      <td>4.02</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d985610bef080473e40f01c53266083c5f0c3169</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Neko-Institute-of-Science/pygmalion-7b</td>
      <td>46.04</td>
      <td>51.37</td>
      <td>77.81</td>
      <td>35.68</td>
      <td>34.54</td>
      <td>72.22</td>
      <td>4.62</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>39.0</td>
      <td>False</td>
      <td>6473f9996d758fde48a181f37cc5de575aff1606</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco-modified2</td>
      <td>46.03</td>
      <td>42.92</td>
      <td>73.97</td>
      <td>48.49</td>
      <td>40.43</td>
      <td>69.69</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>8e1930bbbbdeb4f6f4639e837f09d9878bbf7831</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>qnguyen3/quan-1.8b-chat</td>
      <td>45.91</td>
      <td>39.08</td>
      <td>62.37</td>
      <td>44.09</td>
      <td>43.15</td>
      <td>59.27</td>
      <td>27.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.80</td>
      <td>5.0</td>
      <td>True</td>
      <td>3b648e8a549888292a73a21b7312d958de6e875d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vikash06/mistral_v1</td>
      <td>45.85</td>
      <td>47.01</td>
      <td>67.58</td>
      <td>48.68</td>
      <td>37.53</td>
      <td>64.80</td>
      <td>9.48</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9b7bd68c8105ff8ab2b6a5d6c9ad32f82c3190a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-13b-Instruct-hf</td>
      <td>45.82</td>
      <td>44.54</td>
      <td>64.93</td>
      <td>38.89</td>
      <td>45.88</td>
      <td>68.03</td>
      <td>12.66</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>102.0</td>
      <td>True</td>
      <td>b9f91b7351ecd589118d883afa23d5c93a38c612</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TheBloke/CodeLlama-13B-Instruct-fp16</td>
      <td>45.82</td>
      <td>44.62</td>
      <td>64.94</td>
      <td>38.77</td>
      <td>45.88</td>
      <td>68.03</td>
      <td>12.66</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>28.0</td>
      <td>True</td>
      <td>521c208c7251ccd3e44ccd9500b6bed419bca565</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/llama2-7b-raw-sft</td>
      <td>45.67</td>
      <td>47.44</td>
      <td>75.25</td>
      <td>33.86</td>
      <td>40.77</td>
      <td>73.01</td>
      <td>3.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>cd167d27b6c116b23863da859a07d08c6359c207</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/mistral-7b-raw-sft</td>
      <td>45.67</td>
      <td>47.44</td>
      <td>75.25</td>
      <td>33.86</td>
      <td>40.77</td>
      <td>73.01</td>
      <td>3.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>e1b241a26e35b87137fba8a54e352f1e4c98eebf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Planner-7B-fp16</td>
      <td>45.65</td>
      <td>51.02</td>
      <td>77.82</td>
      <td>35.71</td>
      <td>34.33</td>
      <td>71.43</td>
      <td>3.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>afb4604a06c8541960fb51240259777764c4ce7e</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>huggingface/llama-7b</td>
      <td>45.65</td>
      <td>51.02</td>
      <td>77.82</td>
      <td>35.71</td>
      <td>34.33</td>
      <td>71.43</td>
      <td>3.56</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>f356572651e58fb337d610470d4b36976e7fb802</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>uukuguy/speechless-codellama-platypus-13b</td>
      <td>45.64</td>
      <td>45.31</td>
      <td>68.63</td>
      <td>42.82</td>
      <td>42.38</td>
      <td>65.59</td>
      <td>9.10</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>81cb1bca46ce646b8339501537837e02116de1b8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>DevaMalla/llama-base-7b</td>
      <td>45.62</td>
      <td>50.94</td>
      <td>77.80</td>
      <td>35.67</td>
      <td>34.34</td>
      <td>71.43</td>
      <td>3.56</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e01d89d8e444f7d751ea58feaf22ff8c9af69d2a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WeOpenML/PandaLM-Alpaca-7B-v1</td>
      <td>45.59</td>
      <td>50.85</td>
      <td>77.36</td>
      <td>35.91</td>
      <td>36.63</td>
      <td>71.90</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>7fe5cb1a7009fdade8dfcfec335527997a730fcf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>yeontaek/WizardCoder-Python-13B-LoRa</td>
      <td>45.56</td>
      <td>47.78</td>
      <td>69.60</td>
      <td>38.76</td>
      <td>43.97</td>
      <td>65.43</td>
      <td>7.81</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>32ffc44ffdf1adfe2d8ef219327fbd534f3d5955</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abhinand/tamil-llama-7b-instruct-v0.1</td>
      <td>45.52</td>
      <td>48.04</td>
      <td>70.97</td>
      <td>39.95</td>
      <td>41.70</td>
      <td>70.64</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>36f04b36c781ff994af41060df09491bde54105d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Linly-AI/Chinese-LLaMA-2-7B-hf</td>
      <td>45.44</td>
      <td>48.04</td>
      <td>73.25</td>
      <td>35.04</td>
      <td>39.92</td>
      <td>70.17</td>
      <td>6.22</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>25.0</td>
      <td>False</td>
      <td>a2d55220b3d0693825fe69e1174653dc6cc4a920</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shibing624/chinese-llama-plus-13b-hf</td>
      <td>45.39</td>
      <td>46.25</td>
      <td>71.88</td>
      <td>40.74</td>
      <td>39.89</td>
      <td>73.09</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>19.0</td>
      <td>True</td>
      <td>f17a52b8067d551a814069d2c710e1f5c487a3ce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-7b-chat</td>
      <td>45.39</td>
      <td>46.50</td>
      <td>75.51</td>
      <td>37.62</td>
      <td>40.16</td>
      <td>68.43</td>
      <td>4.09</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>7.00</td>
      <td>500.0</td>
      <td>True</td>
      <td>64e5c9c9fb53a8e89690c2dee75a5add37f7113e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco-modified1</td>
      <td>45.38</td>
      <td>40.87</td>
      <td>73.40</td>
      <td>47.42</td>
      <td>39.87</td>
      <td>69.46</td>
      <td>1.29</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>a7749ff092ef03900de34b69d41c767a6a48ea9e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf</td>
      <td>45.35</td>
      <td>44.97</td>
      <td>70.19</td>
      <td>36.22</td>
      <td>49.99</td>
      <td>69.38</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>dfc8a1e7ac47765466764dc48c285c5bd23de1fd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beomi/llama-2-ko-7b</td>
      <td>45.32</td>
      <td>48.46</td>
      <td>75.28</td>
      <td>39.56</td>
      <td>34.49</td>
      <td>72.14</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.86</td>
      <td>131.0</td>
      <td>False</td>
      <td>d5c58cc2cae21b4fb96aaad2658acc898ab22d99</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>haoranxu/ALMA-7B</td>
      <td>45.32</td>
      <td>50.34</td>
      <td>75.50</td>
      <td>38.04</td>
      <td>35.64</td>
      <td>72.38</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>14.0</td>
      <td>True</td>
      <td>b570315dd26452a07cf15cf6feecce839e1327a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GeneZC/MiniChat-3B</td>
      <td>45.31</td>
      <td>44.03</td>
      <td>67.19</td>
      <td>39.17</td>
      <td>45.67</td>
      <td>65.27</td>
      <td>10.54</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.02</td>
      <td>31.0</td>
      <td>True</td>
      <td>123d23bd291bb2d5fdb3b91dc1570d0b11654a78</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ashercn97/giraffe-7b</td>
      <td>45.29</td>
      <td>47.18</td>
      <td>75.53</td>
      <td>38.89</td>
      <td>38.48</td>
      <td>68.98</td>
      <td>2.65</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9af88449bed5be4709befcfbbba123ee75805479</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>facebook/opt-iml-max-30b</td>
      <td>45.28</td>
      <td>43.86</td>
      <td>72.39</td>
      <td>41.09</td>
      <td>38.16</td>
      <td>73.72</td>
      <td>2.50</td>
      <td></td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>34.0</td>
      <td>True</td>
      <td>291753b04817a31a742631053ee361874d6db8a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-openllama-7b-v12-bf16</td>
      <td>45.28</td>
      <td>42.06</td>
      <td>62.01</td>
      <td>46.53</td>
      <td>45.18</td>
      <td>65.04</td>
      <td>10.84</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>bb94ff691996484b1a9d899a6c0956ef6750d86a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NurtureAI/Orca-2-13B-16k</td>
      <td>45.22</td>
      <td>53.67</td>
      <td>69.48</td>
      <td>41.02</td>
      <td>45.30</td>
      <td>60.06</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>False</td>
      <td>0daee08a5e065d02726e9ae0f05cdfd78992cfba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PotatoOff/HamSter-0.1</td>
      <td>45.19</td>
      <td>46.93</td>
      <td>68.08</td>
      <td>43.03</td>
      <td>51.24</td>
      <td>61.88</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>2.0</td>
      <td>True</td>
      <td>14b81a0c6870d400cd6216682f182d4615203c2a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/llama-shishya-7b-ep3-v1</td>
      <td>45.19</td>
      <td>48.04</td>
      <td>76.63</td>
      <td>46.12</td>
      <td>30.90</td>
      <td>69.46</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>8dc109f45ef36cc7bbd0f5d83fb65ac8e768d1bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Enno-Ai/ennodata-7b</td>
      <td>45.13</td>
      <td>51.02</td>
      <td>77.62</td>
      <td>33.95</td>
      <td>33.53</td>
      <td>70.96</td>
      <td>3.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>7872a492ebbb3c6a899f9acbd34dfd5f7e674fdd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jlevin/guanaco-unchained-llama-2-7b</td>
      <td>45.11</td>
      <td>47.35</td>
      <td>72.16</td>
      <td>41.76</td>
      <td>41.49</td>
      <td>64.48</td>
      <td>3.41</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>43f3de8bcef63eec03a1b00079c08b5932c1a429</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-coding-7b-16k-tora</td>
      <td>45.10</td>
      <td>41.21</td>
      <td>64.45</td>
      <td>39.14</td>
      <td>44.91</td>
      <td>63.61</td>
      <td>17.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>d56b5c4f649d8e722efb927d16d7589967a67fbe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco-modified4</td>
      <td>45.10</td>
      <td>40.70</td>
      <td>73.08</td>
      <td>47.26</td>
      <td>41.59</td>
      <td>67.88</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>715b03c8573df06f3825d1c08b307e2a83fa8bf9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>speechlessai/speechless-coding-7b-16k-tora</td>
      <td>45.05</td>
      <td>41.13</td>
      <td>64.48</td>
      <td>38.86</td>
      <td>44.95</td>
      <td>63.85</td>
      <td>17.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>37281f20d54d895f8e3bc660e68564244c775ac2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</td>
      <td>45.00</td>
      <td>47.35</td>
      <td>69.97</td>
      <td>44.12</td>
      <td>42.87</td>
      <td>65.67</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>7.00</td>
      <td>31.0</td>
      <td>True</td>
      <td>ccbd599ac46bcfbf7020be393afeecef404bce2b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>csitfun/llama-7b-logicot</td>
      <td>44.95</td>
      <td>47.01</td>
      <td>72.56</td>
      <td>38.93</td>
      <td>43.63</td>
      <td>67.56</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>8e9c93c09e6a6c7d504c88d6ca598144829bced8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/WizardLM-7B-Uncensored</td>
      <td>44.92</td>
      <td>47.87</td>
      <td>73.08</td>
      <td>35.42</td>
      <td>41.49</td>
      <td>68.43</td>
      <td>3.26</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>14c23f9fa775ab5ce49010418f00df06d92b0b13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>OpenAssistant/codellama-13b-oasst-sft-v10</td>
      <td>44.85</td>
      <td>45.39</td>
      <td>62.36</td>
      <td>35.36</td>
      <td>45.02</td>
      <td>67.80</td>
      <td>13.19</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>56.0</td>
      <td>True</td>
      <td>612dab2a8b2d77edb4fd36cfc28b3ffbbb20ffc1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shareAI/CodeLLaMA-chat-13b-Chinese</td>
      <td>44.84</td>
      <td>43.26</td>
      <td>63.87</td>
      <td>34.29</td>
      <td>48.97</td>
      <td>67.88</td>
      <td>10.77</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>675b3e35a9601683c2cb4ec7f1b11d2869842f36</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-7b-instruct</td>
      <td>44.83</td>
      <td>50.34</td>
      <td>77.91</td>
      <td>32.35</td>
      <td>35.08</td>
      <td>70.48</td>
      <td>2.81</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>448.0</td>
      <td>True</td>
      <td>925e0d80e50e77aaddaf9c3ced41ca4ea23a1025</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>uukuguy/speechless-codellama-orca-13b</td>
      <td>44.83</td>
      <td>44.37</td>
      <td>65.20</td>
      <td>43.46</td>
      <td>45.94</td>
      <td>64.01</td>
      <td>5.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>6fdfeabe817235df3d560a6e6465c3722bc3a4ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shibing624/chinese-alpaca-plus-7b-hf</td>
      <td>44.77</td>
      <td>49.23</td>
      <td>70.48</td>
      <td>38.39</td>
      <td>39.72</td>
      <td>70.09</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>50.0</td>
      <td>True</td>
      <td>0deb5a13732f1e3e3240ea83f403c57283fe2dc8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>GeneZC/MiniMA-2-3B</td>
      <td>44.75</td>
      <td>44.71</td>
      <td>69.33</td>
      <td>41.22</td>
      <td>38.44</td>
      <td>66.69</td>
      <td>8.11</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>03c9985b5427e143a4e8b513393d65b9bb24a2d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>KnutJaegersberg/Qwen-1_8B-Llamafied</td>
      <td>44.75</td>
      <td>37.71</td>
      <td>58.87</td>
      <td>46.37</td>
      <td>39.41</td>
      <td>61.72</td>
      <td>24.41</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.84</td>
      <td>12.0</td>
      <td>True</td>
      <td>2d58d553f3b54abbb6cc49cdb4f2b47336c3c17e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Writer/palmyra-med-20b</td>
      <td>44.71</td>
      <td>46.93</td>
      <td>73.51</td>
      <td>44.34</td>
      <td>35.47</td>
      <td>65.35</td>
      <td>2.65</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>407810f75698c95000dc0ae1a9a0457be625e972</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TheBloke/Poro-34B-GPTQ</td>
      <td>44.67</td>
      <td>47.01</td>
      <td>73.75</td>
      <td>32.47</td>
      <td>38.37</td>
      <td>71.35</td>
      <td>5.08</td>
      <td>instruction-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>48.06</td>
      <td>2.0</td>
      <td>True</td>
      <td>f6e034384e36b411d6b831157fb6063060ec1169</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>abhinand/tamil-llama-7b-base-v0.1</td>
      <td>44.52</td>
      <td>46.67</td>
      <td>72.85</td>
      <td>40.95</td>
      <td>35.93</td>
      <td>70.72</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>e40f072bf68a157a18247eb08bf5b18ab8138986</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/Project-Baize-v2-7B-GPTQ</td>
      <td>44.50</td>
      <td>45.99</td>
      <td>73.44</td>
      <td>35.46</td>
      <td>39.92</td>
      <td>69.69</td>
      <td>2.50</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>9.04</td>
      <td>4.0</td>
      <td>True</td>
      <td>5dc039834e1ea42ac334458b2e3090fe3705cc59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>qblocks/falcon_7b_norobots</td>
      <td>44.46</td>
      <td>47.87</td>
      <td>77.92</td>
      <td>27.94</td>
      <td>36.81</td>
      <td>71.74</td>
      <td>4.47</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>bbe8e4a0c19ec5a94f6eff680b5a55bd08e11e31</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WeOpenML/Alpaca-7B-v1</td>
      <td>44.41</td>
      <td>49.06</td>
      <td>75.71</td>
      <td>33.76</td>
      <td>36.28</td>
      <td>71.51</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>be5cb84a84a859dd6e5e3efc4648d6d5d1a5d188</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qblocks/falcon_7b_norobots</td>
      <td>44.40</td>
      <td>48.12</td>
      <td>77.90</td>
      <td>28.11</td>
      <td>36.76</td>
      <td>71.59</td>
      <td>3.94</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>bbe8e4a0c19ec5a94f6eff680b5a55bd08e11e31</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>castorini/rank_vicuna_7b_v1_fp16</td>
      <td>44.36</td>
      <td>44.62</td>
      <td>65.67</td>
      <td>44.14</td>
      <td>45.13</td>
      <td>66.61</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>0f3556bb0227cb59bcc652584d879f3bc40102e6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/llama-shishya-7b-ep3-v2</td>
      <td>44.33</td>
      <td>47.35</td>
      <td>75.88</td>
      <td>43.84</td>
      <td>30.16</td>
      <td>68.75</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>679c6cb9e869df686b1ae415ed440e6cfc05f80b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-34b-Instruct-hf</td>
      <td>44.33</td>
      <td>40.78</td>
      <td>35.66</td>
      <td>39.72</td>
      <td>44.29</td>
      <td>74.51</td>
      <td>31.01</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>238.0</td>
      <td>True</td>
      <td>c109b9dde086b31725fa09ff7effdc04c03c033d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/koala-7B-HF</td>
      <td>44.29</td>
      <td>47.10</td>
      <td>73.58</td>
      <td>25.53</td>
      <td>45.96</td>
      <td>69.93</td>
      <td>3.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>d102fe3b68f1a5a50d547e4fd1c8b33b783c993b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>anas-awadalla/mpt-7b</td>
      <td>44.28</td>
      <td>47.70</td>
      <td>77.57</td>
      <td>30.80</td>
      <td>33.44</td>
      <td>72.14</td>
      <td>4.02</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>b772e556c8e8a17d087db6935e7cd019e5eefb0f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>mosaicml/mpt-7b</td>
      <td>44.28</td>
      <td>47.70</td>
      <td>77.57</td>
      <td>30.80</td>
      <td>33.44</td>
      <td>72.14</td>
      <td>4.02</td>
      <td>pretrained</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1117.0</td>
      <td>True</td>
      <td>72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>openlm-research/open_llama_7b_v2</td>
      <td>44.26</td>
      <td>43.69</td>
      <td>72.20</td>
      <td>41.29</td>
      <td>35.54</td>
      <td>69.38</td>
      <td>3.49</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>101.0</td>
      <td>True</td>
      <td>e5961def23172a2384543940e773ab676033c963</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>llm-agents/tora-code-13b-v1.0</td>
      <td>44.19</td>
      <td>44.71</td>
      <td>69.15</td>
      <td>36.69</td>
      <td>34.98</td>
      <td>63.14</td>
      <td>16.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>4bf5b528d95a507b435c24a8986afe80d5951782</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Writer/palmyra-20b-chat</td>
      <td>44.18</td>
      <td>43.52</td>
      <td>72.83</td>
      <td>35.18</td>
      <td>43.17</td>
      <td>66.46</td>
      <td>3.94</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>20.00</td>
      <td>8.0</td>
      <td>False</td>
      <td>3b7442b7e2240846bc9cfac545bd8861c1660aa2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>tiiuae/falcon-7b</td>
      <td>44.17</td>
      <td>47.87</td>
      <td>78.13</td>
      <td>27.79</td>
      <td>34.26</td>
      <td>72.38</td>
      <td>4.62</td>
      <td>pretrained</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>970.0</td>
      <td>True</td>
      <td>378337427557d1df3e742264a2901a49f25d4eb1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>speechlessai/speechless-codellama-airoboros-orca-platypus-13b</td>
      <td>44.10</td>
      <td>44.88</td>
      <td>67.70</td>
      <td>43.16</td>
      <td>40.88</td>
      <td>66.14</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f01d3ab70cc23e31dcf5d6418406b08dc2003153</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qblocks/falcon_7b_DolphinCoder</td>
      <td>44.09</td>
      <td>48.72</td>
      <td>78.03</td>
      <td>27.08</td>
      <td>35.12</td>
      <td>70.48</td>
      <td>5.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>15a07f5340cbb9b6f37db3cda7aa02169feed89f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Zangs3011/falcon_7b_DolphinCoder</td>
      <td>44.09</td>
      <td>48.72</td>
      <td>78.03</td>
      <td>27.08</td>
      <td>35.12</td>
      <td>70.48</td>
      <td>5.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>72558e09e54869de3d8fc9fdd42633b81a1839f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/GPT-JT-6B-v0</td>
      <td>44.05</td>
      <td>42.06</td>
      <td>67.96</td>
      <td>49.34</td>
      <td>38.89</td>
      <td>64.80</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>41bd1937dbc51f9e589d310bddab5b4c1409e783</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>luffycodes/llama-class-shishya-7b-ep3</td>
      <td>43.88</td>
      <td>40.78</td>
      <td>77.04</td>
      <td>46.74</td>
      <td>27.94</td>
      <td>70.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>92802ec9c58b1ed64d758c0f0c8420f4000636ff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/BigTranslate-13B-GPTQ</td>
      <td>43.86</td>
      <td>45.31</td>
      <td>75.10</td>
      <td>31.18</td>
      <td>40.60</td>
      <td>70.96</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>17.99</td>
      <td>16.0</td>
      <td>True</td>
      <td>f2968552d2f522023f3289747234aea5508980e2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>AI-Sweden-Models/gpt-sw3-20b-instruct</td>
      <td>43.70</td>
      <td>43.17</td>
      <td>71.09</td>
      <td>31.32</td>
      <td>41.02</td>
      <td>66.77</td>
      <td>8.79</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>20.92</td>
      <td>9.0</td>
      <td>True</td>
      <td>006477ad4c4875611f20cd927f1fd76bbf5ba5ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>synapsoft/Llama-2-7b-hf-flan2022-1.2M</td>
      <td>43.68</td>
      <td>23.29</td>
      <td>78.46</td>
      <td>42.33</td>
      <td>37.97</td>
      <td>75.53</td>
      <td>4.47</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>False</td>
      <td>792f946a1413a7c58378d7a350b7d75b9df80561</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>souvik0306/falcon_7b_3epoch_norobots</td>
      <td>43.65</td>
      <td>47.61</td>
      <td>77.24</td>
      <td>29.73</td>
      <td>36.27</td>
      <td>69.53</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>55b11c279d1a5b83f59cec0381fb41c31fd02d8d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hyunseoki/ko-ref-llama2-13b</td>
      <td>43.62</td>
      <td>48.38</td>
      <td>73.56</td>
      <td>34.83</td>
      <td>35.82</td>
      <td>69.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>c5d09631c88ab5012b48187ecd90ae773cd4bbd9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>deepseek-ai/deepseek-coder-6.7b-instruct</td>
      <td>43.57</td>
      <td>38.14</td>
      <td>55.09</td>
      <td>39.02</td>
      <td>45.56</td>
      <td>56.83</td>
      <td>26.76</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>149.0</td>
      <td>True</td>
      <td>cbb77d7448ea3168d884758817e7f895e3828d1c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-40b</td>
      <td>43.42</td>
      <td>43.00</td>
      <td>72.37</td>
      <td>34.97</td>
      <td>37.52</td>
      <td>67.96</td>
      <td>4.70</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>39.93</td>
      <td>5.0</td>
      <td>True</td>
      <td>ed18193e7292b5a821e5271d5dac95fffdf9617c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>l3utterfly/minima-3b-layla-v2</td>
      <td>43.39</td>
      <td>44.20</td>
      <td>69.93</td>
      <td>28.53</td>
      <td>43.64</td>
      <td>65.43</td>
      <td>8.64</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>118b6f7cf649f829afdec715eb4720dcd2a572b9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/CodeLlama-13b-hf</td>
      <td>43.35</td>
      <td>40.87</td>
      <td>63.35</td>
      <td>32.81</td>
      <td>43.79</td>
      <td>67.17</td>
      <td>12.13</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b7cfbbce945b966607d15ae275704922a6d04afc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>codellama/CodeLlama-13b-hf</td>
      <td>43.35</td>
      <td>40.87</td>
      <td>63.35</td>
      <td>32.81</td>
      <td>43.79</td>
      <td>67.17</td>
      <td>12.13</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>69.0</td>
      <td>True</td>
      <td>55876f398020b287ac845b34ca08089acf4f4bc3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TigerResearch/tigerbot-7b-sft</td>
      <td>43.35</td>
      <td>41.64</td>
      <td>60.56</td>
      <td>29.89</td>
      <td>58.18</td>
      <td>63.54</td>
      <td>6.29</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>98b847905d63f74624e834db1ff95ee2814cbbd3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cyberagent/calm2-7b-chat</td>
      <td>43.27</td>
      <td>40.27</td>
      <td>68.12</td>
      <td>39.39</td>
      <td>41.96</td>
      <td>64.96</td>
      <td>4.93</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>55.0</td>
      <td>True</td>
      <td>f666a1e43500643cb3ff8c988a6ea5b56afe934a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>tiiuae/falcon-7b-instruct</td>
      <td>43.26</td>
      <td>46.16</td>
      <td>70.85</td>
      <td>25.84</td>
      <td>44.08</td>
      <td>67.96</td>
      <td>4.70</td>
      <td>instruction-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>807.0</td>
      <td>True</td>
      <td>cf4b3c42ce2fdfe24f753f0f0d179202fea59c99</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>JosephusCheung/Guanaco</td>
      <td>43.25</td>
      <td>50.17</td>
      <td>72.69</td>
      <td>30.30</td>
      <td>37.64</td>
      <td>68.67</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>0.00</td>
      <td>219.0</td>
      <td>True</td>
      <td>bed6f3bd18f07a4a379525645cbd86d622b12836</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>l3utterfly/minima-3b-layla-v1</td>
      <td>43.21</td>
      <td>42.32</td>
      <td>67.48</td>
      <td>28.44</td>
      <td>46.46</td>
      <td>65.90</td>
      <td>8.64</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>3.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>844bfa44b1b3cdd1c0e39c13fbb2fdaee82ff874</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>tiiuae/falcon-7b-instruct</td>
      <td>43.16</td>
      <td>45.82</td>
      <td>70.78</td>
      <td>25.66</td>
      <td>44.07</td>
      <td>68.03</td>
      <td>4.62</td>
      <td>instruction-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>807.0</td>
      <td>True</td>
      <td>eb410fb6ffa9028e97adb801f0d6ec46d02f8b07</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ziqingyang/chinese-llama-2-7b</td>
      <td>43.14</td>
      <td>44.45</td>
      <td>69.50</td>
      <td>37.47</td>
      <td>37.00</td>
      <td>68.98</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>557b5cbd48a4a4eb5a08e975c4b6e11ac1ed4cbc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/GPT-JT-6B-v1</td>
      <td>43.13</td>
      <td>40.87</td>
      <td>67.15</td>
      <td>47.19</td>
      <td>37.07</td>
      <td>65.27</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>304.0</td>
      <td>True</td>
      <td>f34aa35f906895602c1f86f5685e598afdea8051</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>u-chom/ex-llm-e1</td>
      <td>43.11</td>
      <td>39.93</td>
      <td>68.11</td>
      <td>39.44</td>
      <td>42.01</td>
      <td>64.88</td>
      <td>4.32</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5838bea0ad7153520a0a105fb81c5b895820f710</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>FreedomIntelligence/phoenix-inst-chat-7b</td>
      <td>43.03</td>
      <td>44.71</td>
      <td>63.23</td>
      <td>39.06</td>
      <td>47.08</td>
      <td>62.83</td>
      <td>1.29</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>43.0</td>
      <td>True</td>
      <td>5ed4d9570e0f76e1becb05bf467a7b4ff7b66055</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/GPT-NeoXT-Chat-Base-20B</td>
      <td>43.02</td>
      <td>45.65</td>
      <td>74.03</td>
      <td>29.92</td>
      <td>34.51</td>
      <td>67.09</td>
      <td>6.90</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>692.0</td>
      <td>True</td>
      <td>d386708e84d862a65f7d2b4989f64750cb657227</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GeorgiaTechResearchInstitute/galpaca-30b</td>
      <td>43.00</td>
      <td>49.57</td>
      <td>58.20</td>
      <td>43.78</td>
      <td>41.16</td>
      <td>62.51</td>
      <td>2.81</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>30.00</td>
      <td>55.0</td>
      <td>True</td>
      <td>a1f0c4bedd65b485a0d4d3a3bd60d7a4599f1eaf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TheBloke/CodeLlama-34B-Instruct-fp16</td>
      <td>43.00</td>
      <td>40.78</td>
      <td>35.66</td>
      <td>39.72</td>
      <td>44.29</td>
      <td>74.51</td>
      <td>23.05</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>7.0</td>
      <td>True</td>
      <td>a4d0ce949de4d5b5f74691641efb5b70736a32a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lyogavin/Anima-7B-100K</td>
      <td>42.98</td>
      <td>46.59</td>
      <td>72.28</td>
      <td>33.40</td>
      <td>37.84</td>
      <td>67.09</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>25.0</td>
      <td>True</td>
      <td>e303cf09e553c38ca5e0c0816d83631801ca5776</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Deita-1_8B</td>
      <td>42.96</td>
      <td>36.52</td>
      <td>60.63</td>
      <td>45.62</td>
      <td>40.02</td>
      <td>59.35</td>
      <td>15.62</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>8.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7709179d3919f48660b0bf58e5efcca2c45e2659</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Writer/InstructPalmyra-20b</td>
      <td>42.91</td>
      <td>47.10</td>
      <td>73.00</td>
      <td>28.26</td>
      <td>41.81</td>
      <td>64.72</td>
      <td>2.58</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>37.0</td>
      <td>True</td>
      <td>c78df447c70d4677b128b1df864b9fff8338d900</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vihangd/dopeyshearedplats-2.7b-v1</td>
      <td>42.90</td>
      <td>46.08</td>
      <td>75.17</td>
      <td>29.01</td>
      <td>44.12</td>
      <td>62.67</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>2.70</td>
      <td>2.0</td>
      <td>True</td>
      <td>c125218041c01662dc4c59b3f344aaa4e53dfd18</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/gpt-neox-20b-full-precision</td>
      <td>42.87</td>
      <td>48.81</td>
      <td>74.44</td>
      <td>26.16</td>
      <td>36.89</td>
      <td>68.27</td>
      <td>2.65</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>20.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>20b347273d90da7c2c9eb4c32d4173dba862a0d2</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Pierre-obi/Mistral_solar-slerp</td>
      <td>42.86</td>
      <td>43.00</td>
      <td>57.93</td>
      <td>40.48</td>
      <td>46.96</td>
      <td>68.19</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>874e9960000eb9abadc57755cc4251bcfe369302</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/landmark-attention-llama7b-fp16</td>
      <td>42.84</td>
      <td>47.35</td>
      <td>65.81</td>
      <td>31.59</td>
      <td>42.63</td>
      <td>68.03</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>bf8bdcb0c30cceb0ceda33cf5fde683807e39a58</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-66b</td>
      <td>42.78</td>
      <td>46.33</td>
      <td>76.25</td>
      <td>26.99</td>
      <td>35.43</td>
      <td>70.01</td>
      <td>1.67</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>66.00</td>
      <td>172.0</td>
      <td>True</td>
      <td>7259969061237fe940036d22bea0fd349e4485e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-agents/tora-code-13b-v1.0</td>
      <td>42.70</td>
      <td>44.45</td>
      <td>69.29</td>
      <td>36.67</td>
      <td>34.98</td>
      <td>62.59</td>
      <td>8.19</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>4bf5b528d95a507b435c24a8986afe80d5951782</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>VMware/open-llama-7b-open-instruct</td>
      <td>42.59</td>
      <td>49.74</td>
      <td>73.67</td>
      <td>31.52</td>
      <td>34.65</td>
      <td>65.43</td>
      <td>0.53</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-3.0</td>
      <td>7.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>fdf9f034163cce67e04d55172155f0e07b1b19a0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Salesforce/codegen-16B-nl</td>
      <td>42.59</td>
      <td>46.76</td>
      <td>71.87</td>
      <td>32.35</td>
      <td>33.95</td>
      <td>67.96</td>
      <td>2.65</td>
      <td>pretrained</td>
      <td>CodeGenForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bsd-3-clause</td>
      <td>16.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>b65951b0cf7c5639f73caea801a892788608ed69</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-1024-20b</td>
      <td>42.58</td>
      <td>48.04</td>
      <td>72.76</td>
      <td>25.96</td>
      <td>39.92</td>
      <td>66.30</td>
      <td>2.50</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>1a5b8d25587eab67d837621a6c9423e7ef6df289</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/oasst-gpt-neox-20b-1000-steps</td>
      <td>42.51</td>
      <td>48.55</td>
      <td>74.61</td>
      <td>26.39</td>
      <td>35.63</td>
      <td>66.77</td>
      <td>3.11</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>20.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4aec11ef19103796fb21387ce925b63c9d61dae1</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vibhorag101/llama-2-13b-chat-hf-phr_mental_therapy</td>
      <td>42.50</td>
      <td>38.82</td>
      <td>72.76</td>
      <td>23.12</td>
      <td>46.92</td>
      <td>65.59</td>
      <td>7.81</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>mit</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>0fe5a48f3d99492cb180fc6efda5b138677ca1de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-oasst1-512-20b</td>
      <td>42.44</td>
      <td>46.93</td>
      <td>72.77</td>
      <td>26.25</td>
      <td>37.50</td>
      <td>68.03</td>
      <td>3.18</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>38.0</td>
      <td>True</td>
      <td>3bdf6f870ca14bcc5587b666fbe57488f7854d30</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Zangs3011/codellama_7b_DolphinCoder</td>
      <td>42.39</td>
      <td>41.98</td>
      <td>65.50</td>
      <td>38.11</td>
      <td>35.45</td>
      <td>63.61</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>600d70148047ad1ec7cb99a596dfeb8ba6a2c42c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qblocks/codellama_7b_DolphinCoder</td>
      <td>42.39</td>
      <td>41.98</td>
      <td>65.50</td>
      <td>38.11</td>
      <td>35.45</td>
      <td>63.61</td>
      <td>9.70</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7a0aaba040ae0b122737172db4581f2d0b1064bf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>JosephusCheung/LL7M</td>
      <td>42.38</td>
      <td>44.97</td>
      <td>68.81</td>
      <td>34.44</td>
      <td>41.39</td>
      <td>64.09</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>0.01</td>
      <td>35.0</td>
      <td>True</td>
      <td>9b31bbf38a43d41eaf166fb3573f706b23cb1c13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/RedPajama-INCITE-7B-Instruct</td>
      <td>42.38</td>
      <td>44.11</td>
      <td>72.02</td>
      <td>37.62</td>
      <td>33.96</td>
      <td>64.96</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>105.0</td>
      <td>True</td>
      <td>95667a602ff2646bf67fe3a57c4eb9a1edec87fe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1</td>
      <td>42.38</td>
      <td>44.11</td>
      <td>72.02</td>
      <td>37.62</td>
      <td>33.96</td>
      <td>64.96</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>95667a602ff2646bf67fe3a57c4eb9a1edec87fe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>openlm-research/open_llama_7b</td>
      <td>42.31</td>
      <td>47.01</td>
      <td>71.98</td>
      <td>30.49</td>
      <td>34.85</td>
      <td>67.96</td>
      <td>1.59</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>115.0</td>
      <td>True</td>
      <td>6fb184ff23774c25bf84b3628e49c8b78372c7be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>occultml/Helios-10.7B-v2</td>
      <td>42.25</td>
      <td>39.16</td>
      <td>46.63</td>
      <td>41.57</td>
      <td>55.51</td>
      <td>70.64</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>79b8aaa82a404ee79cbd724213d3c85910e4dec2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cmarkea/bloomz-7b1-mt-sft-chat</td>
      <td>42.24</td>
      <td>44.03</td>
      <td>62.60</td>
      <td>38.64</td>
      <td>44.34</td>
      <td>63.30</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.07</td>
      <td>12.0</td>
      <td>True</td>
      <td>8c2dc302780fe320ee3428f3db2ee7ff3684dcef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Galpaca-30b-MiniOrca</td>
      <td>42.23</td>
      <td>48.89</td>
      <td>57.80</td>
      <td>43.72</td>
      <td>41.10</td>
      <td>60.06</td>
      <td>1.82</td>
      <td>instruction-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>29.97</td>
      <td>1.0</td>
      <td>True</td>
      <td>681d92f8f71ca3e8425da19afee89ed84baedf1d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/pythia-12b-sft-v8-7k-steps</td>
      <td>42.21</td>
      <td>44.03</td>
      <td>70.28</td>
      <td>26.55</td>
      <td>36.53</td>
      <td>65.27</td>
      <td>10.61</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>275c9b71bfab4e271d1ed85515c61e317b6ef65e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloomz-7b1</td>
      <td>42.21</td>
      <td>42.49</td>
      <td>63.01</td>
      <td>37.85</td>
      <td>45.20</td>
      <td>64.64</td>
      <td>0.08</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.00</td>
      <td>117.0</td>
      <td>True</td>
      <td>2f4c4f3ebcf171dbbe2bae989ea2d2f3d3486a97</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>klosax/open_llama_13b_600bt_preview</td>
      <td>42.21</td>
      <td>44.28</td>
      <td>72.43</td>
      <td>31.47</td>
      <td>34.66</td>
      <td>68.43</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>3465eaca4d293ccc6ce66888e6c8bd9032ae7071</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>occultml/Helios-10.7B</td>
      <td>42.19</td>
      <td>38.91</td>
      <td>46.60</td>
      <td>41.40</td>
      <td>55.52</td>
      <td>70.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7f6e3c76304241500e010979e243d712a0dedb67</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TehVenom/Moderator-Chan_GPT-JT-6b</td>
      <td>42.17</td>
      <td>43.69</td>
      <td>70.77</td>
      <td>35.61</td>
      <td>36.05</td>
      <td>65.59</td>
      <td>1.29</td>
      <td></td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f2b7cda25f6965c1551fa78e9e38676994bc6638</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloomz-7b1-mt</td>
      <td>42.14</td>
      <td>43.86</td>
      <td>62.91</td>
      <td>37.35</td>
      <td>45.65</td>
      <td>63.06</td>
      <td>0.00</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.00</td>
      <td>125.0</td>
      <td>True</td>
      <td>76875e6ea8df98157fb032c48ad6e354fd6a077b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Heng666/EastAsia-4x7B-Moe-experiment</td>
      <td>42.12</td>
      <td>39.51</td>
      <td>48.92</td>
      <td>56.20</td>
      <td>49.83</td>
      <td>58.09</td>
      <td>0.15</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>18.52</td>
      <td>0.0</td>
      <td>True</td>
      <td>44d2f9bfc6538102d101054d2366cb389fb713d9</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT</td>
      <td>42.11</td>
      <td>41.04</td>
      <td>71.26</td>
      <td>28.50</td>
      <td>47.71</td>
      <td>64.17</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.70</td>
      <td>5.0</td>
      <td>True</td>
      <td>802be8903ec44f49a883915882868b479ecdcc3b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Writer/palmyra-large</td>
      <td>42.09</td>
      <td>44.97</td>
      <td>71.85</td>
      <td>28.54</td>
      <td>35.93</td>
      <td>67.88</td>
      <td>3.41</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>20.0</td>
      <td>True</td>
      <td>40086d791942cb28f55e679cd3fb6f6b5ba4effd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-raven-14b</td>
      <td>42.09</td>
      <td>44.62</td>
      <td>71.25</td>
      <td>25.92</td>
      <td>41.93</td>
      <td>66.69</td>
      <td>2.12</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>14.00</td>
      <td>51.0</td>
      <td>False</td>
      <td>359c0649b4f1d10a26ebea32908035bc00d152ee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AlekseyKorshuk/pygmalion-6b-vicuna-chatml</td>
      <td>42.08</td>
      <td>40.61</td>
      <td>67.73</td>
      <td>33.92</td>
      <td>42.76</td>
      <td>63.06</td>
      <td>4.40</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>6.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>ee3ada91a69a194cedfabbfeab98f1499b75cb44</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/Marx-3B-V2</td>
      <td>42.08</td>
      <td>44.03</td>
      <td>72.92</td>
      <td>27.84</td>
      <td>39.92</td>
      <td>66.54</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>25.0</td>
      <td>True</td>
      <td>5fba568304f6f876f5b9e42026f986ea245b836b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NurtureAI/Orca-2-7B-16k</td>
      <td>42.05</td>
      <td>50.60</td>
      <td>63.89</td>
      <td>36.68</td>
      <td>45.37</td>
      <td>54.22</td>
      <td>1.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>4.0</td>
      <td>False</td>
      <td>ab373033e98dcdbcc3aadb51374ae392656c6603</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-tora-code-7b-v1.0</td>
      <td>42.04</td>
      <td>42.66</td>
      <td>65.16</td>
      <td>38.56</td>
      <td>42.06</td>
      <td>62.90</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>f7b1f87a096045f1bba8f68c62e062102218717b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mediocredev/open-llama-3b-v2-instruct</td>
      <td>42.02</td>
      <td>38.48</td>
      <td>70.24</td>
      <td>39.69</td>
      <td>37.96</td>
      <td>65.75</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>4d50e134af1d9806cbdf6bc90795b44ae689deca</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-30b</td>
      <td>42.00</td>
      <td>43.26</td>
      <td>74.07</td>
      <td>26.66</td>
      <td>35.16</td>
      <td>70.64</td>
      <td>2.20</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>132.0</td>
      <td>True</td>
      <td>ceea0a90ac0f6fae7c2c34bcb40477438c152546</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ed001/datascience-coder-6.7b</td>
      <td>41.99</td>
      <td>34.64</td>
      <td>53.83</td>
      <td>37.96</td>
      <td>44.82</td>
      <td>55.72</td>
      <td>24.94</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>02c9e23ecc8d0fdcd84db006ecb608344907c5e1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/oasst-gpt-neox-20b-3000-steps</td>
      <td>41.97</td>
      <td>46.42</td>
      <td>72.08</td>
      <td>26.16</td>
      <td>35.53</td>
      <td>68.75</td>
      <td>2.88</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>20.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f0462a8b7908f61202d86e6a9a2996d8339363b5</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/pythia-12b-sft-v8-2.5k-steps</td>
      <td>41.97</td>
      <td>42.32</td>
      <td>70.15</td>
      <td>27.36</td>
      <td>36.75</td>
      <td>65.67</td>
      <td>9.55</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>142e306db8e279a07c557ea5a919ab7e7a4af17c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-gm-oasst1-multilang-1024-20b</td>
      <td>41.90</td>
      <td>47.44</td>
      <td>72.58</td>
      <td>26.37</td>
      <td>34.39</td>
      <td>68.43</td>
      <td>2.20</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>b3a6bf4250a037c09e451344e2a4e987011b79de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>wenge-research/yayi-7b</td>
      <td>41.88</td>
      <td>46.33</td>
      <td>61.72</td>
      <td>36.34</td>
      <td>43.70</td>
      <td>62.27</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>29.0</td>
      <td>False</td>
      <td>00be6c9e41a8367a855c6f18ebfa08f5ecdb2cc4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>togethercomputer/GPT-JT-Moderation-6B</td>
      <td>41.80</td>
      <td>40.53</td>
      <td>67.66</td>
      <td>41.63</td>
      <td>37.33</td>
      <td>62.67</td>
      <td>0.99</td>
      <td></td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>31.0</td>
      <td>True</td>
      <td>1297870783f6091294769014afddf94499966a78</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/LongAlpaca-13B</td>
      <td>41.74</td>
      <td>42.58</td>
      <td>72.03</td>
      <td>34.91</td>
      <td>36.85</td>
      <td>64.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>6.0</td>
      <td>False</td>
      <td>e80966ae720de9a844441a4a2bbc661106969915</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Danielbrdz/Barcenas-3b</td>
      <td>41.74</td>
      <td>43.17</td>
      <td>67.82</td>
      <td>29.16</td>
      <td>41.56</td>
      <td>66.22</td>
      <td>2.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>3.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>2b6b8bfd3946c02fa4a5182ed008df8ad324a406</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>AI-Sweden-Models/gpt-sw3-6.7b-v2-instruct</td>
      <td>41.72</td>
      <td>40.78</td>
      <td>67.77</td>
      <td>31.57</td>
      <td>40.32</td>
      <td>63.54</td>
      <td>6.37</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>7.11</td>
      <td>7.0</td>
      <td>True</td>
      <td>81ca95a4e93746240994d1e6797ffa64dc796bd9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/Marx-3B</td>
      <td>41.71</td>
      <td>43.17</td>
      <td>72.68</td>
      <td>28.46</td>
      <td>39.09</td>
      <td>65.59</td>
      <td>1.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>12.0</td>
      <td>True</td>
      <td>c0dcc44989cf4e006efae31abbcef7e8be8547c0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/gpt-neox-20b</td>
      <td>41.69</td>
      <td>45.73</td>
      <td>73.45</td>
      <td>25.00</td>
      <td>31.61</td>
      <td>68.90</td>
      <td>5.46</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.74</td>
      <td>468.0</td>
      <td>True</td>
      <td>9369f145ca7b66ef62760f9351af951b2d53b77f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>OpenAssistant/pythia-12b-sft-v8-rlhf-2k-steps</td>
      <td>41.65</td>
      <td>43.43</td>
      <td>70.08</td>
      <td>26.12</td>
      <td>36.06</td>
      <td>64.64</td>
      <td>9.55</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a0debfed4a020d449e3d00f4e75f2c2aefb68db3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vihangd/shearedplats-2.7b-v2</td>
      <td>41.61</td>
      <td>42.41</td>
      <td>72.58</td>
      <td>27.52</td>
      <td>39.76</td>
      <td>65.90</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>2.70</td>
      <td>3.0</td>
      <td>True</td>
      <td>2837296f28d6aa0fb6c1fe382f553e65c8e1e5f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>teilomillet/MiniMerlin-3b-v0.1</td>
      <td>41.60</td>
      <td>40.70</td>
      <td>54.06</td>
      <td>43.32</td>
      <td>49.65</td>
      <td>60.54</td>
      <td>1.36</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2addcbd985f8a7f8bb7a7c21a5ec0e2505e549c6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>glaiveai/glaive-coder-7b</td>
      <td>41.56</td>
      <td>42.66</td>
      <td>64.69</td>
      <td>37.15</td>
      <td>39.88</td>
      <td>59.75</td>
      <td>5.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>50.0</td>
      <td>True</td>
      <td>72a255a58480ef0713eed988312fe82f77f94f37</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>togethercomputer/RedPajama-INCITE-7B-Base</td>
      <td>41.49</td>
      <td>46.25</td>
      <td>71.63</td>
      <td>27.68</td>
      <td>33.03</td>
      <td>67.32</td>
      <td>3.03</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>90.0</td>
      <td>True</td>
      <td>78f7e482443971f4873ba3239f0ac810a367833b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>nomic-ai/gpt4all-j</td>
      <td>41.49</td>
      <td>41.98</td>
      <td>64.06</td>
      <td>28.20</td>
      <td>42.78</td>
      <td>64.72</td>
      <td>7.20</td>
      <td></td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>276.0</td>
      <td>True</td>
      <td>73c15208cb608be2949b7c6e4ba6d88f0176c267</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/oasst-pythia-12b-pretrained-sft</td>
      <td>41.48</td>
      <td>45.31</td>
      <td>67.67</td>
      <td>27.81</td>
      <td>38.16</td>
      <td>65.90</td>
      <td>4.02</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>12.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c21fbece4253841f2d6e15f04f60fe1ba6f990dd</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
      <td>41.46</td>
      <td>41.81</td>
      <td>73.01</td>
      <td>26.36</td>
      <td>38.99</td>
      <td>66.69</td>
      <td>1.90</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>5.0</td>
      <td>True</td>
      <td>4da0c661e6df1235c9997b996c8e395b87248406</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>GeneZC/MiniMA-3B</td>
      <td>41.44</td>
      <td>43.43</td>
      <td>68.06</td>
      <td>28.69</td>
      <td>39.76</td>
      <td>65.98</td>
      <td>2.73</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.02</td>
      <td>18.0</td>
      <td>True</td>
      <td>0a2f9d6bbb3959d68fe52e07ee6f54e8242f91ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>harborwater/open-llama-3b-everything-v2</td>
      <td>41.41</td>
      <td>42.83</td>
      <td>73.28</td>
      <td>26.87</td>
      <td>37.26</td>
      <td>66.61</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>31ce2c1611d9f7d56184ceb5bff6a7e95a180c03</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Fredithefish/ReasonixPajama-3B-HF</td>
      <td>41.41</td>
      <td>39.25</td>
      <td>63.47</td>
      <td>26.09</td>
      <td>55.42</td>
      <td>63.69</td>
      <td>0.53</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.91</td>
      <td>3.0</td>
      <td>False</td>
      <td>fa87c904b5921231b9f6f94b9c537cdda8783b96</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hakurei/mommygpt-3B</td>
      <td>41.36</td>
      <td>41.89</td>
      <td>71.69</td>
      <td>28.74</td>
      <td>37.90</td>
      <td>65.82</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>7.0</td>
      <td>True</td>
      <td>0369335d693b753774050ae44dbaf73bac39e9eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>psmathur/orca_mini_13b</td>
      <td>41.36</td>
      <td>42.06</td>
      <td>63.40</td>
      <td>35.43</td>
      <td>43.10</td>
      <td>64.17</td>
      <td>0.00</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ca900c8f3145de40cd188c559b2901a2e4711546</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>NucleusAI/nucleus-22B-token-500B</td>
      <td>41.33</td>
      <td>40.70</td>
      <td>69.39</td>
      <td>30.11</td>
      <td>39.16</td>
      <td>67.64</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>21.83</td>
      <td>24.0</td>
      <td>True</td>
      <td>49bb1a47c0d32b4bfa6630a4eff04a857adcd4ca</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>chargoddard/llama-2-34b-uncode</td>
      <td>41.33</td>
      <td>39.51</td>
      <td>33.90</td>
      <td>38.49</td>
      <td>40.94</td>
      <td>74.35</td>
      <td>20.77</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>3.0</td>
      <td>True</td>
      <td>d434d06249feb6ca511b0a09162130bcc59d84e3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</td>
      <td>41.31</td>
      <td>45.73</td>
      <td>68.59</td>
      <td>26.82</td>
      <td>37.81</td>
      <td>65.90</td>
      <td>3.03</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>346.0</td>
      <td>True</td>
      <td>626b8c140cfdedb119dfb78c626cd772283dee33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_7b</td>
      <td>41.27</td>
      <td>43.94</td>
      <td>65.22</td>
      <td>29.97</td>
      <td>42.03</td>
      <td>66.06</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ed0dca683685cb5b9e7df599f87d311f00ba6db</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/GPT-NeoX-20B-Erebus</td>
      <td>41.26</td>
      <td>45.48</td>
      <td>72.79</td>
      <td>26.77</td>
      <td>32.15</td>
      <td>68.11</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>74.0</td>
      <td>True</td>
      <td>1a80940a290452af71caf17a8e520955eb338e0f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>togethercomputer/RedPajama-INCITE-Base-7B-v0.1</td>
      <td>41.25</td>
      <td>46.25</td>
      <td>71.63</td>
      <td>27.68</td>
      <td>33.03</td>
      <td>67.32</td>
      <td>1.59</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>78f7e482443971f4873ba3239f0ac810a367833b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CobraMamba/mamba-gpt-3b-v4</td>
      <td>41.24</td>
      <td>42.58</td>
      <td>71.04</td>
      <td>30.04</td>
      <td>37.26</td>
      <td>65.82</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>49cdf710c1a9178ddf616da79211fdcdb2170c3f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aloobun/open-llama-3b-v2-elmv3</td>
      <td>41.14</td>
      <td>42.06</td>
      <td>73.28</td>
      <td>27.61</td>
      <td>35.54</td>
      <td>64.96</td>
      <td>3.41</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>7e43b199ff51dc0e63934ba49758a8a31ff855de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/Griffin-3B</td>
      <td>41.13</td>
      <td>41.81</td>
      <td>72.30</td>
      <td>26.36</td>
      <td>38.33</td>
      <td>67.01</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>edbea6fe86d0bc2673c10269828008a1cb451919</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mwitiderrick/shearedplats-2.7b-v2-instruct-v0.1</td>
      <td>41.13</td>
      <td>40.19</td>
      <td>70.08</td>
      <td>28.12</td>
      <td>41.23</td>
      <td>65.04</td>
      <td>2.12</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.70</td>
      <td>2.0</td>
      <td>True</td>
      <td>8eb300dc6a62166048f7ec997a0a2d8d9a5708f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aloobun/open-llama-3b-v2-elmv3</td>
      <td>41.13</td>
      <td>42.15</td>
      <td>73.26</td>
      <td>27.16</td>
      <td>35.51</td>
      <td>64.96</td>
      <td>3.71</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>7e43b199ff51dc0e63934ba49758a8a31ff855de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-coder-ds-6.7b</td>
      <td>41.11</td>
      <td>36.86</td>
      <td>52.46</td>
      <td>38.08</td>
      <td>41.67</td>
      <td>58.88</td>
      <td>18.73</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.70</td>
      <td>2.0</td>
      <td>True</td>
      <td>808ce4ef532c91bcbf826cbdc29ec5094cbd1769</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>VMware/open-llama-0.7T-7B-open-instruct-v1.1</td>
      <td>41.11</td>
      <td>46.67</td>
      <td>67.67</td>
      <td>28.55</td>
      <td>37.60</td>
      <td>65.43</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>75741b55ad462330e3498d1506f438f835152177</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CobraMamba/mamba-gpt-3b-v3</td>
      <td>41.11</td>
      <td>41.72</td>
      <td>71.05</td>
      <td>27.31</td>
      <td>37.86</td>
      <td>67.48</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>14.0</td>
      <td>True</td>
      <td>d860a90ef6b30c695b985dd2ff382d4bbb80e857</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/pythia-12b-pre-v8-12.5k-steps</td>
      <td>41.10</td>
      <td>41.47</td>
      <td>68.80</td>
      <td>26.58</td>
      <td>36.82</td>
      <td>65.27</td>
      <td>7.66</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>37ca702e957a4b740689d67c58c284224e2fbae2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/GPT-NeoX-20B-Skein</td>
      <td>41.10</td>
      <td>44.97</td>
      <td>72.68</td>
      <td>25.99</td>
      <td>31.64</td>
      <td>68.43</td>
      <td>2.88</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>dd98d514b5aff4e820922c88a73d6d5bf17f332e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
      <td>41.09</td>
      <td>41.21</td>
      <td>72.88</td>
      <td>25.39</td>
      <td>38.87</td>
      <td>66.61</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>5.0</td>
      <td>True</td>
      <td>4da0c661e6df1235c9997b996c8e395b87248406</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xaviviro/FLAMA-0.1-3B</td>
      <td>41.07</td>
      <td>41.72</td>
      <td>71.41</td>
      <td>26.59</td>
      <td>37.19</td>
      <td>66.54</td>
      <td>2.96</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>380f8c1a59a0e60e704b22720af1494801b57e85</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>RobbeD/OpenLlama-Platypus-3B</td>
      <td>41.05</td>
      <td>41.21</td>
      <td>71.67</td>
      <td>29.86</td>
      <td>36.45</td>
      <td>65.98</td>
      <td>1.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>3.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>d3a0bf8e1181be02cc9c4c4cdfedaedacaefbfac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/Puma-3B</td>
      <td>41.02</td>
      <td>41.30</td>
      <td>71.85</td>
      <td>27.51</td>
      <td>38.34</td>
      <td>66.38</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>1159e9cdd05c03d31331f329ba58e4e3444943be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>harborwater/wizard-orca-3b</td>
      <td>41.00</td>
      <td>41.72</td>
      <td>71.78</td>
      <td>24.49</td>
      <td>40.04</td>
      <td>66.93</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>ffc81b58375342f12e38a67272d95458a72e8d09</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>LLM360/Amber</td>
      <td>40.97</td>
      <td>40.96</td>
      <td>73.79</td>
      <td>26.84</td>
      <td>33.56</td>
      <td>67.88</td>
      <td>2.81</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>53.0</td>
      <td>True</td>
      <td>a1fb934dd7bbba8eff8c6052fa469f979803236b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>harborwater/open-llama-3b-claude-30k</td>
      <td>40.93</td>
      <td>41.72</td>
      <td>72.64</td>
      <td>24.03</td>
      <td>38.46</td>
      <td>66.54</td>
      <td>2.20</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>049db7fda44e5ce1e8febf5c3f45e3a93aaaa859</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mediocredev/open-llama-3b-v2-chat</td>
      <td>40.93</td>
      <td>40.61</td>
      <td>70.30</td>
      <td>28.73</td>
      <td>37.84</td>
      <td>65.51</td>
      <td>2.58</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>0d171b62a41b2d249cd2ff235b66638e3a894c98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>princeton-nlp/Sheared-LLaMA-2.7B</td>
      <td>40.84</td>
      <td>41.72</td>
      <td>71.01</td>
      <td>26.92</td>
      <td>37.32</td>
      <td>67.01</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.70</td>
      <td>49.0</td>
      <td>True</td>
      <td>16347024c4df6cd114720958964a850fc287cac0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/GPT-R</td>
      <td>40.80</td>
      <td>41.21</td>
      <td>66.89</td>
      <td>36.50</td>
      <td>34.22</td>
      <td>64.40</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>bigscience-openrail-m</td>
      <td>0.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>92b955a3ff74aa577fa0d8517dfc314847ef60af</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AtAndDev/ShortKing-3b-v0.3</td>
      <td>40.80</td>
      <td>40.96</td>
      <td>70.72</td>
      <td>26.21</td>
      <td>38.78</td>
      <td>66.93</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>4bcf1610eb1f3959568d5acee74833c41502bf04</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>dvruette/oasst-pythia-12b-6000-steps</td>
      <td>40.77</td>
      <td>45.39</td>
      <td>69.68</td>
      <td>25.97</td>
      <td>39.85</td>
      <td>63.22</td>
      <td>0.53</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>12.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e2ccc0ef8d1cc5ffc8b0e2e885f03ef50597ea8a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/oasst-sft-1-pythia-12b</td>
      <td>40.77</td>
      <td>46.42</td>
      <td>70.00</td>
      <td>26.19</td>
      <td>39.19</td>
      <td>62.19</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>279.0</td>
      <td>True</td>
      <td>293df535fe7711a5726987fc2f17dfc87de452a1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/gogpt-7b-bloom</td>
      <td>40.75</td>
      <td>44.62</td>
      <td>62.56</td>
      <td>33.81</td>
      <td>40.61</td>
      <td>62.90</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>8f9996f852db583b982efbd671465d18ad13ffae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hyunseoki/ko-ref-llama2-7b</td>
      <td>40.75</td>
      <td>42.66</td>
      <td>66.58</td>
      <td>30.41</td>
      <td>38.62</td>
      <td>66.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>1ee08c79ae7393473754b77e82b1472ef63d5dd2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>dvruette/oasst-pythia-12b-flash-attn-5000-steps</td>
      <td>40.73</td>
      <td>44.97</td>
      <td>69.75</td>
      <td>26.64</td>
      <td>38.89</td>
      <td>63.14</td>
      <td>0.99</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>12.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>5227ec9c9def4b0bdf6c7ad95d9f77cbf458283d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-20b</td>
      <td>40.71</td>
      <td>41.81</td>
      <td>68.75</td>
      <td>28.47</td>
      <td>37.10</td>
      <td>67.17</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>20.92</td>
      <td>1.0</td>
      <td>True</td>
      <td>36797b7835a9e656af456e0006465a3af48735fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AlekseyKorshuk/chatml-pyg-v1</td>
      <td>40.70</td>
      <td>37.88</td>
      <td>63.29</td>
      <td>32.77</td>
      <td>42.61</td>
      <td>62.51</td>
      <td>5.16</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>79d5a4d53953ca1c26bc2155f168b7e2108f377f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-1024-12b</td>
      <td>40.65</td>
      <td>43.09</td>
      <td>69.75</td>
      <td>25.87</td>
      <td>38.00</td>
      <td>66.14</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>e547fffafb382fd39ef5de35ba3b5afc1b43e74d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/fairseq-dense-13B</td>
      <td>40.62</td>
      <td>40.36</td>
      <td>75.51</td>
      <td>27.07</td>
      <td>32.83</td>
      <td>67.96</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>14.0</td>
      <td>False</td>
      <td>785793f6b216afd9fc664fc63e8e6c776a016825</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>harborwater/open-llama-3b-everythingLM-2048</td>
      <td>40.62</td>
      <td>42.75</td>
      <td>71.72</td>
      <td>27.16</td>
      <td>34.26</td>
      <td>66.30</td>
      <td>1.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>1f9e8d48163feb63ed190eaa982f393542a75d30</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Rallio67/7B-redpajama-conditional-alpha</td>
      <td>40.56</td>
      <td>42.58</td>
      <td>69.91</td>
      <td>26.53</td>
      <td>36.42</td>
      <td>67.17</td>
      <td>0.76</td>
      <td>instruction-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>7.0</td>
      <td>False</td>
      <td>9a3f69a1eba3618930f222d4e013d534102a2af5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Javalion-R</td>
      <td>40.51</td>
      <td>41.72</td>
      <td>68.02</td>
      <td>30.81</td>
      <td>34.44</td>
      <td>65.43</td>
      <td>2.65</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>b881231ab6ea85da2a9a139f282df85d1d18b002</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-oasst1-512-12b</td>
      <td>40.48</td>
      <td>42.32</td>
      <td>70.24</td>
      <td>26.01</td>
      <td>36.41</td>
      <td>66.22</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>26.0</td>
      <td>True</td>
      <td>c6bb0fe363e0105839d34ca757793b61c9606f95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Javelin-R</td>
      <td>40.39</td>
      <td>41.64</td>
      <td>69.01</td>
      <td>30.70</td>
      <td>34.50</td>
      <td>64.80</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>4c4a5caf5d9049a47f5565b72e5a53dede08ac8b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>dvruette/oasst-pythia-12b-reference</td>
      <td>40.33</td>
      <td>43.00</td>
      <td>67.91</td>
      <td>28.33</td>
      <td>36.57</td>
      <td>64.96</td>
      <td>1.21</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>12.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c5a9b7fad884e6c45ce5d2ca551aa1c03db6865f</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WizardLM/WizardCoder-Python-7B-V1.0</td>
      <td>40.32</td>
      <td>41.81</td>
      <td>65.06</td>
      <td>32.29</td>
      <td>36.32</td>
      <td>61.72</td>
      <td>4.70</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>62.0</td>
      <td>True</td>
      <td>e40673a27a4aefcff2c6d2b3b1e0681a38703e4e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Pirr/pythia-13b-deduped-green_devil</td>
      <td>40.31</td>
      <td>42.32</td>
      <td>68.89</td>
      <td>26.01</td>
      <td>35.56</td>
      <td>66.93</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>7faeb395c26189eeab9bf3a98994696687ad31a3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vihangd/smartyplats-3b-v2</td>
      <td>40.29</td>
      <td>41.04</td>
      <td>71.19</td>
      <td>24.32</td>
      <td>36.66</td>
      <td>66.93</td>
      <td>1.59</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>920609897049f674bc4a9678579f6869f6cbed13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/openllama_3b_EvolInstruct_lora_merged</td>
      <td>40.28</td>
      <td>40.27</td>
      <td>71.60</td>
      <td>27.12</td>
      <td>34.78</td>
      <td>67.01</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-4.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>c55e3e114951346f273c519d266170e4d52781e9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>openlm-research/open_llama_3b_v2</td>
      <td>40.28</td>
      <td>40.27</td>
      <td>71.60</td>
      <td>27.12</td>
      <td>34.78</td>
      <td>67.01</td>
      <td>0.91</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>101.0</td>
      <td>True</td>
      <td>bce5d60d3b0c68318862270ec4e794d83308d80a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kfkas/Llama-2-ko-7b-Chat</td>
      <td>40.27</td>
      <td>40.44</td>
      <td>67.16</td>
      <td>30.40</td>
      <td>35.48</td>
      <td>66.85</td>
      <td>1.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>62.0</td>
      <td>False</td>
      <td>3293b98cd8204371988f898dafa9b5a297555cbe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>TheBloke/CodeLlama-34B-Python-fp16</td>
      <td>40.27</td>
      <td>38.14</td>
      <td>34.80</td>
      <td>32.95</td>
      <td>43.57</td>
      <td>72.14</td>
      <td>20.02</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>12.0</td>
      <td>True</td>
      <td>875f9d97fb6c9619d8867887dd1d80918ff0f593</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-34b-Python-hf</td>
      <td>40.27</td>
      <td>40.19</td>
      <td>36.82</td>
      <td>34.79</td>
      <td>44.28</td>
      <td>71.19</td>
      <td>14.33</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>80.0</td>
      <td>True</td>
      <td>3dd8ab05bbd273b9f77088b1d4015b7f1848793d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>l3utterfly/open-llama-3b-v2-layla</td>
      <td>40.25</td>
      <td>38.23</td>
      <td>66.43</td>
      <td>28.56</td>
      <td>44.40</td>
      <td>62.83</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>465669ddafad25393ac3cfe94d3726cced112b30</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kfkas/Llama-2-ko-7b-Chat</td>
      <td>40.25</td>
      <td>40.44</td>
      <td>67.12</td>
      <td>30.19</td>
      <td>35.45</td>
      <td>66.61</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>62.0</td>
      <td>False</td>
      <td>3293b98cd8204371988f898dafa9b5a297555cbe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Javelin-GPTJ</td>
      <td>40.23</td>
      <td>42.66</td>
      <td>70.45</td>
      <td>26.20</td>
      <td>36.08</td>
      <td>64.17</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>bee7068ab002784420a1a30170db3906185359f2</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-agents/tora-code-7b-v1.0</td>
      <td>40.21</td>
      <td>40.70</td>
      <td>65.86</td>
      <td>33.34</td>
      <td>34.84</td>
      <td>61.56</td>
      <td>4.93</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>777501b69bb0ba2675abdcaf7b1309ab05320c2e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Janin-R</td>
      <td>40.19</td>
      <td>40.44</td>
      <td>67.36</td>
      <td>31.24</td>
      <td>34.49</td>
      <td>65.35</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>f6963f77098d8421ff4a1cf4d36f1e94c6c8f44b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/Bean-3B</td>
      <td>40.18</td>
      <td>40.36</td>
      <td>72.00</td>
      <td>26.43</td>
      <td>36.11</td>
      <td>65.67</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>4a1ce189a3fb1d58b3fa47ebe30b3c037592670c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TaylorAI/Flash-Llama-3B</td>
      <td>40.13</td>
      <td>40.10</td>
      <td>71.56</td>
      <td>26.88</td>
      <td>34.74</td>
      <td>66.61</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>b4c7bb49171ff6955cfc1f7e33143383c57f7606</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/Dolly_Shygmalion-6b-Dev_V8P2</td>
      <td>40.11</td>
      <td>41.38</td>
      <td>67.67</td>
      <td>28.48</td>
      <td>36.86</td>
      <td>64.33</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>6413b1d9e8b58df9d3aac91a862e8d505d8c6716</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/gpt-j-6b</td>
      <td>40.10</td>
      <td>41.38</td>
      <td>67.54</td>
      <td>26.78</td>
      <td>35.96</td>
      <td>65.98</td>
      <td>2.96</td>
      <td>pretrained</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>1335.0</td>
      <td>True</td>
      <td>47e169305d2e8376be1d31e765533382721b2cc1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Xilabs/calypso-3b-alpha-v2</td>
      <td>40.09</td>
      <td>41.55</td>
      <td>71.48</td>
      <td>25.82</td>
      <td>35.73</td>
      <td>65.27</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>3.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>933fb9db10f131f7ea54f4e6024ed2acf41c711a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Danielbrdz/CodeBarcenas-7b</td>
      <td>40.09</td>
      <td>42.32</td>
      <td>63.43</td>
      <td>33.39</td>
      <td>38.51</td>
      <td>60.38</td>
      <td>2.50</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>fe7a232baac5394e821f349cb7ef31dbd4ca2078</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/CodeLlama-34b-hf</td>
      <td>40.08</td>
      <td>37.54</td>
      <td>31.84</td>
      <td>37.20</td>
      <td>38.89</td>
      <td>73.40</td>
      <td>21.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>34.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>4e61ec70eb258047f5bc689fa6a66f7753da52b8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-13b</td>
      <td>40.06</td>
      <td>39.93</td>
      <td>71.20</td>
      <td>24.90</td>
      <td>34.10</td>
      <td>68.51</td>
      <td>1.74</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>61.0</td>
      <td>True</td>
      <td>e515202d1e7750da62d245fbccb2723b9c1790f5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-7b-Instruct-hf</td>
      <td>40.05</td>
      <td>36.52</td>
      <td>55.44</td>
      <td>34.54</td>
      <td>41.25</td>
      <td>64.56</td>
      <td>7.96</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>130.0</td>
      <td>True</td>
      <td>7affc442e639b8aa1c4b3e98a10a2f45a21b8b4f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vihangd/DopeyTinyLlama-1.1B-v1</td>
      <td>40.04</td>
      <td>38.40</td>
      <td>63.49</td>
      <td>25.76</td>
      <td>37.36</td>
      <td>73.40</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>34b3b15e9c37be1a89745f06904c1e89ef98d417</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/GPT-J-6B-Skein</td>
      <td>40.02</td>
      <td>42.58</td>
      <td>68.69</td>
      <td>24.88</td>
      <td>38.70</td>
      <td>63.85</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.00</td>
      <td>13.0</td>
      <td>False</td>
      <td>acfe27303f74129930fef5e6fadbc5f58c6b8590</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vihangd/smartyplats-3b-v1</td>
      <td>40.00</td>
      <td>40.53</td>
      <td>70.85</td>
      <td>25.31</td>
      <td>36.53</td>
      <td>65.75</td>
      <td>1.06</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>89272b9edb323f5ace09e097a6449554c0dcd4e7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-tools-7b</td>
      <td>40.00</td>
      <td>38.91</td>
      <td>57.69</td>
      <td>33.24</td>
      <td>44.08</td>
      <td>58.56</td>
      <td>7.51</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>81aefc8983d1192378c2c803f0e0d14d48561117</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Salesforce/codegen-6B-nl</td>
      <td>40.00</td>
      <td>42.32</td>
      <td>68.59</td>
      <td>25.93</td>
      <td>34.47</td>
      <td>66.46</td>
      <td>2.20</td>
      <td>pretrained</td>
      <td>CodeGenForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bsd-3-clause</td>
      <td>6.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>dff91c0aea702edbea3528344d01d8b9aaee6e39</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>KnutJaegersberg/Nanbeige-16B-Base-32K-llama</td>
      <td>39.99</td>
      <td>47.61</td>
      <td>73.08</td>
      <td>45.26</td>
      <td>0.00</td>
      <td>72.93</td>
      <td>1.06</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>15.83</td>
      <td>0.0</td>
      <td>False</td>
      <td>50f59482965671226cfa7f9c107ab8904f1677cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Javalion-GPTJ</td>
      <td>39.97</td>
      <td>41.89</td>
      <td>68.69</td>
      <td>26.85</td>
      <td>35.44</td>
      <td>65.27</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>3ce176bc0f91cae416c78e99f964f54b12472de0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload</td>
      <td>39.95</td>
      <td>41.98</td>
      <td>66.82</td>
      <td>25.69</td>
      <td>39.67</td>
      <td>64.88</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>3.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>e471ec778771f29992293d1660cc108f29c9c69e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Kquant03/Raiden-16x3.43B</td>
      <td>39.93</td>
      <td>41.89</td>
      <td>66.20</td>
      <td>26.24</td>
      <td>39.18</td>
      <td>63.61</td>
      <td>2.43</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>35.78</td>
      <td>1.0</td>
      <td>True</td>
      <td>5403751a298b27603b25c28b1b003cf5f8dbe186</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-4-14b-pile</td>
      <td>39.92</td>
      <td>44.45</td>
      <td>71.07</td>
      <td>26.12</td>
      <td>32.04</td>
      <td>65.43</td>
      <td>0.38</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>14.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>4effb0fa9d15c2f383a1d159f4a40df0e09eb6d5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/WizardLM-30B-GPTQ</td>
      <td>39.90</td>
      <td>28.84</td>
      <td>26.08</td>
      <td>24.62</td>
      <td>49.14</td>
      <td>76.32</td>
      <td>34.42</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>20.0</td>
      <td>True</td>
      <td>e2e97475a9775d2fe7afba098aee37e694b9220f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt</td>
      <td>39.89</td>
      <td>41.30</td>
      <td>62.44</td>
      <td>27.55</td>
      <td>42.00</td>
      <td>64.56</td>
      <td>1.52</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>29604e6e19822531b0d49d3f19abef603a97d0ec</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/Dolly_Shygmalion-6b</td>
      <td>39.89</td>
      <td>41.89</td>
      <td>68.48</td>
      <td>27.58</td>
      <td>33.91</td>
      <td>65.35</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.00</td>
      <td>14.0</td>
      <td>False</td>
      <td>108fabf8a916900525492c294c50998d7c09f10b</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Skegma-GPTJ</td>
      <td>39.87</td>
      <td>43.77</td>
      <td>69.22</td>
      <td>25.37</td>
      <td>34.67</td>
      <td>64.64</td>
      <td>1.52</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>4dff006b2ea7e8d9b067dfe8af8ca1a16bc44dce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/PPO_Shygmalion-V8p4_Dev-6b</td>
      <td>39.85</td>
      <td>40.70</td>
      <td>67.04</td>
      <td>29.31</td>
      <td>35.57</td>
      <td>63.93</td>
      <td>2.58</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>fa3d503bca50c947e7a5bbde4bdd82f699f65c02</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/PPO_Pygway-V8p4_Dev-6b</td>
      <td>39.85</td>
      <td>40.36</td>
      <td>67.15</td>
      <td>29.30</td>
      <td>35.26</td>
      <td>64.40</td>
      <td>2.65</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>f30709dba36c665869f9ac8cd0cef5a8a2e7c8df</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/Pythia-Chat-Base-7B</td>
      <td>39.81</td>
      <td>40.02</td>
      <td>68.67</td>
      <td>27.44</td>
      <td>34.63</td>
      <td>64.01</td>
      <td>4.09</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>63.0</td>
      <td>True</td>
      <td>97aa918c383820e1a69f042801091d7deb996c20</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NousResearch/CodeLlama-7b-hf</td>
      <td>39.81</td>
      <td>39.85</td>
      <td>59.58</td>
      <td>30.47</td>
      <td>38.62</td>
      <td>64.88</td>
      <td>5.46</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>855c92912ea4a8eb5f0be1db4bf776ffd0815dac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>codellama/CodeLlama-7b-hf</td>
      <td>39.81</td>
      <td>39.93</td>
      <td>60.80</td>
      <td>31.12</td>
      <td>37.82</td>
      <td>64.01</td>
      <td>5.16</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>218.0</td>
      <td>True</td>
      <td>be52f4ad322f5a47da121c761aeb5ba20ed77b17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/Dolly_Malion-6b</td>
      <td>39.77</td>
      <td>42.83</td>
      <td>68.43</td>
      <td>27.13</td>
      <td>33.03</td>
      <td>65.43</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>f239eb8d24fe26db3b0a9a69115dc305fc9351af</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mwitiderrick/open_llama_3b_glaive_assistant_v0.1</td>
      <td>39.74</td>
      <td>40.70</td>
      <td>67.45</td>
      <td>27.74</td>
      <td>35.86</td>
      <td>64.72</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>efa950c69b6cbe1f8629400f3a7e0ccd895551fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mwitiderrick/open_llama_3b_glaive_v0.1</td>
      <td>39.74</td>
      <td>40.70</td>
      <td>67.45</td>
      <td>27.74</td>
      <td>35.86</td>
      <td>64.72</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>efa950c69b6cbe1f8629400f3a7e0ccd895551fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mwitiderrick/open_llama_3b_glaive_code_v0.1</td>
      <td>39.74</td>
      <td>40.70</td>
      <td>67.45</td>
      <td>27.74</td>
      <td>35.86</td>
      <td>64.72</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>efa950c69b6cbe1f8629400f3a7e0ccd895551fb</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>heegyu/WizardVicuna-Uncensored-3B-0719</td>
      <td>39.73</td>
      <td>41.38</td>
      <td>66.19</td>
      <td>26.53</td>
      <td>39.35</td>
      <td>63.77</td>
      <td>1.14</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>4.0</td>
      <td>True</td>
      <td>36841c80535bc3e8403e3cc084e8e65884c75076</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/ChanMalion</td>
      <td>39.73</td>
      <td>41.89</td>
      <td>68.25</td>
      <td>27.29</td>
      <td>33.89</td>
      <td>65.35</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>0.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>2667b0e0b705ed23f81f3e2b69673d722e8f4964</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mwitiderrick/open_llama_3b_code_instruct_0.1</td>
      <td>39.72</td>
      <td>41.21</td>
      <td>66.96</td>
      <td>27.82</td>
      <td>35.01</td>
      <td>65.43</td>
      <td>1.90</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>be8055f68a5d53321d98c2b3e0f153034303b96c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-12b-deduped</td>
      <td>39.70</td>
      <td>41.38</td>
      <td>70.26</td>
      <td>25.63</td>
      <td>33.00</td>
      <td>66.46</td>
      <td>1.44</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>50.0</td>
      <td>True</td>
      <td>39c1bd94f9dbe4ebd1d191f364cb33a2e5c47707</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Janin-GPTJ</td>
      <td>39.67</td>
      <td>40.87</td>
      <td>67.29</td>
      <td>27.40</td>
      <td>36.25</td>
      <td>64.25</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a6773861798f2abea3849514aa6f60961518af9c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/GPT-J-Pyg_PPO-6B-Dev-V8p4</td>
      <td>39.61</td>
      <td>40.19</td>
      <td>66.43</td>
      <td>30.39</td>
      <td>34.76</td>
      <td>64.01</td>
      <td>1.90</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>bigscience-openrail-m</td>
      <td>6.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>930dc82245c607ce43558a0e6c0225e77b341ea6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-13B-Erebus</td>
      <td>39.61</td>
      <td>40.02</td>
      <td>70.07</td>
      <td>25.32</td>
      <td>34.93</td>
      <td>66.54</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>175.0</td>
      <td>True</td>
      <td>8a949353677d2b971910a6c4afcc70e95d838c2a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-13B-Nerybus-Mix</td>
      <td>39.61</td>
      <td>39.85</td>
      <td>70.60</td>
      <td>24.90</td>
      <td>34.02</td>
      <td>67.88</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>29.0</td>
      <td>True</td>
      <td>c27a7e2360dd313406719980851e89abf46ebb13</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/GPT-J-6B-Shinen</td>
      <td>39.60</td>
      <td>39.85</td>
      <td>67.06</td>
      <td>27.72</td>
      <td>36.94</td>
      <td>64.09</td>
      <td>1.97</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>afa5a11b24cb23eee708e17c83b920a788e9e07b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/gpt-j-6B-Dolly</td>
      <td>39.60</td>
      <td>41.30</td>
      <td>65.97</td>
      <td>26.78</td>
      <td>37.91</td>
      <td>64.72</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>83d8c754aac12f838d7c847d4352a09396c383d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/GPT-J-Pyg_PPO-6B</td>
      <td>39.60</td>
      <td>42.06</td>
      <td>67.51</td>
      <td>28.52</td>
      <td>31.95</td>
      <td>64.72</td>
      <td>2.81</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>bigscience-openrail-m</td>
      <td>6.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>cde5bab3ae16e1704c5fec54a6a7ff1169c935e6</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-nl2sql-ds-6.7b</td>
      <td>39.59</td>
      <td>36.35</td>
      <td>52.83</td>
      <td>36.80</td>
      <td>40.55</td>
      <td>55.96</td>
      <td>15.09</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>e55ace80c04ed4ace1876ba192e6ecb4ef0353b8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/GPT-J-6B-Janeway</td>
      <td>39.54</td>
      <td>40.87</td>
      <td>67.11</td>
      <td>27.45</td>
      <td>35.74</td>
      <td>64.72</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>036bb03496d648ddc8cf932ad91df8ef1287116c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>amazon/LightGPT</td>
      <td>39.54</td>
      <td>39.93</td>
      <td>63.82</td>
      <td>28.45</td>
      <td>36.69</td>
      <td>64.48</td>
      <td>3.87</td>
      <td></td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>71.0</td>
      <td>True</td>
      <td>1f6ffd8f162030396a3bc1ca2e3504896dbe6434</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-13B-Nerys-v2</td>
      <td>39.53</td>
      <td>39.68</td>
      <td>70.53</td>
      <td>25.36</td>
      <td>33.50</td>
      <td>67.88</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>b0aa4f3630356f7801ca083c00b03d03da13b8bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/RedPajama-INCITE-Chat-3B-v1</td>
      <td>39.53</td>
      <td>42.83</td>
      <td>67.62</td>
      <td>26.23</td>
      <td>34.44</td>
      <td>65.51</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>129.0</td>
      <td>True</td>
      <td>f0e0995eba801096ed04cb87931d96a8316871af</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-6.7b-v2</td>
      <td>39.49</td>
      <td>39.42</td>
      <td>66.39</td>
      <td>30.09</td>
      <td>35.60</td>
      <td>64.25</td>
      <td>1.21</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>7.11</td>
      <td>1.0</td>
      <td>True</td>
      <td>7a7f93d4318658b354c5411cde64e9f0121f6b1f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>heegyu/WizardVicuna-3B-0719</td>
      <td>39.48</td>
      <td>40.70</td>
      <td>65.45</td>
      <td>25.44</td>
      <td>40.71</td>
      <td>63.85</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>62d3d450b8ab2bd2fb9f82383b55d1ecae33a401</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>databricks/dolly-v2-12b</td>
      <td>39.46</td>
      <td>42.41</td>
      <td>72.53</td>
      <td>25.92</td>
      <td>33.83</td>
      <td>60.85</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>12.00</td>
      <td>1909.0</td>
      <td>True</td>
      <td>19308160448536e378e3db21a73a751579ee7fdd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/llama2-ppo</td>
      <td>39.44</td>
      <td>41.64</td>
      <td>49.46</td>
      <td>35.36</td>
      <td>45.08</td>
      <td>64.96</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>8619e9870ce3285bf9c2a74921b5947dd6f9e4ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/PPO_Pygway-6b-Mix</td>
      <td>39.43</td>
      <td>41.81</td>
      <td>67.77</td>
      <td>28.42</td>
      <td>32.50</td>
      <td>64.40</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>20.0</td>
      <td>True</td>
      <td>b31d25819e00d5031ccdb22a9584f0850dcfe39c</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>harborwater/dpo-test-hermes-open-llama-3b</td>
      <td>39.42</td>
      <td>39.25</td>
      <td>67.46</td>
      <td>24.21</td>
      <td>39.81</td>
      <td>64.40</td>
      <td>1.36</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>False</td>
      <td>5cd560152a364f61f92cebe18feaefc181dfd287</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4</td>
      <td>39.38</td>
      <td>41.64</td>
      <td>66.23</td>
      <td>27.26</td>
      <td>36.10</td>
      <td>64.40</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc</td>
      <td>2.91</td>
      <td>3.0</td>
      <td>True</td>
      <td>c588a5924749b86a6cb36a687dafa544c189bb6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/RedPajama-INCITE-7B-Chat</td>
      <td>39.37</td>
      <td>42.06</td>
      <td>70.82</td>
      <td>26.94</td>
      <td>36.09</td>
      <td>59.83</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>91.0</td>
      <td>True</td>
      <td>47b94a739e2f3164b438501c8684acc5d5acc146</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/RedPajama-INCITE-Chat-7B-v0.1</td>
      <td>39.37</td>
      <td>42.06</td>
      <td>70.82</td>
      <td>26.94</td>
      <td>36.09</td>
      <td>59.83</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>47b94a739e2f3164b438501c8684acc5d5acc146</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/LongAlpaca-7B</td>
      <td>39.36</td>
      <td>42.66</td>
      <td>65.89</td>
      <td>27.28</td>
      <td>40.16</td>
      <td>60.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.74</td>
      <td>9.0</td>
      <td>False</td>
      <td>bebfcb894b3f5170ce54e3bb98b6e565fae7b6c0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/PPO_Shygmalion-6b</td>
      <td>39.35</td>
      <td>40.27</td>
      <td>66.88</td>
      <td>27.53</td>
      <td>34.24</td>
      <td>65.35</td>
      <td>1.82</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>573e4546fdccc5c8a52b9d7cb23a2e10f0f2ef51</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>digitous/Adventien-GPTJ</td>
      <td>39.31</td>
      <td>42.49</td>
      <td>69.21</td>
      <td>25.40</td>
      <td>36.95</td>
      <td>60.22</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>4fbfe9eae03a1d6ecf60fda8cf39c4123f0438bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mosaicml/mpt-7b-storywriter</td>
      <td>39.31</td>
      <td>45.65</td>
      <td>74.14</td>
      <td>28.80</td>
      <td>36.12</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>742.0</td>
      <td>True</td>
      <td>a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-6.9b-deduped</td>
      <td>39.30</td>
      <td>41.30</td>
      <td>67.05</td>
      <td>26.48</td>
      <td>35.19</td>
      <td>64.09</td>
      <td>1.67</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.90</td>
      <td>7.0</td>
      <td>True</td>
      <td>372b1c08d9b5b0fc18ce86bbf294930e26e66ed5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/fairseq-dense-6.7B</td>
      <td>39.26</td>
      <td>39.42</td>
      <td>71.26</td>
      <td>26.91</td>
      <td>32.73</td>
      <td>65.27</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>6.70</td>
      <td>2.0</td>
      <td>False</td>
      <td>d62d83b8eb7a6ba012a762752a5b5679add3b40c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>L-R/LLmRA-3B-v0.1</td>
      <td>39.25</td>
      <td>39.42</td>
      <td>59.79</td>
      <td>25.16</td>
      <td>50.62</td>
      <td>59.43</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7d8a4ccf707de28e924653ba719a18caf8c1db05</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>databricks/dolly-v2-7b</td>
      <td>39.24</td>
      <td>44.54</td>
      <td>69.64</td>
      <td>25.18</td>
      <td>34.88</td>
      <td>60.06</td>
      <td>1.14</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.00</td>
      <td>137.0</td>
      <td>True</td>
      <td>d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xaviviro/FLAMA-0.5-3B</td>
      <td>39.23</td>
      <td>37.97</td>
      <td>67.65</td>
      <td>25.73</td>
      <td>41.11</td>
      <td>62.12</td>
      <td>0.83</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>eeec9ee7d50953a27189ac64ee63c93a272d1a12</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/RedPajama-INCITE-Chat-Instruct-3B-V1</td>
      <td>39.23</td>
      <td>42.58</td>
      <td>67.48</td>
      <td>25.99</td>
      <td>33.62</td>
      <td>64.80</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>1.0</td>
      <td>True</td>
      <td>e19eef572d57fc734bf3ea07c7d0098b3901ec9b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>heegyu/RedTulu-Uncensored-3B-0719</td>
      <td>39.19</td>
      <td>40.02</td>
      <td>62.55</td>
      <td>30.37</td>
      <td>37.59</td>
      <td>62.35</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>c92bf022cddc3f57b4552ec3391df487295a2f87</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bigscience/bloom-7b1</td>
      <td>39.18</td>
      <td>41.13</td>
      <td>62.00</td>
      <td>26.25</td>
      <td>38.90</td>
      <td>65.43</td>
      <td>1.36</td>
      <td>pretrained</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.07</td>
      <td>164.0</td>
      <td>True</td>
      <td>e83e90ba86f87f74aa2731cdab25ccf33976bd66</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>DanielSc4/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1</td>
      <td>39.16</td>
      <td>41.30</td>
      <td>66.82</td>
      <td>26.10</td>
      <td>35.04</td>
      <td>65.43</td>
      <td>0.30</td>
      <td>RL-tuned</td>
      <td>?</td>
      <td>Adapter</td>
      <td>8bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a2ee88a9fa1c9ad41e0a8c15217a4b1230ec33c8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dvruette/oasst-pythia-6.9b-4000-steps</td>
      <td>39.15</td>
      <td>41.64</td>
      <td>64.24</td>
      <td>26.26</td>
      <td>40.43</td>
      <td>61.80</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>6.90</td>
      <td>0.0</td>
      <td>False</td>
      <td>0e201b6f344ac6382dda40d389e1c9144a87d027</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>matsuo-lab/weblab-10b-instruction-sft</td>
      <td>39.13</td>
      <td>40.10</td>
      <td>65.30</td>
      <td>26.66</td>
      <td>36.79</td>
      <td>64.09</td>
      <td>1.82</td>
      <td>instruction-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.00</td>
      <td>70.0</td>
      <td>True</td>
      <td>112a5ad9f556078ab14a5cd93511b9db4a0d4413</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/robin-33B-v2-GPTQ</td>
      <td>39.10</td>
      <td>27.73</td>
      <td>26.29</td>
      <td>23.53</td>
      <td>49.54</td>
      <td>79.79</td>
      <td>27.75</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>13.0</td>
      <td>True</td>
      <td>4c2588d65302e9ca634548ed81e8650fb2975686</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-6.7B-Erebus</td>
      <td>39.09</td>
      <td>39.16</td>
      <td>68.66</td>
      <td>24.58</td>
      <td>35.12</td>
      <td>65.98</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.70</td>
      <td>86.0</td>
      <td>True</td>
      <td>9c4d1af96f93224e01d2f69c303fc6d6f686bdcc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>YeungNLP/firefly-bloom-7b1</td>
      <td>39.09</td>
      <td>40.44</td>
      <td>61.20</td>
      <td>26.83</td>
      <td>40.83</td>
      <td>64.56</td>
      <td>0.68</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>6b4385dc45c47d509b6400c41a2ff3665ad1d189</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-6.7b</td>
      <td>39.08</td>
      <td>39.16</td>
      <td>68.66</td>
      <td>24.57</td>
      <td>35.12</td>
      <td>65.98</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.70</td>
      <td>88.0</td>
      <td>True</td>
      <td>a45aa65bbeb77c1558bc99bedc6779195462dab0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>togethercomputer/RedPajama-INCITE-Instruct-3B-v1</td>
      <td>39.06</td>
      <td>41.55</td>
      <td>65.48</td>
      <td>25.03</td>
      <td>36.41</td>
      <td>64.48</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>87.0</td>
      <td>True</td>
      <td>0c66778ee09a036886741707733620b91057909a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/deacon-3b</td>
      <td>39.05</td>
      <td>39.68</td>
      <td>66.42</td>
      <td>27.13</td>
      <td>36.07</td>
      <td>64.64</td>
      <td>0.38</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>c96b846ce7bacf5ad231957630dc94d59f329339</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/ScarletPajama-3B-HF</td>
      <td>39.04</td>
      <td>39.76</td>
      <td>64.89</td>
      <td>27.28</td>
      <td>37.60</td>
      <td>64.48</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>9dd07308b6eb3f270c5762250b6d46abd6f87b6f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psmathur/orca_mini_3b</td>
      <td>39.03</td>
      <td>41.55</td>
      <td>61.52</td>
      <td>26.79</td>
      <td>42.42</td>
      <td>61.80</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>fd2754e80ce80757a3a68a840d7d287dd7def676</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/black_goo_recipe_c</td>
      <td>39.01</td>
      <td>38.74</td>
      <td>66.83</td>
      <td>26.57</td>
      <td>36.54</td>
      <td>64.72</td>
      <td>0.68</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>78c0a6432ac0a6c2e54a2c3aac4cb70f446eb18b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/Guanaco-3B-Uncensored-v2</td>
      <td>38.98</td>
      <td>42.15</td>
      <td>66.72</td>
      <td>26.18</td>
      <td>35.21</td>
      <td>63.30</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>13.0</td>
      <td>True</td>
      <td>e07122091fd4b318dcea105b16c73144d95bc2f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>jb723/cross_lingual_epoch2</td>
      <td>38.97</td>
      <td>39.25</td>
      <td>47.92</td>
      <td>36.66</td>
      <td>47.90</td>
      <td>62.12</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>86e59e85b234e6c882758724849d7a1e4fe0b30a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>mwitiderrick/open_llama_3b_instruct_v_0.2</td>
      <td>38.97</td>
      <td>38.48</td>
      <td>66.77</td>
      <td>25.34</td>
      <td>38.16</td>
      <td>63.46</td>
      <td>1.59</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>6ae4004fe8901c1dae19108bc37e8b744cd08539</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Guanaco-3B-Uncensored-v2-GPTQ</td>
      <td>38.95</td>
      <td>41.64</td>
      <td>64.76</td>
      <td>26.25</td>
      <td>36.58</td>
      <td>64.33</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>GPTQ</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>4.78</td>
      <td>14.0</td>
      <td>True</td>
      <td>c80e2f01377d551ad17c8c9bac3f52578c38d653</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/Guanaco-3B-Uncensored</td>
      <td>38.94</td>
      <td>42.49</td>
      <td>66.99</td>
      <td>25.55</td>
      <td>34.71</td>
      <td>63.38</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>3.0</td>
      <td>True</td>
      <td>084a12f767b31c1fde681bebb14e9a291e506ea8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>health360/Healix-3B</td>
      <td>38.93</td>
      <td>37.71</td>
      <td>65.94</td>
      <td>26.02</td>
      <td>37.40</td>
      <td>65.75</td>
      <td>0.76</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>52297e0b6845b3c1b26f336fd2a2c9b2f56ce6ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>CobraMamba/mamba-gpt-3b</td>
      <td>38.87</td>
      <td>40.53</td>
      <td>64.94</td>
      <td>25.35</td>
      <td>37.14</td>
      <td>65.04</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>4.0</td>
      <td>True</td>
      <td>21a8212e3641dd14924d6bdead0774b64dda8ce0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>OpenAssistant/galactica-6.7b-finetuned</td>
      <td>38.84</td>
      <td>41.55</td>
      <td>51.01</td>
      <td>38.03</td>
      <td>41.65</td>
      <td>57.70</td>
      <td>3.11</td>
      <td></td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.70</td>
      <td>34.0</td>
      <td>False</td>
      <td>d86db70e16111175ff7900f71d40806ccf4b8491</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>frank098/orca_mini_3b_juniper</td>
      <td>38.83</td>
      <td>40.87</td>
      <td>61.73</td>
      <td>26.37</td>
      <td>43.19</td>
      <td>60.30</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>3.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>c08749034baa053834f1b709b6e7b88b914cd1fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-6.7B-Nerybus-Mix</td>
      <td>38.83</td>
      <td>39.16</td>
      <td>68.63</td>
      <td>24.47</td>
      <td>34.84</td>
      <td>65.11</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.70</td>
      <td>20.0</td>
      <td>True</td>
      <td>9afe4dca5a9dbd71cb90d1050d142837f4c739f6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-12b</td>
      <td>38.82</td>
      <td>39.59</td>
      <td>68.82</td>
      <td>26.76</td>
      <td>31.85</td>
      <td>64.17</td>
      <td>1.74</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>123.0</td>
      <td>True</td>
      <td>35c9d7f32fbb108fb8b5bdd574eb03369d1eed49</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>heegyu/WizardVicuna-open-llama-3b-v2</td>
      <td>38.77</td>
      <td>37.71</td>
      <td>66.60</td>
      <td>27.23</td>
      <td>36.80</td>
      <td>63.30</td>
      <td>0.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>1c69905286171d7d3ef3f95f8e1bbc9150bad3cd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/black_goo_recipe_a</td>
      <td>38.73</td>
      <td>38.14</td>
      <td>66.56</td>
      <td>25.75</td>
      <td>37.46</td>
      <td>63.93</td>
      <td>0.53</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>7067f68d4d9e7b10a1aa2c9fa97456bc04678867</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-6B-nerys-v2</td>
      <td>38.72</td>
      <td>38.40</td>
      <td>68.57</td>
      <td>24.34</td>
      <td>34.73</td>
      <td>65.59</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>6.00</td>
      <td>21.0</td>
      <td>True</td>
      <td>9e1f1498391df2c28ce35a9290a5a24b8022a43b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hakurei/instruct-12b</td>
      <td>38.63</td>
      <td>42.58</td>
      <td>66.76</td>
      <td>26.79</td>
      <td>31.96</td>
      <td>63.46</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>ff4699b502b79c716330b6f761002588a65dcba6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-oig-oasst1-256-6_9b</td>
      <td>38.62</td>
      <td>39.93</td>
      <td>65.42</td>
      <td>26.39</td>
      <td>35.00</td>
      <td>63.38</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>9.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>f1c9bac89b74d3487cb092788ce828fb9520c1a7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>matsuo-lab/weblab-10b</td>
      <td>38.59</td>
      <td>39.51</td>
      <td>65.76</td>
      <td>26.29</td>
      <td>36.02</td>
      <td>62.51</td>
      <td>1.44</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>10.00</td>
      <td>57.0</td>
      <td>True</td>
      <td>d6fc432983b1633a4c1568d121c60de6b8c3e511</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/black_goo_recipe_d</td>
      <td>38.57</td>
      <td>37.80</td>
      <td>66.50</td>
      <td>26.64</td>
      <td>36.46</td>
      <td>63.61</td>
      <td>0.38</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>fdf7f93837808958f9463d3c683314e7f649a088</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>RWKV/rwkv-raven-7b</td>
      <td>38.55</td>
      <td>39.42</td>
      <td>66.48</td>
      <td>23.64</td>
      <td>38.56</td>
      <td>62.90</td>
      <td>0.30</td>
      <td>instruction-tuned</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>16.0</td>
      <td>False</td>
      <td>a2dfc9f659be13556a25d9e38da642c6f67aeee3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>togethercomputer/RedPajama-INCITE-Base-3B-v1</td>
      <td>38.54</td>
      <td>40.19</td>
      <td>64.77</td>
      <td>27.03</td>
      <td>33.23</td>
      <td>64.72</td>
      <td>1.29</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>85.0</td>
      <td>True</td>
      <td>094fbdd0c911feb485ce55de1952ab2e75277e1e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Lazycuber/pyg-instruct-wizardlm</td>
      <td>38.54</td>
      <td>40.96</td>
      <td>66.71</td>
      <td>26.33</td>
      <td>31.93</td>
      <td>63.69</td>
      <td>1.59</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>f00ef7a7b0cc6f02af2a11ac764270dfd61b9e2f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-30B-Erebus</td>
      <td>38.53</td>
      <td>36.69</td>
      <td>65.60</td>
      <td>24.80</td>
      <td>38.76</td>
      <td>65.11</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>30.00</td>
      <td>49.0</td>
      <td>True</td>
      <td>a1041efcf9599c962822274e92040710579a5bf2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/CrimsonPajama</td>
      <td>38.52</td>
      <td>40.19</td>
      <td>65.47</td>
      <td>25.95</td>
      <td>33.78</td>
      <td>65.19</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>ff054eeff9e3541464383d40b36d182057d01113</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-oig-oasst1-512-6_9b</td>
      <td>38.52</td>
      <td>40.44</td>
      <td>65.58</td>
      <td>24.90</td>
      <td>36.68</td>
      <td>62.51</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>9.00</td>
      <td>16.0</td>
      <td>True</td>
      <td>029a787e0d98fcd3fecffbfbeb4a75a425474937</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/guanaco-33B-GPTQ</td>
      <td>38.51</td>
      <td>28.16</td>
      <td>26.34</td>
      <td>24.94</td>
      <td>48.98</td>
      <td>78.85</td>
      <td>23.81</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>74.0</td>
      <td>True</td>
      <td>8e42e031bfc8be3bbf31dc546d7c51fb991ff6e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/LLongMA-3b-LIMA</td>
      <td>38.51</td>
      <td>39.08</td>
      <td>67.15</td>
      <td>26.43</td>
      <td>34.71</td>
      <td>63.38</td>
      <td>0.30</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>3.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>333b8c41e42a46a6f3aecaf8f3fa8a17c6d83990</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>pszemraj/pythia-6.9b-HC3</td>
      <td>38.51</td>
      <td>36.52</td>
      <td>61.76</td>
      <td>26.94</td>
      <td>45.05</td>
      <td>60.77</td>
      <td>0.00</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.90</td>
      <td>2.0</td>
      <td>True</td>
      <td>c5c60ea656e921e6c5415f6feaebac4dd9b2aa2a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/black_goo_recipe_b</td>
      <td>38.49</td>
      <td>37.63</td>
      <td>66.72</td>
      <td>25.68</td>
      <td>37.09</td>
      <td>63.77</td>
      <td>0.08</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>42faec8429cee8c9f4f5db58ffa193f6f8e0d498</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Fredithefish/RedPajama-INCITE-Chat-3B-ShareGPT-11K</td>
      <td>38.47</td>
      <td>40.61</td>
      <td>64.84</td>
      <td>26.13</td>
      <td>35.41</td>
      <td>63.54</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>ec33d12d08d61ed821e67b1a55ad404dc3457ebf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/pygmalion-6b</td>
      <td>38.47</td>
      <td>40.53</td>
      <td>67.47</td>
      <td>25.73</td>
      <td>32.53</td>
      <td>62.51</td>
      <td>2.05</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>6.00</td>
      <td>705.0</td>
      <td>True</td>
      <td>30e2405100eac6bd53f75964cc7345eeafd19f7d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ</td>
      <td>38.43</td>
      <td>27.39</td>
      <td>26.03</td>
      <td>25.81</td>
      <td>48.90</td>
      <td>77.90</td>
      <td>24.56</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>43.0</td>
      <td>True</td>
      <td>1c65902c620fcdf6b9c8e36ce17f21360e186a1e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>anhnv125/pygmalion-6b-roleplay</td>
      <td>38.34</td>
      <td>40.53</td>
      <td>67.47</td>
      <td>25.73</td>
      <td>32.53</td>
      <td>62.67</td>
      <td>1.14</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>e49ed0bde45de0a436bff678ec4872069e8f230c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TehVenom/DiffMerge_Pygmalion_Main-onto-V8P4</td>
      <td>38.31</td>
      <td>40.53</td>
      <td>67.48</td>
      <td>25.68</td>
      <td>32.55</td>
      <td>62.51</td>
      <td>1.14</td>
      <td></td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>f855780745aa34c3bdbe020e4c51253d538cb21e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>acrastt/OmegLLaMA-3B</td>
      <td>38.28</td>
      <td>40.36</td>
      <td>66.13</td>
      <td>28.00</td>
      <td>33.31</td>
      <td>61.64</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>5.0</td>
      <td>True</td>
      <td>520c5f1ceb5c90d4011887e2a8d3becf15e7e66e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>openlm-research/open_llama_3b</td>
      <td>38.26</td>
      <td>39.85</td>
      <td>62.65</td>
      <td>26.94</td>
      <td>34.97</td>
      <td>64.72</td>
      <td>0.45</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>136.0</td>
      <td>True</td>
      <td>141067009124b9c0aea62c76b3eb952174864057</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>aihub-app/zyte-1B</td>
      <td>38.23</td>
      <td>37.88</td>
      <td>61.37</td>
      <td>24.61</td>
      <td>42.14</td>
      <td>61.96</td>
      <td>1.44</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>5.0</td>
      <td>True</td>
      <td>6c2b31ee038f8df37547c013d73b91c4a07e41a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xaviviro/FLOR-6.3B-xat</td>
      <td>38.23</td>
      <td>38.65</td>
      <td>63.76</td>
      <td>26.54</td>
      <td>37.96</td>
      <td>62.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.25</td>
      <td>0.0</td>
      <td>True</td>
      <td>edd1cbf53f584c6bc7b38a31a0b7beed8e942e8f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aihub-app/zyte-1.1B</td>
      <td>38.22</td>
      <td>37.88</td>
      <td>61.37</td>
      <td>24.62</td>
      <td>42.15</td>
      <td>61.96</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>False</td>
      <td>4537b28d9b2e9958c53b6d4aa6e16f46f85c1867</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>venkycs/zyte-v1-1.1B</td>
      <td>38.21</td>
      <td>37.29</td>
      <td>61.41</td>
      <td>24.60</td>
      <td>42.59</td>
      <td>62.04</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>False</td>
      <td>b75c703a236c6f0394f7f8641c4ecee016c2e43f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewof/koishi-instruct-3b</td>
      <td>38.16</td>
      <td>40.96</td>
      <td>64.54</td>
      <td>26.58</td>
      <td>31.65</td>
      <td>64.09</td>
      <td>1.14</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>2.91</td>
      <td>3.0</td>
      <td>False</td>
      <td>2bb7f3842398b048efa4ae2d1aafb9e2f18a8586</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-6.7b</td>
      <td>38.06</td>
      <td>40.10</td>
      <td>65.00</td>
      <td>24.64</td>
      <td>32.85</td>
      <td>64.72</td>
      <td>1.06</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>6.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>b666a6e46eeade607c73ed1334ecda3b9345e4bf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>saberai/Zro1.5_3B</td>
      <td>38.02</td>
      <td>35.92</td>
      <td>61.11</td>
      <td>25.55</td>
      <td>36.89</td>
      <td>58.72</td>
      <td>9.93</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>1.0</td>
      <td>True</td>
      <td>434e3ac9bb300779d677486d5e04d774fc514169</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-4-7b-pile</td>
      <td>37.95</td>
      <td>39.68</td>
      <td>66.31</td>
      <td>24.96</td>
      <td>33.65</td>
      <td>62.35</td>
      <td>0.76</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>922e22a761427e50d7be457b31a76b1126021b8b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/Galactica-6.7B-EssayWriter</td>
      <td>37.75</td>
      <td>40.10</td>
      <td>50.29</td>
      <td>33.88</td>
      <td>40.27</td>
      <td>58.48</td>
      <td>3.49</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>6.66</td>
      <td>0.0</td>
      <td>True</td>
      <td>ac74fdd938de1ffd34832d66a25db20b0230983e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Devio/test-22B</td>
      <td>37.71</td>
      <td>39.42</td>
      <td>64.51</td>
      <td>27.13</td>
      <td>37.13</td>
      <td>57.70</td>
      <td>0.38</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>21.83</td>
      <td>0.0</td>
      <td>False</td>
      <td>cd72f5954ab5801dd2c1b499e59265f7504f9ee6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/TinyLlama-MoE-Chat</td>
      <td>37.71</td>
      <td>34.73</td>
      <td>59.29</td>
      <td>29.71</td>
      <td>39.35</td>
      <td>62.19</td>
      <td>0.99</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.43</td>
      <td>0.0</td>
      <td>False</td>
      <td>2d786c9077b949d7ee3f5201813d7edccc7bd2da</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aihub-app/zyte-1.1b</td>
      <td>37.70</td>
      <td>37.54</td>
      <td>60.82</td>
      <td>24.57</td>
      <td>39.46</td>
      <td>62.04</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>False</td>
      <td>3d4e61bc3c090a28355cceba8da106c31e3bbb84</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ericzzz/falcon-rw-1b-instruct-openorca</td>
      <td>37.63</td>
      <td>34.56</td>
      <td>60.93</td>
      <td>28.77</td>
      <td>37.42</td>
      <td>60.69</td>
      <td>3.41</td>
      <td>instruction-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>4.0</td>
      <td>True</td>
      <td>bb5f86170d8d01aa850bb216bb2797899570c13e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>euclaise/falcon_1b_stage2</td>
      <td>37.59</td>
      <td>35.49</td>
      <td>65.56</td>
      <td>23.83</td>
      <td>38.32</td>
      <td>62.35</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>c3ef73a8c9dc06fae4bfe4460d2f293147aecbb0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ikala/bloom-zh-3b-chat</td>
      <td>37.58</td>
      <td>38.82</td>
      <td>54.71</td>
      <td>31.62</td>
      <td>41.25</td>
      <td>58.64</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-openrail-m</td>
      <td>3.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>4ea0ad223a2623fc15e8824c1c4f8e6539bc40b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2</td>
      <td>37.55</td>
      <td>36.43</td>
      <td>61.41</td>
      <td>25.01</td>
      <td>37.59</td>
      <td>64.64</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>fdc6ff469295d0aaabec8948525b70d6688728ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/CodeLlama-13B-Python-fp16</td>
      <td>37.52</td>
      <td>33.19</td>
      <td>44.50</td>
      <td>25.94</td>
      <td>43.99</td>
      <td>67.40</td>
      <td>10.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>25.0</td>
      <td>True</td>
      <td>442282f4207442b828953a72c51a919c332cba5c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>HiTZ/GoLLIE-7B</td>
      <td>37.48</td>
      <td>36.09</td>
      <td>57.93</td>
      <td>29.38</td>
      <td>39.27</td>
      <td>58.96</td>
      <td>3.26</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>d3e41fef45f6a7d438c46ba7d9fce5d0d486c7a9</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/fairseq-dense-2.7B</td>
      <td>37.41</td>
      <td>33.79</td>
      <td>65.74</td>
      <td>26.44</td>
      <td>34.57</td>
      <td>63.93</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>2.78</td>
      <td>2.0</td>
      <td>False</td>
      <td>4201f4b101bad2992efc8452009317a354ec52d2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cerebras/Cerebras-GPT-13B</td>
      <td>37.40</td>
      <td>38.14</td>
      <td>60.01</td>
      <td>25.92</td>
      <td>39.19</td>
      <td>59.83</td>
      <td>1.29</td>
      <td>pretrained</td>
      <td>GPT2Model</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>635.0</td>
      <td>True</td>
      <td>7e97fa4b15edd955094c4395d62e6f4290e365b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>ericzzz/falcon-rw-1b-chat</td>
      <td>37.37</td>
      <td>35.58</td>
      <td>61.12</td>
      <td>24.51</td>
      <td>39.62</td>
      <td>61.72</td>
      <td>1.67</td>
      <td>instruction-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>3.0</td>
      <td>True</td>
      <td>61c2b3f27c8d32912d0b9ff47ebf687af2eb9e86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Dampish/StellarX-4B-V0</td>
      <td>37.31</td>
      <td>36.95</td>
      <td>61.90</td>
      <td>26.85</td>
      <td>34.30</td>
      <td>63.85</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>4.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>0a79832bd57a8cdadc61626fb77bdc26c85b9fa4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>
      <td>37.28</td>
      <td>36.09</td>
      <td>61.10</td>
      <td>25.39</td>
      <td>37.48</td>
      <td>61.25</td>
      <td>2.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>661.0</td>
      <td>True</td>
      <td>de253fa9783f8bd558c9ed398c8ffbe3c55cedb3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/WizardLM-30B-Uncensored-GPTQ</td>
      <td>37.27</td>
      <td>29.44</td>
      <td>26.47</td>
      <td>24.35</td>
      <td>49.15</td>
      <td>73.16</td>
      <td>21.08</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>112.0</td>
      <td>True</td>
      <td>43c701ddbe0bceac26c860307e06763cc5203500</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>DanielSc4/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1</td>
      <td>37.27</td>
      <td>38.65</td>
      <td>63.53</td>
      <td>25.16</td>
      <td>36.07</td>
      <td>60.14</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>8bit</td>
      <td>False</td>
      <td>?</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f477d24b00e05fe4c5f8d5f933080994cfd90e4e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>GeorgiaTechResearchInstitute/galactica-6.7b-evol-instruct-70k</td>
      <td>37.27</td>
      <td>42.58</td>
      <td>49.30</td>
      <td>32.96</td>
      <td>42.10</td>
      <td>56.27</td>
      <td>0.38</td>
      <td></td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>6.70</td>
      <td>18.0</td>
      <td>True</td>
      <td>14fa470051d0bc38fd871643186a9edfd3a8a9aa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>euclaise/falcon_1b_stage1</td>
      <td>37.25</td>
      <td>35.15</td>
      <td>62.40</td>
      <td>24.47</td>
      <td>40.00</td>
      <td>61.48</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f85d91ff3f6cadc93f7222a19b9c4930c8842366</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-6.7b</td>
      <td>37.23</td>
      <td>36.35</td>
      <td>60.75</td>
      <td>26.00</td>
      <td>39.04</td>
      <td>60.69</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>7b20cb87e793e1b73b6a73da5261c6010f2b5410</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>sreeramajay/TinyLlama-1.1B-orca-v1.0</td>
      <td>37.17</td>
      <td>36.35</td>
      <td>61.23</td>
      <td>25.18</td>
      <td>36.58</td>
      <td>61.40</td>
      <td>2.27</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>7dbbc8ccc85c1c3f1ce7cffbb62b97ca6d2ca046</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>
      <td>37.17</td>
      <td>35.92</td>
      <td>61.11</td>
      <td>25.00</td>
      <td>37.38</td>
      <td>61.17</td>
      <td>2.43</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>661.0</td>
      <td>True</td>
      <td>de253fa9783f8bd558c9ed398c8ffbe3c55cedb3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kevin009/lamatama</td>
      <td>37.15</td>
      <td>36.35</td>
      <td>61.12</td>
      <td>24.72</td>
      <td>37.67</td>
      <td>60.77</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>bb8349cd64652df9a62bc46c12c24f3226662a5c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>princeton-nlp/Sheared-LLaMA-1.3B-ShareGPT</td>
      <td>37.14</td>
      <td>33.96</td>
      <td>62.55</td>
      <td>26.42</td>
      <td>43.03</td>
      <td>56.83</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.30</td>
      <td>8.0</td>
      <td>True</td>
      <td>d2f3cfae7746c4ff07353b39828985ea0f36b07d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-2.7b</td>
      <td>37.09</td>
      <td>37.37</td>
      <td>60.74</td>
      <td>25.86</td>
      <td>35.40</td>
      <td>62.12</td>
      <td>1.06</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>2.91</td>
      <td>0.0</td>
      <td>True</td>
      <td>b9d8cace80b1a97f5ed380711aea31f2d1b24310</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Deathsquad10/TinyLlama-repeat</td>
      <td>37.09</td>
      <td>35.24</td>
      <td>60.25</td>
      <td>26.07</td>
      <td>38.78</td>
      <td>60.46</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>14728ff7c51471faec92a4c86261951cc4175f61</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>davanstrien/TinyLlama-1.1B-Chat-v1.0-intel-dpo</td>
      <td>37.09</td>
      <td>35.84</td>
      <td>61.29</td>
      <td>25.05</td>
      <td>37.38</td>
      <td>61.01</td>
      <td>1.97</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>da2b792b7edf3d30b6e8ed05ebc8e9bbde442b5c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>tiiuae/falcon-rw-1b</td>
      <td>37.07</td>
      <td>35.07</td>
      <td>63.56</td>
      <td>25.28</td>
      <td>35.96</td>
      <td>62.04</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>82.0</td>
      <td>True</td>
      <td>e4b9872bb803165eb22f0a867d4e6a64d34fce19</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Phind/Phind-CodeLlama-34B-v1</td>
      <td>37.06</td>
      <td>27.13</td>
      <td>28.28</td>
      <td>28.94</td>
      <td>44.94</td>
      <td>72.61</td>
      <td>20.47</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>318.0</td>
      <td>True</td>
      <td>b073c9bb418ae52ca76b4ab48ac2dfbc8622f434</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloomz-3b</td>
      <td>37.03</td>
      <td>36.86</td>
      <td>54.95</td>
      <td>32.91</td>
      <td>40.34</td>
      <td>57.14</td>
      <td>0.00</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>3.00</td>
      <td>73.0</td>
      <td>True</td>
      <td>31eefcb2bcd69632925adf07e090debafe95436d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alexredna/TinyLlama-1.1B-Chat-v1.0-reasoning-v2-dpo</td>
      <td>37.03</td>
      <td>34.39</td>
      <td>61.87</td>
      <td>26.34</td>
      <td>36.13</td>
      <td>63.46</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>f61da97b0c79b404f3dbe88f9379d1c918777338</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aabbhishekk/TinyLlama-1.1B-miniguanaco</td>
      <td>37.02</td>
      <td>35.15</td>
      <td>60.26</td>
      <td>26.26</td>
      <td>38.84</td>
      <td>60.14</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>e2495b16f1d812485842b199a026438e037f15f5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-13b-Python-hf</td>
      <td>37.00</td>
      <td>32.59</td>
      <td>43.94</td>
      <td>27.23</td>
      <td>44.59</td>
      <td>65.04</td>
      <td>8.64</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>31.0</td>
      <td>True</td>
      <td>ea1b775799b477fe22e64f8ac9107f28950b5c87</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AIChenKai/TinyLlama-1.1B-Chat-v1.0-x2-MoE</td>
      <td>36.98</td>
      <td>36.01</td>
      <td>61.04</td>
      <td>24.81</td>
      <td>37.37</td>
      <td>60.38</td>
      <td>2.27</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.86</td>
      <td>0.0</td>
      <td>True</td>
      <td>fe49be7cae7eb8362e176e4d371fb9dd8c68422d</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-2.7B-Erebus</td>
      <td>36.96</td>
      <td>34.39</td>
      <td>60.91</td>
      <td>26.70</td>
      <td>37.82</td>
      <td>61.64</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>2.70</td>
      <td>33.0</td>
      <td>True</td>
      <td>39ca914ceb82f7f14a38484023bc04f0cd5d0a8d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jan-hq/LlamaCorn-1.1B</td>
      <td>36.94</td>
      <td>34.13</td>
      <td>59.33</td>
      <td>29.01</td>
      <td>36.78</td>
      <td>61.96</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>309e861eb3291666e9bd4e899fc95c8513beda4d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cmarkea/bloomz-3b-sft-chat</td>
      <td>36.94</td>
      <td>36.86</td>
      <td>54.34</td>
      <td>31.49</td>
      <td>39.69</td>
      <td>58.88</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>3.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>a35b6ae6809891e253b45fb5795979c33992e548</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azure99/blossom-v1-3b</td>
      <td>36.90</td>
      <td>36.86</td>
      <td>55.10</td>
      <td>26.70</td>
      <td>43.45</td>
      <td>58.88</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>3235ee41e3793c98749b7bbd2bb80882a12ac889</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Phind/Phind-CodeLlama-34B-v2</td>
      <td>36.89</td>
      <td>24.57</td>
      <td>27.60</td>
      <td>25.76</td>
      <td>48.37</td>
      <td>71.82</td>
      <td>23.20</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>639.0</td>
      <td>True</td>
      <td>949f61e203f91b412efe8f679c798f09f0ff4b0c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-7b-Python-hf</td>
      <td>36.89</td>
      <td>31.31</td>
      <td>52.86</td>
      <td>27.32</td>
      <td>42.21</td>
      <td>63.06</td>
      <td>4.55</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>77.0</td>
      <td>True</td>
      <td>ec4dd26f30674fdee00ef161b55f464ce28f9c20</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Rallio67/3B-redpajama-conditional-alpha</td>
      <td>36.88</td>
      <td>36.26</td>
      <td>61.90</td>
      <td>25.42</td>
      <td>36.31</td>
      <td>60.77</td>
      <td>0.61</td>
      <td>instruction-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>3.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>7e2156c14b4b7981a4cd6db7b878888a98144df0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>euclaise/falcon_1b_stage2</td>
      <td>36.88</td>
      <td>33.11</td>
      <td>63.19</td>
      <td>24.22</td>
      <td>38.40</td>
      <td>62.35</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>025c77e9ee457c6771c5a36dbacd064c269642a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-2.7B-Nerybus-Mix</td>
      <td>36.88</td>
      <td>33.70</td>
      <td>61.21</td>
      <td>26.60</td>
      <td>37.57</td>
      <td>62.04</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>2.70</td>
      <td>9.0</td>
      <td>True</td>
      <td>b4131723cfff1fa42f6cbab546c5b4bb0d19fd83</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenBuddy/openbuddy-openllama-3b-v10-bf16</td>
      <td>36.87</td>
      <td>36.26</td>
      <td>58.38</td>
      <td>23.89</td>
      <td>42.04</td>
      <td>59.67</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>7f24d32de53aa4bc150f04ca2418604475173921</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Writer/camel-5b-hf</td>
      <td>36.81</td>
      <td>35.15</td>
      <td>57.62</td>
      <td>26.07</td>
      <td>40.65</td>
      <td>61.01</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>5.00</td>
      <td>106.0</td>
      <td>True</td>
      <td>d1438e22a33b9115af0e47ab3a0fe844cbf588a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>appvoid/palmer-002</td>
      <td>36.79</td>
      <td>34.47</td>
      <td>59.41</td>
      <td>25.94</td>
      <td>37.06</td>
      <td>62.67</td>
      <td>1.21</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>8b79b8c2126483baeb3a503c51cd4ffa9d7c11a6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TFLai/pythia-2.8b-4bit-alpaca</td>
      <td>36.77</td>
      <td>34.73</td>
      <td>58.96</td>
      <td>25.53</td>
      <td>39.14</td>
      <td>61.64</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>2.80</td>
      <td>0.0</td>
      <td>True</td>
      <td>40e84b6d38aac92a0302c2a682498794ef0fd901</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-2.7B-Nerys-v2</td>
      <td>36.75</td>
      <td>33.28</td>
      <td>61.23</td>
      <td>26.44</td>
      <td>37.23</td>
      <td>62.04</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>2.70</td>
      <td>6.0</td>
      <td>True</td>
      <td>91d7afd6dbf3bbd1e4ccc6b9a2618d632a8cbb92</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vihangd/dopeyshearedplats-1.3b-v1</td>
      <td>36.74</td>
      <td>34.39</td>
      <td>64.31</td>
      <td>25.40</td>
      <td>38.21</td>
      <td>57.38</td>
      <td>0.76</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>True</td>
      <td>45aa5d406bb6975deb801e5fffa27ca23e5724a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-2.7b</td>
      <td>36.74</td>
      <td>33.96</td>
      <td>61.43</td>
      <td>25.43</td>
      <td>37.43</td>
      <td>61.96</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>2.70</td>
      <td>52.0</td>
      <td>True</td>
      <td>397f71a473a150c00f0fe3fc4a2f78ff3ccaf82d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>L-R/LLmRa-2.7B</td>
      <td>36.72</td>
      <td>37.03</td>
      <td>60.65</td>
      <td>25.58</td>
      <td>35.23</td>
      <td>61.56</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>2.70</td>
      <td>1.0</td>
      <td>True</td>
      <td>93201b7d778272fb3252481c1cbd56f726d43e6b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-2.8b-deduped</td>
      <td>36.72</td>
      <td>36.26</td>
      <td>60.66</td>
      <td>26.78</td>
      <td>35.56</td>
      <td>60.22</td>
      <td>0.83</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.91</td>
      <td>13.0</td>
      <td>True</td>
      <td>7d977fed8c4ce9649816af8cd5fe36a639cbe5b2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/chopt-2_7b</td>
      <td>36.72</td>
      <td>36.01</td>
      <td>63.38</td>
      <td>25.44</td>
      <td>37.71</td>
      <td>57.77</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>45f57352c10a1fb1ec13c4bf387a15552ca1fe65</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/TinyLlama-MoE-Chat-0.1</td>
      <td>36.70</td>
      <td>34.39</td>
      <td>56.72</td>
      <td>29.36</td>
      <td>37.82</td>
      <td>59.67</td>
      <td>2.27</td>
      <td>instruction-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.43</td>
      <td>0.0</td>
      <td>False</td>
      <td>2ebc34217cafbff7812e85fd59c682550bbeb4f8</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>danielhanchen/open_llama_3b_600bt_preview</td>
      <td>36.65</td>
      <td>36.86</td>
      <td>59.96</td>
      <td>25.97</td>
      <td>32.81</td>
      <td>63.69</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>d8fddf7651dfcae5aefda59d9e868c9111d8bdb3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>42dot/42dot_LLM-SFT-1.3B</td>
      <td>36.61</td>
      <td>36.09</td>
      <td>58.96</td>
      <td>25.51</td>
      <td>39.98</td>
      <td>58.41</td>
      <td>0.68</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.44</td>
      <td>22.0</td>
      <td>True</td>
      <td>7474cafe5dc60549c19f89f7c49392a8a32b9199</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>PSanni/Deer-3b</td>
      <td>36.55</td>
      <td>38.48</td>
      <td>57.41</td>
      <td>25.64</td>
      <td>39.98</td>
      <td>57.46</td>
      <td>0.30</td>
      <td>instruction-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>53ea8f8862fc1820f0cd31f62953b7290fd79867</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BEE-spoke-data/TinyLlama-3T-1.1bee</td>
      <td>36.46</td>
      <td>33.79</td>
      <td>60.29</td>
      <td>25.86</td>
      <td>38.13</td>
      <td>60.22</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>eca8e79df61b9872b84df24f61f0d8f0573d383e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>codellama/CodeLlama-7b-Python-hf</td>
      <td>36.42</td>
      <td>29.27</td>
      <td>50.12</td>
      <td>28.37</td>
      <td>41.61</td>
      <td>64.01</td>
      <td>5.16</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>77.0</td>
      <td>True</td>
      <td>ec4dd26f30674fdee00ef161b55f464ce28f9c20</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T</td>
      <td>36.42</td>
      <td>33.87</td>
      <td>60.31</td>
      <td>26.04</td>
      <td>37.32</td>
      <td>59.51</td>
      <td>1.44</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>80.0</td>
      <td>True</td>
      <td>df4c1907f152969ce2850c097e414d79c3a1665a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/xglm-7.5B</td>
      <td>36.38</td>
      <td>34.13</td>
      <td>60.77</td>
      <td>27.79</td>
      <td>36.66</td>
      <td>58.72</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.50</td>
      <td>50.0</td>
      <td>True</td>
      <td>732d59308a844004bd9a4def972cc7c3896a38e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SanjiWatsuki/WoolyHermes-1.1B</td>
      <td>36.37</td>
      <td>34.30</td>
      <td>59.37</td>
      <td>25.59</td>
      <td>37.58</td>
      <td>59.35</td>
      <td>2.05</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf2c67039648176ffe45e3ffb9892557a95d3405</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Phind/Phind-CodeLlama-34B-Python-v1</td>
      <td>36.33</td>
      <td>24.66</td>
      <td>29.77</td>
      <td>27.95</td>
      <td>45.27</td>
      <td>68.82</td>
      <td>21.53</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.00</td>
      <td>243.0</td>
      <td>True</td>
      <td>3aabef8c9bc1b3ec2fffed053645bc1e2d829b6c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cerebras/Cerebras-GPT-6.7B</td>
      <td>36.27</td>
      <td>35.07</td>
      <td>59.36</td>
      <td>25.93</td>
      <td>38.02</td>
      <td>58.72</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.70</td>
      <td>62.0</td>
      <td>True</td>
      <td>4f56c6e28f9a2a1c470626f1a064238806f19f09</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T</td>
      <td>36.26</td>
      <td>33.53</td>
      <td>59.38</td>
      <td>26.22</td>
      <td>36.79</td>
      <td>60.22</td>
      <td>1.44</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>37.0</td>
      <td>True</td>
      <td>03978af6c0997cda809de070e056ee5ddb7e7188</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/gpt-neo-2.7B</td>
      <td>36.20</td>
      <td>33.36</td>
      <td>56.24</td>
      <td>26.45</td>
      <td>39.78</td>
      <td>60.06</td>
      <td>1.29</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>2.72</td>
      <td>371.0</td>
      <td>True</td>
      <td>e24fa291132763e59f4a5422741b424fb5d59056</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bertin-project/bertin-gpt-j-6B-alpaca</td>
      <td>36.19</td>
      <td>36.01</td>
      <td>54.30</td>
      <td>27.66</td>
      <td>43.38</td>
      <td>55.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>6.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>636b17d6044189343475d1889f076aba73036905</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>euclaise/falcon_1b_stage3_2</td>
      <td>36.19</td>
      <td>34.56</td>
      <td>58.37</td>
      <td>23.87</td>
      <td>39.89</td>
      <td>60.46</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>aec2f59879ea6dfa5233611c4cf83cf3cb974d40</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Dampish/StellarX-4B-V0.2</td>
      <td>36.15</td>
      <td>34.64</td>
      <td>56.74</td>
      <td>25.55</td>
      <td>38.55</td>
      <td>61.40</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>4.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>605b6812956400dbde24ad7b8649a744a2ddfc8e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bigscience/bloom-3b</td>
      <td>36.07</td>
      <td>35.75</td>
      <td>54.37</td>
      <td>26.59</td>
      <td>40.57</td>
      <td>57.62</td>
      <td>1.52</td>
      <td>pretrained</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>3.00</td>
      <td>75.0</td>
      <td>True</td>
      <td>52bc5b43010b4844513826b8be3f78c7344c37d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ</td>
      <td>36.06</td>
      <td>29.61</td>
      <td>25.47</td>
      <td>25.34</td>
      <td>50.25</td>
      <td>75.77</td>
      <td>9.93</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>272.0</td>
      <td>True</td>
      <td>d9b00ec47ae3546398432f0693fe2d5d92bf143b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Deacon-1_8b</td>
      <td>36.03</td>
      <td>33.70</td>
      <td>52.33</td>
      <td>33.97</td>
      <td>39.05</td>
      <td>57.14</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.84</td>
      <td>0.0</td>
      <td>True</td>
      <td>77056bdfc4f05eb933a9e9af3af6fe68f89eb0b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Azure99/blossom-v2-3b</td>
      <td>35.98</td>
      <td>35.32</td>
      <td>54.10</td>
      <td>23.99</td>
      <td>43.11</td>
      <td>58.80</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1a403344de52ddb7f18548a526a927714adfe4d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>vihangd/shearedplats-1.3b-v1</td>
      <td>35.97</td>
      <td>35.41</td>
      <td>62.75</td>
      <td>24.75</td>
      <td>33.93</td>
      <td>58.48</td>
      <td>0.53</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>True</td>
      <td>7ac93152e1807ec1d732500255a747e27922fb1a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>remyxai/localmentor_25K_3epochs_tinyllama</td>
      <td>35.96</td>
      <td>34.22</td>
      <td>59.01</td>
      <td>24.93</td>
      <td>36.07</td>
      <td>60.46</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>670b3f3be7ee5fd09922c033d0fa2d539f98344a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>princeton-nlp/Sheared-LLaMA-1.3B</td>
      <td>35.95</td>
      <td>32.85</td>
      <td>60.91</td>
      <td>25.71</td>
      <td>37.14</td>
      <td>58.64</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.30</td>
      <td>72.0</td>
      <td>True</td>
      <td>b1c3f74c8495e27b3963d64af0781d4a611794f3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/GPT-J-6B-Adventure</td>
      <td>35.95</td>
      <td>37.12</td>
      <td>61.26</td>
      <td>25.94</td>
      <td>34.56</td>
      <td>55.96</td>
      <td>0.83</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>6.00</td>
      <td>15.0</td>
      <td>False</td>
      <td>e2c00dc99f986f2430f5d34c0214969cee786755</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ehartford/CodeLlama-34b-Python-hf</td>
      <td>35.92</td>
      <td>38.05</td>
      <td>34.79</td>
      <td>32.96</td>
      <td>43.57</td>
      <td>66.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>33.48</td>
      <td>0.0</td>
      <td>True</td>
      <td>45f38e53a579a2b39298cc57ab04078722bebec0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/TinyLamma-SFT</td>
      <td>35.88</td>
      <td>34.39</td>
      <td>59.14</td>
      <td>24.26</td>
      <td>37.20</td>
      <td>58.64</td>
      <td>1.67</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>4dbfdc67f096a0a801d95c4f4c74cd6dd0c52e1c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>MayaPH/opt-flan-iml-6.7b</td>
      <td>35.84</td>
      <td>30.12</td>
      <td>58.82</td>
      <td>25.12</td>
      <td>36.74</td>
      <td>64.25</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>6.66</td>
      <td>1.0</td>
      <td>True</td>
      <td>cbe8d60db6f3c52e653ca73e23a1c34c08127d02</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>RWKV/rwkv-raven-3b</td>
      <td>35.81</td>
      <td>36.69</td>
      <td>59.78</td>
      <td>24.87</td>
      <td>35.60</td>
      <td>57.46</td>
      <td>0.45</td>
      <td>instruction-tuned</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>3.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>1ddeea6a7313c8ba8824645d7aa88d5449458f67</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Ba2han/Tinypus-1.5B</td>
      <td>35.73</td>
      <td>33.45</td>
      <td>57.35</td>
      <td>25.53</td>
      <td>39.35</td>
      <td>57.70</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.45</td>
      <td>0.0</td>
      <td>True</td>
      <td>5bfbd6b5920189dad68217576e0e23be4d2265d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Dans-DiscountModels/ShearedLlama-1.3b-FFT-Test1</td>
      <td>35.71</td>
      <td>32.68</td>
      <td>59.99</td>
      <td>25.69</td>
      <td>36.97</td>
      <td>58.72</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.30</td>
      <td>1.0</td>
      <td>True</td>
      <td>68e43c006a01764d3ff2bcaeaec5289f2ddad36a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>42dot/42dot_LLM-PLM-1.3B</td>
      <td>35.70</td>
      <td>32.42</td>
      <td>56.39</td>
      <td>27.09</td>
      <td>38.68</td>
      <td>58.88</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.44</td>
      <td>16.0</td>
      <td>True</td>
      <td>a72bf57eb02cd4ea4388a344b4a5893aa95698da</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>sartmis1/starcoder-finetune-selfinstruct</td>
      <td>35.65</td>
      <td>31.23</td>
      <td>47.66</td>
      <td>29.52</td>
      <td>41.63</td>
      <td>57.77</td>
      <td>6.07</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b21bd307ea7417185e7dc59557c399a3e4e0092b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>freecs/Llama-3-7b</td>
      <td>35.60</td>
      <td>34.64</td>
      <td>56.39</td>
      <td>24.51</td>
      <td>38.03</td>
      <td>59.67</td>
      <td>0.38</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.91</td>
      <td>0.0</td>
      <td>False</td>
      <td>778db38d13be6ed3384fa049114a95d56cf420d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>freecs/Tiny-Llama-3-7b</td>
      <td>35.60</td>
      <td>34.64</td>
      <td>56.39</td>
      <td>24.51</td>
      <td>38.03</td>
      <td>59.67</td>
      <td>0.38</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.91</td>
      <td>0.0</td>
      <td>True</td>
      <td>778db38d13be6ed3384fa049114a95d56cf420d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>zyh3826/20231206094523-pretrain-Llama-2-13b-hf-76000</td>
      <td>35.58</td>
      <td>31.06</td>
      <td>52.03</td>
      <td>24.43</td>
      <td>44.71</td>
      <td>61.25</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.25</td>
      <td>0.0</td>
      <td>True</td>
      <td>28b3ae089b5610053f2294d24667fe248405f031</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>habanoz/tinyllama-oasst1-top1-instruct-full-lr1-5-v0.1</td>
      <td>35.58</td>
      <td>32.85</td>
      <td>58.16</td>
      <td>25.96</td>
      <td>38.35</td>
      <td>57.70</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>e55b262cbd0ee52f7a4cbda136dbf1a027987c47</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/wizard-vicuna-13B-GPTQ</td>
      <td>35.56</td>
      <td>28.67</td>
      <td>25.94</td>
      <td>25.84</td>
      <td>48.53</td>
      <td>74.74</td>
      <td>9.63</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>99.0</td>
      <td>True</td>
      <td>936a51c0219744d7a9598d0c65a7d18e01660601</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PY007/TinyLlama-1.1B-Chat-v0.3</td>
      <td>35.56</td>
      <td>35.07</td>
      <td>57.70</td>
      <td>25.53</td>
      <td>36.67</td>
      <td>57.70</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>20dd44d78aa09480bf15ca0ecc0c0780951d49a9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>pythainlp/wangchanglm-7.5B-sft-en-sharded</td>
      <td>35.55</td>
      <td>34.47</td>
      <td>59.81</td>
      <td>26.37</td>
      <td>34.15</td>
      <td>58.25</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.50</td>
      <td>0.0</td>
      <td>True</td>
      <td>dd22eaea8be3fcb8c28f61b513a89d1adac00ffd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Dans-DiscountModels/TinyLlama-1.1B-FFT-Test2</td>
      <td>35.53</td>
      <td>34.22</td>
      <td>57.96</td>
      <td>25.54</td>
      <td>36.32</td>
      <td>58.80</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>dfedea2fbf66c27c88cd4b2eeb0ff0f5041e3b59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>HuggingFaceH4/starchat-alpha</td>
      <td>35.49</td>
      <td>31.57</td>
      <td>49.43</td>
      <td>30.76</td>
      <td>43.66</td>
      <td>55.09</td>
      <td>2.43</td>
      <td></td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigcode-openrail-m</td>
      <td>15.52</td>
      <td>225.0</td>
      <td>True</td>
      <td>b693a7a7d52bed1cd7cc0fe00399db838b09c74f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16</td>
      <td>35.46</td>
      <td>32.00</td>
      <td>53.88</td>
      <td>31.43</td>
      <td>38.59</td>
      <td>56.83</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>32.53</td>
      <td>2.0</td>
      <td>False</td>
      <td>1dd7804dbbb547c1be852652ce74568ba41d4e73</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>AtAndDev/ShortKingv0.1</td>
      <td>35.45</td>
      <td>34.22</td>
      <td>54.59</td>
      <td>25.78</td>
      <td>41.64</td>
      <td>56.04</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.42</td>
      <td>0.0</td>
      <td>True</td>
      <td>6cd9b5bc13ee15b5e7e7cfb46477bc6a7c0b5d47</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>habanoz/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-2.2epochs-oasst1-top1-instruct-V1</td>
      <td>35.45</td>
      <td>31.48</td>
      <td>54.40</td>
      <td>25.47</td>
      <td>42.34</td>
      <td>57.54</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>74cd9eba94e77832b3081689fc5c99c37c063790</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nnpy/Nape-0</td>
      <td>35.43</td>
      <td>32.68</td>
      <td>58.68</td>
      <td>24.88</td>
      <td>38.99</td>
      <td>57.30</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.10</td>
      <td>3.0</td>
      <td>True</td>
      <td>47e07bd518b989890a7f694d39e2772e703384c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lizhuang144/starcoder_mirror</td>
      <td>35.43</td>
      <td>31.31</td>
      <td>45.82</td>
      <td>29.29</td>
      <td>43.38</td>
      <td>57.22</td>
      <td>5.53</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>eb5f39bac15ccab9463001aa203e33d49f4ff7cb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>habanoz/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-3epochs-oasst1-top1-instruct-V1</td>
      <td>35.42</td>
      <td>31.40</td>
      <td>54.24</td>
      <td>25.36</td>
      <td>42.47</td>
      <td>57.70</td>
      <td>1.36</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1ec2a1e08eb790b9a32a43053316650921af943</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/openchat_v2_openorca_preview-GPTQ</td>
      <td>35.38</td>
      <td>27.99</td>
      <td>26.06</td>
      <td>24.24</td>
      <td>50.08</td>
      <td>70.64</td>
      <td>13.27</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>15.0</td>
      <td>True</td>
      <td>5a4c2ea612b71d7c00118f796db7189bc1a0c930</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/chopt-1_3b</td>
      <td>35.32</td>
      <td>31.48</td>
      <td>56.63</td>
      <td>25.35</td>
      <td>40.19</td>
      <td>58.25</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>fdd3691978f557baf9d1c20d4ede900c47f7e135</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Walter-Llama-1B</td>
      <td>35.29</td>
      <td>32.85</td>
      <td>61.05</td>
      <td>27.46</td>
      <td>33.93</td>
      <td>56.43</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>2.0</td>
      <td>True</td>
      <td>ae782b5a37bc961d0860e6a8edb10547bb5285d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>vihangd/dopeyplats-1.1b-2T-v1</td>
      <td>35.28</td>
      <td>33.11</td>
      <td>54.31</td>
      <td>24.55</td>
      <td>39.26</td>
      <td>58.80</td>
      <td>1.67</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>3.0</td>
      <td>True</td>
      <td>4ca47b470296de0e7bf3261e377aabaff9ad5c06</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>habanoz/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-4epochs-oasst1-top1-instruct-V1</td>
      <td>35.28</td>
      <td>31.14</td>
      <td>54.31</td>
      <td>25.42</td>
      <td>41.72</td>
      <td>57.77</td>
      <td>1.29</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>7cd6d5ad10180127771e4326772eae3d40fa8445</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Josephgflowers/TinyLlama-3T-Cinder-v1.2</td>
      <td>35.26</td>
      <td>34.39</td>
      <td>56.51</td>
      <td>26.14</td>
      <td>36.78</td>
      <td>57.70</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.10</td>
      <td>2.0</td>
      <td>True</td>
      <td>15c3d37d6d0a6ec7294ce9b5c84851b739f47508</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-4-3b-pile</td>
      <td>35.25</td>
      <td>36.01</td>
      <td>59.66</td>
      <td>24.67</td>
      <td>32.14</td>
      <td>58.33</td>
      <td>0.68</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>7fdda3c5570d4a9711f8f02cc3a20941a5623cd3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/platypus-1_8b</td>
      <td>35.24</td>
      <td>33.28</td>
      <td>50.76</td>
      <td>33.25</td>
      <td>40.73</td>
      <td>52.96</td>
      <td>0.45</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.84</td>
      <td>0.0</td>
      <td>True</td>
      <td>688223a26ae6c3f6102bc3f524594cf21ebb752a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beberik/TinyExperts-v0-4x1B</td>
      <td>35.23</td>
      <td>31.40</td>
      <td>52.29</td>
      <td>25.87</td>
      <td>41.13</td>
      <td>60.14</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>cc-by-nc-4.0</td>
      <td>2.62</td>
      <td>0.0</td>
      <td>True</td>
      <td>cf8144d3b97b9f0154d0a84be2ee758cc60ca33c</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Deacon-1b</td>
      <td>35.21</td>
      <td>32.42</td>
      <td>58.62</td>
      <td>24.89</td>
      <td>35.05</td>
      <td>59.59</td>
      <td>0.68</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>77f16fd4c605fe043033d4335024fb887cedef69</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>facebook/opt-iml-max-1.3b</td>
      <td>35.21</td>
      <td>30.72</td>
      <td>53.81</td>
      <td>27.61</td>
      <td>38.34</td>
      <td>60.22</td>
      <td>0.53</td>
      <td></td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.30</td>
      <td>40.0</td>
      <td>True</td>
      <td>d60fa58f50def19751da2075791da359ca19d273</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Writer/palmyra-base</td>
      <td>35.18</td>
      <td>31.91</td>
      <td>55.39</td>
      <td>27.15</td>
      <td>37.57</td>
      <td>58.09</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>39.0</td>
      <td>True</td>
      <td>df2f3bdb7cbe4295d69cf0cbc35f3ceaf451de82</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/wizard-mega-13B-GPTQ</td>
      <td>35.18</td>
      <td>27.73</td>
      <td>26.01</td>
      <td>24.97</td>
      <td>48.69</td>
      <td>74.74</td>
      <td>8.95</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>103.0</td>
      <td>True</td>
      <td>848bf2514f804799dd28c188e5428d497dc983fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/fairseq-dense-1.3B</td>
      <td>35.16</td>
      <td>31.14</td>
      <td>58.39</td>
      <td>24.98</td>
      <td>37.43</td>
      <td>59.04</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>1.41</td>
      <td>4.0</td>
      <td>False</td>
      <td>20bf1732212ea81adb45b782a25ce69e65a01ad2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ</td>
      <td>35.15</td>
      <td>27.99</td>
      <td>26.10</td>
      <td>25.72</td>
      <td>49.68</td>
      <td>74.51</td>
      <td>6.90</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>16.22</td>
      <td>6.0</td>
      <td>True</td>
      <td>c4246e4b8d3fc77b9fe4ebb1ead61cda4b83575b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>maywell/TinyWand-DPO</td>
      <td>35.13</td>
      <td>31.66</td>
      <td>50.42</td>
      <td>26.22</td>
      <td>45.80</td>
      <td>54.78</td>
      <td>1.90</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.63</td>
      <td>0.0</td>
      <td>True</td>
      <td>7bf42524d664785d92243576b1f7d3b3ed463819</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HWERI/pythia-1.4b-deduped-sharegpt</td>
      <td>35.11</td>
      <td>34.30</td>
      <td>54.49</td>
      <td>24.00</td>
      <td>41.81</td>
      <td>55.25</td>
      <td>0.83</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.42</td>
      <td>2.0</td>
      <td>True</td>
      <td>5b50336208840f557ef3301d841e7994caaa63bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beaugogh/pythia-1.4b-deduped-sharegpt</td>
      <td>35.11</td>
      <td>34.30</td>
      <td>54.49</td>
      <td>24.00</td>
      <td>41.81</td>
      <td>55.25</td>
      <td>0.83</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.42</td>
      <td>0.0</td>
      <td>False</td>
      <td>03dfdc25c111a6a4a16d3da12190697611936426</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>pythainlp/wangchanglm-7.5B-sft-enth</td>
      <td>35.11</td>
      <td>33.79</td>
      <td>58.99</td>
      <td>24.52</td>
      <td>34.90</td>
      <td>57.93</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.50</td>
      <td>6.0</td>
      <td>True</td>
      <td>eeee33ea6778a5e66184eeb4bf4294d4316b1933</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/metharme-1.3b</td>
      <td>35.04</td>
      <td>34.39</td>
      <td>55.94</td>
      <td>25.07</td>
      <td>37.68</td>
      <td>56.43</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.52</td>
      <td>20.0</td>
      <td>True</td>
      <td>62ec4ff53042f692ef0661e54f371747214707a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/falcon-1b-t-sft</td>
      <td>35.02</td>
      <td>32.94</td>
      <td>57.24</td>
      <td>25.26</td>
      <td>38.49</td>
      <td>55.88</td>
      <td>0.30</td>
      <td>instruction-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>3b891a0c37f8fa98301c85fcf34baae876e4cac1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>L-R/LLmRa-1.3B</td>
      <td>35.00</td>
      <td>32.68</td>
      <td>58.77</td>
      <td>23.23</td>
      <td>36.21</td>
      <td>59.04</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>8d5e8bb336cb886e20a7570bc00c2381792338a5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-1.4b-deduped</td>
      <td>35.00</td>
      <td>32.68</td>
      <td>54.96</td>
      <td>25.56</td>
      <td>38.66</td>
      <td>57.30</td>
      <td>0.83</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.40</td>
      <td>20.0</td>
      <td>True</td>
      <td>77f320b24ccae4aa85a5890dbb9514bd11267bb3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>habanoz/TinyLlama-1.1B-intermediate-step-715k-1.5T-lr-5-1epch-airoboros3.1-1k-instruct-V1</td>
      <td>34.98</td>
      <td>30.72</td>
      <td>54.32</td>
      <td>24.78</td>
      <td>41.67</td>
      <td>57.62</td>
      <td>0.76</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>2b961bacab9fcd4bf9a0d6979b024fe23f61555e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>euclaise/falcon_1b_stage3</td>
      <td>34.95</td>
      <td>33.11</td>
      <td>54.08</td>
      <td>25.11</td>
      <td>37.92</td>
      <td>59.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>593e48197e91537b203ba288260f6580b9cbcbe6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TinyLlama/TinyLlama-1.1B-Chat-v0.6</td>
      <td>34.94</td>
      <td>31.66</td>
      <td>55.79</td>
      <td>25.98</td>
      <td>34.72</td>
      <td>59.35</td>
      <td>2.12</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>53.0</td>
      <td>True</td>
      <td>bf9ae1c8bf026667e6f810768de259bb4a7f4777</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Deathsquad10/TinyLlama-1.1B-Remix-V.2</td>
      <td>34.91</td>
      <td>33.19</td>
      <td>56.62</td>
      <td>25.99</td>
      <td>34.64</td>
      <td>58.09</td>
      <td>0.91</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>d60a0c010610de653e55fe498585a44a7202c8b1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>OpenAssistant/stablelm-7b-sft-v7-epoch-3</td>
      <td>34.85</td>
      <td>36.01</td>
      <td>55.81</td>
      <td>25.01</td>
      <td>37.02</td>
      <td>54.85</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>7.00</td>
      <td>66.0</td>
      <td>False</td>
      <td>4c454bfc0e3618b3d574e28ba71369607e637e91</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Jiayi-Pan/Tiny-Vicuna-1B</td>
      <td>34.76</td>
      <td>33.45</td>
      <td>55.92</td>
      <td>25.45</td>
      <td>33.82</td>
      <td>58.41</td>
      <td>1.52</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>4.0</td>
      <td>True</td>
      <td>175336a0000f36b508575ef1a2da05755faf48c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>w95/megachat</td>
      <td>34.75</td>
      <td>30.80</td>
      <td>54.35</td>
      <td>25.55</td>
      <td>39.85</td>
      <td>56.99</td>
      <td>0.99</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>789b259a18ca7b168ced4995138ad6195cd2e8e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/lamini-neo-1.3b</td>
      <td>34.73</td>
      <td>32.76</td>
      <td>49.13</td>
      <td>28.79</td>
      <td>41.05</td>
      <td>56.51</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5c7ecc4d908e7a9469d080308af64ae775c733d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/LaMini-GPT-1.5B</td>
      <td>34.67</td>
      <td>31.40</td>
      <td>48.38</td>
      <td>29.92</td>
      <td>42.47</td>
      <td>55.88</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.50</td>
      <td>35.0</td>
      <td>True</td>
      <td>88ca6f5abe2335bac317e82684e574afdd6046b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WizardLM/WizardCoder-15B-V1.0</td>
      <td>34.64</td>
      <td>32.34</td>
      <td>47.20</td>
      <td>29.43</td>
      <td>41.56</td>
      <td>55.17</td>
      <td>2.12</td>
      <td>fine-tuned</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-openrail-m</td>
      <td>15.00</td>
      <td>701.0</td>
      <td>True</td>
      <td>926ca1b215c4631bc5f8c3e47173381452c23e5c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bn22/tinyllama_frankenmerge</td>
      <td>34.64</td>
      <td>30.20</td>
      <td>51.01</td>
      <td>26.11</td>
      <td>40.18</td>
      <td>58.72</td>
      <td>1.59</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>1.54</td>
      <td>0.0</td>
      <td>True</td>
      <td>086cd453c6d72be4960b6ff15fa5c97dc47993cc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>maywell/TinyWand-SFT</td>
      <td>34.61</td>
      <td>31.40</td>
      <td>49.96</td>
      <td>25.98</td>
      <td>43.08</td>
      <td>55.17</td>
      <td>2.05</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.63</td>
      <td>4.0</td>
      <td>True</td>
      <td>ac1dffae8e8a8324fdac7a266a8ce82e6d033577</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>facebook/opt-1.3b</td>
      <td>34.60</td>
      <td>29.52</td>
      <td>54.53</td>
      <td>24.96</td>
      <td>38.71</td>
      <td>59.75</td>
      <td>0.15</td>
      <td></td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.30</td>
      <td>128.0</td>
      <td>True</td>
      <td>8c7b10754972749675d22364c25c428b29face51</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>0x7194633/fialka-13B-v3</td>
      <td>34.58</td>
      <td>30.97</td>
      <td>48.83</td>
      <td>26.36</td>
      <td>40.58</td>
      <td>59.43</td>
      <td>1.29</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>81bde04594320c0e8174644be352a98c7b073a88</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PY007/TinyLlama-1.1B-Chat-v0.1</td>
      <td>34.57</td>
      <td>32.00</td>
      <td>54.21</td>
      <td>26.71</td>
      <td>39.03</td>
      <td>54.93</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>7abc14e7779eabc3a028bc695342869d0410dea2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T</td>
      <td>34.56</td>
      <td>30.29</td>
      <td>54.84</td>
      <td>26.47</td>
      <td>36.07</td>
      <td>58.33</td>
      <td>1.36</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>28.0</td>
      <td>True</td>
      <td>f62ecb34ea0d4acea9d896040a4616a9538e2f36</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>AI-Sweden-Models/gpt-sw3-1.3b-instruct</td>
      <td>34.54</td>
      <td>30.97</td>
      <td>51.42</td>
      <td>26.17</td>
      <td>40.31</td>
      <td>56.75</td>
      <td>1.59</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>1.44</td>
      <td>3.0</td>
      <td>True</td>
      <td>5f2f03167dedc59192ee02694e07424a890d9206</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>habanoz/TinyLlama-1.1B-step-2T-lr-5-5ep-oasst1-top1-instruct-V1</td>
      <td>34.53</td>
      <td>31.06</td>
      <td>55.02</td>
      <td>26.41</td>
      <td>35.08</td>
      <td>58.01</td>
      <td>1.59</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>3.0</td>
      <td>True</td>
      <td>586c223b539e05fd8a63733c6a540f292460e639</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
      <td>34.53</td>
      <td>25.34</td>
      <td>26.66</td>
      <td>23.36</td>
      <td>49.51</td>
      <td>73.72</td>
      <td>8.57</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>32.53</td>
      <td>4.0</td>
      <td>False</td>
      <td>468225a547a8cb0a62758d813cf9606b58506ab4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/tinyllama-1.1b-chat-v0.3_platypus</td>
      <td>34.50</td>
      <td>30.29</td>
      <td>55.12</td>
      <td>26.13</td>
      <td>39.15</td>
      <td>55.80</td>
      <td>0.53</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.10</td>
      <td>6.0</td>
      <td>True</td>
      <td>0bb6ebe1d41d394bae0ed9107ec8d776d9d76a68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-1.3b</td>
      <td>34.46</td>
      <td>31.14</td>
      <td>51.43</td>
      <td>26.55</td>
      <td>39.24</td>
      <td>57.38</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>34b668ff0acfe56f2d541aa46b385557ee39eb3f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>NYTK/PULI-GPTrio</td>
      <td>34.42</td>
      <td>30.72</td>
      <td>53.49</td>
      <td>24.73</td>
      <td>39.03</td>
      <td>57.77</td>
      <td>0.76</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>c85efce322a0f6d93d64f7b9096525753da6913e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>gpt2-xl</td>
      <td>34.38</td>
      <td>30.29</td>
      <td>51.36</td>
      <td>26.54</td>
      <td>38.54</td>
      <td>58.25</td>
      <td>1.29</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.61</td>
      <td>0.0</td>
      <td>False</td>
      <td>33cdb5c0db5423c1879b1b9f16c352988e8754a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>PY007/TinyLlama-1.1B-intermediate-step-480k-1T</td>
      <td>34.37</td>
      <td>30.89</td>
      <td>52.97</td>
      <td>25.00</td>
      <td>39.55</td>
      <td>57.30</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>098830e58452a0a08f90eb0189ec5925803fd48b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/EverythingLM-13B-16K-GPTQ</td>
      <td>34.37</td>
      <td>29.27</td>
      <td>26.24</td>
      <td>25.40</td>
      <td>48.58</td>
      <td>71.35</td>
      <td>5.38</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>16.23</td>
      <td>13.0</td>
      <td>True</td>
      <td>f14d3df05577f3e1ac35e2c4ec32ce0d39b97508</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>stabilityai/stablelm-base-alpha-7b</td>
      <td>34.37</td>
      <td>32.00</td>
      <td>51.78</td>
      <td>26.21</td>
      <td>40.19</td>
      <td>55.41</td>
      <td>0.61</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.00</td>
      <td>209.0</td>
      <td>True</td>
      <td>38366357b5a45e002af2d254ff3d559444ec2147</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt</td>
      <td>34.32</td>
      <td>34.04</td>
      <td>50.51</td>
      <td>24.66</td>
      <td>41.80</td>
      <td>54.93</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>12.0</td>
      <td>True</td>
      <td>754e0c90ed5d9241fdfd5a188572b3ea2152eaa7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/xglm-4.5B</td>
      <td>34.31</td>
      <td>31.48</td>
      <td>57.95</td>
      <td>25.43</td>
      <td>35.84</td>
      <td>54.93</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>5.08</td>
      <td>14.0</td>
      <td>True</td>
      <td>dc6a67fac06c8bca7860b84656a0cb736293a7a8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-1.3b</td>
      <td>34.31</td>
      <td>30.38</td>
      <td>50.40</td>
      <td>26.14</td>
      <td>39.97</td>
      <td>58.88</td>
      <td>0.08</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.44</td>
      <td>3.0</td>
      <td>True</td>
      <td>b0d9545a27cfaf9a937adac72ed6953f2dc597de</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>winglian/llama-2-4b</td>
      <td>34.23</td>
      <td>31.23</td>
      <td>53.29</td>
      <td>24.22</td>
      <td>38.72</td>
      <td>57.46</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td></td>
      <td>4.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>fbba77f9894cf738ad8d7d08fc6874856fb42507</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>L-R/LLmRa-1.3B_V2</td>
      <td>34.21</td>
      <td>30.46</td>
      <td>53.03</td>
      <td>26.06</td>
      <td>36.46</td>
      <td>59.27</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>a760ebda8f736988eafea879173c5be468ea68d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v2-1_5b</td>
      <td>34.20</td>
      <td>32.59</td>
      <td>53.98</td>
      <td>24.93</td>
      <td>38.77</td>
      <td>54.70</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>5.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>97440ff1b6ef749423758e3495cdce1b5e68ee92</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LoupGarou/WizardCoder-Guanaco-15B-V1.1</td>
      <td>34.19</td>
      <td>32.59</td>
      <td>45.42</td>
      <td>25.88</td>
      <td>42.33</td>
      <td>56.04</td>
      <td>2.88</td>
      <td>fine-tuned</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[apache-2.0]</td>
      <td>15.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>979531c84ec0b4e1712d6a5cec6907126a21e605</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct</td>
      <td>34.15</td>
      <td>32.68</td>
      <td>47.60</td>
      <td>28.63</td>
      <td>40.41</td>
      <td>55.56</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigcode-openrail-m</td>
      <td>0.00</td>
      <td>76.0</td>
      <td>True</td>
      <td>d866b68daa719239dc44979dbf39a608ed6f7bce</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2-xl_lima</td>
      <td>34.12</td>
      <td>31.14</td>
      <td>51.28</td>
      <td>25.43</td>
      <td>38.74</td>
      <td>57.22</td>
      <td>0.91</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7db5b1db521abd7578b95138e737637e0037ca5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>0x7194633/fialka-13B-v3.1</td>
      <td>34.11</td>
      <td>29.95</td>
      <td>47.28</td>
      <td>25.41</td>
      <td>43.03</td>
      <td>58.48</td>
      <td>0.53</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>5d7ce7a375b6641a133485c47542d522d7096f2e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/Walter-Falcon-1B</td>
      <td>34.07</td>
      <td>31.06</td>
      <td>54.92</td>
      <td>24.58</td>
      <td>38.47</td>
      <td>55.41</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>3.0</td>
      <td>True</td>
      <td>9cc302810282152eea488e8649e45dbc332313e3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>habanoz/TinyLlama-1.1B-2T-lr-2e-4-3ep-dolly-15k-instruct-v1</td>
      <td>34.04</td>
      <td>30.55</td>
      <td>53.70</td>
      <td>26.07</td>
      <td>35.85</td>
      <td>58.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>152436a0dd6ca1603b3993bbf08a227ea131f85d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stabilityai/stablelm-tuned-alpha-7b</td>
      <td>34.04</td>
      <td>31.91</td>
      <td>53.59</td>
      <td>24.41</td>
      <td>40.37</td>
      <td>53.12</td>
      <td>0.83</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>7.00</td>
      <td>356.0</td>
      <td>True</td>
      <td>25071b093c15c0d1cb2b2876c6deb621b764fcf5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Josephgflowers/TinyLlama-3T-Cinder-v1.1</td>
      <td>34.03</td>
      <td>34.04</td>
      <td>50.40</td>
      <td>25.75</td>
      <td>37.57</td>
      <td>56.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>False</td>
      <td>1f1bc965140150b7c7a5012abe1e0e0fcce93d68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Deathsquad10/TinyLlama-Remix</td>
      <td>34.00</td>
      <td>31.14</td>
      <td>49.50</td>
      <td>27.34</td>
      <td>40.53</td>
      <td>55.41</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>e5ba81a66f14d23a72053b2d6bdcd31c111d81ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>jzjiao/opt-1.3b-rlhf</td>
      <td>33.99</td>
      <td>28.92</td>
      <td>52.77</td>
      <td>25.39</td>
      <td>37.44</td>
      <td>58.96</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>1.30</td>
      <td>0.0</td>
      <td>False</td>
      <td>5b12df71b21b6b7d76ca9d56de6751f25022e854</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloom-1b7</td>
      <td>33.98</td>
      <td>30.63</td>
      <td>47.60</td>
      <td>27.48</td>
      <td>41.31</td>
      <td>56.04</td>
      <td>0.83</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>1.72</td>
      <td>105.0</td>
      <td>True</td>
      <td>cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/pygmalion-2.7b</td>
      <td>33.98</td>
      <td>32.76</td>
      <td>54.13</td>
      <td>23.28</td>
      <td>37.17</td>
      <td>56.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>creativeml-openrail-m</td>
      <td>2.70</td>
      <td>48.0</td>
      <td>True</td>
      <td>9533805293bc48e8ddfe9dc1940d8cbc5662113e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>LoupGarou/WizardCoder-Guanaco-15B-V1.0</td>
      <td>33.96</td>
      <td>30.46</td>
      <td>45.59</td>
      <td>26.79</td>
      <td>46.39</td>
      <td>53.12</td>
      <td>1.44</td>
      <td>fine-tuned</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[apache-2.0]</td>
      <td>15.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>ab5ea678d63eb2324658dcc8cfae267eabc366ef</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/gogpt-3b-bloom</td>
      <td>33.96</td>
      <td>31.91</td>
      <td>50.32</td>
      <td>25.20</td>
      <td>41.79</td>
      <td>54.38</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>fe942d5d0faca8156eaf456ecdf569993eab8062</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt-2-xl_camel-ai-physics</td>
      <td>33.96</td>
      <td>29.52</td>
      <td>50.62</td>
      <td>26.79</td>
      <td>39.12</td>
      <td>57.54</td>
      <td>0.15</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>e20cf5a8c89441f4dc15fd2af12dbe72b7df8e60</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ</td>
      <td>33.78</td>
      <td>28.41</td>
      <td>26.05</td>
      <td>24.71</td>
      <td>49.54</td>
      <td>68.67</td>
      <td>5.31</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>35.58</td>
      <td>76.0</td>
      <td>True</td>
      <td>cd07cc7c55b46524f61214012653c25226d24c0d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>PY007/TinyLlama-1.1B-intermediate-step-240k-503b</td>
      <td>33.72</td>
      <td>29.27</td>
      <td>49.71</td>
      <td>26.26</td>
      <td>40.17</td>
      <td>56.59</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>213ebf60d7fdd3258fa5574840b06c97a7e8cf5d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/gpt-neo-1.3B</td>
      <td>33.58</td>
      <td>31.23</td>
      <td>48.47</td>
      <td>24.82</td>
      <td>39.63</td>
      <td>56.91</td>
      <td>0.45</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.37</td>
      <td>219.0</td>
      <td>True</td>
      <td>8282180b53cba30a1575e49de1530019e5931739</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>RWKV/rwkv-raven-1b5</td>
      <td>33.56</td>
      <td>31.83</td>
      <td>52.60</td>
      <td>25.96</td>
      <td>37.09</td>
      <td>53.91</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>1.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>571a3bd891ce33f2ee3fc6de09218178edb0dae2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>lxe/Cerebras-GPT-2.7B-Alpaca-SP</td>
      <td>33.50</td>
      <td>30.80</td>
      <td>48.88</td>
      <td>25.12</td>
      <td>40.24</td>
      <td>55.41</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.70</td>
      <td>10.0</td>
      <td>True</td>
      <td>ae7f22e90cb968b0a73355aa2001d6bc7df28477</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>postbot/gpt-neo-1.3B-emailgen</td>
      <td>33.47</td>
      <td>29.95</td>
      <td>47.95</td>
      <td>24.11</td>
      <td>42.55</td>
      <td>56.27</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>True</td>
      <td>accdf0e43c0d1b313bc6d1fb307d67f1921ef3ca</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Josephgflowers/TinyLlama-3T-Cinder-v1</td>
      <td>33.47</td>
      <td>33.53</td>
      <td>46.36</td>
      <td>26.03</td>
      <td>38.32</td>
      <td>56.59</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>False</td>
      <td>52ccb7253aaa88f675ff117917d541ec7e49d56d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BEE-spoke-data/TinyLlama-1.1bee</td>
      <td>33.38</td>
      <td>30.55</td>
      <td>51.80</td>
      <td>24.25</td>
      <td>39.01</td>
      <td>54.46</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>5889ec467cf80a83c4092b55686f8121e81bf001</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>l3utterfly/llama2-3b-distilled-layla-v1</td>
      <td>33.36</td>
      <td>30.46</td>
      <td>46.05</td>
      <td>23.91</td>
      <td>42.14</td>
      <td>57.38</td>
      <td>0.23</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1ba0a65e5262bc134dbc562a9faf80865b0a72f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v1-1_5b</td>
      <td>33.35</td>
      <td>31.66</td>
      <td>49.69</td>
      <td>25.62</td>
      <td>37.08</td>
      <td>55.96</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>5.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>4ac21faec255e3544e96aeb3591c27bdee5ebf45</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/polyglot-ko-12.8b</td>
      <td>33.33</td>
      <td>27.05</td>
      <td>51.68</td>
      <td>26.64</td>
      <td>34.69</td>
      <td>59.75</td>
      <td>0.15</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.06</td>
      <td>69.0</td>
      <td>True</td>
      <td>09dfc839067bf44e7f52976eca8adbc17f04e1b0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MrNJK/gpt2-xl-sft</td>
      <td>33.31</td>
      <td>30.03</td>
      <td>49.17</td>
      <td>25.56</td>
      <td>38.78</td>
      <td>55.56</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>53250831436460254b7ee9afc4014d4d3156b372</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/Quokka_2.7b</td>
      <td>33.26</td>
      <td>31.06</td>
      <td>47.72</td>
      <td>24.80</td>
      <td>40.14</td>
      <td>55.49</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.79</td>
      <td>0.0</td>
      <td>True</td>
      <td>abe5e0f574d32f3234035b6e8c5d68bbb201e03c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cerebras/Cerebras-GPT-2.7B</td>
      <td>33.25</td>
      <td>29.10</td>
      <td>49.29</td>
      <td>25.17</td>
      <td>41.37</td>
      <td>54.14</td>
      <td>0.45</td>
      <td>pretrained</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.70</td>
      <td>40.0</td>
      <td>True</td>
      <td>4383dfd80aafdbcfd0876419d246de51e6cbf7c1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-4-1b5-pile</td>
      <td>33.25</td>
      <td>31.83</td>
      <td>52.25</td>
      <td>25.77</td>
      <td>35.80</td>
      <td>53.83</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>1.00</td>
      <td>5.0</td>
      <td>False</td>
      <td>643585471eaf5821d94dfcb498ab5b94a36b42cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shaohang/Sparse0.5_OPT-1.3</td>
      <td>33.19</td>
      <td>27.13</td>
      <td>48.69</td>
      <td>25.60</td>
      <td>39.11</td>
      <td>58.56</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>06249d582b0cfefac537dd6bee2e578002ffff00</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>shaohang/SparseOPT-1.3B</td>
      <td>33.19</td>
      <td>27.13</td>
      <td>48.69</td>
      <td>25.60</td>
      <td>39.11</td>
      <td>58.56</td>
      <td>0.08</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>06249d582b0cfefac537dd6bee2e578002ffff00</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TurkuNLP/gpt3-finnish-13B</td>
      <td>32.95</td>
      <td>24.66</td>
      <td>46.76</td>
      <td>23.49</td>
      <td>44.47</td>
      <td>58.01</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>BloomModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>True</td>
      <td>ade35fd78ac2c29f7a56ffd3087321d297bb97a9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v2-774m</td>
      <td>32.86</td>
      <td>30.12</td>
      <td>47.68</td>
      <td>25.37</td>
      <td>40.00</td>
      <td>53.99</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.77</td>
      <td>8.0</td>
      <td>True</td>
      <td>0ea894a33e491912cd1a65dde47b4af03f03c4f2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-1b-deduped</td>
      <td>32.78</td>
      <td>29.10</td>
      <td>49.65</td>
      <td>24.27</td>
      <td>38.94</td>
      <td>53.59</td>
      <td>1.14</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.08</td>
      <td>14.0</td>
      <td>True</td>
      <td>7199d8fc61a6d565cd1f3c62bf11525b563e13b2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096</td>
      <td>32.68</td>
      <td>30.63</td>
      <td>52.63</td>
      <td>25.04</td>
      <td>34.96</td>
      <td>52.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.41</td>
      <td>0.0</td>
      <td>True</td>
      <td>657e40fe890c2baa1705b45084a93a70b98842eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>w601sxs/b1ade-1b</td>
      <td>32.59</td>
      <td>28.58</td>
      <td>46.08</td>
      <td>25.11</td>
      <td>41.34</td>
      <td>53.83</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b4b0fd71589e6590089e1ec14a840ecab10894ae</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TFLai/gpt-neo-1.3B-4bit-alpaca</td>
      <td>32.58</td>
      <td>28.24</td>
      <td>46.35</td>
      <td>25.19</td>
      <td>39.26</td>
      <td>56.20</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>4bit</td>
      <td>False</td>
      <td>?</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>True</td>
      <td>137d483d1dc757c81c59bd190016f7c5df01f978</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloom-1b1</td>
      <td>32.47</td>
      <td>28.33</td>
      <td>42.78</td>
      <td>26.70</td>
      <td>41.80</td>
      <td>55.01</td>
      <td>0.23</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>1.06</td>
      <td>48.0</td>
      <td>True</td>
      <td>6f4195539db0eef1c9d010289f32e0645d9a2354</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>rinna/bilingual-gpt-neox-4b-instruction-sft</td>
      <td>32.46</td>
      <td>28.07</td>
      <td>47.50</td>
      <td>23.12</td>
      <td>43.76</td>
      <td>52.33</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>3.80</td>
      <td>16.0</td>
      <td>True</td>
      <td>c20e42bd49a3b1b0d0a07151899a322c4760e871</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Kunhao/pile-7b-250b-tokens</td>
      <td>32.44</td>
      <td>29.27</td>
      <td>46.29</td>
      <td>25.25</td>
      <td>40.49</td>
      <td>52.80</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>caefdf7a7c177905b0b16fbe9d4c7ba08def97c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>MBZUAI/LaMini-GPT-774M</td>
      <td>32.43</td>
      <td>27.65</td>
      <td>43.81</td>
      <td>26.30</td>
      <td>40.26</td>
      <td>56.59</td>
      <td>0.00</td>
      <td></td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.77</td>
      <td>9.0</td>
      <td>True</td>
      <td>4f3bd4b37d249e6aa335be677afd39f417e05b5d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Salesforce/codegen-6B-multi</td>
      <td>32.43</td>
      <td>27.22</td>
      <td>41.11</td>
      <td>25.71</td>
      <td>45.65</td>
      <td>53.91</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>CodeGenForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bsd-3-clause</td>
      <td>6.00</td>
      <td>18.0</td>
      <td>True</td>
      <td>2d58b1e73791e8f0be7ea59c2720dccb6f4d0f06</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>FabbriSimo01/Bloom_1b_Quantized</td>
      <td>32.41</td>
      <td>27.73</td>
      <td>42.83</td>
      <td>26.28</td>
      <td>41.82</td>
      <td>55.64</td>
      <td>0.15</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f31188966c6735bd894edacfee8371a6eaf7dbc7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>deepseek-ai/deepseek-coder-1.3b-instruct</td>
      <td>32.40</td>
      <td>28.58</td>
      <td>39.87</td>
      <td>28.47</td>
      <td>44.02</td>
      <td>52.41</td>
      <td>1.06</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>1.30</td>
      <td>41.0</td>
      <td>True</td>
      <td>e04e04028d6345ab3225644cd615e2573ffb9b8c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>Locutusque/gpt2-large-conversational</td>
      <td>32.33</td>
      <td>26.96</td>
      <td>44.98</td>
      <td>26.33</td>
      <td>39.60</td>
      <td>56.04</td>
      <td>0.08</td>
      <td>RL-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>openrail</td>
      <td>0.77</td>
      <td>4.0</td>
      <td>True</td>
      <td>6674ad1ed9f518054561b866172eb88b7a769413</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>xaviviro/FLOR-1.3B-xat</td>
      <td>32.27</td>
      <td>26.79</td>
      <td>41.63</td>
      <td>26.65</td>
      <td>44.38</td>
      <td>53.43</td>
      <td>0.76</td>
      <td>instruction-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>35cdda0d2b7ade43fd39f3fb4ffad25f0c2730ea</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>rinna/bilingual-gpt-neox-4b-8k</td>
      <td>32.23</td>
      <td>28.58</td>
      <td>43.94</td>
      <td>25.38</td>
      <td>47.48</td>
      <td>47.99</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>3.95</td>
      <td>24.0</td>
      <td>True</td>
      <td>ad56d7fc86db4ad5a7036bc9f80e11cd6f435a60</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Rachneet/gpt2-xl-alpaca</td>
      <td>32.21</td>
      <td>26.79</td>
      <td>43.85</td>
      <td>26.31</td>
      <td>39.40</td>
      <td>56.91</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>1.61</td>
      <td>0.0</td>
      <td>False</td>
      <td>a1a19acc0ef161bfa35f460c15ed3015595714d8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Devio/test-3b</td>
      <td>32.20</td>
      <td>27.65</td>
      <td>44.79</td>
      <td>23.53</td>
      <td>41.42</td>
      <td>55.49</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>3.50</td>
      <td>0.0</td>
      <td>False</td>
      <td>b81c038ee2fa2addd285acde08b1a7ca3cb2854d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>rinna/bilingual-gpt-neox-4b</td>
      <td>32.14</td>
      <td>29.18</td>
      <td>43.73</td>
      <td>23.10</td>
      <td>45.00</td>
      <td>51.85</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>3.95</td>
      <td>24.0</td>
      <td>True</td>
      <td>f02f6f3c8da0093f3c1ce59220409bc2fa9fbb17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>stabilityai/stablelm-tuned-alpha-3b</td>
      <td>32.14</td>
      <td>27.82</td>
      <td>44.06</td>
      <td>23.08</td>
      <td>42.33</td>
      <td>55.01</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[cc-by-nc-sa-4.0]</td>
      <td>3.00</td>
      <td>110.0</td>
      <td>True</td>
      <td>d1c03d2114451d562416b9efe4281d319ceff99e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mohammed-Altaf/Medical-ChatBot</td>
      <td>32.13</td>
      <td>30.55</td>
      <td>38.63</td>
      <td>25.98</td>
      <td>41.25</td>
      <td>55.41</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>9e2d5d7a6189762164690a2fe714b00ce497b253</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mohammed-Altaf/Medical-ChatBot</td>
      <td>31.98</td>
      <td>30.46</td>
      <td>38.60</td>
      <td>25.96</td>
      <td>41.04</td>
      <td>54.85</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>9e2d5d7a6189762164690a2fe714b00ce497b253</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mohammed-Altaf/Medical-ChatBot</td>
      <td>31.87</td>
      <td>30.46</td>
      <td>38.55</td>
      <td>25.91</td>
      <td>41.02</td>
      <td>54.22</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>9e2d5d7a6189762164690a2fe714b00ce497b253</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>PY007/TinyLlama-1.1B-step-50K-105b</td>
      <td>31.86</td>
      <td>25.85</td>
      <td>44.10</td>
      <td>26.78</td>
      <td>39.51</td>
      <td>54.38</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>c1f1ef67c12e4bb85fe0bdf1747c645a202cc118</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikivis/gpt2-large-lora-sft</td>
      <td>31.82</td>
      <td>26.79</td>
      <td>44.15</td>
      <td>25.82</td>
      <td>39.06</td>
      <td>55.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.77</td>
      <td>1.0</td>
      <td>True</td>
      <td>1c0c5a686f3c83692e033416197155557e4d3a0d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>YeungNLP/firefly-bloom-2b6-v2</td>
      <td>31.82</td>
      <td>27.65</td>
      <td>39.23</td>
      <td>25.24</td>
      <td>42.27</td>
      <td>54.78</td>
      <td>1.74</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>2.00</td>
      <td>9.0</td>
      <td>False</td>
      <td>8334b22c39937c0404e09dd22a867e2e2a6fc9e0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0</td>
      <td>31.77</td>
      <td>26.88</td>
      <td>44.78</td>
      <td>23.12</td>
      <td>45.19</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>68282fe744c69ea2e4420a4a6833c0b9168215eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/orca_mini_13B-GPTQ</td>
      <td>31.73</td>
      <td>27.30</td>
      <td>25.85</td>
      <td>25.31</td>
      <td>48.06</td>
      <td>63.77</td>
      <td>0.08</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>16.22</td>
      <td>45.0</td>
      <td>True</td>
      <td>8ec18e5c597da86fa123c08b6e6bef7da6ec7440</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>llm-jp/llm-jp-13b-instruct-full-jaster-v1.0</td>
      <td>31.63</td>
      <td>27.22</td>
      <td>44.70</td>
      <td>23.12</td>
      <td>44.69</td>
      <td>50.04</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>13.0</td>
      <td>True</td>
      <td>b44eac954eac7ddbceba4f510325fd710c977eab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/fairseq-dense-355M</td>
      <td>31.58</td>
      <td>25.43</td>
      <td>46.67</td>
      <td>25.30</td>
      <td>39.19</td>
      <td>52.88</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>None</td>
      <td>0.40</td>
      <td>6.0</td>
      <td>False</td>
      <td>24da1ea670f0638c2df911596e95c764bcd5fb44</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-410m</td>
      <td>31.55</td>
      <td>26.19</td>
      <td>40.85</td>
      <td>27.25</td>
      <td>41.22</td>
      <td>53.12</td>
      <td>0.68</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.51</td>
      <td>14.0</td>
      <td>True</td>
      <td>9879c9b5f8bea9051dcb0e68dff21493d67e9d4f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-13b-chat-longlora-32k-sft</td>
      <td>31.54</td>
      <td>26.54</td>
      <td>26.10</td>
      <td>23.12</td>
      <td>49.16</td>
      <td>64.33</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>21.0</td>
      <td>False</td>
      <td>6f2924e354c3ab035aa2ff7c7e28d0e5327e2667</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v1-774m</td>
      <td>31.51</td>
      <td>28.07</td>
      <td>44.35</td>
      <td>25.91</td>
      <td>36.11</td>
      <td>54.62</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>True</td>
      <td>d3f5401d07965fb13c2cb8b458ffaed9a5a79c2d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikivis/gpt2-large-lora-stf4</td>
      <td>31.50</td>
      <td>26.88</td>
      <td>42.17</td>
      <td>25.53</td>
      <td>40.84</td>
      <td>53.59</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>82eff3a62116fd589ad7319c9d75ff6b12f42f72</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>stabilityai/stablelm-base-alpha-3b</td>
      <td>31.50</td>
      <td>26.45</td>
      <td>42.24</td>
      <td>25.43</td>
      <td>40.50</td>
      <td>53.91</td>
      <td>0.45</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>[cc-by-sa-4.0]</td>
      <td>3.00</td>
      <td>83.0</td>
      <td>True</td>
      <td>99567ccfe45fabe467c71393aa6716106edb83c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-13b-chat-longlora-32k-sft</td>
      <td>31.43</td>
      <td>26.11</td>
      <td>26.17</td>
      <td>23.12</td>
      <td>49.07</td>
      <td>64.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.02</td>
      <td>21.0</td>
      <td>False</td>
      <td>6f2924e354c3ab035aa2ff7c7e28d0e5327e2667</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>facebook/xglm-1.7B</td>
      <td>31.42</td>
      <td>25.85</td>
      <td>45.68</td>
      <td>25.10</td>
      <td>37.21</td>
      <td>53.91</td>
      <td>0.76</td>
      <td></td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.70</td>
      <td>13.0</td>
      <td>True</td>
      <td>d23a5e8e2164af31a84a26756b9b17f925143050</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-coder-ds-1.3b</td>
      <td>31.40</td>
      <td>26.54</td>
      <td>39.49</td>
      <td>24.85</td>
      <td>42.12</td>
      <td>53.04</td>
      <td>2.35</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>True</td>
      <td>8045ddf0d93e582dd6ed80c9f62fd0b6c7d8f806</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikivis/gpt2-large-lora-sft2</td>
      <td>31.33</td>
      <td>26.62</td>
      <td>42.68</td>
      <td>24.72</td>
      <td>40.31</td>
      <td>53.67</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>False</td>
      <td>1244efb5d20765beb54f6b4a4e1426cf6d5daf44</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>nicholasKluge/Aira-2-774M</td>
      <td>31.33</td>
      <td>28.75</td>
      <td>40.80</td>
      <td>25.10</td>
      <td>41.33</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.77</td>
      <td>3.0</td>
      <td>True</td>
      <td>f43044cfe7bf0827a176f0d319c63251c2b29373</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/gpt-2-xl-EvolInstruct</td>
      <td>31.32</td>
      <td>27.39</td>
      <td>38.46</td>
      <td>25.67</td>
      <td>42.76</td>
      <td>53.51</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>3e68735b9bfbca5c2e6a8e4367f003ab3d3c1512</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>FabbriSimo01/Cerebras_1.3b_Quantized</td>
      <td>31.31</td>
      <td>25.94</td>
      <td>38.56</td>
      <td>26.79</td>
      <td>42.67</td>
      <td>53.51</td>
      <td>0.38</td>
      <td></td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>False</td>
      <td>e2126a42a1c8a938553dd513e4adafec41cb793e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cerebras/Cerebras-GPT-1.3B</td>
      <td>31.30</td>
      <td>26.28</td>
      <td>38.54</td>
      <td>26.59</td>
      <td>42.70</td>
      <td>53.43</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.30</td>
      <td>44.0</td>
      <td>True</td>
      <td>5b95400ee8d1e3cc9f79f0dec7182ed9c1009c34</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-410m-deduped</td>
      <td>31.29</td>
      <td>24.83</td>
      <td>41.29</td>
      <td>25.99</td>
      <td>40.95</td>
      <td>54.38</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.51</td>
      <td>20.0</td>
      <td>True</td>
      <td>c4fc8d586d62df497f1f9b69d66d3ca419992d3e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v2-355m</td>
      <td>31.20</td>
      <td>28.33</td>
      <td>40.54</td>
      <td>26.77</td>
      <td>38.76</td>
      <td>52.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.36</td>
      <td>7.0</td>
      <td>True</td>
      <td>f51d310aebc16a9fe0d999d2a437b5faff635716</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>winglian/basilisk-4b</td>
      <td>31.15</td>
      <td>25.85</td>
      <td>39.60</td>
      <td>24.61</td>
      <td>43.74</td>
      <td>53.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>None</td>
      <td>4.00</td>
      <td>4.0</td>
      <td>False</td>
      <td>b91c2e5389f4f0ce2d6042fdce5927343d8dcb06</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/pygmalion-1.3b</td>
      <td>31.14</td>
      <td>28.07</td>
      <td>46.96</td>
      <td>24.12</td>
      <td>37.64</td>
      <td>50.04</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>agpl-3.0</td>
      <td>1.52</td>
      <td>56.0</td>
      <td>True</td>
      <td>bef2c90128c00ff6f16c0f397463423b7d988e17</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikivis/gpt2-large-lora-sft1</td>
      <td>31.01</td>
      <td>24.66</td>
      <td>42.67</td>
      <td>24.89</td>
      <td>39.37</td>
      <td>54.46</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>False</td>
      <td>8e26a8d2dc1661d87a8652c75f00b805d63e7330</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>nicholasKluge/Aira-2-355M</td>
      <td>31.00</td>
      <td>27.56</td>
      <td>38.92</td>
      <td>27.26</td>
      <td>38.53</td>
      <td>53.75</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.36</td>
      <td>0.0</td>
      <td>True</td>
      <td>2479f5b1bb62251ec88e60182ba81390a4c19cf9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>None</td>
      <td>baseline</td>
      <td>31.00</td>
      <td>25.00</td>
      <td>25.00</td>
      <td>25.00</td>
      <td>25.00</td>
      <td>50.00</td>
      <td>0.21</td>
      <td></td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>False</td>
      <td>None</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None</td>
      <td>N/A</td>
      <td>False</td>
      <td>None</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>SummerSigh/GPTNeo350M-Instruct-SFT</td>
      <td>31.00</td>
      <td>25.94</td>
      <td>38.55</td>
      <td>25.76</td>
      <td>45.25</td>
      <td>50.20</td>
      <td>0.30</td>
      <td>instruction-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.46</td>
      <td>0.0</td>
      <td>False</td>
      <td>5e41660ced3edf13c47e933112efd280b710b977</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KaeriJenti/kaori-34b-v4</td>
      <td>30.97</td>
      <td>23.89</td>
      <td>28.97</td>
      <td>25.59</td>
      <td>49.46</td>
      <td>57.22</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>94628cc31b1acac36a464edbfea09949bca139b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KaeriJenti/Kaori-34b-v2</td>
      <td>30.97</td>
      <td>23.89</td>
      <td>28.97</td>
      <td>25.59</td>
      <td>49.46</td>
      <td>57.22</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>34.39</td>
      <td>0.0</td>
      <td>True</td>
      <td>e28a7b27201045a0ca9b1504c5bae53428f2c0ba</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>postbot/emailgen-pythia-410m-deduped</td>
      <td>30.93</td>
      <td>27.90</td>
      <td>40.04</td>
      <td>27.35</td>
      <td>38.20</td>
      <td>52.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.51</td>
      <td>0.0</td>
      <td>True</td>
      <td>e0208b02990c49138350da791f0b6fcb8a65e738</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>AI-Sweden-Models/gpt-sw3-356m-instruct</td>
      <td>30.93</td>
      <td>26.96</td>
      <td>38.01</td>
      <td>25.53</td>
      <td>40.74</td>
      <td>52.57</td>
      <td>1.74</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>0.47</td>
      <td>0.0</td>
      <td>True</td>
      <td>85615b7c700ca7f38c32db8c7efabfa97668f1c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/Quokka_1.3b</td>
      <td>30.86</td>
      <td>27.73</td>
      <td>37.91</td>
      <td>26.66</td>
      <td>40.14</td>
      <td>52.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.42</td>
      <td>0.0</td>
      <td>True</td>
      <td>8a8d738e841a524d658897d89b9e39e7b9272ed8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Corianas/1.3b</td>
      <td>30.76</td>
      <td>27.30</td>
      <td>38.30</td>
      <td>26.77</td>
      <td>39.02</td>
      <td>53.04</td>
      <td>0.15</td>
      <td></td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>1.42</td>
      <td>2.0</td>
      <td>True</td>
      <td>9831f95df82155ef95ff46a505506bf6194b131a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Technoculture/Mediquad-4x7b</td>
      <td>30.74</td>
      <td>27.47</td>
      <td>28.21</td>
      <td>28.66</td>
      <td>49.56</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>19.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>91cd7ebc2a1ec9f88073842ce9cbd92a6943fd55</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cmarkea/bloomz-560m-sft-chat</td>
      <td>30.72</td>
      <td>27.47</td>
      <td>37.05</td>
      <td>23.93</td>
      <td>42.35</td>
      <td>53.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>10.0</td>
      <td>True</td>
      <td>e2bbcbdd534c7d75b7d2f9408e74f6682cf3a05e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>player1537/dolphinette</td>
      <td>30.65</td>
      <td>24.91</td>
      <td>37.33</td>
      <td>25.37</td>
      <td>42.08</td>
      <td>54.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>20529d47b0a82343014727edd1639a9a6a6b09e6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloomz-560m</td>
      <td>30.63</td>
      <td>23.55</td>
      <td>36.31</td>
      <td>25.10</td>
      <td>45.69</td>
      <td>53.12</td>
      <td>0.00</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>89.0</td>
      <td>True</td>
      <td>a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/medalpaca-13B-GPTQ-4bit</td>
      <td>30.62</td>
      <td>29.35</td>
      <td>26.32</td>
      <td>25.44</td>
      <td>49.51</td>
      <td>53.12</td>
      <td>0.00</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>16.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>12190f743a19e91dfe1f5c77abc0c1bf486073dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v1-355m</td>
      <td>30.54</td>
      <td>27.13</td>
      <td>39.07</td>
      <td>27.12</td>
      <td>37.13</td>
      <td>52.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.36</td>
      <td>2.0</td>
      <td>True</td>
      <td>c5f4b5a61e6a66a5c7613164d99a70db5bf7e9a2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>xhyi/PT_GPTNEO350_ATG</td>
      <td>30.46</td>
      <td>25.43</td>
      <td>37.59</td>
      <td>24.79</td>
      <td>43.05</td>
      <td>51.46</td>
      <td>0.45</td>
      <td>fine-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>18.0</td>
      <td>False</td>
      <td>56ab08aaa6802d0f830d42c352d5d536be72811d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TehVenom/DiffMerge-DollyGPT-Pygmalion</td>
      <td>30.45</td>
      <td>23.63</td>
      <td>34.38</td>
      <td>24.41</td>
      <td>46.48</td>
      <td>53.83</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>0.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>6a00b371146d4bd2903890814485ee1b775162e7</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-4-430m-pile</td>
      <td>30.45</td>
      <td>26.71</td>
      <td>40.01</td>
      <td>24.85</td>
      <td>39.58</td>
      <td>51.14</td>
      <td>0.38</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.43</td>
      <td>6.0</td>
      <td>False</td>
      <td>a4f6ec80438d4262d1bbc8f385feb2ef1a4a9d6b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>TheTravellingEngineer/bloom-560m-RLHF-v2</td>
      <td>30.43</td>
      <td>26.45</td>
      <td>37.67</td>
      <td>23.95</td>
      <td>43.51</td>
      <td>50.91</td>
      <td>0.08</td>
      <td>RL-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.56</td>
      <td>3.0</td>
      <td>False</td>
      <td>7128cbfcdaf67f1eff27e45d875c35e7b47618db</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/mistral-inst-v02-dpo</td>
      <td>30.43</td>
      <td>27.90</td>
      <td>26.08</td>
      <td>27.02</td>
      <td>50.80</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d907e70ac8d48e22b85f57b4fb715dfef9f4cfc8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-356m</td>
      <td>30.41</td>
      <td>23.63</td>
      <td>37.05</td>
      <td>25.93</td>
      <td>42.55</td>
      <td>53.04</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>0.47</td>
      <td>1.0</td>
      <td>True</td>
      <td>15ba8a812d3eb265342f62cb0ee9ab6a45fdbd89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>robowaifudev/megatron-gpt2-345m</td>
      <td>30.40</td>
      <td>24.23</td>
      <td>39.18</td>
      <td>24.32</td>
      <td>41.51</td>
      <td>52.96</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.38</td>
      <td>5.0</td>
      <td>True</td>
      <td>b39f8d00fb9f33da4271be2035da848da896a23b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>uukuguy/speechless-codellama-orca-airoboros-13b-0.10e</td>
      <td>30.36</td>
      <td>29.44</td>
      <td>25.71</td>
      <td>25.43</td>
      <td>49.64</td>
      <td>51.93</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>KnutJaegersberg/megatron-gpt2-345m-evol_instruct_v2</td>
      <td>30.31</td>
      <td>26.37</td>
      <td>38.39</td>
      <td>23.60</td>
      <td>41.19</td>
      <td>52.33</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.36</td>
      <td>2.0</td>
      <td>False</td>
      <td>2866eeaaf62014a7a6e939d18b6e27f44df48428</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Felladrin/Llama-160M-Chat-v1</td>
      <td>30.27</td>
      <td>24.74</td>
      <td>35.29</td>
      <td>26.13</td>
      <td>44.16</td>
      <td>51.30</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.16</td>
      <td>4.0</td>
      <td>True</td>
      <td>06b255f112080b26c62e72404331421ffcb95293</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AkiGogikar/KnowledgeNinja-LiteLlama-460Mx6MoE-1T</td>
      <td>30.23</td>
      <td>25.17</td>
      <td>38.45</td>
      <td>26.16</td>
      <td>41.57</td>
      <td>50.04</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>1.97</td>
      <td>0.0</td>
      <td>True</td>
      <td>04c91b4a7759f67dc236e8d61846f0cf756da9fa</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>porkorbeef/Llama-2-13b-sf</td>
      <td>30.22</td>
      <td>29.52</td>
      <td>26.49</td>
      <td>25.98</td>
      <td>48.97</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>06253ee259e6b205c4734ab6ec3fa850737b2110</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-orca-airoboros-13b-0.10e</td>
      <td>30.22</td>
      <td>29.27</td>
      <td>25.74</td>
      <td>25.69</td>
      <td>49.61</td>
      <td>50.99</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>yec019/fbopt-350m-8bit</td>
      <td>30.21</td>
      <td>23.55</td>
      <td>36.60</td>
      <td>26.22</td>
      <td>40.97</td>
      <td>52.64</td>
      <td>1.29</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>unknown</td>
      <td>0.33</td>
      <td>0.0</td>
      <td>True</td>
      <td>305f804054d75a406a85a568ea99dca17cfc998d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098</td>
      <td>30.18</td>
      <td>26.02</td>
      <td>40.39</td>
      <td>24.45</td>
      <td>37.57</td>
      <td>52.41</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.38</td>
      <td>0.0</td>
      <td>True</td>
      <td>e31777c9d3b8c5c9f803b23f49550c009cbdcf6d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>ahxt/llama2_xs_460M_experimental</td>
      <td>30.17</td>
      <td>24.91</td>
      <td>38.47</td>
      <td>26.17</td>
      <td>41.59</td>
      <td>49.88</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.46</td>
      <td>11.0</td>
      <td>False</td>
      <td>c8db281477559f5c969a9be794ce236f8a99e1a0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>ahxt/LiteLlama-460M-1T</td>
      <td>30.16</td>
      <td>24.83</td>
      <td>38.39</td>
      <td>25.96</td>
      <td>41.59</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.46</td>
      <td>128.0</td>
      <td>True</td>
      <td>77b8a976440e7d1ea5a890eaf1e0175b1cac0078</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>kevin009/flyingllama</td>
      <td>30.16</td>
      <td>24.74</td>
      <td>38.35</td>
      <td>26.14</td>
      <td>41.60</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.46</td>
      <td>0.0</td>
      <td>True</td>
      <td>57297d80cdbd91415b76b2ef58d272262a627a98</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>uukuguy/Orca-2-7b-f16</td>
      <td>30.15</td>
      <td>29.61</td>
      <td>25.62</td>
      <td>26.70</td>
      <td>48.36</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f6b2f717467dc12b2b19cad90ed4362153863ad9</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-350M-Erebus</td>
      <td>30.14</td>
      <td>23.81</td>
      <td>34.35</td>
      <td>26.23</td>
      <td>43.58</td>
      <td>52.57</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.33</td>
      <td>12.0</td>
      <td>True</td>
      <td>83ce2f4e78d308968cf7ecd03d86a1f64aea8336</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>TheTravellingEngineer/bloom-1b1-RLHF</td>
      <td>30.14</td>
      <td>27.99</td>
      <td>26.19</td>
      <td>26.86</td>
      <td>48.88</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>65bd72580520a1d4a0c19fcb23f68c1f28464e1b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>bigscience/bloom-560m</td>
      <td>30.13</td>
      <td>24.74</td>
      <td>37.15</td>
      <td>24.22</td>
      <td>42.44</td>
      <td>51.93</td>
      <td>0.30</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>298.0</td>
      <td>True</td>
      <td>4f42c91d806a19ae1a46af6c3fb5f4990d884cd6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yhyhy3/med-orca-instruct-33b</td>
      <td>30.12</td>
      <td>28.84</td>
      <td>25.63</td>
      <td>26.50</td>
      <td>49.26</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>None</td>
      <td>33.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1d636881854338e571825226c712180da06be72c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>porkorbeef/Llama-2-13b</td>
      <td>30.11</td>
      <td>29.35</td>
      <td>26.35</td>
      <td>24.94</td>
      <td>48.32</td>
      <td>51.70</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>06253ee259e6b205c4734ab6ec3fa850737b2110</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>health360/Healix-410M</td>
      <td>30.10</td>
      <td>25.09</td>
      <td>32.02</td>
      <td>24.94</td>
      <td>44.42</td>
      <td>54.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>0.41</td>
      <td>0.0</td>
      <td>False</td>
      <td>df5a3cec54a0bdd22e1644bfe576c7b58eca6bfd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>marcchew/Marcoroni-7B-LaMini-80K</td>
      <td>30.09</td>
      <td>28.75</td>
      <td>26.13</td>
      <td>24.46</td>
      <td>49.71</td>
      <td>51.46</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ea7a283403ec1a40570bfc25f2c4b8fcb089b6bb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>doas/test5</td>
      <td>30.06</td>
      <td>28.41</td>
      <td>26.63</td>
      <td>25.36</td>
      <td>47.34</td>
      <td>52.64</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b0dae937b7137790d8946794375e1affd51c760a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/lamini-cerebras-1.3b</td>
      <td>30.05</td>
      <td>26.88</td>
      <td>37.96</td>
      <td>28.43</td>
      <td>36.45</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>502e70081df53edc8a9156acf5a26a11a9dad8fb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/megatron-GPT-2-345m-EvolInstruct</td>
      <td>30.01</td>
      <td>24.06</td>
      <td>35.12</td>
      <td>24.48</td>
      <td>41.25</td>
      <td>54.78</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.38</td>
      <td>4.0</td>
      <td>True</td>
      <td>dc95fda9f1e51d94870e28751e35410c66563d18</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-350m</td>
      <td>30.01</td>
      <td>23.55</td>
      <td>36.73</td>
      <td>26.02</td>
      <td>40.83</td>
      <td>52.64</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.35</td>
      <td>97.0</td>
      <td>True</td>
      <td>cb32f77e905cccbca1d970436fb0f5e6b58ee3c5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shitshow123/mistral7b_sft_dpo</td>
      <td>30.00</td>
      <td>27.56</td>
      <td>25.53</td>
      <td>24.05</td>
      <td>49.68</td>
      <td>53.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>0191ecaf158b047b4c2f87edfcbe5c144c509d38</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yeen214/test_llama2_ko_7b</td>
      <td>29.99</td>
      <td>29.95</td>
      <td>26.94</td>
      <td>25.62</td>
      <td>49.03</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>45901e1d6ccb22f5ed8aec3f9dd366823fdd1c33</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vikp/phi2</td>
      <td>29.98</td>
      <td>22.87</td>
      <td>30.70</td>
      <td>27.55</td>
      <td>46.10</td>
      <td>52.01</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>9fd01ce09da870fc66af88616d43e53db642ef46</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-codellama-orca-platypus-13b-0.10e</td>
      <td>29.96</td>
      <td>28.92</td>
      <td>25.76</td>
      <td>25.28</td>
      <td>49.22</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>119abfc73f9ce541a40779f167fe21e95faed4e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1</td>
      <td>29.96</td>
      <td>27.99</td>
      <td>26.00</td>
      <td>27.04</td>
      <td>48.59</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>13.00</td>
      <td>20.0</td>
      <td>True</td>
      <td>826e83e411df32f358893ab21f5eae680499ae9a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>PygmalionAI/pygmalion-350m</td>
      <td>29.95</td>
      <td>25.00</td>
      <td>37.80</td>
      <td>25.68</td>
      <td>40.41</td>
      <td>50.28</td>
      <td>0.53</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.35</td>
      <td>51.0</td>
      <td>False</td>
      <td>d65832d913f6b396e2ffb64c373d9383c9da9303</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>senseable/moe-x33</td>
      <td>29.95</td>
      <td>26.19</td>
      <td>26.44</td>
      <td>24.93</td>
      <td>51.14</td>
      <td>50.99</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>58.94</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ce4ba7ce76392721be10c3c05b63853be98b686</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hoskinson-center/proofGPT-v0.1</td>
      <td>29.94</td>
      <td>22.87</td>
      <td>28.66</td>
      <td>25.96</td>
      <td>51.64</td>
      <td>50.43</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>1e4dd330ca90c0ef6d77ca71bd49cbe3d71f26b8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shitshow123/TinyLlama-1.1B-ChatStrong-DPO-PPO</td>
      <td>29.93</td>
      <td>30.38</td>
      <td>25.75</td>
      <td>24.17</td>
      <td>48.87</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.03</td>
      <td>0.0</td>
      <td>False</td>
      <td>8bf7ba0c5552fd7377c75e0ad8e6030a16234f86</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fionazhang/mistral-environment-adapter</td>
      <td>29.93</td>
      <td>29.18</td>
      <td>25.81</td>
      <td>25.38</td>
      <td>48.75</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>28910193dcfc67b615e918c6cd90162b9ef12446</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>marcchew/LaMini-40k-Platypus2-7B</td>
      <td>29.91</td>
      <td>28.50</td>
      <td>26.32</td>
      <td>27.04</td>
      <td>47.39</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e8c03e43eab479a216b5f4f182a711c3624f38bd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/OPT-350M-Nerys-v2</td>
      <td>29.90</td>
      <td>23.63</td>
      <td>35.49</td>
      <td>25.91</td>
      <td>42.08</td>
      <td>51.62</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.35</td>
      <td>5.0</td>
      <td>True</td>
      <td>59b1019c35ab17a7d77ea1ad32b45a8375ba6e89</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>postbot/gpt2-medium-emailgen</td>
      <td>29.87</td>
      <td>26.45</td>
      <td>34.31</td>
      <td>24.10</td>
      <td>43.96</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>[apache-2.0]</td>
      <td>0.38</td>
      <td>2.0</td>
      <td>True</td>
      <td>1b9b03d00b2b300d3c04c37fe3782c180ef51a27</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>rishiraj/cutie</td>
      <td>29.87</td>
      <td>26.96</td>
      <td>27.02</td>
      <td>24.17</td>
      <td>48.42</td>
      <td>52.64</td>
      <td>0.00</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>eab22794d6cf39c945f7dc326c9785a5abf88ddd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>doas/test2</td>
      <td>29.87</td>
      <td>29.61</td>
      <td>26.65</td>
      <td>24.34</td>
      <td>48.49</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f08d224deae510ebf1408ce38bc2610b1e4c77eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>TheTravellingEngineer/bloom-560m-RLHF</td>
      <td>29.86</td>
      <td>24.40</td>
      <td>36.96</td>
      <td>23.63</td>
      <td>40.76</td>
      <td>53.12</td>
      <td>0.30</td>
      <td>RL-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.56</td>
      <td>1.0</td>
      <td>False</td>
      <td>b1769e92f325d8a28e7db1c21f133e6c85b84e78</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>TheBloke/WizardLM-7B-uncensored-GPTQ</td>
      <td>29.86</td>
      <td>28.50</td>
      <td>25.37</td>
      <td>24.85</td>
      <td>50.86</td>
      <td>49.57</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>9.04</td>
      <td>169.0</td>
      <td>True</td>
      <td>cc30c031fd795ee3d3a50312ab4549415bfbdb46</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>uukuguy/speechless-codellama-orca-platypus-13b-0.10e</td>
      <td>29.83</td>
      <td>28.75</td>
      <td>25.88</td>
      <td>25.36</td>
      <td>49.27</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>119abfc73f9ce541a40779f167fe21e95faed4e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>IDEA-CCNL/Ziya-LLaMA-13B-v1</td>
      <td>29.82</td>
      <td>27.73</td>
      <td>25.96</td>
      <td>27.04</td>
      <td>48.65</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>gpl-3.0</td>
      <td>13.00</td>
      <td>260.0</td>
      <td>True</td>
      <td>fccf34387d2c9f2f95ff59ae380e6de3718e41ff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k</td>
      <td>29.81</td>
      <td>25.43</td>
      <td>31.97</td>
      <td>23.43</td>
      <td>47.00</td>
      <td>51.07</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>33.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>b6d0002b10d43ab48aa14e365d9e7b40655ec160</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vihangd/neuralfalcon-1b-v1</td>
      <td>29.80</td>
      <td>26.37</td>
      <td>26.56</td>
      <td>25.93</td>
      <td>49.03</td>
      <td>50.75</td>
      <td>0.15</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f788af66f22a933ad60e732ebaede3dfb5679bd4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>marcchew/Marcoroni-7B-LaMini-40K</td>
      <td>29.78</td>
      <td>27.65</td>
      <td>26.23</td>
      <td>26.92</td>
      <td>47.40</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>27868e4faed5d68d059c8c57dbd3e24e4933ca28</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MayaPH/FinOPT-Franklin</td>
      <td>29.78</td>
      <td>27.73</td>
      <td>24.91</td>
      <td>23.12</td>
      <td>52.40</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>1.32</td>
      <td>5.0</td>
      <td>True</td>
      <td>1b13331834190bfe49a176f1661ba4d8309a5051</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>NEU-HAI/mental-alpaca</td>
      <td>29.77</td>
      <td>28.58</td>
      <td>26.02</td>
      <td>27.04</td>
      <td>48.61</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>f5f24d4a11ed52b4a224f365b6a694cf4e27c1bc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/PM_modelV2</td>
      <td>29.77</td>
      <td>25.09</td>
      <td>26.45</td>
      <td>26.14</td>
      <td>51.36</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.32</td>
      <td>0.0</td>
      <td>False</td>
      <td>4edde209eea33af491206f8651c0c47e70e08289</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>hoskinson-center/proofGPT-v0.1-6.7B</td>
      <td>29.72</td>
      <td>23.29</td>
      <td>28.45</td>
      <td>24.57</td>
      <td>50.87</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.70</td>
      <td>9.0</td>
      <td>True</td>
      <td>02f405f08ca0e5b1aaa90a7c3b11303b5f245102</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Felladrin/Llama-68M-Chat-v1</td>
      <td>29.72</td>
      <td>23.29</td>
      <td>28.27</td>
      <td>25.18</td>
      <td>47.27</td>
      <td>54.30</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>f60895b5cf4e4f2c9387c6c851a4f6691c40ce95</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vihangd/neuralfalcon-1b-v1</td>
      <td>29.72</td>
      <td>26.79</td>
      <td>26.56</td>
      <td>26.22</td>
      <td>48.93</td>
      <td>49.57</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f788af66f22a933ad60e732ebaede3dfb5679bd4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>danielpark/gorani-100k-llama2-13b-instruct</td>
      <td>29.69</td>
      <td>28.07</td>
      <td>26.30</td>
      <td>25.17</td>
      <td>48.96</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>f7d38ee654e505ad7a454f192d5e3d85cb60b3b8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alnrg2arg/test2</td>
      <td>29.69</td>
      <td>27.22</td>
      <td>26.25</td>
      <td>24.64</td>
      <td>50.14</td>
      <td>49.88</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Delta</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>6310110a31918d27d42116942bc2ba3941784ae9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TFLai/gpt2-turkish-uncased</td>
      <td>29.68</td>
      <td>24.49</td>
      <td>25.08</td>
      <td>26.59</td>
      <td>52.30</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>4807e7df1dfb9d60c6d98e3cfeff62cb6b9a1579</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>porkorbeef/Llama-2-13b-12_153950</td>
      <td>29.68</td>
      <td>28.58</td>
      <td>26.58</td>
      <td>20.79</td>
      <td>49.03</td>
      <td>53.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ee9b0cf26f521b5cb2322d743880e8b6bfadb0b7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>marcchew/Platypus-2-7B-LaMini-14K</td>
      <td>29.64</td>
      <td>29.52</td>
      <td>26.15</td>
      <td>23.13</td>
      <td>48.29</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>50199ba51c4d002cc86cf3fb2ac921ec52bf4828</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openbmb/UltraRM-13b</td>
      <td>29.58</td>
      <td>28.16</td>
      <td>26.13</td>
      <td>25.96</td>
      <td>47.91</td>
      <td>49.33</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>4b231ae58c15244e6e15f0d2f4e26ec37b846229</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vicgalle/alpaca-7b</td>
      <td>29.57</td>
      <td>28.07</td>
      <td>25.83</td>
      <td>25.31</td>
      <td>48.49</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>7f22882125208d1f54765c21abf84fd162aa454a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mavihsrr/GetCode-slerp</td>
      <td>29.57</td>
      <td>26.54</td>
      <td>26.20</td>
      <td>23.12</td>
      <td>49.78</td>
      <td>51.78</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>c4e9a5c09be34872e7a1db125d851ae1210d15ff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>SebastianSchramm/Cerebras-GPT-111M-instruction</td>
      <td>29.57</td>
      <td>24.40</td>
      <td>26.05</td>
      <td>25.87</td>
      <td>49.46</td>
      <td>51.62</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.11</td>
      <td>3.0</td>
      <td>False</td>
      <td>09f1ec782ae2243fc605b24eb13ec8d5e4fd2734</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>golaxy/gogpt-560m</td>
      <td>29.56</td>
      <td>26.37</td>
      <td>31.86</td>
      <td>25.29</td>
      <td>43.12</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>82bd8b88b95068eee614a35b790388c5d2415705</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HWERI/pythia-70m-deduped-cleansharegpt</td>
      <td>29.56</td>
      <td>25.68</td>
      <td>25.40</td>
      <td>23.12</td>
      <td>51.15</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.07</td>
      <td>1.0</td>
      <td>True</td>
      <td>6ea42abd94cb0017918f6fe5e71d78bcb7c75548</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/xglm-564M</td>
      <td>29.55</td>
      <td>24.57</td>
      <td>34.64</td>
      <td>25.18</td>
      <td>40.43</td>
      <td>52.25</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.56</td>
      <td>33.0</td>
      <td>True</td>
      <td>f3059f01b98ccc877c673149e0178c0e957660f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Abe13/juniper-certificate-Llama-2-7b-chat-hf</td>
      <td>29.55</td>
      <td>29.10</td>
      <td>27.63</td>
      <td>24.02</td>
      <td>48.23</td>
      <td>48.30</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>90ed388e5503c02f5e6ba8dbc7286687a85ce1c1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Technoculture/Medtulu-4x7B</td>
      <td>29.54</td>
      <td>28.75</td>
      <td>25.74</td>
      <td>24.41</td>
      <td>47.91</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>19.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>b9b38d1b9039038d7d4e5177884bb35300f2fdf1</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>winglian/Llama-2-3b-hf</td>
      <td>29.53</td>
      <td>26.96</td>
      <td>26.52</td>
      <td>23.33</td>
      <td>50.71</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td></td>
      <td>3.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>293f071b223efd7959f9e1fac66285369aaa959d</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ai-forever/rugpt3large_based_on_gpt2</td>
      <td>29.53</td>
      <td>22.61</td>
      <td>32.84</td>
      <td>24.90</td>
      <td>43.39</td>
      <td>53.12</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>62.0</td>
      <td>False</td>
      <td>8201db0de8deb68f25e7309db04d163b71970494</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>bigcode/santacoder</td>
      <td>29.51</td>
      <td>26.28</td>
      <td>25.60</td>
      <td>25.89</td>
      <td>51.24</td>
      <td>48.07</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadCustomModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigcode-openrail-m</td>
      <td>0.00</td>
      <td>315.0</td>
      <td>True</td>
      <td>132eb6b6cedaf579c2f333f1ecd78a16d7e45978</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Cartinoe5930/iDUS</td>
      <td>29.51</td>
      <td>27.73</td>
      <td>26.65</td>
      <td>24.91</td>
      <td>48.58</td>
      <td>49.17</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>680101b4b43110627f526cd4d05856cf624a6ce2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>WangZeJun/bloom-820m-chat</td>
      <td>29.50</td>
      <td>23.38</td>
      <td>34.16</td>
      <td>25.98</td>
      <td>40.32</td>
      <td>53.20</td>
      <td>0.00</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.75</td>
      <td>2.0</td>
      <td>True</td>
      <td>f98b1f9c1bd358dd837d05d443d992c495497606</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>janhq/supermario-v1</td>
      <td>29.49</td>
      <td>27.73</td>
      <td>25.83</td>
      <td>27.04</td>
      <td>47.27</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>22a88e62529dc2cc95991478cd87e6c588237258</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>huggingtweets/bladeecity-jerma985</td>
      <td>29.49</td>
      <td>22.87</td>
      <td>30.53</td>
      <td>26.56</td>
      <td>44.99</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9bf3a0db7f6bc960c51f2c0dc6fb66ed982b0180</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>abhinand/mistral7b-test001</td>
      <td>29.49</td>
      <td>24.66</td>
      <td>26.78</td>
      <td>23.12</td>
      <td>50.07</td>
      <td>52.33</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>52d285a1d9bdd52e50a4cd10b9de43f2f4332517</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Panchovix/airoboros-33b-gpt4-1.2-SuperHOT-8k</td>
      <td>29.48</td>
      <td>24.66</td>
      <td>31.23</td>
      <td>23.13</td>
      <td>47.44</td>
      <td>50.43</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>33.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>47c14f699cbbc9bd24458edd86eb70d87552b623</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>marcchew/test1</td>
      <td>29.48</td>
      <td>27.65</td>
      <td>26.17</td>
      <td>24.55</td>
      <td>48.33</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>7444355ad764584ef05805f58ccf174bb03e0f46</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>codeparrot/codeparrot</td>
      <td>29.48</td>
      <td>21.67</td>
      <td>28.34</td>
      <td>25.55</td>
      <td>50.87</td>
      <td>50.20</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>93.0</td>
      <td>False</td>
      <td>065248a99f051da363b1c2cbf05da943c8b6211b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>kz919/mistral-7b-dpo-open-orca-flan-50k-synthetic-5-models</td>
      <td>29.48</td>
      <td>25.51</td>
      <td>25.52</td>
      <td>26.82</td>
      <td>48.81</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>abe276881262a4571412e6b1bf545c3d61c9e49e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/gpt-neo-125m</td>
      <td>29.47</td>
      <td>22.95</td>
      <td>30.26</td>
      <td>25.97</td>
      <td>45.58</td>
      <td>51.78</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.15</td>
      <td>145.0</td>
      <td>True</td>
      <td>6cb0d322a3a484e99667e7cb240e22f1ac036b99</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>beomi/KoAlpaca-Polyglot-5.8B</td>
      <td>29.46</td>
      <td>27.65</td>
      <td>35.58</td>
      <td>24.72</td>
      <td>39.74</td>
      <td>49.01</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>51.0</td>
      <td>True</td>
      <td>1051dacf82ca9fba0ba4a4ff67f1d98a81ef7a2e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/MusePy-1-2</td>
      <td>29.46</td>
      <td>25.77</td>
      <td>25.94</td>
      <td>25.22</td>
      <td>49.33</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6c1725158a74a41a10f21696a48510d45b4b425b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>porkorbeef/Llama-2-13b-public</td>
      <td>29.45</td>
      <td>29.95</td>
      <td>26.65</td>
      <td>22.74</td>
      <td>49.01</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e1b32a8fcfc0f37fd5f50cf765151897574c73c7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>openbmb/UltraLM-13b</td>
      <td>29.45</td>
      <td>29.44</td>
      <td>25.99</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>13.00</td>
      <td>70.0</td>
      <td>False</td>
      <td>2c732c2899fc329036d97e5c6f0a61eaff19d97d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BEE-spoke-data/smol_llama-220M-GQA</td>
      <td>29.44</td>
      <td>24.83</td>
      <td>29.76</td>
      <td>25.85</td>
      <td>44.55</td>
      <td>50.99</td>
      <td>0.68</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.22</td>
      <td>6.0</td>
      <td>True</td>
      <td>2d144b9a69b3620110e8a14790d383076ac87925</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/lamini-neo-125m</td>
      <td>29.44</td>
      <td>24.57</td>
      <td>30.22</td>
      <td>26.74</td>
      <td>42.85</td>
      <td>52.25</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>f01e73ba67da96f6645be3067158cc493b0cbbcb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KoboldAI/fairseq-dense-125M</td>
      <td>29.41</td>
      <td>24.06</td>
      <td>34.14</td>
      <td>23.98</td>
      <td>43.72</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>XGLMForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.16</td>
      <td>3.0</td>
      <td>False</td>
      <td>c8fb975220512b34e7b4a9fc570ca333ddcaf9b5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bigcode/tiny_starcoder_py</td>
      <td>29.41</td>
      <td>20.99</td>
      <td>28.77</td>
      <td>26.79</td>
      <td>47.68</td>
      <td>51.22</td>
      <td>0.99</td>
      <td>pretrained</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigcode-openrail-m</td>
      <td>0.16</td>
      <td>64.0</td>
      <td>True</td>
      <td>8547527bef0bc927268c1653cce6948c5c242dd1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cerebras/Cerebras-GPT-256M</td>
      <td>29.38</td>
      <td>22.01</td>
      <td>28.99</td>
      <td>26.83</td>
      <td>45.98</td>
      <td>52.49</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.26</td>
      <td>21.0</td>
      <td>True</td>
      <td>d77812ac95aece1f1edef6745ae2a1b325ad01a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-160m-deduped</td>
      <td>29.38</td>
      <td>24.06</td>
      <td>31.39</td>
      <td>24.86</td>
      <td>44.34</td>
      <td>51.38</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.21</td>
      <td>2.0</td>
      <td>True</td>
      <td>582159a2dfe3e712a8d47ae83dec95ae3bde8e7e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>breadlicker45/dough-base-001</td>
      <td>29.37</td>
      <td>23.89</td>
      <td>24.76</td>
      <td>23.13</td>
      <td>53.40</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>e42b65191f97d786eadaba450f1d34baea470734</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>breadlicker45/dough-instruct-base-001</td>
      <td>29.37</td>
      <td>23.89</td>
      <td>24.76</td>
      <td>23.13</td>
      <td>53.40</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.19</td>
      <td>0.0</td>
      <td>False</td>
      <td>3e1b0bf0a887feeb342982eee4f6d8041772a7dd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Deci/DeciCoder-1b</td>
      <td>29.37</td>
      <td>21.16</td>
      <td>31.09</td>
      <td>24.34</td>
      <td>47.05</td>
      <td>50.83</td>
      <td>1.74</td>
      <td>pretrained</td>
      <td>DeciCoderForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.11</td>
      <td>237.0</td>
      <td>True</td>
      <td>af2ef45ef8cbe82eb7eb4074f260412bc14c7b11</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>amazingvince/zephyr-smol_llama-100m-dpo-full</td>
      <td>29.37</td>
      <td>25.00</td>
      <td>28.54</td>
      <td>25.18</td>
      <td>45.75</td>
      <td>51.07</td>
      <td>0.68</td>
      <td>RL-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>3.0</td>
      <td>True</td>
      <td>be3400c89d66ed66f0aa96f1b8131604c118b67b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>yhyhy3/med-orca-instruct-33b</td>
      <td>29.36</td>
      <td>27.39</td>
      <td>25.89</td>
      <td>25.37</td>
      <td>49.60</td>
      <td>47.91</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>33.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>1d636881854338e571825226c712180da06be72c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>chargoddard/SmolLlamix-8x101M-take2</td>
      <td>29.35</td>
      <td>23.98</td>
      <td>28.43</td>
      <td>25.07</td>
      <td>45.87</td>
      <td>52.25</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.40</td>
      <td>0.0</td>
      <td>True</td>
      <td>c9f73e5f63546ca506bbae944ab546a8d8e42d24</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Technoculture/Medorca-4x7b</td>
      <td>29.35</td>
      <td>29.35</td>
      <td>25.72</td>
      <td>24.28</td>
      <td>48.42</td>
      <td>48.30</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>19.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>70fa312cca9f7d966c37ccb52f0ce6a2aa2fd3a0</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Technoculture/Mediquad-orca-20B</td>
      <td>29.35</td>
      <td>29.35</td>
      <td>25.72</td>
      <td>24.28</td>
      <td>48.42</td>
      <td>48.30</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>19.73</td>
      <td>0.0</td>
      <td>False</td>
      <td>6a5a811206e5c255dff8128334c06924347ae324</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>FINDA-FIT/llama-r</td>
      <td>29.34</td>
      <td>21.59</td>
      <td>30.18</td>
      <td>26.13</td>
      <td>45.38</td>
      <td>52.17</td>
      <td>0.61</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6bdde9a227da60c2db803024d5b2e3a53a41cf0b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>BEE-spoke-data/smol_llama-220M-openhermes</td>
      <td>29.34</td>
      <td>25.17</td>
      <td>28.98</td>
      <td>26.17</td>
      <td>43.08</td>
      <td>52.01</td>
      <td>0.61</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>356848c3ced75332f875abf0896e0157a33abd8e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>BEE-spoke-data/zephyr-220m-dpo-full</td>
      <td>29.33</td>
      <td>25.43</td>
      <td>29.15</td>
      <td>26.43</td>
      <td>43.44</td>
      <td>50.99</td>
      <td>0.53</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>58b624e62557ea56b525ead061b6bd92dae37970</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>BEE-spoke-data/zephyr-220m-sft-full</td>
      <td>29.33</td>
      <td>25.26</td>
      <td>29.03</td>
      <td>26.45</td>
      <td>43.23</td>
      <td>51.62</td>
      <td>0.38</td>
      <td>RL-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>49f3c45163e7eb65b9b9deb971f1f69424d5d261</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>nicholasKluge/Aira-2-1B1</td>
      <td>29.32</td>
      <td>23.21</td>
      <td>26.97</td>
      <td>24.86</td>
      <td>50.63</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>2.0</td>
      <td>True</td>
      <td>a53eb20b72ae86441566f99acc204d9bb527bf32</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>yyjjtt/test-model</td>
      <td>29.31</td>
      <td>24.40</td>
      <td>30.17</td>
      <td>25.88</td>
      <td>44.59</td>
      <td>50.83</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3ea8330f61a47f16861415359f09ff0c6a210f27</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TaylorAI/Flash-Llama-30M-20001</td>
      <td>29.31</td>
      <td>23.89</td>
      <td>25.76</td>
      <td>24.09</td>
      <td>51.29</td>
      <td>50.83</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.03</td>
      <td>0.0</td>
      <td>False</td>
      <td>6ff84442217565875450bd7a0457121dcedf6b0b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Harshvir/LaMini-Neo-1.3B-Mental-Health_lora</td>
      <td>29.30</td>
      <td>25.77</td>
      <td>25.67</td>
      <td>27.00</td>
      <td>48.21</td>
      <td>49.17</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>1.30</td>
      <td>1.0</td>
      <td>False</td>
      <td>9f1c45d5ce88a8eaf7ec03b760a4adfb5fda07eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>klosax/pythia-160m-deduped-step92k-193bt</td>
      <td>29.30</td>
      <td>24.23</td>
      <td>32.33</td>
      <td>24.54</td>
      <td>43.49</td>
      <td>50.83</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>0.16</td>
      <td>0.0</td>
      <td>False</td>
      <td>9eac24dad1bd7194e38ce8083a0197cee456456c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bit-dny/MindLLM</td>
      <td>29.28</td>
      <td>22.44</td>
      <td>34.11</td>
      <td>25.50</td>
      <td>43.48</td>
      <td>49.33</td>
      <td>0.83</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b3554c83555a098c94b626c3ab67247bfd024fb5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>bsp-albz/llama2-13b-platypus-ckpt-1000</td>
      <td>29.28</td>
      <td>28.16</td>
      <td>26.55</td>
      <td>23.17</td>
      <td>48.79</td>
      <td>49.01</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d9f3e490df2134784afc3a86f5c617a9bab8db4d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/DialoGPT-large</td>
      <td>29.27</td>
      <td>23.38</td>
      <td>25.77</td>
      <td>23.81</td>
      <td>50.27</td>
      <td>52.41</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>237.0</td>
      <td>True</td>
      <td>04e3e47b52dadbcf7688aa61a7ed0438ecf9184c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>voidful/changpt-bart</td>
      <td>29.27</td>
      <td>28.67</td>
      <td>26.41</td>
      <td>23.12</td>
      <td>47.94</td>
      <td>49.49</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.18</td>
      <td>0.0</td>
      <td>True</td>
      <td>e3d26f736b8b47d5275421be6133b81bef84db7d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MayaPH/FinOPT-Lincoln</td>
      <td>29.27</td>
      <td>26.71</td>
      <td>25.60</td>
      <td>23.00</td>
      <td>50.59</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>0.33</td>
      <td>1.0</td>
      <td>True</td>
      <td>7ddc381fa3968df22f72acb6cf03b75d3ac49661</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>victor123/WizardLM-13B-1.0</td>
      <td>29.27</td>
      <td>28.50</td>
      <td>25.97</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>49.41</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ea86d3c02ca0c2abb086a2145e1e85eaea4a23e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BEE-spoke-data/NanoLlama-GQA-L10-A32_KV8-v13-KI</td>
      <td>29.23</td>
      <td>23.81</td>
      <td>29.39</td>
      <td>25.37</td>
      <td>44.77</td>
      <td>51.14</td>
      <td>0.91</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>29fc3a802ee639be914d2a54fa6d9f595036ecf2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>FabbriSimo01/GPT_Large_Quantized</td>
      <td>29.21</td>
      <td>27.05</td>
      <td>26.29</td>
      <td>24.12</td>
      <td>48.46</td>
      <td>49.33</td>
      <td>0.00</td>
      <td></td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>unknown</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c2df1904aa18de22d03ba0fee925e831d8468898</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2-dolly</td>
      <td>29.21</td>
      <td>22.70</td>
      <td>30.15</td>
      <td>25.81</td>
      <td>44.97</td>
      <td>51.46</td>
      <td>0.15</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>2.0</td>
      <td>True</td>
      <td>7e75e6f4626437305e4d3e7b2aa36f617c517247</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>concedo/Pythia-70M-ChatSalad</td>
      <td>29.20</td>
      <td>20.99</td>
      <td>27.28</td>
      <td>24.78</td>
      <td>49.74</td>
      <td>52.41</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.10</td>
      <td>5.0</td>
      <td>True</td>
      <td>692289413c47c219cf83b1596783a8e9223541eb</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-7b-longlora-32k-ft</td>
      <td>29.20</td>
      <td>27.90</td>
      <td>25.61</td>
      <td>23.08</td>
      <td>49.57</td>
      <td>49.01</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>ab48674ffc55568ffe2a1207ef0e711c2febbaaf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>BEE-spoke-data/smol_llama-220M-open_instruct</td>
      <td>29.19</td>
      <td>25.00</td>
      <td>29.71</td>
      <td>26.11</td>
      <td>44.06</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.22</td>
      <td>0.0</td>
      <td>True</td>
      <td>6d4735f86c74c881857659efb7d981c5f50bee77</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/DialoGPT-small</td>
      <td>29.19</td>
      <td>25.77</td>
      <td>25.79</td>
      <td>25.81</td>
      <td>47.49</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.18</td>
      <td>67.0</td>
      <td>True</td>
      <td>97d0fec744c2cb4d48f5db51d17e3258e185858e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>fionazhang/mistral-environment-all</td>
      <td>29.18</td>
      <td>29.44</td>
      <td>25.89</td>
      <td>23.12</td>
      <td>47.92</td>
      <td>48.70</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba2832b0dbd70860408d7786026549407c951a8a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>buildingthemoon/testfinetunedmodel</td>
      <td>29.18</td>
      <td>25.85</td>
      <td>31.40</td>
      <td>26.07</td>
      <td>40.75</td>
      <td>50.99</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>9efeae0561a9af68ea7f9b26c5184838760372bc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-13b-longlora-16k-ft</td>
      <td>29.17</td>
      <td>25.85</td>
      <td>27.60</td>
      <td>23.10</td>
      <td>48.89</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>13.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>5f0cfdef590fc9bd7642042fb5f1ed9679260b93</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/CodeGPT-small-py</td>
      <td>29.17</td>
      <td>22.70</td>
      <td>27.26</td>
      <td>25.05</td>
      <td>51.23</td>
      <td>48.78</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>21.0</td>
      <td>False</td>
      <td>e5f31df92bfb7b7a808ea8d1c7557488e1bdff7f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>pszemraj/pythia-31m-KI_v1-2048-scratch</td>
      <td>29.15</td>
      <td>23.12</td>
      <td>25.23</td>
      <td>23.12</td>
      <td>51.67</td>
      <td>51.78</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>b29a3229f8d5317adeabafeb20677ec7bea9d703</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>facebook/opt-125m</td>
      <td>29.15</td>
      <td>22.87</td>
      <td>31.47</td>
      <td>26.02</td>
      <td>42.87</td>
      <td>51.62</td>
      <td>0.08</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.12</td>
      <td>94.0</td>
      <td>True</td>
      <td>3d2b5f275bdf882b8775f902e1bfdb790e2cfc32</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ogimgio/gpt-neo-125m-neurallinguisticpioneers</td>
      <td>29.15</td>
      <td>22.44</td>
      <td>30.36</td>
      <td>25.14</td>
      <td>45.64</td>
      <td>51.22</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>515fd7753c5fecbf4a2951f7cebb2846d91324b3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>cerebras/Cerebras-GPT-590M</td>
      <td>29.14</td>
      <td>23.72</td>
      <td>32.40</td>
      <td>25.97</td>
      <td>44.15</td>
      <td>48.15</td>
      <td>0.45</td>
      <td></td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.59</td>
      <td>19.0</td>
      <td>True</td>
      <td>67a653304fd782a34906d59f3795a37f9e053397</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>roneneldan/TinyStories-1M</td>
      <td>29.14</td>
      <td>23.46</td>
      <td>25.23</td>
      <td>24.57</td>
      <td>49.40</td>
      <td>52.17</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>27.0</td>
      <td>False</td>
      <td>8cd14d5339178f1b285f55baee14a0deff7103ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>TheBloke/Llama-2-7b-Chat-AWQ</td>
      <td>29.14</td>
      <td>27.22</td>
      <td>25.48</td>
      <td>24.67</td>
      <td>49.95</td>
      <td>47.51</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>1.13</td>
      <td>0.0</td>
      <td>True</td>
      <td>a065961fd627aa3b3e6dde21e77fd5e20f712189</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>euclaise/crow-1b</td>
      <td>29.12</td>
      <td>25.51</td>
      <td>25.87</td>
      <td>24.80</td>
      <td>48.28</td>
      <td>49.41</td>
      <td>0.83</td>
      <td>RL-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>False</td>
      <td>dbbcb8892474ce1571297eb68b6c1ef971fa0cf8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yash21/TinyYi-7B-Test</td>
      <td>29.11</td>
      <td>26.88</td>
      <td>26.14</td>
      <td>24.41</td>
      <td>46.35</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>e680a6b8244e9a4871aa419e2faca079d4f42381</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yash21/TinyYi-7b-Test</td>
      <td>29.11</td>
      <td>26.88</td>
      <td>26.14</td>
      <td>24.41</td>
      <td>46.35</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>?</td>
      <td>6.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>e680a6b8244e9a4871aa419e2faca079d4f42381</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Aspik101/tulu-7b-instruct-pl-lora_unload</td>
      <td>29.11</td>
      <td>28.67</td>
      <td>26.05</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>48.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>962d4e5d8da5a4ec0ec047b6f8f08f1bb9e509fe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TurkuNLP/gpt3-finnish-large</td>
      <td>29.11</td>
      <td>21.76</td>
      <td>32.88</td>
      <td>24.11</td>
      <td>44.35</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>BloomModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>b9a3dd97387fc70d07010d469888a918842d3449</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>euclaise/gpt-neox-122m-minipile-digits</td>
      <td>29.10</td>
      <td>20.73</td>
      <td>27.03</td>
      <td>25.31</td>
      <td>49.19</td>
      <td>52.33</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc0-1.0</td>
      <td>0.17</td>
      <td>2.0</td>
      <td>True</td>
      <td>3e9187385d31234b04021ddc8b03cbd5cfef9fb4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-7b-longlora-100k-ft</td>
      <td>29.08</td>
      <td>28.16</td>
      <td>25.43</td>
      <td>23.48</td>
      <td>49.06</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>48.0</td>
      <td>False</td>
      <td>242c6469cab41b41d30826e850afa4687e422f24</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>KnutJaegersberg/internlm-20b-llamafied</td>
      <td>29.08</td>
      <td>26.79</td>
      <td>26.40</td>
      <td>25.40</td>
      <td>48.06</td>
      <td>47.83</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>LlamaModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>19.56</td>
      <td>0.0</td>
      <td>False</td>
      <td>f859dfb710431ad6cd7d4e8389297d0f0b196278</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>anas-awadalla/mpt-1b-redpajama-200b</td>
      <td>29.05</td>
      <td>25.77</td>
      <td>26.08</td>
      <td>24.50</td>
      <td>47.57</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MosaicGPT</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>fc98636655efb7c091bbe5d8014eb138ddfc5471</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/gpt-YA-1-1_160M</td>
      <td>29.03</td>
      <td>22.95</td>
      <td>27.29</td>
      <td>26.25</td>
      <td>47.02</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.16</td>
      <td>2.0</td>
      <td>False</td>
      <td>b9b3577df726f7984721e4d73741296db50fa782</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alibidaran/medical_transcription_generator</td>
      <td>29.03</td>
      <td>22.78</td>
      <td>30.60</td>
      <td>23.84</td>
      <td>46.50</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.14</td>
      <td>1.0</td>
      <td>False</td>
      <td>f622239151c89c2db0f1cef495d1b42afd16ce64</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-160m</td>
      <td>29.02</td>
      <td>22.78</td>
      <td>30.34</td>
      <td>24.95</td>
      <td>44.26</td>
      <td>51.54</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.21</td>
      <td>18.0</td>
      <td>True</td>
      <td>50f5173d932e8e61f858120bcb800b97af589f46</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Locutusque/gpt2-conversational-or-qa</td>
      <td>29.01</td>
      <td>21.42</td>
      <td>27.61</td>
      <td>26.51</td>
      <td>47.31</td>
      <td>51.14</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>0.14</td>
      <td>1.0</td>
      <td>True</td>
      <td>f881c740c82ee9bc3191b886ad53f18d741960ea</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abhishek/hepu-o4zf-ravz-7-0</td>
      <td>29.01</td>
      <td>24.49</td>
      <td>25.36</td>
      <td>23.27</td>
      <td>51.67</td>
      <td>49.25</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b73d869edfc259dea27c15d06cf65ee08ec3c2c7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>klosax/pythia-70m-deduped-step44k-92bt</td>
      <td>29.00</td>
      <td>22.10</td>
      <td>28.21</td>
      <td>26.03</td>
      <td>46.12</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>other</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>False</td>
      <td>aac86fff08965d84d8bfc3e7c14559d48b8c4c99</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>chargoddard/SmolLlamix-8x101M</td>
      <td>28.98</td>
      <td>22.70</td>
      <td>28.50</td>
      <td>24.69</td>
      <td>46.09</td>
      <td>51.30</td>
      <td>0.61</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.40</td>
      <td>10.0</td>
      <td>True</td>
      <td>02909f5f76561cc02059b0802d4b894f4a8f9b5a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>huggingtweets/jerma985</td>
      <td>28.97</td>
      <td>21.67</td>
      <td>30.91</td>
      <td>26.57</td>
      <td>44.01</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>816206ad02a397161be78dcb70eeda67e0c53132</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BEE-spoke-data/smol_llama-101M-GQA</td>
      <td>28.97</td>
      <td>23.55</td>
      <td>28.77</td>
      <td>24.24</td>
      <td>45.76</td>
      <td>50.67</td>
      <td>0.83</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>13.0</td>
      <td>True</td>
      <td>cac68b3377fd0a1eb1aca92a2e661d81f59d8b08</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BEE-spoke-data/smol_llama-101M-GQA</td>
      <td>28.96</td>
      <td>23.46</td>
      <td>28.73</td>
      <td>24.35</td>
      <td>45.80</td>
      <td>50.67</td>
      <td>0.76</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>13.0</td>
      <td>True</td>
      <td>cac68b3377fd0a1eb1aca92a2e661d81f59d8b08</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>WizardLM/WizardLM-30B-V1.0</td>
      <td>28.96</td>
      <td>27.39</td>
      <td>25.94</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>48.70</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>74.0</td>
      <td>False</td>
      <td>815e2dd7daabe446c429f3c9f70ef01582528f81</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>concedo/OPT-19M-ChatSalad</td>
      <td>28.96</td>
      <td>24.40</td>
      <td>25.15</td>
      <td>23.12</td>
      <td>51.36</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.02</td>
      <td>16.0</td>
      <td>True</td>
      <td>3930ca6bf3976e9b603815403cb373398ae509e5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>WizardLM/WizardLM-30B-V1.0</td>
      <td>28.95</td>
      <td>27.39</td>
      <td>25.94</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>48.62</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td></td>
      <td>30.00</td>
      <td>74.0</td>
      <td>False</td>
      <td>815e2dd7daabe446c429f3c9f70ef01582528f81</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/DiscordPy</td>
      <td>28.94</td>
      <td>23.29</td>
      <td>26.15</td>
      <td>25.04</td>
      <td>48.16</td>
      <td>50.99</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a5405585aec0b60c5de7d942ccd58421fe9239be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-70m</td>
      <td>28.93</td>
      <td>21.59</td>
      <td>27.29</td>
      <td>25.90</td>
      <td>47.06</td>
      <td>51.46</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ab25ed47af79376eed2baaf8bbb7a192a0c73ff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>anton-l/gpt-j-tiny-random</td>
      <td>28.92</td>
      <td>26.37</td>
      <td>25.76</td>
      <td>24.46</td>
      <td>47.44</td>
      <td>49.49</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTJForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td></td>
      <td>0.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>feea91564dac0081f73aeb6744979c6cfe553fff</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Corianas/590m</td>
      <td>28.88</td>
      <td>24.15</td>
      <td>31.91</td>
      <td>26.61</td>
      <td>42.19</td>
      <td>48.38</td>
      <td>0.08</td>
      <td></td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>ec721c97ef0e6ebfc578ab98b3ff6e2bd19b3e27</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/gpt-YA-1-1_70M</td>
      <td>28.88</td>
      <td>22.53</td>
      <td>27.37</td>
      <td>25.38</td>
      <td>47.09</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>False</td>
      <td>218e8da522cf6fb5566314f37624f27412ae2259</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cyberagent/open-calm-large</td>
      <td>28.88</td>
      <td>20.73</td>
      <td>29.56</td>
      <td>25.23</td>
      <td>46.52</td>
      <td>51.14</td>
      <td>0.08</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>0.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>f9b7a3222967b15169a09bcc86b118ac68a1ad62</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Zangs3011/gpt2_137m_DolphinCoder</td>
      <td>28.87</td>
      <td>21.84</td>
      <td>31.35</td>
      <td>25.40</td>
      <td>41.58</td>
      <td>52.01</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>a558223f774bbd315d1a3890d93ab80dc720fbb1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qblocks/gpt2_137m_DolphinCoder</td>
      <td>28.87</td>
      <td>21.84</td>
      <td>31.35</td>
      <td>25.40</td>
      <td>41.58</td>
      <td>52.01</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>906d8a02bdb444159b189a153f1f5589071ed74e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>microsoft/DialoGPT-medium</td>
      <td>28.86</td>
      <td>24.49</td>
      <td>26.21</td>
      <td>25.84</td>
      <td>47.06</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>265.0</td>
      <td>True</td>
      <td>9d5c5fadcc072b693fb5a5e29416bbf3f503c26c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Quake24/easyTermsSummerizer</td>
      <td>28.86</td>
      <td>25.77</td>
      <td>25.81</td>
      <td>23.12</td>
      <td>47.69</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.41</td>
      <td>0.0</td>
      <td>True</td>
      <td>8df9f96cc14be8f681c40bd1672b3f3540b70e31</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MayaPH/FinOPT-Washington</td>
      <td>28.85</td>
      <td>25.17</td>
      <td>26.25</td>
      <td>24.83</td>
      <td>45.80</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>cdd8a6cde7902de39757cf31d73af1f51df0d8e8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</td>
      <td>28.85</td>
      <td>23.12</td>
      <td>25.66</td>
      <td>23.11</td>
      <td>51.32</td>
      <td>49.88</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>1.0</td>
      <td>True</td>
      <td>01a3cd918dd7c233bc0c3c0c948a9a462a5359d1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/StoryPy</td>
      <td>28.85</td>
      <td>22.35</td>
      <td>26.19</td>
      <td>24.37</td>
      <td>49.10</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.10</td>
      <td>1.0</td>
      <td>False</td>
      <td>5c32081bd3bc1404c2f5b8dbb6f888048bcb7cd7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>postbot/distilgpt2-emailgen</td>
      <td>28.84</td>
      <td>21.76</td>
      <td>27.52</td>
      <td>25.97</td>
      <td>46.17</td>
      <td>51.62</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.09</td>
      <td>2.0</td>
      <td>True</td>
      <td>fe96d63cc2edcbd1ae444ada293cc59d1e01a6ad</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>ethzanalytics/pythia-31m</td>
      <td>28.81</td>
      <td>21.84</td>
      <td>27.00</td>
      <td>24.97</td>
      <td>49.10</td>
      <td>49.72</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>eeea0b6b80603d162fe7de4e80a5bf4a8e9c6207</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yukang/Llama-2-7b-longlora-16k-ft</td>
      <td>28.81</td>
      <td>26.37</td>
      <td>26.37</td>
      <td>23.75</td>
      <td>47.76</td>
      <td>48.62</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c86de31b80866d047e680e08dbd3572e2965d4c5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>ByteWave/Yi-8B-Llama</td>
      <td>28.78</td>
      <td>25.68</td>
      <td>26.79</td>
      <td>24.14</td>
      <td>47.79</td>
      <td>48.30</td>
      <td>0.00</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>8.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>4f3f4d73ff3962487d1c51702b02d795bf1f33a4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nthngdy/pythia-owt2-70m-100k</td>
      <td>28.78</td>
      <td>20.90</td>
      <td>28.34</td>
      <td>25.02</td>
      <td>45.12</td>
      <td>53.28</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>b288893319b6cdce499148f4482043c350116560</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Locutusque/TinyMistral-248M-v2</td>
      <td>28.78</td>
      <td>21.25</td>
      <td>26.56</td>
      <td>23.39</td>
      <td>49.60</td>
      <td>51.85</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.25</td>
      <td>9.0</td>
      <td>True</td>
      <td>937ed7abdec98b7a9868b95e3b8a0d757b902325</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Tincando/fiction_story_generator</td>
      <td>28.77</td>
      <td>23.29</td>
      <td>28.68</td>
      <td>26.72</td>
      <td>43.79</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>377b080cf96e10d50289aa3e1fd79c330265f45a</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/256_5epoch</td>
      <td>28.76</td>
      <td>22.27</td>
      <td>28.99</td>
      <td>26.62</td>
      <td>41.71</td>
      <td>52.72</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>0.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1fe75844a07832acd405a4d989a26f6ab7b1c00</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>abhiramtirumala/DialoGPT-sarcastic-medium</td>
      <td>28.73</td>
      <td>23.29</td>
      <td>25.93</td>
      <td>23.76</td>
      <td>46.04</td>
      <td>53.35</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.14</td>
      <td>1.0</td>
      <td>False</td>
      <td>292596e120591887383011c4520bc5b57e7e8993</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Felladrin/Smol-Llama-101M-Chat-v1</td>
      <td>28.73</td>
      <td>22.87</td>
      <td>28.69</td>
      <td>24.93</td>
      <td>45.76</td>
      <td>50.04</td>
      <td>0.08</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>5.0</td>
      <td>True</td>
      <td>b7c10b0e04ef6f9811ac7f57b3a947546d288eea</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>nthngdy/pythia-owt2-70m-50k</td>
      <td>28.71</td>
      <td>21.50</td>
      <td>28.15</td>
      <td>25.70</td>
      <td>44.50</td>
      <td>52.41</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>9fce9b8252f7891dbd50299a8c3bd71cd25454db</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>distilgpt2</td>
      <td>28.71</td>
      <td>22.27</td>
      <td>27.58</td>
      <td>24.81</td>
      <td>44.49</td>
      <td>53.12</td>
      <td>0.00</td>
      <td></td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.09</td>
      <td>308.0</td>
      <td>True</td>
      <td>38cc92ec43315abd5136313225e95acc5986876c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>HWERI/pythia-70m-deduped-cleansharegpt-en</td>
      <td>28.71</td>
      <td>21.16</td>
      <td>27.16</td>
      <td>25.24</td>
      <td>48.57</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>a97ff56bc68a81a9f6147f1590e53511246d1040</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BEE-spoke-data/verysmol_llama-v11-KIx2</td>
      <td>28.70</td>
      <td>22.70</td>
      <td>27.60</td>
      <td>25.28</td>
      <td>44.75</td>
      <td>51.54</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.06</td>
      <td>3.0</td>
      <td>True</td>
      <td>1cd271d3d62a9e1dc4b7c2978e54806d74705439</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Yash21/SuperChat-7B</td>
      <td>28.67</td>
      <td>23.98</td>
      <td>26.40</td>
      <td>23.24</td>
      <td>47.21</td>
      <td>50.20</td>
      <td>0.99</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b416e3a17d1954d488c29bcc50841dd735527b52</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>RWKV/rwkv-4-169m-pile</td>
      <td>28.64</td>
      <td>23.63</td>
      <td>31.74</td>
      <td>23.18</td>
      <td>41.92</td>
      <td>50.91</td>
      <td>0.45</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.17</td>
      <td>8.0</td>
      <td>False</td>
      <td>46bdc280eb97b6141d5d51a935e0c4870ecaefcc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>postbot/distilgpt2-emailgen-V2</td>
      <td>28.64</td>
      <td>20.99</td>
      <td>26.78</td>
      <td>25.53</td>
      <td>46.51</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.09</td>
      <td>2.0</td>
      <td>True</td>
      <td>9750ba00e79a02e1bf98d3faa3d49b8ae0f8ae63</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>pszemraj/pythia-31m-simplewiki-scratch-bf16</td>
      <td>28.61</td>
      <td>22.78</td>
      <td>25.61</td>
      <td>23.12</td>
      <td>49.65</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>4eaec0542e7609fd3f364cb34491f05d7c61a3d0</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</td>
      <td>28.60</td>
      <td>21.59</td>
      <td>25.79</td>
      <td>24.99</td>
      <td>50.62</td>
      <td>48.62</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>91f011eb99502e667ebc2803f354ce5f5209ccf1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2_open-platypus</td>
      <td>28.58</td>
      <td>22.18</td>
      <td>31.29</td>
      <td>26.19</td>
      <td>40.35</td>
      <td>51.30</td>
      <td>0.15</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>745c1864b752525789cad2b75166c519a327325e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>beomi/KoAlpaca-KoRWKV-6B</td>
      <td>28.57</td>
      <td>23.46</td>
      <td>31.65</td>
      <td>24.89</td>
      <td>39.83</td>
      <td>51.62</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>6.53</td>
      <td>7.0</td>
      <td>True</td>
      <td>427ee72c4350f26de1b287a0c07b842e7d168dbc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KnutJaegersberg/RWKV-4-PilePlus-169M-20230520-done-ctx4096</td>
      <td>28.57</td>
      <td>23.98</td>
      <td>32.25</td>
      <td>23.37</td>
      <td>42.29</td>
      <td>49.17</td>
      <td>0.38</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.13</td>
      <td>0.0</td>
      <td>True</td>
      <td>1134d31db1aee9fc970d3e9dc4e7314fb8bba500</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>yeen214/llama2_7b_small_tuning_v1</td>
      <td>28.56</td>
      <td>22.44</td>
      <td>25.00</td>
      <td>25.51</td>
      <td>48.70</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>3f9b43b4db2da4fe3785071dd52c9fc92aa0801d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aloobun/falcon-1b-cot-t2</td>
      <td>28.56</td>
      <td>24.74</td>
      <td>24.75</td>
      <td>23.12</td>
      <td>48.38</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>FalconForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>fed367016c8adcd499f18eab5e8a9eda71c5e647</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>qiyinmiss/My_GPT2</td>
      <td>28.55</td>
      <td>21.93</td>
      <td>31.59</td>
      <td>25.84</td>
      <td>40.73</td>
      <td>50.51</td>
      <td>0.68</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>4145e280b85ec619906dfc5a624e17cde8ffbea6</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>gpt2</td>
      <td>28.53</td>
      <td>22.01</td>
      <td>31.53</td>
      <td>25.83</td>
      <td>40.69</td>
      <td>50.43</td>
      <td>0.68</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>1597.0</td>
      <td>True</td>
      <td>11c5a3d5811f50298f278a704980280950aedb10</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/Quokka_590m</td>
      <td>28.53</td>
      <td>24.40</td>
      <td>31.61</td>
      <td>25.36</td>
      <td>39.59</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>ae0ac41e9be016f6dceac06821fbf6ebacc7edb9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2_guanaco-dolly-platypus</td>
      <td>28.52</td>
      <td>23.55</td>
      <td>31.03</td>
      <td>26.40</td>
      <td>40.02</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>6bf0a8146cf255c829ec2ad83926c8b80945b431</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2_platypus-dolly-guanaco</td>
      <td>28.51</td>
      <td>23.21</td>
      <td>31.04</td>
      <td>26.16</td>
      <td>40.31</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>bfa144d3eb087e54f1798fd2e2fb17e894cc39d3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>bigcode/gpt_bigcode-santacoder</td>
      <td>28.49</td>
      <td>21.16</td>
      <td>30.84</td>
      <td>24.97</td>
      <td>45.64</td>
      <td>47.83</td>
      <td>0.53</td>
      <td>pretrained</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>openrail</td>
      <td>1.12</td>
      <td>23.0</td>
      <td>True</td>
      <td>291931872cae83498cf984b16319f47f5e9e7a07</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/lamini-cerebras-256m</td>
      <td>28.49</td>
      <td>21.76</td>
      <td>28.70</td>
      <td>26.66</td>
      <td>41.81</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.26</td>
      <td>0.0</td>
      <td>True</td>
      <td>72df0b6d62d64002575687ea2edbb0df05712678</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-126m</td>
      <td>28.49</td>
      <td>22.18</td>
      <td>29.54</td>
      <td>24.43</td>
      <td>44.03</td>
      <td>50.67</td>
      <td>0.08</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>0.19</td>
      <td>0.0</td>
      <td>True</td>
      <td>9272f5a996cf785b8ab706a27d1e7dff1228dc70</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>0x7194633/nanoFialka-v1</td>
      <td>28.48</td>
      <td>22.01</td>
      <td>28.12</td>
      <td>25.03</td>
      <td>45.26</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>96023dad08cf1f9a300c95c8834e28631ca7167b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>blueapple8259/TinyStories-Alpaca</td>
      <td>28.46</td>
      <td>23.98</td>
      <td>24.92</td>
      <td>23.35</td>
      <td>46.68</td>
      <td>51.85</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.07</td>
      <td>2.0</td>
      <td>True</td>
      <td>18e0bde7e72e477757832f0624a0410efc066216</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/gpt-Youtube</td>
      <td>28.46</td>
      <td>23.29</td>
      <td>26.34</td>
      <td>23.54</td>
      <td>48.63</td>
      <td>48.93</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.21</td>
      <td>2.0</td>
      <td>False</td>
      <td>de88554a0212c16fdfeda030afb58f831ebcd895</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BEE-spoke-data/Mixtral-GQA-400m-v2</td>
      <td>28.45</td>
      <td>20.22</td>
      <td>27.78</td>
      <td>26.10</td>
      <td>46.55</td>
      <td>49.96</td>
      <td>0.08</td>
      <td>pretrained</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.01</td>
      <td>1.0</td>
      <td>True</td>
      <td>6f8c51d1bf60da6f8e64ba7fb75fb747d9b124cf</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>AI-Sweden-Models/gpt-sw3-126m</td>
      <td>28.45</td>
      <td>22.01</td>
      <td>29.56</td>
      <td>24.53</td>
      <td>44.07</td>
      <td>50.43</td>
      <td>0.08</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.19</td>
      <td>0.0</td>
      <td>True</td>
      <td>c9d5a2f3fe905557cf0acba496a903255a11907c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Sayan01/Llama-Flan-XL2base</td>
      <td>28.44</td>
      <td>20.65</td>
      <td>25.33</td>
      <td>23.19</td>
      <td>50.58</td>
      <td>50.91</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>2.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>5ffcaeaf5645d96c3f04ed632a820590d3f87c6c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>roneneldan/TinyStories-28M</td>
      <td>28.44</td>
      <td>22.78</td>
      <td>25.83</td>
      <td>23.53</td>
      <td>48.08</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.03</td>
      <td>6.0</td>
      <td>False</td>
      <td>52dabea9997faf578489d619249616926e54ed18</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>EleutherAI/pythia-70m-deduped</td>
      <td>28.44</td>
      <td>21.08</td>
      <td>27.17</td>
      <td>25.26</td>
      <td>47.51</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>20.0</td>
      <td>True</td>
      <td>e93a9faa9c77e5d09219f6c868bfc7a1bd65593c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>budecosystem/boomer-1b</td>
      <td>28.44</td>
      <td>22.78</td>
      <td>31.58</td>
      <td>25.66</td>
      <td>39.17</td>
      <td>50.51</td>
      <td>0.91</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>f8f24b5480fa43f23d858f0eb8d1af1b7ad0af59</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>roneneldan/TinyStories-33M</td>
      <td>28.41</td>
      <td>24.23</td>
      <td>25.69</td>
      <td>23.82</td>
      <td>47.64</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.03</td>
      <td>75.0</td>
      <td>False</td>
      <td>190d22e37cba4b12ddae57d6738a0c65f6ab1aa5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2_platypus-camel_physics</td>
      <td>28.41</td>
      <td>23.04</td>
      <td>31.32</td>
      <td>26.91</td>
      <td>39.56</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>66165ff32ed8de6c39f3524a810f5e97ba6d3347</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2_camel_physics-platypus</td>
      <td>28.41</td>
      <td>23.04</td>
      <td>31.32</td>
      <td>26.91</td>
      <td>39.56</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>66165ff32ed8de6c39f3524a810f5e97ba6d3347</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dpv/finetuned-gpt2-tiny</td>
      <td>28.40</td>
      <td>21.84</td>
      <td>31.60</td>
      <td>25.86</td>
      <td>40.67</td>
      <td>50.12</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>379e02101b4dccba48e7ae792708d2fe7f0bbca2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>SaylorTwift/gpt2_test</td>
      <td>28.40</td>
      <td>21.84</td>
      <td>31.60</td>
      <td>25.86</td>
      <td>40.67</td>
      <td>50.12</td>
      <td>0.30</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>ef61310a16ffda93bf8f6132e02658482ffc2bcc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>behnamsh/gpt2_platypus-camel_physics</td>
      <td>28.40</td>
      <td>22.78</td>
      <td>31.24</td>
      <td>25.87</td>
      <td>38.95</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>cd4d700d13b3bc9371bf45616ef74ac20d165c3d</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>MBZUAI/lamini-cerebras-590m</td>
      <td>28.38</td>
      <td>24.32</td>
      <td>31.58</td>
      <td>25.57</td>
      <td>40.72</td>
      <td>47.91</td>
      <td>0.15</td>
      <td></td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.59</td>
      <td>0.0</td>
      <td>True</td>
      <td>bab37eb7ba63f6ff9f0eb36a85727146b82ae5ed</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cloudyu/Qwen-72Bx2-MoE-120B</td>
      <td>28.37</td>
      <td>25.94</td>
      <td>24.91</td>
      <td>23.27</td>
      <td>48.91</td>
      <td>47.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>120.61</td>
      <td>0.0</td>
      <td>False</td>
      <td>8074fa7f9d97775efe3bcb8b11c04cdcbf3a9810</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>?</td>
      <td>mncai/SGPT-1.3B-insurance-epoch10</td>
      <td>28.37</td>
      <td>24.57</td>
      <td>24.25</td>
      <td>25.23</td>
      <td>45.24</td>
      <td>50.91</td>
      <td>0.00</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>1.30</td>
      <td>0.0</td>
      <td>False</td>
      <td>df685c0bbf838f0627383c28f48e577ee901ba68</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vicgalle/gpt2-alpaca-gpt4</td>
      <td>28.34</td>
      <td>22.61</td>
      <td>31.17</td>
      <td>25.76</td>
      <td>38.04</td>
      <td>52.17</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>19.0</td>
      <td>True</td>
      <td>282e9bd56f0cab5d48e6954793647eecaa0871d9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/Quokka_256m</td>
      <td>28.32</td>
      <td>22.87</td>
      <td>28.84</td>
      <td>26.48</td>
      <td>39.47</td>
      <td>52.25</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>d4e69f714d360d39979eb7b8cbc9decdb7190c88</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>roneneldan/TinyStories-8M</td>
      <td>28.31</td>
      <td>24.66</td>
      <td>25.03</td>
      <td>23.33</td>
      <td>46.54</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.01</td>
      <td>4.0</td>
      <td>False</td>
      <td>8612e3b15c66ffa94eaa6ee0de5c96edd2d630af</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>xzuyn/GPT-2-SlimOrcaDeduped-airoboros-3.1-MetaMathQA-SFT-124M</td>
      <td>28.30</td>
      <td>24.57</td>
      <td>29.43</td>
      <td>25.82</td>
      <td>38.84</td>
      <td>49.01</td>
      <td>2.12</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>e12dbd27ee148ce4af6faf742aa936d38c26536f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>ethzanalytics/pythia-31m</td>
      <td>28.30</td>
      <td>19.97</td>
      <td>26.34</td>
      <td>24.27</td>
      <td>50.12</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>8a3c2f1555de8a3c53d67d73b5d0d53a66a6c6c2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v2-124m</td>
      <td>28.30</td>
      <td>23.98</td>
      <td>31.10</td>
      <td>25.29</td>
      <td>38.98</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>5.0</td>
      <td>True</td>
      <td>bc719f990748ea72be4b6c270df34fc3d37291dc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>instructkr/ko-wand-136M</td>
      <td>28.29</td>
      <td>21.33</td>
      <td>25.00</td>
      <td>23.58</td>
      <td>50.68</td>
      <td>49.17</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>[apache-2.0]</td>
      <td>0.14</td>
      <td>1.0</td>
      <td>True</td>
      <td>86cc9bf25c45c60cc16ea6002609121fdcd83609</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>huggingtweets/gladosystem</td>
      <td>28.29</td>
      <td>24.40</td>
      <td>29.71</td>
      <td>23.18</td>
      <td>41.78</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>02a1bbcee7b584ace743b2fe4885cc0eaf2179ac</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/lamini-cerebras-111m</td>
      <td>28.29</td>
      <td>22.10</td>
      <td>27.12</td>
      <td>25.51</td>
      <td>43.79</td>
      <td>51.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>e8e347b02f9305e4bc144eb9be2821c518d43183</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>gpt2</td>
      <td>28.28</td>
      <td>21.59</td>
      <td>31.58</td>
      <td>25.40</td>
      <td>41.15</td>
      <td>49.57</td>
      <td>0.38</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>1597.0</td>
      <td>True</td>
      <td>11c5a3d5811f50298f278a704980280950aedb10</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>pszemraj/pythia-31m-simplewiki-2048</td>
      <td>28.27</td>
      <td>22.18</td>
      <td>25.55</td>
      <td>23.12</td>
      <td>49.37</td>
      <td>49.41</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>1.0</td>
      <td>True</td>
      <td>95d47818055661250b55144c7d9beaf05dc126d8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>cyberagent/open-calm-7b</td>
      <td>28.21</td>
      <td>20.48</td>
      <td>30.65</td>
      <td>25.22</td>
      <td>44.15</td>
      <td>48.54</td>
      <td>0.23</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-sa-4.0</td>
      <td>7.00</td>
      <td>196.0</td>
      <td>True</td>
      <td>276a5fb67510554e11ef191a2da44c919acccdf5</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>crumb/gpt2023</td>
      <td>28.20</td>
      <td>21.93</td>
      <td>31.11</td>
      <td>25.05</td>
      <td>40.71</td>
      <td>50.12</td>
      <td>0.30</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>15.0</td>
      <td>True</td>
      <td>e3620b53d164529575db66d9d4f4382311dd713c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>AI-Sweden-Models/gpt-sw3-126m-instruct</td>
      <td>28.20</td>
      <td>23.38</td>
      <td>29.88</td>
      <td>23.78</td>
      <td>42.65</td>
      <td>48.54</td>
      <td>0.99</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>other</td>
      <td>0.19</td>
      <td>2.0</td>
      <td>True</td>
      <td>5f353e1eb1b579ef62e10302b7c0bb843ee8eba9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Felladrin/TinyMistral-248M-SFT-v4</td>
      <td>28.20</td>
      <td>24.91</td>
      <td>28.15</td>
      <td>26.04</td>
      <td>39.56</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.25</td>
      <td>18.0</td>
      <td>True</td>
      <td>ec0ff201527cd9b50eb9b4fc754d6c08f1242ea1</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>beomi/KoRWKV-6B</td>
      <td>28.19</td>
      <td>22.10</td>
      <td>32.18</td>
      <td>24.69</td>
      <td>39.05</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>RwkvForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>6.53</td>
      <td>3.0</td>
      <td>True</td>
      <td>541600070459baf0f1be9560181d5ceb77794085</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>roneneldan/TinyStories-3M</td>
      <td>28.19</td>
      <td>22.01</td>
      <td>25.58</td>
      <td>24.99</td>
      <td>47.33</td>
      <td>49.25</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPTNeoForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>0.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>cfaf26ec85ecdfc1bd7c2638104cce55cb67f894</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Locutusque/TinyMistral-248M-Instruct</td>
      <td>28.19</td>
      <td>24.32</td>
      <td>27.52</td>
      <td>25.18</td>
      <td>41.94</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.25</td>
      <td>7.0</td>
      <td>True</td>
      <td>32a9317176bd8562bbb6497eef43a95f2c0261c3</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>pszemraj/distilgpt2-HC3</td>
      <td>28.18</td>
      <td>24.66</td>
      <td>27.99</td>
      <td>23.95</td>
      <td>42.10</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.09</td>
      <td>1.0</td>
      <td>True</td>
      <td>6f9ad473a3793d0271df34a55882ad30846a6788</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>lgaalves/gpt2-dolly</td>
      <td>28.18</td>
      <td>21.76</td>
      <td>30.77</td>
      <td>24.66</td>
      <td>42.22</td>
      <td>49.57</td>
      <td>0.08</td>
      <td>instruction-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.12</td>
      <td>2.0</td>
      <td>True</td>
      <td>52fcf61a8eef255a981be6efde187481086e1a48</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>BEE-spoke-data/smol_llama-81M-tied</td>
      <td>28.17</td>
      <td>22.18</td>
      <td>29.33</td>
      <td>24.06</td>
      <td>43.97</td>
      <td>49.25</td>
      <td>0.23</td>
      <td>pretrained</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.08</td>
      <td>4.0</td>
      <td>True</td>
      <td>096e543bd36d067a819ea867c66f14d946849053</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>MBZUAI/LaMini-GPT-124M</td>
      <td>28.01</td>
      <td>24.32</td>
      <td>30.82</td>
      <td>24.99</td>
      <td>36.57</td>
      <td>51.38</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>0.12</td>
      <td>16.0</td>
      <td>True</td>
      <td>5c67c8c03c08e82d6138ce2a1eddf5317fac3a6b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Locutusque/LocutusqueXFelladrin-TinyMistral248M-Instruct</td>
      <td>27.98</td>
      <td>24.74</td>
      <td>27.79</td>
      <td>26.12</td>
      <td>40.12</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>True</td>
      <td>apache-2.0</td>
      <td>0.25</td>
      <td>5.0</td>
      <td>True</td>
      <td>646fc1eaf46fcd7f1f9141da8a259715ff7528be</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shitshow123/tinylamma-20000</td>
      <td>27.95</td>
      <td>23.81</td>
      <td>32.45</td>
      <td>25.37</td>
      <td>34.87</td>
      <td>51.22</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>51b5eea5679f69d00571d94fb87ee12496cb8104</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>TurkuNLP/gpt3-finnish-small</td>
      <td>27.95</td>
      <td>20.48</td>
      <td>28.09</td>
      <td>24.47</td>
      <td>46.47</td>
      <td>48.22</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>BloomModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>9.0</td>
      <td>True</td>
      <td>20a19af481bf59f38610a2977b2b513e9df51e3a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Mikivis/xuanxuan</td>
      <td>27.88</td>
      <td>23.46</td>
      <td>31.12</td>
      <td>26.27</td>
      <td>35.97</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba6ae2b347bc613ae38980e059ec8c5ec8b26038</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>vicgalle/gpt2-alpaca</td>
      <td>27.86</td>
      <td>22.87</td>
      <td>31.14</td>
      <td>26.26</td>
      <td>36.22</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>0.14</td>
      <td>9.0</td>
      <td>True</td>
      <td>e06875a588f7b3386c18a6efdc8cc7583d95b21b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aisquared/dlite-v1-124m</td>
      <td>27.86</td>
      <td>24.32</td>
      <td>31.16</td>
      <td>25.08</td>
      <td>36.38</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>f6fd5f3960f31881e6cee23f5a872ecc80b40283</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>psyche/kogpt</td>
      <td>27.83</td>
      <td>21.16</td>
      <td>28.11</td>
      <td>26.56</td>
      <td>42.06</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.39</td>
      <td>3.0</td>
      <td>True</td>
      <td>4c02d48f548103ba53a5e481b8aa81bf7a259287</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>cerebras/Cerebras-GPT-111M</td>
      <td>27.75</td>
      <td>20.22</td>
      <td>26.73</td>
      <td>25.51</td>
      <td>46.31</td>
      <td>47.75</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>?</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.11</td>
      <td>69.0</td>
      <td>True</td>
      <td>d2b54d7af419055f204690fe0385959616a1723e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>Locutusque/TinyMistral-248m</td>
      <td>27.73</td>
      <td>22.87</td>
      <td>28.02</td>
      <td>23.15</td>
      <td>42.52</td>
      <td>49.80</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>True</td>
      <td>8f03f72bca0542aa164c29ba41f02cba6f9d7748</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>ai-forever/mGPT</td>
      <td>27.61</td>
      <td>23.81</td>
      <td>26.37</td>
      <td>25.17</td>
      <td>39.62</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>201.0</td>
      <td>True</td>
      <td>40897bd7c8b47a76802c411108ca6220438b8b40</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>huashiyiqike/testmodel</td>
      <td>27.60</td>
      <td>19.71</td>
      <td>26.68</td>
      <td>25.28</td>
      <td>43.72</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>0.15</td>
      <td>1.0</td>
      <td>True</td>
      <td>1ac5d244402e2433b6abfcff1fe65e84af15766b</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Corianas/111m</td>
      <td>27.60</td>
      <td>19.71</td>
      <td>26.68</td>
      <td>25.28</td>
      <td>43.72</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>0.15</td>
      <td>2.0</td>
      <td>True</td>
      <td>ee58d79e27f8b9e3984aab29235c5851d2be01d4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Felladrin/TinyMistral-248M-SFT-v3</td>
      <td>27.45</td>
      <td>21.93</td>
      <td>28.26</td>
      <td>22.91</td>
      <td>40.03</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>True</td>
      <td>7a4787dfed21a432924d24575e6c65a97e1dd98a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>databricks/dolly-v2-3b</td>
      <td>22.83</td>
      <td>25.26</td>
      <td>26.55</td>
      <td>24.70</td>
      <td>0.00</td>
      <td>59.43</td>
      <td>1.06</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>3.00</td>
      <td>256.0</td>
      <td>True</td>
      <td>f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>v1olet/v1olet_marcoroni-go-bruins-7B</td>
      <td>22.43</td>
      <td>29.10</td>
      <td>28.30</td>
      <td>25.09</td>
      <td>0.00</td>
      <td>52.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>05868b30f81600b703a1029c4806683f7f5a89fc</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>v1olet/v1olet_mistral_7B</td>
      <td>22.16</td>
      <td>29.18</td>
      <td>28.13</td>
      <td>26.24</td>
      <td>0.00</td>
      <td>49.41</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>aaf2184ac642ce0171d2703bdb3db8fde855e4c9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>FabbriSimo01/Facebook_opt_1.3b_Quantized</td>
      <td>21.78</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>59.67</td>
      <td>0.15</td>
      <td>pretrained</td>
      <td>OPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>1.30</td>
      <td>0.0</td>
      <td>False</td>
      <td>7ef72ccee9d91d06967809e4e63ffbef62a9ad4a</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>KevinNi/mistral-class-bio-tutor</td>
      <td>21.59</td>
      <td>28.07</td>
      <td>28.02</td>
      <td>23.79</td>
      <td>0.00</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>c0e782c571209e1238e3a3170dcd187f9a436df2</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>maximuslee07/llama-2-13b-rockwellautomation</td>
      <td>21.48</td>
      <td>28.16</td>
      <td>25.77</td>
      <td>25.14</td>
      <td>0.00</td>
      <td>49.80</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Delta</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2bec12a875dd8cb22550c02082ae81e138018ebe</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>jslin09/bloom-560m-finetuned-fraud</td>
      <td>21.37</td>
      <td>26.96</td>
      <td>28.87</td>
      <td>24.03</td>
      <td>0.00</td>
      <td>48.38</td>
      <td>0.00</td>
      <td></td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>1.0</td>
      <td>True</td>
      <td>5571f87f557b909e863005c6e3870bc2e77341a7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ahnyeonchan/OpenOrca-AYT-13B</td>
      <td>21.35</td>
      <td>27.22</td>
      <td>26.03</td>
      <td>25.11</td>
      <td>0.00</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Delta</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>1357abceda30e8389007a023907824cc3a11e397</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Andron00e/YetAnother_Open-Llama-3B-LoRA</td>
      <td>21.29</td>
      <td>25.94</td>
      <td>25.76</td>
      <td>24.65</td>
      <td>0.00</td>
      <td>51.38</td>
      <td>0.00</td>
      <td></td>
      <td>LlamaForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>3.43</td>
      <td>0.0</td>
      <td>False</td>
      <td>52c5cb0178831908ed0571f1750fcb0f0fb125f9</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>Andron00e/YetAnother_Open-Llama-3B-LoRA-OpenOrca</td>
      <td>21.20</td>
      <td>25.94</td>
      <td>25.76</td>
      <td>24.65</td>
      <td>0.00</td>
      <td>50.83</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>07d9d32cd091148295d4e13802ba63486599aff4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Dampish/Dante-2.8B</td>
      <td>21.12</td>
      <td>25.09</td>
      <td>26.05</td>
      <td>24.51</td>
      <td>0.00</td>
      <td>51.07</td>
      <td>0.00</td>
      <td></td>
      <td>GPTNeoXForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td>cc-by-nc-4.0</td>
      <td>2.80</td>
      <td>0.0</td>
      <td>True</td>
      <td>fb2a8f95c0286f957c830af640fd5c989081e8e4</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>BreadAi/MuseCan</td>
      <td>21.06</td>
      <td>28.07</td>
      <td>25.00</td>
      <td>24.19</td>
      <td>0.00</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPT2LMHeadModel</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>None</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f441866d78feaead3dede6efd9e23990bb74c21e</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>team-lucid/mptk-1b</td>
      <td>20.84</td>
      <td>22.70</td>
      <td>25.48</td>
      <td>27.11</td>
      <td>0.00</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>MptForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>aea467410ae0cead4fded6b98a3575e92b22862f</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>mindy-labs/mindy-7b</td>
      <td>20.52</td>
      <td>23.63</td>
      <td>25.82</td>
      <td>24.15</td>
      <td>0.00</td>
      <td>49.49</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b859eae30d69b065060e268b4e918601dabcc36c</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alnrg2arg/test</td>
      <td>20.45</td>
      <td>23.04</td>
      <td>25.23</td>
      <td>23.28</td>
      <td>0.00</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>True</td>
      <td>mit</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>9750a39173e3052074bf940c1e41badf993ee960</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>amu/zen</td>
      <td>20.33</td>
      <td>23.98</td>
      <td>25.08</td>
      <td>23.26</td>
      <td>0.00</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>1.0</td>
      <td>True</td>
      <td>2d41f336037eadddf1dcd75d622813ab8e956067</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>alnrg2arg/test_wanda_240109</td>
      <td>20.24</td>
      <td>22.95</td>
      <td>25.26</td>
      <td>23.32</td>
      <td>0.00</td>
      <td>49.88</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td>mit</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>1c11cdf3b6aa1e175dc76609affbaec7da4494ab</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>kyujinpy/Sakura-SOLAR-Instruct-DPO-v1</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>RL-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>56cd9f8992e05fa799b33db83e967d1b5f0d5724</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>clibrain/Llama-2-ft-instruct-es</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>17.0</td>
      <td>True</td>
      <td>42f07d6a86fac5574febb7b8fa13c3b1e14fcebd</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>ewqr2130/mistral-moe-scratch</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>efda3aed6b33db48bde86d2d3a7200895350e490</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🟦</td>
      <td>TheTravellingEngineer/bloom-1b1-RLHF-v2</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>RL-tuned</td>
      <td>BloomForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td></td>
      <td>1.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>05f7f0fd82fb3a5798d4bb284b6c10dd9d380f22</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>dfurman/llama-2-13b-dolphin-peft</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>Unknown</td>
      <td>Adapter</td>
      <td>float16</td>
      <td>False</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>5d17f6b5f394f0745bd4377c8a1290c68051e351</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shitshow123/moe_scratch</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MixtralForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>46.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>86bd0f657bfc81b8c42bcc2c958949e21258f97e</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>shitshow123/stablelm_sft_dpo</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.87</td>
      <td>0.0</td>
      <td>True</td>
      <td>c8c9b1fb2217a2b659de4f9396feeca5d15f53ee</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>?</td>
      <td>Rardilit/Panther_v1</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td></td>
      <td>LLaMAForCausalLM</td>
      <td>Delta</td>
      <td>float16</td>
      <td>False</td>
      <td>other</td>
      <td>0.00</td>
      <td>1.0</td>
      <td>True</td>
      <td></td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>APMIC/caigun-lora-model-33B</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>8bit</td>
      <td>False</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>18.25</td>
      <td>0.0</td>
      <td>True</td>
      <td>43789c7afafa495cbcb75185c8f48b11488c0408</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>aiplanet/panda-coder-13B</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>LlamaForCausalLM</td>
      <td>Original</td>
      <td>4bit</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>823a8320224cdac88e927aee00338ffa79395faa</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🟢</td>
      <td>wtang06/mpt-125m-c4</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>pretrained</td>
      <td>MPTForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>55f8f1874aa8bf4fc28c0abc92c7fbd1271ff7d7</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>🔶</td>
      <td>uukuguy/speechless-mistral-six-in-one-7b-orth-1.0</td>
      <td>20.07</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>fine-tuned</td>
      <td>MistralForCausalLM</td>
      <td>Original</td>
      <td>bfloat16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>e500285ba420cb3865d72aa0cc3b1fb9cc0bfee8</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>⭕</td>
      <td>Felladrin/Pythia-31M-Chat-v1</td>
      <td>19.92</td>
      <td>22.70</td>
      <td>25.60</td>
      <td>23.24</td>
      <td>0.00</td>
      <td>47.99</td>
      <td>0.00</td>
      <td>instruction-tuned</td>
      <td>GPTNeoXForCausalLM</td>
      <td>Original</td>
      <td>float16</td>
      <td>False</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>3.0</td>
      <td>True</td>
      <td>e6a52e4ac98e20c7f9e39aaba9368dd6faacdad9</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>