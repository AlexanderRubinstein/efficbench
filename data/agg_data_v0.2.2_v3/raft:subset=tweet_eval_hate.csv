model,data_augmentations,instance_id,train_trial_index,score,score_name,unique_id_inc_seed,split,sub_split
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
AlephAlpha_luminous-base,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-base,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_gpt-j-6b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-j-6b,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
microsoft_TNLGv2_530B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_530B,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id10,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id20,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_text-davinci-002,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-002,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id11,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id40,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id11,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id40,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_t5-11b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t5-11b,data_augmentation=canonical,stop=hash",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id18,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id25,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id27,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id29,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id36,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id11,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id25,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id27,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id29,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id36,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id40,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id47,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id27,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id29,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_text-ada-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-ada-001,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id11,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
microsoft_TNLGv2_7B,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=microsoft_TNLGv2_7B,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id11,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id20,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id25,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id27,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id29,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id36,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id39,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id40,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id43,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id46,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id47,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id49,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id11,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id19,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id24,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id27,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id29,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id36,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id40,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id43,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id46,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id47,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id48,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_yalm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_yalm,data_augmentation=canonical",id49,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id20,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id20,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id39,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id39,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
anthropic_stanford-online-all-v4-s3,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=anthropic_stanford-online-all-v4-s3,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_glm,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_glm,data_augmentation=canonical,stop=hash",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
ai21_j1-large,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-large,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id10,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id15,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id25,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id42,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id25,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_text-davinci-003,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-davinci-003,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_curie,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_curie,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_xlarge-20220609,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20220609,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id10,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id18,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id19,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id25,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id40,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id48,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id11,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id25,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id40,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id19,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id24,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id40,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id48,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_text-babbage-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-babbage-001,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
ai21_j1-grande-v2-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande-v2-beta,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id15,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_opt-175b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-175b,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
AlephAlpha_luminous-supreme,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-supreme,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_medium-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20221108,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id46,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
AlephAlpha_luminous-extended,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=AlephAlpha_luminous-extended,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id20,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id48,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_ul2,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_ul2,data_augmentation=canonical,stop=hash,global_prefix=nlg",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_opt-66b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_opt-66b,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id10,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id19,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id24,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id42,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id48,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id19,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id24,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_small-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_small-20220720,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_large-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_large-20220720,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_medium-20220720,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_medium-20220720,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_bloom,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_bloom,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id11,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id15,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id18,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id19,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id20,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id24,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id25,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id29,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id40,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id42,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id49,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id11,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id20,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id25,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id40,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id49,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id11,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id19,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id24,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id40,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_text-curie-001,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_text-curie-001,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id10,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id11,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id14,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id15,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id16,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id18,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id19,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id20,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id21,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id23,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id24,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id25,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id29,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id39,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id42,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id43,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id48,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id49,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id10,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id14,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id20,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id23,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id25,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id29,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id39,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id43,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id44,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id49,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id10,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id11,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id16,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id19,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id24,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id29,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id39,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id43,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id48,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_xlarge-20221108,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_xlarge-20221108,data_augmentation=canonical",id49,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id40,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_gpt-neox-20b,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_gpt-neox-20b,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id11,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id12,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id17,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id18,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id19,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id24,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id27,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id29,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id36,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id39,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id40,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id42,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id43,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id46,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id47,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id48,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id49,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id11,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id12,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id17,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id18,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id19,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id24,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id27,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id29,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id36,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id37,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id39,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id40,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id42,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id43,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id46,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id47,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id48,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id49,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id11,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id12,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id18,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id19,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id24,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id27,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id29,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id36,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id39,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id40,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id42,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id43,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id46,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id47,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id48,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
together_t0pp,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=together_t0pp,data_augmentation=canonical,stop=hash",id49,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_command-medium-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-medium-beta,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
cohere_command-xlarge-beta,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=cohere_command-xlarge-beta,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id38,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id41,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id44,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id22,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id23,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_davinci,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_davinci,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_ada,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_ada,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id26,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id30,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id31,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id32,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id35,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id13,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id21,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id26,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id30,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id32,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id33,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id34,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id38,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id45,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id13,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id26,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id33,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id34,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id45,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
openai_babbage,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=openai_babbage,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id11,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id13,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id22,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id28,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id29,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id33,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id34,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id37,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id45,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id15,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id16,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id28,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id29,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id31,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id35,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id41,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id14,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id15,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id17,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id20,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id21,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id22,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id25,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id28,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id29,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id30,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id31,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id32,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id35,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id37,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id38,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id41,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id44,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
ai21_j1-jumbo,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-jumbo,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id10,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id11,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id12,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id13,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id14,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id15,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id16,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id17,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id18,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id19,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id20,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id21,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id22,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id23,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id24,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id25,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id26,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id27,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id28,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id29,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id30,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id31,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id32,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id33,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id34,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id35,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id36,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id37,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id38,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id39,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id40,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id41,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id42,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id43,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id44,0,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id45,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id46,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id47,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id48,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id49,0,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_0,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id10,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id11,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id12,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id13,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id14,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id15,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id16,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id17,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id18,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id19,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id20,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id21,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id22,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id23,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id24,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id25,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id26,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id27,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id28,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id29,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id30,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id31,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id32,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id33,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id34,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id35,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id36,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id37,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id38,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id39,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id40,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id41,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id42,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id43,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id44,1,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id45,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id46,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id47,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id48,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id49,1,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_1,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id10,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id10_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id11,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id11_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id12,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id12_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id13,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id13_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id14,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id14_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id15,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id15_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id16,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id16_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id17,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id17_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id18,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id18_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id19,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id19_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id20,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id20_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id21,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id21_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id22,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id22_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id23,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id23_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id24,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id24_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id25,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id25_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id26,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id26_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id27,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id27_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id28,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id28_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id29,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id29_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id30,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id30_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id31,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id31_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id32,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id32_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id33,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id33_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id34,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id34_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id35,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id35_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id36,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id36_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id37,2,0.0,quasi_exact_match,raft:subset=tweet_eval_hate_id37_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id38,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id38_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id39,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id39_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id40,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id40_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id41,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id41_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id42,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id42_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id43,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id43_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id44,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id44_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id45,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id45_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id46,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id46_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id47,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id47_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id48,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id48_2,test,
ai21_j1-grande,"/dccstor/nlgforbi/fm-eval/dev/research-helm/benchmark_output/runs/v0.2.2/raft:subset=tweet_eval_hate,model=ai21_j1-grande,data_augmentation=canonical",id49,2,1.0,quasi_exact_match,raft:subset=tweet_eval_hate_id49_2,test,
