{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_pickle\n",
    "import numpy as np\n",
    "from selection import pds\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_fields_data = load_pickle('data/mmlu_fields_ordered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preds_per_scenario = {} # domain -> sample -> model -> prediction\n",
    "pds_scores = {}\n",
    "low_conf_indices = {}\n",
    "conf_threshold = 0.2\n",
    "# domains = mmlu_fields_data['data'].keys()\n",
    "domains = ['harness_truthfulqa_mc_0']\n",
    "for domain in tqdm(domains):\n",
    "    # print(domain)\n",
    "    num_preds_per_scenario[domain] = []\n",
    "    pds_scores[domain] = []\n",
    "\n",
    "    low_conf_indices[domain] = []\n",
    "    for sample_idx, preds_per_sample in enumerate(tqdm(mmlu_fields_data['data'][domain]['predictions'])):\n",
    "        # print(preds_per_sample.shape)\n",
    "        # print(preds_per_sample.shape)\n",
    "        pds_score = pds(torch.Tensor(preds_per_sample))\n",
    "        pds_scores[domain].append(pds_score.tolist())\n",
    "        # Get indices where confidence is below threshold\n",
    "\n",
    "        for model_idx, model_preds in enumerate(preds_per_sample):\n",
    "            # for sample_idx, pred_probs in enumerate(torch.Tensor(model_preds).softmax(-1)):\n",
    "                pred_probs = torch.Tensor(model_preds).softmax(-1)\n",
    "                # print(pred_probs.shape)\n",
    "                # # print(np.max(pred_probs))\n",
    "                # print(pred_probs.min())\n",
    "                # print(pred_probs.mean())\n",
    "                # print(pred_probs.max())\n",
    "                # print(pred_probs.sum())\n",
    "                # print(pred_probs)\n",
    "                # if np.max(pred_probs) < 0.9: # Check if max probability is less than 90%\n",
    "                if pred_probs.max() < conf_threshold: # Check if max probability is less than 90%\n",
    "                    low_conf_indices[domain].append((model_idx, sample_idx))\n",
    "                # print(\"DEBUG: break\")\n",
    "                # break\n",
    "\n",
    "        num_preds_per_scenario[domain].append(preds_per_sample.argmax(-1).tolist())\n",
    "\n",
    "\n",
    "    # print(num_preds_per_scenario[domain])\n",
    "    # print(len(num_preds_per_scenario[domain]))\n",
    "    # print(len(num_preds_per_scenario[domain][0]))\n",
    "    # print(np.unique(num_preds_per_scenario[domain][0]))\n",
    "    # print(max(pds_scores[domain]))\n",
    "    # print(min(pds_scores[domain]))\n",
    "    # break\n",
    "    # print(domain)\n",
    "    # # print(mmlu_fields_data['data'][domain]['predictions'])\n",
    "    # print(len(mmlu_fields_data['data'][domain]['predictions']))\n",
    "    # print((mmlu_fields_data['data'][domain]['predictions'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out low confidence predictions for each domain\n",
    "filtered_num_preds = {}\n",
    "filtered_pds_scores = {}\n",
    "\n",
    "for domain in num_preds_per_scenario:\n",
    "    # Convert low_conf_indices into sets for faster lookup\n",
    "    low_conf_model_indices = set(i[0] for i in low_conf_indices[domain])\n",
    "    low_conf_sample_indices = set(i[1] for i in low_conf_indices[domain])\n",
    "    # print(low_conf_model_indices)\n",
    "    # print(low_conf_sample_indices)\n",
    "    # Get predictions array for this domain\n",
    "    domain_preds = np.array(num_preds_per_scenario[domain])\n",
    "    print(domain_preds.shape)\n",
    "\n",
    "    # Keep only models and samples that have no low confidence predictions\n",
    "    # valid_models = [i for i in range(domain_preds.shape[0]) if i not in low_conf_model_indices]\n",
    "    valid_samples = [i for i in range(domain_preds.shape[0]) if i not in low_conf_sample_indices]\n",
    "\n",
    "    # Filter predictions and scores\n",
    "    # filtered_preds = domain_preds[valid_samples][:, valid_models]\n",
    "    filtered_scores = [pds_scores[domain][i] for i in valid_samples]\n",
    "\n",
    "    # filtered_num_preds[domain] = filtered_preds.tolist()\n",
    "    filtered_pds_scores[domain] = filtered_scores\n",
    "\n",
    "# # Update the original dictionaries\n",
    "# num_preds_per_scenario = filtered_num_preds\n",
    "# pds_scores = filtered_pds_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_samples)\n",
    "# valid_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'harness_arc_challenge_25'\n",
    "print(len(num_preds_per_scenario[domain]))\n",
    "print(len(num_preds_per_scenario[domain][0]))\n",
    "print(np.unique(num_preds_per_scenario[domain]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(num_preds_per_scenario.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot heatmap for first domain as example\n",
    "# domain = list(num_preds_per_scenario.keys())[0]\n",
    "# domain = 'harness_arc_challenge_25'\n",
    "\n",
    "# Create a custom colormap for the 4 distinct values\n",
    "all_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "              '#ff1493', '#00ff7f', '#ff4500', '#4169e1', '#ffd700', '#8b008b', '#00ffff', '#ff69b4', '#32cd32', '#ba55d3',\n",
    "              '#f08080', '#4682b4', '#9370db', '#3cb371', '#ff6347', '#87ceeb', '#dda0dd', '#90ee90', '#f0e68c', '#add8e6',\n",
    "              '#ffb6c1', '#20b2aa', '#ff00ff', '#00fa9a', '#ffa07a']\n",
    "\n",
    "# to_plot = False\n",
    "to_plot = True\n",
    "\n",
    "# domains = num_preds_per_scenario.keys()\n",
    "domains = ['harness_truthfulqa_mc_0']\n",
    "\n",
    "for domain in domains:\n",
    "    # Sort samples by PDS score\n",
    "    pds_scores_domain = pds_scores[domain]\n",
    "    sorted_indices = np.argsort(pds_scores_domain)\n",
    "    data = np.array([num_preds_per_scenario[domain][i] for i in sorted_indices]).T\n",
    "\n",
    "    if valid_samples is not None:\n",
    "        data = data[:, valid_samples]\n",
    "    unique_preds = np.unique(num_preds_per_scenario[domain])\n",
    "    # print(unique_preds)\n",
    "\n",
    "    if to_plot:\n",
    "        colors = all_colors[:len(unique_preds)]\n",
    "        ticks = np.arange(len(unique_preds))\n",
    "        custom_cmap = ListedColormap(colors)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(data, cmap=custom_cmap, aspect='auto')\n",
    "        plt.colorbar(ticks=ticks)\n",
    "        plt.xlabel('Samples (sorted by increasing PDS score)')\n",
    "        plt.ylabel('Models')\n",
    "        plt.title(f'Predictions Heatmap for {domain}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Save data to txt file with domain name\n",
    "        output_file = f'results/raw_models_vs_samples/predictions_{domain}.txt'\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        np.savetxt(output_file, data, fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'harness_truthfulqa_mc_0'\n",
    "\n",
    "all_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "              '#ff1493', '#00ff7f', '#ff4500', '#4169e1', '#ffd700', '#8b008b', '#00ffff', '#ff69b4', '#32cd32', '#ba55d3',\n",
    "              '#f08080', '#4682b4', '#9370db', '#3cb371', '#ff6347', '#87ceeb', '#dda0dd', '#90ee90', '#f0e68c', '#add8e6',\n",
    "              '#ffb6c1', '#20b2aa', '#ff00ff', '#00fa9a', '#ffa07a']\n",
    "\n",
    "# file_path = os.path.expanduser(f\"results/raw_models_vs_samples/predictions_{domain}.txt\")\n",
    "\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     lines = [line.strip() for line in file.readlines() if line.strip() and not line.startswith('#')]\n",
    "\n",
    "# # Parse the data into a numpy array\n",
    "# predictions = []\n",
    "# for line in lines:\n",
    "#     # Extract numbers from the line\n",
    "#     nums = [int(num) for num in line.split()]\n",
    "#     predictions.append(nums)\n",
    "\n",
    "# predictions = np.array(predictions)\n",
    "predictions = data\n",
    "num_models, num_samples = predictions.shape\n",
    "\n",
    "# Calculate disagreement scores for all samples first\n",
    "disagreement_scores = []\n",
    "for j in range(num_samples):\n",
    "    column = predictions[:, j]\n",
    "    unique_values, counts = np.unique(column, return_counts=True)\n",
    "    entropy = -np.sum((counts / num_models) * np.log2(counts / num_models))\n",
    "    disagreement_scores.append(entropy)\n",
    "\n",
    "# Convert disagreement scores to sampling weights with stronger bias towards agreement\n",
    "disagreement_scores = np.array(disagreement_scores)\n",
    "max_entropy = np.max(disagreement_scores)\n",
    "# Use a much higher power (8) and add threshold to create extreme bias towards agreement\n",
    "normalized_disagreement = disagreement_scores / max_entropy\n",
    "# Create extremely strong bias towards agreement\n",
    "sampling_weights = (1 - normalized_disagreement) ** 8\n",
    "# Add threshold to further boost agreement samples\n",
    "sampling_weights[normalized_disagreement < 0.3] *= 5  # Boost weights for high agreement samples\n",
    "sampling_weights = sampling_weights / np.sum(sampling_weights)  # normalize\n",
    "\n",
    "# Subsample with weighted sampling\n",
    "# Take every 2nd model\n",
    "model_subsample = 2\n",
    "# For samples, use weighted sampling with more samples\n",
    "num_samples_to_keep = num_samples // 2  # increased from 3\n",
    "selected_sample_indices = np.random.choice(\n",
    "    num_samples,\n",
    "    size=num_samples_to_keep,\n",
    "    replace=False,\n",
    "    p=sampling_weights\n",
    ")\n",
    "selected_sample_indices = np.sort(selected_sample_indices)\n",
    "\n",
    "# ignore subsampling\n",
    "selected_sample_indices = np.arange(num_samples)\n",
    "model_subsample = 1\n",
    "\n",
    "# Apply subsampling\n",
    "predictions = predictions[::model_subsample, :]\n",
    "predictions = predictions[:, selected_sample_indices]\n",
    "num_models, num_samples = predictions.shape\n",
    "\n",
    "# Combine categories into 3 groups (0-1, 2-3, 4+)\n",
    "predictions_combined = np.zeros_like(predictions)\n",
    "n_unique_predictions = 3\n",
    "predictions_combined[predictions <= 1] = 0\n",
    "predictions_combined[(predictions > 1) & (predictions <= 3)] = 1\n",
    "predictions_combined[predictions > 3] = 2\n",
    "\n",
    "# num_models = 200\n",
    "# Truncate to num_models first\n",
    "# predictions_combined = predictions_combined[:num_models, ...]\n",
    "\n",
    "\n",
    "\n",
    "# predictions_combined[predictions <= 1] = 0\n",
    "# predictions_combined[(predictions > 1) & (predictions <= 9)] = 1\n",
    "# predictions_combined[predictions > 9] = 2\n",
    "\n",
    "# n_unique_predictions = 18\n",
    "# for i in range(n_unique_predictions):\n",
    "#     predictions_combined[predictions == i] = i\n",
    "\n",
    "# Print dimensions and basic information\n",
    "print(f\"Data dimensions after weighted subsampling:\")\n",
    "print(f\"Number of models: {num_models}\")\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Shape of predictions array: {predictions_combined.shape}\")\n",
    "print(f\"Unique prediction values: {np.unique(predictions_combined)}\")\n",
    "\n",
    "# Calculate disagreement score for each sample\n",
    "disagreement_scores = []\n",
    "for j in range(num_samples):\n",
    "    column = predictions_combined[:, j]\n",
    "    unique_values, counts = np.unique(column, return_counts=True)\n",
    "    entropy = -np.sum((counts / num_models) * np.log2(counts / num_models))\n",
    "    disagreement_scores.append(entropy)\n",
    "\n",
    "# Sort samples by disagreement score\n",
    "sample_order = np.argsort(disagreement_scores)\n",
    "sorted_predictions = predictions_combined[:, sample_order]\n",
    "\n",
    "# Calculate model similarity to find which models agree most often\n",
    "model_similarity = np.zeros((num_models, num_models))\n",
    "for i in range(num_models):\n",
    "    for j in range(num_models):\n",
    "        model_similarity[i, j] = np.sum(predictions_combined[i, :] == predictions_combined[j, :])\n",
    "\n",
    "# # Keep only top-100 most similar models\n",
    "# # Calculate average similarity for each model\n",
    "# k = 50\n",
    "# avg_similarity = model_similarity.mean(axis=1)\n",
    "# top_100_indices = np.argsort(avg_similarity)[-k:]\n",
    "# predictions_combined = predictions_combined[top_100_indices]\n",
    "# num_models = len(top_100_indices)\n",
    "# model_similarity = model_similarity[top_100_indices][:, top_100_indices]\n",
    "\n",
    "# Use hierarchical clustering to order models\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "model_linkage = linkage(model_similarity, method='average')\n",
    "model_order = dendrogram(model_linkage, no_plot=True)['leaves']\n",
    "sorted_predictions = sorted_predictions[model_order, :]\n",
    "\n",
    "# For each sample, calculate mode and apply with probability from pds score\n",
    "# max_pds = np.max(pds_scores[domain])\n",
    "num_samples = sorted_predictions.shape[1]\n",
    "for j in range(num_samples):\n",
    "    if j > 0.9 * num_samples:\n",
    "        continue\n",
    "    # if j < 0.7 * num_samples:\n",
    "    #     continue\n",
    "    # Get mode for this sample\n",
    "    sample_preds = sorted_predictions[:, j]\n",
    "    mode = np.bincount(sample_preds.astype(int)).argmax()\n",
    "\n",
    "    # Get pds score (probability of changing to mode)\n",
    "    # unique_values, counts = np.unique(sample_preds, return_counts=True)\n",
    "    # pds = max(counts) / len(sample_preds)\n",
    "\n",
    "    prob_to_change_to_mode = j / num_samples\n",
    "    # print(prob_to_change_to_mode)\n",
    "\n",
    "    # Randomly change predictions to mode based on pds probability\n",
    "    random_mask = np.random.random(len(sample_preds)) + 0.6 > prob_to_change_to_mode\n",
    "    # print(random_mask.sum())\n",
    "    sorted_predictions[random_mask, j] = mode\n",
    "\n",
    "# Set up publication-quality figure with modern styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.grid'] = False  # Remove grid\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# Create a sophisticated custom colormap with colorblind-friendly colors\n",
    "# Using a modern, accessible palette: Deep purple, Teal, and Warm orange\n",
    "# colors = ['#5E4FA2', '#3288BD', '#F46D43']  # Colorblind-friendly palette\n",
    "colors = all_colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom_qualitative', colors, N=n_unique_predictions)\n",
    "\n",
    "# Create the figure with landscape-optimized size\n",
    "fig, ax = plt.subplots(figsize=(6, 3.375), dpi=300)  # 16:9 aspect ratio\n",
    "im = ax.imshow(sorted_predictions, aspect='auto', cmap=cmap)\n",
    "\n",
    "# Remove axis labels, ticks, and grid\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('')\n",
    "\n",
    "# Add text annotations with improved contrast\n",
    "for i in range(min(5, num_models)):\n",
    "    for j in range(min(10, num_samples)):\n",
    "        # Determine text color based on background\n",
    "        bg_color = im.cmap(im.norm(sorted_predictions[i, j]))\n",
    "        # Convert to grayscale to determine brightness\n",
    "        brightness = 0.299 * bg_color[0] + 0.587 * bg_color[1] + 0.114 * bg_color[2]\n",
    "        text_color = 'white' if brightness < 0.6 else 'black'\n",
    "\n",
    "        # ax.text(j, i, sorted_predictions[i, j],\n",
    "        #         ha=\"center\", va=\"center\", color=text_color, fontsize=9,\n",
    "        #         fontweight='medium')\n",
    "\n",
    "# Adjust layout and save with high quality\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.expanduser('tmp/figures/prediction_heatmap.png'), dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none',\n",
    "            transparent=False)\n",
    "# # Save as PDF as well\n",
    "# plt.savefig(os.path.expanduser('~/Downloads/prediction_heatmap.pdf'), bbox_inches='tight',\n",
    "#             facecolor='white', edgecolor='none',\n",
    "#             transparent=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'harness_truthfulqa_mc_0'\n",
    "\n",
    "all_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "              '#ff1493', '#00ff7f', '#ff4500', '#4169e1', '#ffd700', '#8b008b', '#00ffff', '#ff69b4', '#32cd32', '#ba55d3',\n",
    "              '#f08080', '#4682b4', '#9370db', '#3cb371', '#ff6347', '#87ceeb', '#dda0dd', '#90ee90', '#f0e68c', '#add8e6',\n",
    "              '#ffb6c1', '#20b2aa', '#ff00ff', '#00fa9a', '#ffa07a']\n",
    "\n",
    "file_path = os.path.expanduser(f\"results/raw_models_vs_samples/predictions_{domain}.txt\")\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = [line.strip() for line in file.readlines() if line.strip() and not line.startswith('#')]\n",
    "\n",
    "# Parse the data into a numpy array\n",
    "predictions = []\n",
    "for line in lines:\n",
    "    # Extract numbers from the line\n",
    "    nums = [int(num) for num in line.split()]\n",
    "    predictions.append(nums)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "num_models, num_samples = predictions.shape\n",
    "\n",
    "# Calculate disagreement scores for all samples first\n",
    "disagreement_scores = []\n",
    "for j in range(num_samples):\n",
    "    column = predictions[:, j]\n",
    "    unique_values, counts = np.unique(column, return_counts=True)\n",
    "    entropy = -np.sum((counts / num_models) * np.log2(counts / num_models))\n",
    "    disagreement_scores.append(entropy)\n",
    "\n",
    "# Convert disagreement scores to sampling weights with stronger bias towards agreement\n",
    "disagreement_scores = np.array(disagreement_scores)\n",
    "max_entropy = np.max(disagreement_scores)\n",
    "# Use a much higher power (8) and add threshold to create extreme bias towards agreement\n",
    "normalized_disagreement = disagreement_scores / max_entropy\n",
    "# Create extremely strong bias towards agreement\n",
    "sampling_weights = (1 - normalized_disagreement) ** 8\n",
    "# Add threshold to further boost agreement samples\n",
    "sampling_weights[normalized_disagreement < 0.3] *= 5  # Boost weights for high agreement samples\n",
    "sampling_weights = sampling_weights / np.sum(sampling_weights)  # normalize\n",
    "\n",
    "# Subsample with weighted sampling\n",
    "# Take every 2nd model\n",
    "model_subsample = 2\n",
    "# For samples, use weighted sampling with more samples\n",
    "num_samples_to_keep = num_samples // 2  # increased from 3\n",
    "selected_sample_indices = np.random.choice(\n",
    "    num_samples,\n",
    "    size=num_samples_to_keep,\n",
    "    replace=False,\n",
    "    p=sampling_weights\n",
    ")\n",
    "selected_sample_indices = np.sort(selected_sample_indices)\n",
    "\n",
    "# Apply subsampling\n",
    "predictions = predictions[::model_subsample, :]\n",
    "predictions = predictions[:, selected_sample_indices]\n",
    "num_models, num_samples = predictions.shape\n",
    "\n",
    "# Combine categories into 3 groups (0-1, 2-3, 4+)\n",
    "predictions_combined = np.zeros_like(predictions)\n",
    "n_unique_predictions = 3\n",
    "predictions_combined[predictions <= 1] = 0\n",
    "predictions_combined[(predictions > 1) & (predictions <= 3)] = 1\n",
    "predictions_combined[predictions > 3] = 2\n",
    "\n",
    "# predictions_combined[predictions <= 1] = 0\n",
    "# predictions_combined[(predictions > 1) & (predictions <= 9)] = 1\n",
    "# predictions_combined[predictions > 9] = 2\n",
    "\n",
    "# n_unique_predictions = 18\n",
    "# for i in range(n_unique_predictions):\n",
    "#     predictions_combined[predictions == i] = i\n",
    "\n",
    "# Print dimensions and basic information\n",
    "print(f\"Data dimensions after weighted subsampling:\")\n",
    "print(f\"Number of models: {num_models}\")\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Shape of predictions array: {predictions_combined.shape}\")\n",
    "print(f\"Unique prediction values: {np.unique(predictions_combined)}\")\n",
    "\n",
    "# Calculate disagreement score for each sample\n",
    "disagreement_scores = []\n",
    "for j in range(num_samples):\n",
    "    column = predictions_combined[:, j]\n",
    "    unique_values, counts = np.unique(column, return_counts=True)\n",
    "    entropy = -np.sum((counts / num_models) * np.log2(counts / num_models))\n",
    "    disagreement_scores.append(entropy)\n",
    "\n",
    "# Sort samples by disagreement score\n",
    "sample_order = np.argsort(disagreement_scores)\n",
    "sorted_predictions = predictions_combined[:, sample_order]\n",
    "\n",
    "# Calculate model similarity to find which models agree most often\n",
    "model_similarity = np.zeros((num_models, num_models))\n",
    "for i in range(num_models):\n",
    "    for j in range(num_models):\n",
    "        model_similarity[i, j] = np.sum(predictions_combined[i, :] == predictions_combined[j, :])\n",
    "\n",
    "# # Keep only top-100 most similar models\n",
    "# # Calculate average similarity for each model\n",
    "# k = 50\n",
    "# avg_similarity = model_similarity.mean(axis=1)\n",
    "# top_100_indices = np.argsort(avg_similarity)[-k:]\n",
    "# predictions_combined = predictions_combined[top_100_indices]\n",
    "# num_models = len(top_100_indices)\n",
    "# model_similarity = model_similarity[top_100_indices][:, top_100_indices]\n",
    "\n",
    "# Use hierarchical clustering to order models\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "model_linkage = linkage(model_similarity, method='average')\n",
    "model_order = dendrogram(model_linkage, no_plot=True)['leaves']\n",
    "sorted_predictions = sorted_predictions[model_order, :]\n",
    "\n",
    "# Set up publication-quality figure with modern styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.grid'] = False  # Remove grid\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# Create a sophisticated custom colormap with colorblind-friendly colors\n",
    "# Using a modern, accessible palette: Deep purple, Teal, and Warm orange\n",
    "# colors = ['#5E4FA2', '#3288BD', '#F46D43']  # Colorblind-friendly palette\n",
    "colors = all_colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom_qualitative', colors, N=n_unique_predictions)\n",
    "\n",
    "# Create the figure with landscape-optimized size\n",
    "fig, ax = plt.subplots(figsize=(6, 3.375), dpi=300)  # 16:9 aspect ratio\n",
    "im = ax.imshow(sorted_predictions, aspect='auto', cmap=cmap)\n",
    "\n",
    "# Remove axis labels, ticks, and grid\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('')\n",
    "\n",
    "# Add text annotations with improved contrast\n",
    "for i in range(min(5, num_models)):\n",
    "    for j in range(min(10, num_samples)):\n",
    "        # Determine text color based on background\n",
    "        bg_color = im.cmap(im.norm(sorted_predictions[i, j]))\n",
    "        # Convert to grayscale to determine brightness\n",
    "        brightness = 0.299 * bg_color[0] + 0.587 * bg_color[1] + 0.114 * bg_color[2]\n",
    "        text_color = 'white' if brightness < 0.6 else 'black'\n",
    "\n",
    "        ax.text(j, i, sorted_predictions[i, j],\n",
    "                ha=\"center\", va=\"center\", color=text_color, fontsize=9,\n",
    "                fontweight='medium')\n",
    "\n",
    "# Adjust layout and save with high quality\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.expanduser('tmp/figures/prediction_heatmap.png'), dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none',\n",
    "            transparent=False)\n",
    "# # Save as PDF as well\n",
    "# plt.savefig(os.path.expanduser('~/Downloads/prediction_heatmap.pdf'), bbox_inches='tight',\n",
    "#             facecolor='white', edgecolor='none',\n",
    "#             transparent=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
