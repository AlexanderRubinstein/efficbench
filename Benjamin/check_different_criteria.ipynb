{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:36:04.325795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-15 09:36:04.472025: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 09:36:05.747442: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/extras/CUPTI/lib64:/opt/rh/gcc-toolset-10/root/usr/lib64:/opt/rh/gcc-toolset-10/root/usr/lib:/opt/rh/gcc-toolset-10/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-10/root/usr/lib/dyninst:/opt/rh/gcc-toolset-10/root/usr/lib64:/opt/rh/gcc-toolset-10/root/usr/lib\n",
      "2025-05-15 09:36:05.747546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/2023/cuda/12.2/extras/CUPTI/lib64:/opt/rh/gcc-toolset-10/root/usr/lib64:/opt/rh/gcc-toolset-10/root/usr/lib:/opt/rh/gcc-toolset-10/root/usr/lib64/dyninst:/opt/rh/gcc-toolset-10/root/usr/lib/dyninst:/opt/rh/gcc-toolset-10/root/usr/lib64:/opt/rh/gcc-toolset-10/root/usr/lib\n",
      "2025-05-15 09:36:05.747551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utility'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(figure_generation_helpers\u001b[38;5;241m.\u001b[39mhelpers)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfigure_generation_helpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNModel, baseline_mae, baseline_empirical_rank_score, SimpleNN, MLPRegressor, HemmingNNModel, calculate_model_distance_matrix_for_confidences_tensor, get_average_absolute_error_with_model_class, get_distance_matrix_list_with_bootstrapping, create_model_of_class, plot_average_model_performance_over_sample_size\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     read_old_checkpoint,\n\u001b[1;32m     36\u001b[0m     get_device\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utility'"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# env: /weka/<PATH>/github/model-selection/envs/benj/bin/python - 3.10.4\n",
    "# original notebook: ../tmp/model-selection/notebooks/check_different_criteria.ipynb\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids # pip install scikit-learn-extra\n",
    "import datetime\n",
    "import importlib\n",
    "import random\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# local modules\n",
    "# sys.path.insert(\n",
    "#     0,\n",
    "#     os.path.join(\n",
    "#         os.path.dirname(os.path.abspath('')), \"src\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "import figure_generation_helpers.helpers\n",
    "importlib.reload(figure_generation_helpers.helpers)\n",
    "\n",
    "from figure_generation_helpers.helpers import NNModel, baseline_mae, baseline_empirical_rank_score, SimpleNN, MLPRegressor, HemmingNNModel, calculate_model_distance_matrix_for_confidences_tensor, get_average_absolute_error_with_model_class, get_distance_matrix_list_with_bootstrapping, create_model_of_class, plot_average_model_performance_over_sample_size\n",
    "\n",
    "from utility.utils import (\n",
    "    read_old_checkpoint,\n",
    "    get_device\n",
    ")\n",
    "\n",
    "# sys.path.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "stable-diffusion-sdkit 2.1.4 requires open-clip-torch==2.0.2, but you have open-clip-torch 2.24.0 which is incompatible.\n",
      "stable-diffusion-sdkit 2.1.4 requires transformers==4.26.1, but you have transformers 4.44.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q scikit-learn-extra umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_968094/796740258.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  timm_model_confidence_for_val_samples_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_val_samples.tensor\"))\n"
     ]
    }
   ],
   "source": [
    "path_to_save_folder = \"/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints\"\n",
    "\n",
    "timm_model_confidence_for_val_samples_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_val_samples.tensor\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_968094/1071778926.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  open_clip_model_confidence_for_val_samples_tensor = torch.load(f=os.path.join(path_to_save_folder, \"open_clip_model_confidences_cifar_100.tensor\"))\n"
     ]
    }
   ],
   "source": [
    "path_to_save_folder = \"/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints\"\n",
    "\n",
    "open_clip_model_confidence_for_val_samples_tensor = torch.load(f=os.path.join(path_to_save_folder, \"open_clip_model_confidences_cifar_100.tensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_save_folder, \"timm_model_catalog_with_per_class.json\"), mode=\"r\") as fp:\n",
    "    timm_model_catalog = json.load(fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_save_folder, \"open_clip_model_catalog_with_per_class.json\"), mode=\"r\") as fp:\n",
    "    open_clip_model_catalog = json.load(fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_along_axis(function, x, axis: int = 0):\n",
    "    return torch.stack([\n",
    "        function(x_i) for x_i in torch.unbind(x, dim=axis)\n",
    "    ], dim=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_entropy(x):\n",
    "    counts = torch.bincount(x)\n",
    "    counts = counts[counts!=0]\n",
    "    frequencies = counts / counts.sum()\n",
    "    return - (frequencies*frequencies.log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_entropy_per_sample(tensor: torch.tensor):\n",
    "    predicted_class_tensor = torch.argmax(tensor, dim=2)\n",
    "\n",
    "    per_sample_entropy = apply_along_axis(tensor_entropy,\n",
    "                                          x=predicted_class_tensor,\n",
    "                                          axis=0)\n",
    "    return per_sample_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_1_imagenet.pkl_model_0//ViT-H-14-378-quickgelu+dfn5b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_1_imagenet.pkl_model_1//ViT-H-14-quickgelu+dfn5b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_1_imagenet.pkl_model_2//ViT-SO400M-14-SigLIP-384+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_2_imagenet.pkl_model_0//ViT-SO400M-14-SigLIP+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_2_imagenet.pkl_model_1//ViT-L-14-quickgelu+dfn2b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_2_imagenet.pkl_model_2//ViT-L-16-SigLIP-384+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_3_imagenet.pkl_model_0//ViT-H-14-CLIPA-336+datacomp1b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_3_imagenet.pkl_model_1//ViT-H-14-quickgelu+metaclip_fullcc': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_3_imagenet.pkl_model_2//ViT-H-14-CLIPA+datacomp1b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_4_imagenet.pkl_model_0//ViT-L-14-quickgelu+metaclip_fullcc': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_4_imagenet.pkl_model_1//EVA02-L-14-336+merged2b_s6b_b61k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_4_imagenet.pkl_model_2//ViT-L-14-CLIPA-336+datacomp1b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_5_imagenet.pkl_model_0//ViT-L-16-SigLIP-256+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_5_imagenet.pkl_model_1//EVA02-L-14+merged2b_s4b_b131k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_5_imagenet.pkl_model_2//ViT-B-16-SigLIP-512+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_6_imagenet.pkl_model_0//ViT-H-14-CLIPA-336+laion2b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_6_imagenet.pkl_model_1//ViT-H-14+laion2b_s32b_b79k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_6_imagenet.pkl_model_2//convnext_large_d_320+laion2b_s29b_b131k_ft_soup': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_7_imagenet.pkl_model_0//ViT-B-16-SigLIP-384+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_7_imagenet.pkl_model_1//ViT-L-14+commonpool_xl_laion_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_7_imagenet.pkl_model_2//convnext_large_d_320+laion2b_s29b_b131k_ft': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_8_imagenet.pkl_model_0//coca_ViT-L-14+laion2b_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_8_imagenet.pkl_model_1//convnext_large_d+laion2b_s26b_b102k_augreg': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_8_imagenet.pkl_model_2//ViT-L-14-quickgelu+metaclip_400m': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_9_imagenet.pkl_model_0//ViT-B-16-SigLIP+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_9_imagenet.pkl_model_1//ViT-B-16-SigLIP-256+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_9_imagenet.pkl_model_2//ViT-L-14+commonpool_xl_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_10_imagenet.pkl_model_0//ViT-L-14+laion2b_s32b_b82k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_10_imagenet.pkl_model_1//ViT-L-14+openai': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_10_imagenet.pkl_model_2//coca_ViT-L-14+mscoco_finetuned_laion2b_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_11_imagenet.pkl_model_0//ViT-B-16+datacomp_xl_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_11_imagenet.pkl_model_1//ViT-B-16+dfn2b': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_11_imagenet.pkl_model_2//ViT-B-32-256+datacomp_s34b_b86k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_12_imagenet.pkl_model_0//ViT-B-16-SigLIP-i18n-256+webli': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_12_imagenet.pkl_model_1//RN50x64+openai': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_12_imagenet.pkl_model_2//ViT-B-16-quickgelu+metaclip_fullcc': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_13_imagenet.pkl_model_0//ViT-L-14+laion400m_e32': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_13_imagenet.pkl_model_1//ViT-L-14+laion400m_e31': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_13_imagenet.pkl_model_2//EVA02-B-16+merged2b_s8b_b131k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_14_imagenet.pkl_model_0//convnext_base_w_320+laion_aesthetic_s13b_b82k_augreg': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_14_imagenet.pkl_model_1//ViT-B-16+laion2b_s34b_b88k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_14_imagenet.pkl_model_2//convnext_base_w+laion2b_s13b_b82k_augreg': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_15_imagenet.pkl_model_0//ViT-B-32+datacomp_xl_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_15_imagenet.pkl_model_1//ViT-B-16-quickgelu+metaclip_400m': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_15_imagenet.pkl_model_2//convnext_base_w+laion_aesthetic_s13b_b82k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_16_imagenet.pkl_model_0//convnext_base_w+laion2b_s13b_b82k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_16_imagenet.pkl_model_1//ViT-B-16-plus-240+laion400m_e32': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_16_imagenet.pkl_model_2//ViT-B-16-plus-240+laion400m_e31': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_17_imagenet.pkl_model_0//ViT-B-32+laion2b_s34b_b79k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_17_imagenet.pkl_model_1//RN50x16+openai': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_17_imagenet.pkl_model_2//convnext_base_w_320+laion_aesthetic_s13b_b82k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_18_imagenet.pkl_model_0//xlm-roberta-base-ViT-B-32+laion5b_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_18_imagenet.pkl_model_1//ViT-B-16+openai': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_18_imagenet.pkl_model_2//ViT-B-16+laion400m_e32': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_19_imagenet.pkl_model_0//ViT-B-16+laion400m_e31': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_19_imagenet.pkl_model_1//ViT-B-32-quickgelu+metaclip_fullcc': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_19_imagenet.pkl_model_2//convnext_base+laion400m_s13b_b51k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_20_imagenet.pkl_model_0//coca_ViT-B-32+laion2b_s13b_b90k': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_20_imagenet.pkl_model_1//ViT-B-32+laion2b_e16': datetime.datetime(2022, 12, 2, 0, 0), '/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints/open_clip_models_for_ensemble_20_imagenet.pkl_model_2//roberta-ViT-B-32+laion2b_s12b_b32k': datetime.datetime(2022, 12, 2, 0, 0)}\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{key+'//'+open_clip_model_catalog[key]['open_clip_model']['model_name']+'+'+open_clip_model_catalog[key]['open_clip_model']['pretrained']: datetime.datetime(year=2022, month=12, day=2) for key in open_clip_model_catalog.keys()}.__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model_date_mapping = {'maxvit_large_tf_224.in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'rexnet_100.nav_in1k': datetime.datetime(2023, 3, 20, 0, 0),\n",
    " 'inception_next_base.sail_in1k_384': datetime.datetime(2023, 8, 24, 0, 0),\n",
    " 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'vit_base_patch32_224.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'tf_mobilenetv3_large_100.in1k': datetime.datetime(2022, 12, 16, 0, 0),\n",
    " 'tf_efficientnet_es.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'convnextv2_nano.fcmae_ft_in22k_in1k_384': datetime.datetime(2023, 1, 5, 0, 0),\n",
    " 'vit_base_patch16_224.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'mobilevitv2_075.cvnets_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'volo_d4_448.sail_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'lambda_resnet26t.c1_in1k': datetime.datetime(2023, 4, 16, 0, 0),\n",
    " 'xcit_large_24_p8_224.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'repghostnet_111.in1k': datetime.datetime(2023, 8, 20, 0, 0),\n",
    " 'mobilevitv2_200.cvnets_in22k_ft_in1k_384': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'coat_lite_small.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'coatnext_nano_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'hiera_large_224.mae_in1k_ft_in1k': datetime.datetime(2024, 5, 13, 0, 0),\n",
    " 'repvit_m2_3.dist_450e_in1k': datetime.datetime(2023, 10, 20, 0, 0),\n",
    " 'tiny_vit_5m_224.dist_in22k_ft_in1k': datetime.datetime(2023, 9, 1, 0, 0),\n",
    " 'convformer_m36.sail_in1k_384': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'deit3_base_patch16_384.fb_in22k_ft_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'xcit_large_24_p16_224.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'tf_efficientnet_b2.in1k': datetime.datetime(2023, 4, 27, 0, 0),\n",
    " 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'vit_large_patch14_clip_336.laion2b_ft_in1k': datetime.datetime(2022, 11, 7, 0, 0),\n",
    " 'fastvit_sa24.apple_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'rexnet_150.nav_in1k': datetime.datetime(2023, 3, 20, 0, 0),\n",
    " 'maxxvit_rmlp_small_rw_256.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'beitv2_base_patch16_224.in1k_ft_in22k_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'resnet50.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'poolformer_m36.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'vit_base_patch16_224.augreg2_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'xcit_small_12_p16_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'ghostnetv2_160.in1k': datetime.datetime(2023, 8, 20, 0, 0),\n",
    " 'caformer_b36.sail_in22k_ft_in1k_384': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'cait_xxs36_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'flexivit_small.1200ep_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'tf_efficientnet_b7.ns_jft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'swin_small_patch4_window7_224.ms_in22k_ft_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'beit_base_patch16_384.in22k_ft_in22k_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'tf_mixnet_l.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'vit_small_patch16_224.augreg_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'beitv2_base_patch16_224.in1k_ft_in1k': datetime.datetime(2023, 3, 9, 0, 0),\n",
    " 'efficientvit_b1.r256_in1k': datetime.datetime(2023, 8, 19, 0, 0),\n",
    " 'swinv2_base_window16_256.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'efficientnet_b3_pruned.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'poolformerv2_m48.sail_in1k': datetime.datetime(2023, 5, 3, 0, 0),\n",
    " 'xcit_tiny_24_p16_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'vit_wee_patch16_reg1_gap_256.sbb_in1k': datetime.datetime(2024, 5, 13, 0, 0),\n",
    " 'haloregnetz_b.ra3_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'vit_base_patch32_clip_224.openai_ft_in1k': datetime.datetime(2022, 11, 10, 0, 0),\n",
    " 'deit3_small_patch16_384.fb_in22k_ft_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'tf_efficientnet_b1.in1k': datetime.datetime(2023, 4, 27, 0, 0),\n",
    " 'vit_betwixt_patch16_reg1_gap_256.sbb_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'ghostnetv2_130.in1k': datetime.datetime(2023, 8, 20, 0, 0),\n",
    " 'regnety_064.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'gmixer_24_224.ra3_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'repvit_m2.dist_in1k': datetime.datetime(2023, 8, 25, 0, 0),\n",
    " 'resnext50_32x4d.tv_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'mobilevitv2_125.cvnets_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'resnetv2_101x3_bit.goog_in21k_ft_in1k': datetime.datetime(2023, 3, 22, 0, 0),\n",
    " 'repvit_m1.dist_in1k': datetime.datetime(2023, 8, 25, 0, 0),\n",
    " 'efficientvit_l1.r224_in1k': datetime.datetime(2023, 11, 21, 0, 0),\n",
    " 'dla102.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k': datetime.datetime(2022, 11, 28, 0, 0),\n",
    " 'convformer_s36.sail_in1k_384': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'selecsls60.in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'xcit_small_24_p8_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'hrnet_w18_small_v2.ms_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'convnext_xlarge.fb_in22k_ft_in1k_384': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'vit_large_patch14_clip_224.openai_ft_in1k': datetime.datetime(2022, 11, 2, 0, 0),\n",
    " 'gcvit_xxtiny.in1k': datetime.datetime(2022, 12, 27, 0, 0),\n",
    " 'crossvit_9_dagger_240.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'efficientformer_l7.snap_dist_in1k': datetime.datetime(2023, 2, 3, 0, 0),\n",
    " 'tf_efficientnet_b0.aa_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'regnety_320.swag_lc_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'tf_efficientnet_cc_b1_8e.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'xception71.tf_in1k': datetime.datetime(2023, 4, 22, 0, 0),\n",
    " 'maxvit_base_tf_512.in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'volo_d3_448.sail_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'regnety_160.swag_lc_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'resnet152s.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'resnest101e.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'efficientvit_b3.r288_in1k': datetime.datetime(2023, 8, 19, 0, 0),\n",
    " 'vit_small_patch16_224.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'coat_small.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'pvtv2_b3.in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'repvit_m1_0.dist_450e_in1k': datetime.datetime(2023, 10, 20, 0, 0),\n",
    " 'fastvit_sa24.apple_dist_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'resnet152d.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'poolformer_s36.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'mobileone_s1.apple_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'cait_xs24_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'eca_halonext26ts.c1_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'fastvit_ma36.apple_dist_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'vit_small_patch16_384.augreg_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'vit_base_patch16_clip_384.openai_ft_in12k_in1k': datetime.datetime(2022, 12, 1, 0, 0),\n",
    " 'skresnet34.ra_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'regnety_080_tv.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'regnetx_080.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'vit_base_patch16_224.orig_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'convit_base.fb_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'maxvit_large_tf_512.in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'coat_mini.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'vit_relpos_small_patch16_224.sw_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'mobilenetv3_large_100.ra_in1k': datetime.datetime(2022, 12, 16, 0, 0),\n",
    " 'coatnet_rmlp_2_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'xcit_tiny_24_p8_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'regnetx_008.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'densenet201.tv_in1k': datetime.datetime(2023, 4, 22, 0, 0),\n",
    " 'caformer_b36.sail_in1k_384': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'beitv2_large_patch16_224.in1k_ft_in22k_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'davit_tiny.msft_in1k': datetime.datetime(2023, 1, 27, 0, 0),\n",
    " 'xcit_large_24_p8_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'resmlp_24_224.fb_distilled_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'mobilevitv2_150.cvnets_in22k_ft_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'deit3_medium_patch16_224.fb_in22k_ft_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'vit_tiny_patch16_224.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'inception_v3.tv_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'xcit_small_12_p16_224.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'volo_d4_224.sail_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'regnety_008_tv.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384': datetime.datetime(2023, 2, 7, 0, 0),\n",
    " 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'convmixer_1024_20_ks9_p14.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'repvgg_b1g4.rvgg_in1k': datetime.datetime(2023, 3, 22, 0, 0),\n",
    " 'res2net50_26w_6s.in1k': datetime.datetime(2024, 3, 10, 0, 0),\n",
    " 'tf_efficientnet_b6.ns_jft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'efficientnet_lite0.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'mobilevitv2_200.cvnets_in22k_ft_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'mobilenetv2_120d.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'bat_resnext26ts.ch_in1k': datetime.datetime(2023, 3, 22, 0, 0),\n",
    " 'cait_s24_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'lambda_resnet26rpt_256.c1_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'coatnet_bn_0_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'nextvit_small.bd_ssld_6m_in1k': datetime.datetime(2024, 2, 11, 0, 0),\n",
    " 'convformer_s18.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'tiny_vit_21m_224.in1k': datetime.datetime(2023, 9, 11, 0, 0),\n",
    " 'eva_large_patch14_196.in22k_ft_in22k_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'mobileone_s3.apple_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'caformer_m36.sail_in1k_384': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'deit_base_distilled_patch16_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'nest_base_jx.goog_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'inception_next_base.sail_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'eva02_large_patch14_448.mim_in22k_ft_in1k': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'tf_efficientnet_b1.ap_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'cspdarknet53.ra_in1k': datetime.datetime(2023, 4, 12, 0, 0),\n",
    " 'repghostnet_200.in1k': datetime.datetime(2023, 8, 20, 0, 0),\n",
    " 'efficientvit_b2.r288_in1k': datetime.datetime(2023, 8, 19, 0, 0),\n",
    " 'seresnext101_32x4d.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'tf_efficientnet_b0.ap_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'sehalonet33ts.ra2_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'regnety_320.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'nest_small_jx.goog_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'halo2botnet50ts_256.a1h_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'coatnet_rmlp_1_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'pvt_v2_b5.in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'crossvit_18_dagger_240.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'convnext_base.clip_laiona_augreg_ft_in1k_384': datetime.datetime(2023, 2, 3, 0, 0),\n",
    " 'xcit_medium_24_p16_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'deit_base_patch16_384.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'resnext101_32x4d.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'levit_conv_256.fb_dist_in1k': datetime.datetime(2023, 2, 3, 0, 0),\n",
    " 'swin_small_patch4_window7_224.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'maxvit_tiny_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'hardcorenas_a.miil_green_in1k': datetime.datetime(2023, 4, 21, 0, 0),\n",
    " 'regnety_160.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'resnext101_64x4d.tv_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'convit_small.fb_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'deit3_large_patch16_224.fb_in22k_ft_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'vit_base_patch16_rpn_224.sw_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'resnet101c.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'repghostnet_150.in1k': datetime.datetime(2023, 8, 20, 0, 0),\n",
    " 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'res2net101_26w_4s.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'deit_small_patch16_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'vit_base_patch16_224.sam_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'efficientvit_b2.r256_in1k': datetime.datetime(2023, 8, 19, 0, 0),\n",
    " 'convformer_m36.sail_in22k_ft_in1k_384': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'tf_efficientnet_b2.aa_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'dla60x.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'tf_efficientnet_b5.aa_in1k': datetime.datetime(2023, 4, 27, 0, 0),\n",
    " 'gmlp_s16_224.ra3_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'tresnet_xl.miil_in1k': datetime.datetime(2023, 4, 21, 0, 0),\n",
    " 'poolformerv2_s24.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'pit_s_224.in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'resmlp_big_24_224.fb_distilled_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'xcit_small_12_p16_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'caformer_s18.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'maxvit_base_tf_384.in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'vit_base_patch16_384.orig_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'xcit_medium_24_p8_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'efficientnet_em.ra2_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'mixnet_m.ft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'legacy_seresnet50.in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'tf_efficientnet_b8.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'vit_base_patch16_clip_384.laion2b_ft_in1k': datetime.datetime(2022, 11, 9, 0, 0),\n",
    " 'deit3_base_patch16_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'convnext_small.fb_in22k_ft_in1k_384': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'caformer_m36.sail_in22k_ft_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'gcvit_small.in1k': datetime.datetime(2024, 3, 10, 0, 0),\n",
    " 'deit3_small_patch16_224.fb_in22k_ft_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'nextvit_base.bd_in1k': datetime.datetime(2024, 2, 11, 0, 0),\n",
    " 'xcit_nano_12_p8_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'flexivit_large.1200ep_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'tiny_vit_21m_512.dist_in22k_ft_in1k': datetime.datetime(2023, 11, 1, 0, 0),\n",
    " 'caformer_m36.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'beit_base_patch16_224.in22k_ft_in22k_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k': datetime.datetime(2022, 11, 2, 0, 0),\n",
    " 'tf_efficientnet_b2.ns_jft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'tf_efficientnet_b7.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'swinv2_base_window8_256.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'regnetx_032.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'pvt_v2_b2.in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'efficientvit_l2.r288_in1k': datetime.datetime(2023, 11, 21, 0, 0),\n",
    " 'pit_xs_224.in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'crossvit_15_dagger_240.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'hardcorenas_b.miil_green_in1k': datetime.datetime(2023, 4, 21, 0, 0),\n",
    " 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'mobilevitv2_175.cvnets_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'gernet_m.idstcv_in1k': datetime.datetime(2023, 5, 22, 0, 0),\n",
    " 'convformer_s18.sail_in22k_ft_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'coatnet_2_rw_224.sw_in12k_ft_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'tf_efficientnet_lite1.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'vit_base_patch16_clip_224.laion2b_ft_in1k': datetime.datetime(2022, 11, 28, 0, 0),\n",
    " 'pvt_v2_b4.in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'resmlp_big_24_224.fb_in22k_ft_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'convformer_m36.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'res2net50d.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'efficientvit_b3.r224_in1k': datetime.datetime(2023, 8, 19, 0, 0),\n",
    " 'deit3_base_patch16_224.fb_in22k_ft_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'vit_relpos_base_patch16_clsgap_224.sw_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'tf_efficientnet_el.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'swin_s3_tiny_224.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'tf_mixnet_m.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'crossvit_base_240.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'convnext_large.fb_in22k_ft_in1k_384': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'res2net101d.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'volo_d2_384.sail_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'tf_efficientnet_b5.in1k': datetime.datetime(2023, 4, 27, 0, 0),\n",
    " 'convformer_b36.sail_in22k_ft_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'nextvit_small.bd_in1k_384': datetime.datetime(2024, 2, 11, 0, 0),\n",
    " 'inception_v3.gluon_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'coatnet_0_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'maxvit_large_tf_384.in21k_ft_in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'xcit_tiny_24_p8_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'mobilenetv2_110d.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'resnet50.fb_ssl_yfcc100m_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'swin_large_patch4_window12_384.ms_in22k_ft_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'vit_base_patch32_384.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'densenet161.tv_in1k': datetime.datetime(2023, 4, 22, 0, 0),\n",
    " 'regnety_640.seer_ft_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'maxxvit_rmlp_nano_rw_256.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'regnety_016.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'regnety_320.seer_ft_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'focalnet_tiny_lrf.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'maxvit_large_tf_512.in21k_ft_in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'resnet152c.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'swinv2_small_window8_256.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'swin_base_patch4_window12_384.ms_in22k_ft_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'caformer_s36.sail_in22k_ft_in1k_384': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'coatnet_nano_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'tf_efficientnet_lite3.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'res2next50.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'xcit_small_12_p8_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'xception41p.ra3_in1k': datetime.datetime(2023, 4, 22, 0, 0),\n",
    " 'regnetx_040.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'swin_base_patch4_window12_384.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'tresnet_l.miil_in1k_448': datetime.datetime(2023, 4, 21, 0, 0),\n",
    " 'repvit_m1_1.dist_300e_in1k': datetime.datetime(2023, 10, 20, 0, 0),\n",
    " 'flexivit_small.300ep_in1k': datetime.datetime(2022, 12, 22, 0, 0),\n",
    " 'res2net50_26w_8s.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'coatnet_rmlp_nano_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'repvit_m1_1.dist_450e_in1k': datetime.datetime(2023, 10, 20, 0, 0),\n",
    " 'maxvit_tiny_tf_384.in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'xcit_tiny_24_p16_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'mobilevitv2_175.cvnets_in22k_ft_in1k_384': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'mvitv2_large.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'crossvit_18_dagger_408.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'xcit_tiny_12_p16_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'mobilenetv3_large_100.miil_in21k_ft_in1k': datetime.datetime(2022, 12, 16, 0, 0),\n",
    " 'nextvit_large.bd_ssld_6m_in1k_384': datetime.datetime(2024, 2, 11, 0, 0),\n",
    " 'tf_efficientnet_lite4.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'gcvit_xtiny.in1k': datetime.datetime(2024, 3, 7, 0, 0),\n",
    " 'tf_efficientnet_b5.ns_jft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'vit_base_patch16_clip_224.openai_ft_in12k_in1k': datetime.datetime(2022, 11, 28, 0, 0),\n",
    " 'efficientformerv2_l.snap_dist_in1k': datetime.datetime(2023, 2, 3, 0, 0),\n",
    " 'maxvit_large_tf_384.in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'swin_large_patch4_window7_224.ms_in22k_ft_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'pvt_v2_b2_li.in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'tf_efficientnet_cc_b0_4e.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'legacy_seresnext26_32x4d.in1k': datetime.datetime(2024, 3, 7, 0, 0),\n",
    " 'deit3_small_patch16_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'fbnetc_100.rmsp_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'xcit_tiny_12_p8_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'beit_large_patch16_384.in22k_ft_in22k_in1k': datetime.datetime(2022, 12, 2, 0, 0),\n",
    " 'deit3_medium_patch16_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'beitv2_large_patch16_224.in1k_ft_in1k': datetime.datetime(2023, 5, 9, 0, 0),\n",
    " 'caformer_b36.sail_in22k_ft_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'volo_d2_224.sail_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'resnet50c.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'mobileone_s4.apple_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'hrnet_w40.ms_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'resnext101_32x32d.fb_wsl_ig1b_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'regnety_320.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'maxvit_rmlp_pico_rw_256.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'xcit_small_24_p16_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'regnety_032.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'dla102x2.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'convformer_s36.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'maxvit_rmlp_nano_rw_256.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'levit_128s.fb_dist_in1k': datetime.datetime(2023, 2, 3, 0, 0),\n",
    " 'eva02_base_patch14_448.mim_in22k_ft_in1k': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'eva02_large_patch14_448.mim_m38m_ft_in1k': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'repvgg_b3.rvgg_in1k': datetime.datetime(2023, 3, 22, 0, 0),\n",
    " 'vit_small_r26_s32_384.augreg_in21k_ft_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'resnet101.gluon_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'dla60_res2net.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'levit_128.fb_dist_in1k': datetime.datetime(2023, 2, 3, 0, 0),\n",
    " 'efficientnet_el.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'xcit_tiny_12_p16_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'fastvit_sa12.apple_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'twins_svt_small.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'sequencer2d_l.in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'tresnet_m.miil_in1k': datetime.datetime(2023, 4, 21, 0, 0),\n",
    " 'resnext101_32x16d.fb_wsl_ig1b_ft_in1k': datetime.datetime(2023, 4, 5, 0, 0),\n",
    " 'vit_large_patch14_clip_224.openai_ft_in12k_in1k': datetime.datetime(2022, 11, 3, 0, 0),\n",
    " 'maxvit_rmlp_small_rw_224.sw_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'hrnet_w44.ms_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'hrnet_w30.ms_in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'regnety_004.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'pit_s_distilled_224.in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'tf_efficientnet_b4.ap_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'dla60_res2next.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'deit3_large_patch16_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'efficientvit_l3.r320_in1k': datetime.datetime(2023, 11, 21, 0, 0),\n",
    " 'fastvit_s12.apple_in1k': datetime.datetime(2023, 8, 23, 0, 0),\n",
    " 'vit_relpos_base_patch16_224.sw_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'crossvit_small_240.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'vit_little_patch16_reg1_gap_256.sbb_in12k_ft_in1k': datetime.datetime(2024, 5, 27, 0, 0),\n",
    " 'cait_s24_384.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'tf_mixnet_s.in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'regnetx_160.tv2_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k': datetime.datetime(2022, 11, 5, 0, 0),\n",
    " 'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'botnet26t_256.c1_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'lambda_resnet50ts.a1h_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'tf_efficientnet_b5.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k': datetime.datetime(2023, 1, 20, 0, 0),\n",
    " 'resnest50d_1s4x24d.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'vit_medium_patch16_reg4_gap_256.sbb_in12k_ft_in1k': datetime.datetime(2024, 5, 21, 0, 0),\n",
    " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384': datetime.datetime(2023, 3, 22, 0, 0),\n",
    " 'nextvit_base.bd_ssld_6m_in1k_384': datetime.datetime(2024, 2, 11, 0, 0),\n",
    " 'resmlp_12_224.fb_in1k': datetime.datetime(2023, 3, 28, 0, 0),\n",
    " 'efficientnet_b5.sw_in12k_ft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k': datetime.datetime(2022, 11, 5, 0, 0),\n",
    " 'tf_efficientnet_b0.ns_jft_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'mvitv2_small.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'xcit_medium_24_p16_224.fb_dist_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k': datetime.datetime(2023, 3, 31, 0, 0),\n",
    " 'regnety_006.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'ghostnetv2_100.in1k': datetime.datetime(2023, 8, 20, 0, 0),\n",
    " 'efficientnet_es.ra_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'tf_efficientnet_b3.aa_in1k': datetime.datetime(2022, 12, 13, 0, 0),\n",
    " 'focalnet_base_lrf.ms_in1k': datetime.datetime(2023, 3, 18, 0, 0),\n",
    " 'eca_botnext26ts_256.c1_in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'efficientvit_l3.r256_in1k': datetime.datetime(2023, 11, 21, 0, 0),\n",
    " 'mvitv2_base.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'regnetx_320.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'vit_relpos_medium_patch16_rpn_224.sw_in1k': datetime.datetime(2022, 12, 23, 0, 0),\n",
    " 'efficientvit_b2.r224_in1k': datetime.datetime(2023, 8, 19, 0, 0),\n",
    " 'xcit_small_24_p8_224.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'vit_medium_patch16_reg1_gap_256.sbb_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'regnety_120.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'caformer_b36.sail_in1k': datetime.datetime(2023, 5, 5, 0, 0),\n",
    " 'vit_pwee_patch16_reg1_gap_256.sbb_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'dpn92.mx_in1k': datetime.datetime(2023, 4, 21, 0, 0),\n",
    " 'res2net50_48w_2s.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'regnety_080.pycls_in1k': datetime.datetime(2023, 3, 21, 0, 0),\n",
    " 'sequencer2d_m.in1k': datetime.datetime(2023, 4, 26, 0, 0),\n",
    " 'vit_base_patch16_rope_reg1_gap_256.sbb_in1k': datetime.datetime(2024, 5, 11, 0, 0),\n",
    " 'repvgg_b3g4.rvgg_in1k': datetime.datetime(2023, 3, 22, 0, 0),\n",
    " 'inception_next_small.sail_in1k': datetime.datetime(2023, 8, 24, 0, 0),\n",
    " 'repvit_m2_3.dist_300e_in1k': datetime.datetime(2023, 10, 20, 0, 0),\n",
    " 'xcit_tiny_24_p8_224.fb_in1k': datetime.datetime(2023, 4, 13, 0, 0),\n",
    " 'seresnextaa201d_32x8d.sw_in12k_ft_in1k_384': datetime.datetime(2023, 7, 26, 0, 0),\n",
    " 'twins_svt_base.in1k': datetime.datetime(2023, 4, 24, 0, 0),\n",
    " 'pnasnet5large.tf_in1k': datetime.datetime(2023, 4, 25, 0, 0),\n",
    " 'tf_efficientnet_b3.in1k': datetime.datetime(2023, 4, 27, 0, 0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(np.array(list(timm_model_date_mapping.values())))\n",
    "timm_train_model_indices = sorted_indices[:301]\n",
    "timm_val_model_indices = sorted_indices[301:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 4, 25, 0, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(timm_model_date_mapping.values())[sorted_indices[301]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model_confidence_for_val_samples_tensor_train = timm_model_confidence_for_val_samples_tensor[:, timm_train_model_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip_model_date_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = list(range(60))# np.argsort(np.array(list(open_clip_model_date_mapping.values())))\n",
    "np.random.shuffle(sorted_indices) # surrogate for missing history\n",
    "open_clip_train_model_indices = sorted_indices[:45]\n",
    "open_clip_val_model_indices = sorted_indices[45:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip_model_confidence_for_val_samples_tensor_train = open_clip_model_confidence_for_val_samples_tensor[:, open_clip_train_model_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 300, 1000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_model_confidence_for_val_samples_tensor_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pds(logits):\n",
    "\n",
    "    def are_probs(logits):\n",
    "        if (\n",
    "                logits.min() >= 0\n",
    "            and\n",
    "                logits.max() <= 1\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def get_probs(logits):\n",
    "        if are_probs(logits):\n",
    "            probs = logits\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "\n",
    "    # input shape: (num_models, num_samples, num_classes)\n",
    "    # output shape: (num_samples)\n",
    "\n",
    "    probs = get_probs(logits).clone()\n",
    "\n",
    "    max_by_model = probs.max(0).values\n",
    "    sum_over_classes = max_by_model.sum(-1)\n",
    "\n",
    "    return torch.Tensor(sum_over_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disagreement_scores(predictions_train, n_guiding_models):\n",
    "    if n_guiding_models is not None:\n",
    "        guiding_models_indices = np.random.choice(predictions_train.shape[0], n_guiding_models, replace=False)\n",
    "        predictions_train = predictions_train[guiding_models_indices, ...]\n",
    "    pds_per_sample = pds(torch.Tensor(predictions_train))\n",
    "    return pds_per_sample.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_disagreement_scores = get_disagreement_scores(timm_model_confidence_for_val_samples_tensor_train.transpose(0,1), n_guiding_models=300)\n",
    "timm_top_disagreement_indices = timm_disagreement_scores.argsort()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip_disagreement_scores = get_disagreement_scores(open_clip_model_confidence_for_val_samples_tensor_train.transpose(0,1), n_guiding_models=45)\n",
    "open_clip_top_disagreement_indices = open_clip_disagreement_scores.argsort()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_per_sample_entropy = get_categorical_entropy_per_sample(tensor=timm_model_confidence_for_val_samples_tensor)\n",
    "timm_top_samples_entropy = timm_per_sample_entropy.sort(descending=True).indices[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip_per_sample_entropy = get_categorical_entropy_per_sample(tensor=timm_model_confidence_for_val_samples_tensor)\n",
    "open_clip_top_samples_entropy = open_clip_per_sample_entropy.sort(descending=True).indices[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_389788/689691087.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  l1_distance_tensor = torch.load(os.path.join(path_to_save_folder, \"timm_output_distances_1000_l1.json\"))\n"
     ]
    }
   ],
   "source": [
    "l1_distance_tensor = torch.load(os.path.join(path_to_save_folder, \"timm_output_distances_1000_l1.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_distance_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_389788/1028495380.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  l2_distance_tensor = torch.load(os.path.join(path_to_save_folder, \"timm_output_distances_1000_l2.json\"))\n"
     ]
    }
   ],
   "source": [
    "l2_distance_tensor = torch.load(os.path.join(path_to_save_folder, \"timm_output_distances_1000_l2.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_distance_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m kmedoids \u001b[38;5;241m=\u001b[39m KMedoids(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpam\u001b[39m\u001b[38;5;124m'\u001b[39m, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk-medoids++\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mkmedoids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml1_distance_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m l1_dist_top_indices \u001b[38;5;241m=\u001b[39m kmedoids\u001b[38;5;241m.\u001b[39mmedoid_indices_\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(l1_dist_top_indices)\n",
      "File \u001b[0;32m~/.conda/envs/model-selection/lib/python3.10/site-packages/sklearn_extra/cluster/_k_medoids.py:273\u001b[0m, in \u001b[0;36mKMedoids.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    272\u001b[0m     not_medoid_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(D)), medoid_idxs)\n\u001b[0;32m--> 273\u001b[0m     optimal_swap \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_optimal_swap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmedoid_idxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnot_medoid_idxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimal_swap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         i, j, _ \u001b[38;5;241m=\u001b[39m optimal_swap\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kmedoids = KMedoids(n_clusters=100, metric='precomputed', method='pam', init='k-medoids++', random_state=42)\n",
    "kmedoids.fit(l1_distance_tensor)\n",
    "l1_dist_top_indices = kmedoids.medoid_indices_\n",
    "print(l1_dist_top_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KMeans.__init__() got an unexpected keyword argument 'metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecomputed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk-medoids++\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(l1_distance_tensor)\n\u001b[1;32m      3\u001b[0m l1_dist_top_indices \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_\n",
      "\u001b[0;31mTypeError\u001b[0m: KMeans.__init__() got an unexpected keyword argument 'metric'"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=100, metric='precomputed', method='pam', init='k-medoids++', random_state=42)\n",
    "kmeans.fit(l1_distance_tensor)\n",
    "l1_dist_top_indices = kmeans.cluster_centers_\n",
    "print(l1_dist_top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids = KMedoids(n_clusters=100, metric='precomputed', method='pam', init='k-medoids++', random_state=42)\n",
    "kmedoids.fit(l1_distance_tensor)\n",
    "l2_dist_top_indices = kmedoids.medoid_indices_\n",
    "print(l2_dist_top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_distance_selection(distance_matrix, num_samples=100):\n",
    "    \"\"\"\n",
    "    Selects a subset of points to maximize the minimum pairwise distance.\n",
    "\n",
    "    Parameters:\n",
    "    - distance_matrix (numpy.ndarray): Precomputed 10,000 x 10,000 distance matrix.\n",
    "    - num_samples (int): Number of samples to select.\n",
    "\n",
    "    Returns:\n",
    "    - selected_indices (list): Indices of the selected samples.\n",
    "    \"\"\"\n",
    "    num_points = distance_matrix.shape[0]\n",
    "    selected_indices = [np.random.randint(num_points)]  # Start with a random point\n",
    "    remaining_indices = list(set(range(num_points)) - set(selected_indices))\n",
    "\n",
    "    for i in range(1, num_samples):\n",
    "        print('iteration:', i)\n",
    "        # Compute the minimum distance from each unselected point to the selected set\n",
    "        min_distances = np.array([min(distance_matrix[i, selected_indices]) for i in remaining_indices])\n",
    "        # Select the point with the maximum of these minimum distances\n",
    "        next_index = list(remaining_indices)[np.argmax(min_distances)]\n",
    "        selected_indices.append(remaining_indices[next_index])\n",
    "        remaining_indices.pop(next_index)\n",
    "\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_min_distance_selection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l1_dist_top_indices \u001b[38;5;241m=\u001b[39m \u001b[43mmax_min_distance_selection\u001b[49m(l1_distance_tensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_min_distance_selection' is not defined"
     ]
    }
   ],
   "source": [
    "l1_dist_top_indices = max_min_distance_selection(l1_distance_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "iteration: 20\n",
      "iteration: 21\n",
      "iteration: 22\n",
      "iteration: 23\n",
      "iteration: 24\n",
      "iteration: 25\n",
      "iteration: 26\n",
      "iteration: 27\n",
      "iteration: 28\n",
      "iteration: 29\n",
      "iteration: 30\n",
      "iteration: 31\n",
      "iteration: 32\n",
      "iteration: 33\n",
      "iteration: 34\n",
      "iteration: 35\n",
      "iteration: 36\n",
      "iteration: 37\n",
      "iteration: 38\n",
      "iteration: 39\n",
      "iteration: 40\n",
      "iteration: 41\n",
      "iteration: 42\n",
      "iteration: 43\n",
      "iteration: 44\n",
      "iteration: 45\n",
      "iteration: 46\n",
      "iteration: 47\n",
      "iteration: 48\n",
      "iteration: 49\n",
      "iteration: 50\n",
      "iteration: 51\n",
      "iteration: 52\n",
      "iteration: 53\n",
      "iteration: 54\n",
      "iteration: 55\n",
      "iteration: 56\n",
      "iteration: 57\n",
      "iteration: 58\n",
      "iteration: 59\n",
      "iteration: 60\n",
      "iteration: 61\n",
      "iteration: 62\n",
      "iteration: 63\n",
      "iteration: 64\n",
      "iteration: 65\n",
      "iteration: 66\n",
      "iteration: 67\n",
      "iteration: 68\n",
      "iteration: 69\n",
      "iteration: 70\n",
      "iteration: 71\n",
      "iteration: 72\n",
      "iteration: 73\n",
      "iteration: 74\n",
      "iteration: 75\n",
      "iteration: 76\n",
      "iteration: 77\n",
      "iteration: 78\n",
      "iteration: 79\n",
      "iteration: 80\n",
      "iteration: 81\n",
      "iteration: 82\n",
      "iteration: 83\n",
      "iteration: 84\n",
      "iteration: 85\n",
      "iteration: 86\n",
      "iteration: 87\n",
      "iteration: 88\n",
      "iteration: 89\n",
      "iteration: 90\n",
      "iteration: 91\n",
      "iteration: 92\n",
      "iteration: 93\n",
      "iteration: 94\n",
      "iteration: 95\n",
      "iteration: 96\n",
      "iteration: 97\n",
      "iteration: 98\n",
      "iteration: 99\n"
     ]
    }
   ],
   "source": [
    "l2_dist_top_indices = max_min_distance_selection(l2_distance_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_968094/1717516195.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  timm_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_main_set.tensor\"))\n"
     ]
    }
   ],
   "source": [
    "path_to_save_folder = \"/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints\"\n",
    "timm_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_main_set.tensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_result_performances = []\n",
    "for model_info in timm_model_catalog.values():\n",
    "    timm_result_performances.append(model_info['results']['imagenet1k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_rank_score_dict: {'imagenet1k': [0.7500030184554329, 0.7271816243493379, 0.5498106498159113, 0.7845073208201873, 0.7051698425650463]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.7439743248667223, 0.8111314360209833, 0.8089262035061523, 0.8525280247341112, 0.8794079454965471]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.7984837506086049, 0.9100794140782342, 0.7832281385112357, 0.7883913877244001, 0.8067107224854533]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.895013878128575, 0.7884206912088992, 0.8686936506866381, 0.8371345715525711, 0.8225423314934496]}\n",
      "Epoch [10/700], Train-Loss: 0.3022, Val-Loss: 0.2517\n",
      "Epoch [20/700], Train-Loss: 0.0272, Val-Loss: 0.0456\n",
      "Epoch [30/700], Train-Loss: 0.0018, Val-Loss: 0.0009\n",
      "Epoch [40/700], Train-Loss: 0.0117, Val-Loss: 0.0097\n",
      "Epoch [50/700], Train-Loss: 0.0032, Val-Loss: 0.0033\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0008, Val-Loss: 0.0006\n",
      "Epoch [80/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [90/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [150/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [320/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [330/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [340/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [350/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [360/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [370/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [380/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [390/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [400/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [410/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [420/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [430/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [440/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [450/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [460/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [470/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [480/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [490/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [500/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [510/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [520/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [530/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "x: tensor([[9.2254e-05, 9.4417e-05, 2.0619e-04,  ..., 9.6760e-05, 1.1498e-04,\n",
      "         8.6502e-05],\n",
      "        [1.2946e-04, 1.6005e-04, 1.0275e-04,  ..., 1.6591e-04, 1.9208e-04,\n",
      "         1.5882e-04],\n",
      "        [1.9181e-04, 1.1549e-04, 1.3919e-04,  ..., 9.3348e-05, 1.9701e-04,\n",
      "         2.5289e-04],\n",
      "        ...,\n",
      "        [6.5453e-05, 1.0364e-04, 8.2907e-05,  ..., 1.8335e-04, 1.0841e-04,\n",
      "         1.5794e-04],\n",
      "        [7.8019e-05, 4.5392e-05, 5.2113e-05,  ..., 3.5779e-05, 8.3198e-05,\n",
      "         6.8713e-05],\n",
      "        [3.0060e-05, 1.2940e-04, 1.5330e-05,  ..., 8.6923e-05, 5.4292e-05,\n",
      "         1.9698e-04]], device='cuda:0')\n",
      "pred: tensor([0.8367, 0.8358, 0.8031, 0.8247, 0.8038, 0.8252, 0.8260, 0.8301, 0.8055,\n",
      "        0.8352, 0.8111, 0.8426, 0.7877, 0.8009, 0.8220, 0.8154, 0.8234, 0.8319,\n",
      "        0.7962, 0.7976, 0.8168, 0.8412, 0.8694, 0.8508, 0.8696, 0.8181, 0.8583,\n",
      "        0.8220, 0.8495, 0.8380, 0.8443, 0.8590, 0.8564, 0.8935, 0.8497, 0.8582,\n",
      "        0.8439, 0.8491, 0.8305, 0.8461, 0.8462, 0.8635, 0.7923, 0.8409, 0.8242,\n",
      "        0.8331, 0.8487, 0.8132, 0.8263, 0.8161, 0.7696, 0.7758, 0.7222, 0.7692,\n",
      "        0.7974, 0.7760, 0.8180, 0.8286, 0.7215, 0.8381, 0.7233, 0.8146, 0.7961,\n",
      "        0.7968, 0.8388, 0.8413, 0.7768, 0.7459, 0.7765, 0.8347, 0.8272, 0.7498,\n",
      "        0.8222, 0.8068, 0.7724, 0.8488, 0.8521, 0.8534, 0.8473, 0.8544, 0.8238,\n",
      "        0.8424, 0.8641, 0.8393, 0.8576, 0.8169, 0.7704, 0.8392, 0.8122, 0.8388,\n",
      "        0.8345, 0.8488, 0.8177, 0.8387, 0.8536, 0.8473, 0.8164, 0.8506, 0.8514,\n",
      "        0.8438], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3461, Val-Loss: 0.2898\n",
      "Epoch [20/700], Train-Loss: 0.0416, Val-Loss: 0.0631\n",
      "Epoch [30/700], Train-Loss: 0.0009, Val-Loss: 0.0025\n",
      "Epoch [40/700], Train-Loss: 0.0104, Val-Loss: 0.0076\n",
      "Epoch [50/700], Train-Loss: 0.0041, Val-Loss: 0.0040\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0008\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "x: tensor([[1.0156e-04, 5.2574e-05, 1.1353e-04,  ..., 1.5951e-04, 4.0137e-04,\n",
      "         3.8161e-04],\n",
      "        [1.3745e-04, 8.8664e-05, 1.0320e-04,  ..., 2.6409e-04, 1.4996e-03,\n",
      "         4.8585e-04],\n",
      "        [1.7196e-04, 2.1893e-04, 1.8467e-04,  ..., 3.7697e-04, 1.6434e-03,\n",
      "         8.1344e-04],\n",
      "        ...,\n",
      "        [1.4351e-04, 2.2328e-04, 2.3357e-04,  ..., 1.1595e-04, 4.7516e-04,\n",
      "         1.4626e-04],\n",
      "        [9.2173e-05, 8.9375e-05, 1.3104e-04,  ..., 5.1715e-05, 1.6033e-04,\n",
      "         8.3683e-04],\n",
      "        [6.4485e-05, 4.7476e-05, 1.3882e-04,  ..., 4.8785e-05, 2.8225e-04,\n",
      "         4.0443e-04]], device='cuda:0')\n",
      "pred: tensor([0.8405, 0.8165, 0.7850, 0.8293, 0.8029, 0.8411, 0.8130, 0.8293, 0.8038,\n",
      "        0.8339, 0.8017, 0.8331, 0.8087, 0.8253, 0.8287, 0.7959, 0.8299, 0.8247,\n",
      "        0.7704, 0.7816, 0.8238, 0.8401, 0.8606, 0.8554, 0.8886, 0.8075, 0.8859,\n",
      "        0.8255, 0.8652, 0.8409, 0.8605, 0.8605, 0.8691, 0.8583, 0.8505, 0.8700,\n",
      "        0.8277, 0.8659, 0.8283, 0.8172, 0.8494, 0.8601, 0.8070, 0.8771, 0.8117,\n",
      "        0.7827, 0.8258, 0.8388, 0.8261, 0.8343, 0.7669, 0.7921, 0.7453, 0.7547,\n",
      "        0.7698, 0.7642, 0.8234, 0.8251, 0.7629, 0.8402, 0.8148, 0.8174, 0.8158,\n",
      "        0.8158, 0.8424, 0.8523, 0.7974, 0.7776, 0.7906, 0.8314, 0.8351, 0.7892,\n",
      "        0.8339, 0.7898, 0.8023, 0.8674, 0.8439, 0.8528, 0.8508, 0.8484, 0.8369,\n",
      "        0.8430, 0.8561, 0.8358, 0.8640, 0.8205, 0.7762, 0.8369, 0.7841, 0.8491,\n",
      "        0.8370, 0.8386, 0.8141, 0.8409, 0.8446, 0.8494, 0.8279, 0.8571, 0.8350,\n",
      "        0.8427], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3599, Val-Loss: 0.3072\n",
      "Epoch [20/700], Train-Loss: 0.0292, Val-Loss: 0.0520\n",
      "Epoch [30/700], Train-Loss: 0.0011, Val-Loss: 0.0015\n",
      "Epoch [40/700], Train-Loss: 0.0117, Val-Loss: 0.0090\n",
      "Epoch [50/700], Train-Loss: 0.0042, Val-Loss: 0.0040\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0010\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "x: tensor([[1.6200e-04, 1.8651e-04, 3.1826e-04,  ..., 8.4435e-05, 1.3965e-04,\n",
      "         1.2168e-04],\n",
      "        [1.2615e-04, 1.6175e-04, 3.8554e-04,  ..., 9.2869e-05, 9.6369e-05,\n",
      "         1.2789e-04],\n",
      "        [4.4194e-05, 1.1475e-04, 1.0632e-04,  ..., 2.1390e-04, 2.7876e-04,\n",
      "         1.7988e-04],\n",
      "        ...,\n",
      "        [1.4980e-04, 1.8691e-04, 2.2413e-04,  ..., 2.7776e-05, 7.5475e-05,\n",
      "         1.2152e-04],\n",
      "        [1.3914e-04, 1.7739e-04, 2.8585e-04,  ..., 2.4491e-04, 8.5043e-05,\n",
      "         7.8248e-04],\n",
      "        [1.1795e-05, 4.6851e-06, 5.1555e-05,  ..., 5.8640e-05, 5.9663e-05,\n",
      "         1.6926e-04]], device='cuda:0')\n",
      "pred: tensor([0.8259, 0.8187, 0.8221, 0.8071, 0.7942, 0.8315, 0.8182, 0.8253, 0.7910,\n",
      "        0.7863, 0.7957, 0.8409, 0.8012, 0.8176, 0.8541, 0.8163, 0.8181, 0.8280,\n",
      "        0.8160, 0.7952, 0.8089, 0.8246, 0.8557, 0.8661, 0.8664, 0.7719, 0.8590,\n",
      "        0.8032, 0.8632, 0.8432, 0.8641, 0.8579, 0.8496, 0.8627, 0.8631, 0.8616,\n",
      "        0.8369, 0.8581, 0.8181, 0.8299, 0.8243, 0.8597, 0.7843, 0.8550, 0.8281,\n",
      "        0.8138, 0.8645, 0.8452, 0.8539, 0.8474, 0.7358, 0.7928, 0.7732, 0.7453,\n",
      "        0.7957, 0.7932, 0.8230, 0.8110, 0.7818, 0.8340, 0.7876, 0.8089, 0.8061,\n",
      "        0.7864, 0.8407, 0.8523, 0.7927, 0.7591, 0.7779, 0.8357, 0.8228, 0.8054,\n",
      "        0.8221, 0.7432, 0.8020, 0.8572, 0.8619, 0.8456, 0.8661, 0.8658, 0.8205,\n",
      "        0.8385, 0.8477, 0.8267, 0.8427, 0.8274, 0.7627, 0.8298, 0.7907, 0.8319,\n",
      "        0.8292, 0.8288, 0.8093, 0.8417, 0.8487, 0.8317, 0.8116, 0.8294, 0.8404,\n",
      "        0.8175], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3260, Val-Loss: 0.2837\n",
      "Epoch [20/700], Train-Loss: 0.0057, Val-Loss: 0.0190\n",
      "Epoch [30/700], Train-Loss: 0.0064, Val-Loss: 0.0018\n",
      "Epoch [40/700], Train-Loss: 0.0120, Val-Loss: 0.0109\n",
      "Epoch [50/700], Train-Loss: 0.0020, Val-Loss: 0.0023\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0009, Val-Loss: 0.0008\n",
      "Epoch [80/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [90/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [100/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [150/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [160/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0004\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0004\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "x: tensor([[1.6911e-04, 1.4435e-04, 1.5827e-04,  ..., 1.9337e-04, 2.4344e-04,\n",
      "         1.1344e-04],\n",
      "        [1.9565e-04, 3.6320e-05, 3.6020e-05,  ..., 9.0513e-05, 3.2166e-04,\n",
      "         6.4391e-05],\n",
      "        [1.9549e-04, 1.7155e-04, 1.7832e-04,  ..., 1.0272e-04, 1.6961e-04,\n",
      "         1.5103e-04],\n",
      "        ...,\n",
      "        [2.0578e-04, 2.0447e-04, 1.2028e-04,  ..., 1.1367e-04, 7.3368e-05,\n",
      "         1.2591e-04],\n",
      "        [1.2164e-04, 8.0910e-05, 8.5194e-05,  ..., 1.1929e-04, 7.2028e-05,\n",
      "         1.1524e-04],\n",
      "        [1.6376e-04, 5.6809e-05, 6.2129e-05,  ..., 1.1963e-04, 1.1126e-04,\n",
      "         4.5518e-05]], device='cuda:0')\n",
      "pred: tensor([0.8522, 0.8099, 0.8028, 0.7880, 0.7621, 0.8056, 0.8214, 0.7880, 0.7942,\n",
      "        0.8409, 0.8089, 0.8184, 0.8034, 0.8251, 0.8286, 0.8245, 0.8335, 0.8254,\n",
      "        0.8222, 0.8104, 0.8337, 0.8580, 0.8538, 0.8620, 0.8686, 0.8054, 0.8623,\n",
      "        0.8474, 0.8511, 0.8354, 0.8479, 0.8713, 0.8600, 0.8537, 0.8522, 0.8589,\n",
      "        0.8380, 0.8540, 0.8413, 0.5917, 0.8369, 0.8628, 0.7888, 0.8663, 0.8389,\n",
      "        0.7975, 0.8476, 0.8441, 0.8432, 0.8220, 0.7866, 0.7795, 0.7580, 0.8102,\n",
      "        0.8103, 0.7295, 0.8368, 0.8464, 0.7322, 0.8557, 0.8083, 0.8043, 0.7871,\n",
      "        0.7972, 0.8224, 0.8442, 0.7973, 0.7440, 0.8339, 0.8413, 0.8189, 0.7590,\n",
      "        0.8253, 0.8116, 0.8065, 0.8777, 0.8533, 0.8483, 0.8579, 0.8559, 0.8447,\n",
      "        0.8385, 0.8646, 0.8461, 0.8594, 0.8165, 0.7933, 0.8350, 0.8156, 0.8441,\n",
      "        0.8470, 0.8458, 0.8186, 0.8330, 0.8511, 0.8480, 0.8044, 0.8516, 0.8173,\n",
      "        0.8332], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2226, Val-Loss: 0.1754\n",
      "Epoch [20/700], Train-Loss: 0.0578, Val-Loss: 0.0615\n",
      "Epoch [30/700], Train-Loss: 0.0041, Val-Loss: 0.0076\n",
      "Epoch [40/700], Train-Loss: 0.0048, Val-Loss: 0.0029\n",
      "Epoch [50/700], Train-Loss: 0.0038, Val-Loss: 0.0029\n",
      "Epoch [60/700], Train-Loss: 0.0014, Val-Loss: 0.0015\n",
      "Epoch [70/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[1.0362e-04, 2.3926e-04, 6.3781e-04,  ..., 6.9296e-05, 8.9544e-05,\n",
      "         7.3640e-05],\n",
      "        [1.4193e-04, 2.1596e-04, 1.1537e-04,  ..., 2.1474e-04, 1.2660e-04,\n",
      "         1.7848e-04],\n",
      "        [1.8593e-04, 2.5489e-04, 3.8255e-04,  ..., 3.4540e-04, 3.3806e-04,\n",
      "         1.9275e-04],\n",
      "        ...,\n",
      "        [1.1842e-04, 1.7097e-04, 1.4428e-04,  ..., 1.1711e-04, 1.1646e-04,\n",
      "         1.0485e-04],\n",
      "        [1.6468e-04, 2.9775e-04, 6.2728e-05,  ..., 4.9688e-05, 1.2666e-04,\n",
      "         5.7969e-05],\n",
      "        [1.5991e-04, 4.3841e-04, 1.8657e-04,  ..., 5.2883e-05, 4.5980e-05,\n",
      "         1.0328e-04]], device='cuda:0')\n",
      "pred: tensor([0.8433, 0.8260, 0.8058, 0.8204, 0.7870, 0.8055, 0.8239, 0.8231, 0.8125,\n",
      "        0.8186, 0.7920, 0.8339, 0.7997, 0.8275, 0.8577, 0.7874, 0.8287, 0.8251,\n",
      "        0.7945, 0.7916, 0.8139, 0.8291, 0.8599, 0.8468, 0.8661, 0.7940, 0.8612,\n",
      "        0.8288, 0.8510, 0.8241, 0.8504, 0.8677, 0.8640, 0.8558, 0.8496, 0.8558,\n",
      "        0.8172, 0.8649, 0.8066, 0.8422, 0.8287, 0.8518, 0.8095, 0.8599, 0.8324,\n",
      "        0.8049, 0.8493, 0.8234, 0.8367, 0.8430, 0.7362, 0.8031, 0.7757, 0.7931,\n",
      "        0.7826, 0.7886, 0.8219, 0.8330, 0.7682, 0.8319, 0.7757, 0.8117, 0.7885,\n",
      "        0.7976, 0.8299, 0.8334, 0.8221, 0.8004, 0.8068, 0.8308, 0.8195, 0.7815,\n",
      "        0.8400, 0.8073, 0.8091, 0.8555, 0.8422, 0.8362, 0.8480, 0.8470, 0.8436,\n",
      "        0.8395, 0.8653, 0.8434, 0.8731, 0.8288, 0.7585, 0.8360, 0.8047, 0.8397,\n",
      "        0.8411, 0.8336, 0.8202, 0.8272, 0.8513, 0.8452, 0.8097, 0.8601, 0.8373,\n",
      "        0.8327], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9138433843384338, 0.9353735373537353, 0.8730873087308729, 0.8849444944494449, 0.9085628562856285]}\n",
      "Epoch [10/700], Train-Loss: 0.2547, Val-Loss: 0.1943\n",
      "Epoch [20/700], Train-Loss: 0.0792, Val-Loss: 0.0752\n",
      "Epoch [30/700], Train-Loss: 0.0085, Val-Loss: 0.0132\n",
      "Epoch [40/700], Train-Loss: 0.0030, Val-Loss: 0.0015\n",
      "Epoch [50/700], Train-Loss: 0.0038, Val-Loss: 0.0023\n",
      "Epoch [60/700], Train-Loss: 0.0018, Val-Loss: 0.0020\n",
      "Epoch [70/700], Train-Loss: 0.0008, Val-Loss: 0.0007\n",
      "Epoch [80/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[3.0756e-04, 3.0300e-04, 4.0017e-04,  ..., 1.8443e-04, 1.4441e-04,\n",
      "         1.4970e-04],\n",
      "        [3.7191e-05, 7.0353e-05, 5.7824e-05,  ..., 5.6323e-05, 9.9687e-05,\n",
      "         1.0754e-04],\n",
      "        [3.4678e-04, 4.7422e-04, 2.7627e-04,  ..., 2.7763e-04, 1.0912e-04,\n",
      "         1.2364e-04],\n",
      "        ...,\n",
      "        [2.3804e-04, 2.0203e-04, 2.4897e-04,  ..., 1.0973e-04, 1.3814e-04,\n",
      "         1.1828e-04],\n",
      "        [5.9972e-05, 7.5861e-05, 6.5762e-05,  ..., 1.4568e-04, 1.3412e-04,\n",
      "         1.2971e-04],\n",
      "        [4.0844e-05, 8.8912e-05, 6.8472e-05,  ..., 8.5910e-05, 1.3873e-04,\n",
      "         1.2897e-04]], device='cuda:0')\n",
      "pred: tensor([0.8333, 0.8295, 0.8037, 0.8243, 0.8112, 0.8215, 0.8224, 0.8080, 0.7976,\n",
      "        0.8227, 0.8068, 0.8307, 0.8029, 0.8035, 0.8300, 0.8127, 0.8129, 0.8269,\n",
      "        0.7829, 0.8167, 0.8242, 0.8392, 0.8556, 0.8357, 0.8695, 0.8136, 0.8634,\n",
      "        0.8072, 0.8345, 0.8201, 0.8291, 0.8602, 0.8543, 0.8533, 0.8324, 0.8550,\n",
      "        0.8296, 0.8428, 0.8366, 0.8293, 0.8400, 0.8602, 0.8060, 0.8574, 0.8191,\n",
      "        0.7861, 0.8336, 0.8078, 0.8184, 0.8283, 0.7709, 0.8027, 0.7481, 0.7778,\n",
      "        0.7904, 0.7791, 0.8252, 0.8239, 0.7329, 0.8386, 0.8196, 0.7879, 0.7765,\n",
      "        0.8020, 0.8398, 0.8380, 0.7870, 0.7681, 0.8055, 0.8363, 0.8411, 0.8096,\n",
      "        0.8215, 0.8141, 0.8145, 0.8618, 0.8306, 0.8274, 0.8363, 0.8355, 0.8388,\n",
      "        0.8295, 0.8614, 0.8471, 0.8634, 0.8092, 0.7593, 0.8320, 0.8059, 0.8364,\n",
      "        0.8464, 0.8461, 0.7951, 0.8241, 0.8574, 0.8373, 0.7975, 0.8593, 0.8373,\n",
      "        0.8374], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0780, Val-Loss: 0.0359\n",
      "Epoch [20/700], Train-Loss: 0.0258, Val-Loss: 0.0105\n",
      "Epoch [30/700], Train-Loss: 0.0164, Val-Loss: 0.0148\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0042\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0012\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.2674e-05, 9.9807e-05, 1.9681e-04,  ..., 2.8562e-04, 2.8271e-04,\n",
      "         2.5141e-04],\n",
      "        [7.6622e-05, 7.8521e-05, 1.2992e-04,  ..., 2.3063e-04, 3.7762e-04,\n",
      "         2.8923e-04],\n",
      "        [4.9267e-05, 1.2197e-04, 9.9808e-05,  ..., 7.6309e-04, 6.2996e-04,\n",
      "         1.4350e-03],\n",
      "        ...,\n",
      "        [4.1868e-05, 7.6611e-05, 1.3134e-04,  ..., 6.4458e-05, 1.1919e-04,\n",
      "         2.9457e-04],\n",
      "        [4.7052e-05, 1.1495e-04, 4.9247e-04,  ..., 6.2631e-05, 7.2811e-05,\n",
      "         1.5094e-04],\n",
      "        [1.5067e-05, 3.2040e-05, 5.4885e-05,  ..., 4.5313e-05, 2.8598e-04,\n",
      "         2.1515e-04]], device='cuda:0')\n",
      "pred: tensor([0.8332, 0.8173, 0.7925, 0.8234, 0.7861, 0.8359, 0.8251, 0.8271, 0.8132,\n",
      "        0.8233, 0.7901, 0.8262, 0.7787, 0.8114, 0.8243, 0.8241, 0.8196, 0.8280,\n",
      "        0.7835, 0.7942, 0.8248, 0.8426, 0.8802, 0.8486, 0.8807, 0.8131, 0.8752,\n",
      "        0.8118, 0.8425, 0.8450, 0.8516, 0.8474, 0.8755, 0.8685, 0.8528, 0.8647,\n",
      "        0.8310, 0.8366, 0.8402, 0.8190, 0.8408, 0.8681, 0.7985, 0.8691, 0.8328,\n",
      "        0.7919, 0.8319, 0.8339, 0.8382, 0.8295, 0.7418, 0.8059, 0.7743, 0.7470,\n",
      "        0.7765, 0.7853, 0.8226, 0.8167, 0.7723, 0.8333, 0.7973, 0.8005, 0.7932,\n",
      "        0.7984, 0.8279, 0.8436, 0.7643, 0.7820, 0.8044, 0.8404, 0.8302, 0.8086,\n",
      "        0.8224, 0.7865, 0.7935, 0.8605, 0.8474, 0.8444, 0.8534, 0.8581, 0.8317,\n",
      "        0.8334, 0.8728, 0.8550, 0.8713, 0.8175, 0.7770, 0.8377, 0.7938, 0.8450,\n",
      "        0.8370, 0.8354, 0.8149, 0.8369, 0.8627, 0.8516, 0.8195, 0.8774, 0.8305,\n",
      "        0.8527], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1094, Val-Loss: 0.0547\n",
      "Epoch [20/700], Train-Loss: 0.0450, Val-Loss: 0.0233\n",
      "Epoch [30/700], Train-Loss: 0.0218, Val-Loss: 0.0216\n",
      "Epoch [40/700], Train-Loss: 0.0029, Val-Loss: 0.0038\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0008\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "x: tensor([[1.4626e-04, 1.4899e-04, 1.6015e-04,  ..., 1.0963e-04, 1.0881e-04,\n",
      "         8.0436e-05],\n",
      "        [2.4368e-04, 2.2652e-04, 1.1787e-04,  ..., 1.6136e-04, 2.3103e-04,\n",
      "         1.9621e-04],\n",
      "        [1.2225e-04, 2.5591e-04, 1.6261e-04,  ..., 2.3443e-04, 3.7616e-04,\n",
      "         1.3850e-04],\n",
      "        ...,\n",
      "        [7.9184e-05, 9.7182e-05, 6.6286e-05,  ..., 1.0857e-04, 6.8576e-05,\n",
      "         9.0601e-05],\n",
      "        [2.2464e-04, 1.2889e-04, 7.3443e-05,  ..., 1.0320e-04, 7.8249e-05,\n",
      "         6.3393e-05],\n",
      "        [1.8503e-04, 1.4176e-04, 4.2435e-05,  ..., 8.8556e-05, 5.3949e-05,\n",
      "         7.0657e-05]], device='cuda:0')\n",
      "pred: tensor([0.8400, 0.8163, 0.8110, 0.8079, 0.7907, 0.8258, 0.7939, 0.8149, 0.7901,\n",
      "        0.8211, 0.8083, 0.8269, 0.8002, 0.8104, 0.8443, 0.8235, 0.8277, 0.8197,\n",
      "        0.7849, 0.7818, 0.8147, 0.8470, 0.8627, 0.8503, 0.8836, 0.8041, 0.8749,\n",
      "        0.8307, 0.8411, 0.8408, 0.8423, 0.8677, 0.8709, 0.8728, 0.8501, 0.8716,\n",
      "        0.8329, 0.8391, 0.8523, 0.8207, 0.8345, 0.8733, 0.7898, 0.8697, 0.8225,\n",
      "        0.8072, 0.8373, 0.8167, 0.8203, 0.8312, 0.7653, 0.7775, 0.7312, 0.7679,\n",
      "        0.7944, 0.7357, 0.8196, 0.8335, 0.7547, 0.8370, 0.7766, 0.8079, 0.7471,\n",
      "        0.8156, 0.8421, 0.8427, 0.7946, 0.7581, 0.8033, 0.8356, 0.8281, 0.7796,\n",
      "        0.8151, 0.7932, 0.8005, 0.8688, 0.8436, 0.8349, 0.8760, 0.8529, 0.8363,\n",
      "        0.8330, 0.8374, 0.8295, 0.8430, 0.8218, 0.7656, 0.8386, 0.8054, 0.8445,\n",
      "        0.8364, 0.8452, 0.8222, 0.8427, 0.8588, 0.8437, 0.8086, 0.8620, 0.8649,\n",
      "        0.8351], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0849, Val-Loss: 0.0333\n",
      "Epoch [20/700], Train-Loss: 0.0287, Val-Loss: 0.0108\n",
      "Epoch [30/700], Train-Loss: 0.0216, Val-Loss: 0.0189\n",
      "Epoch [40/700], Train-Loss: 0.0054, Val-Loss: 0.0060\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "x: tensor([[1.8698e-04, 1.5137e-04, 1.0802e-04,  ..., 1.3641e-04, 1.0981e-04,\n",
      "         1.3031e-04],\n",
      "        [3.1481e-04, 1.9240e-04, 9.6780e-05,  ..., 1.2693e-04, 1.0142e-04,\n",
      "         1.9747e-04],\n",
      "        [6.6207e-04, 4.4603e-04, 2.1509e-04,  ..., 3.0984e-05, 5.8755e-05,\n",
      "         7.8322e-05],\n",
      "        ...,\n",
      "        [1.3845e-04, 1.6502e-04, 8.8544e-05,  ..., 8.3072e-05, 1.7450e-04,\n",
      "         6.0429e-05],\n",
      "        [1.1910e-04, 9.0837e-05, 6.7098e-05,  ..., 1.2889e-04, 7.8400e-05,\n",
      "         1.3415e-04],\n",
      "        [7.7189e-05, 4.8619e-05, 6.9472e-05,  ..., 6.5306e-05, 2.4195e-05,\n",
      "         1.0436e-04]], device='cuda:0')\n",
      "pred: tensor([0.8480, 0.8310, 0.7849, 0.8244, 0.7876, 0.8157, 0.8295, 0.8197, 0.7877,\n",
      "        0.8072, 0.7897, 0.8345, 0.8024, 0.8102, 0.8381, 0.8198, 0.8235, 0.8349,\n",
      "        0.7838, 0.8063, 0.8357, 0.8280, 0.8619, 0.8471, 0.8629, 0.8148, 0.8645,\n",
      "        0.8184, 0.8527, 0.8276, 0.8549, 0.8482, 0.8578, 0.8674, 0.8430, 0.8629,\n",
      "        0.8340, 0.8508, 0.8277, 0.8042, 0.8320, 0.8732, 0.8082, 0.8699, 0.8326,\n",
      "        0.7908, 0.8506, 0.8378, 0.8421, 0.8519, 0.7583, 0.7774, 0.7459, 0.7659,\n",
      "        0.7885, 0.7897, 0.8422, 0.8439, 0.7616, 0.8373, 0.7989, 0.8037, 0.7709,\n",
      "        0.8097, 0.8397, 0.8410, 0.8160, 0.7936, 0.8038, 0.8480, 0.8413, 0.8156,\n",
      "        0.8234, 0.8075, 0.7887, 0.8482, 0.8438, 0.8386, 0.8525, 0.8560, 0.8348,\n",
      "        0.8378, 0.8516, 0.8280, 0.8510, 0.7944, 0.7819, 0.8532, 0.7855, 0.8238,\n",
      "        0.8498, 0.8390, 0.7929, 0.8329, 0.8435, 0.8464, 0.8117, 0.8592, 0.8396,\n",
      "        0.8374], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0723, Val-Loss: 0.0356\n",
      "Epoch [20/700], Train-Loss: 0.0313, Val-Loss: 0.0164\n",
      "Epoch [30/700], Train-Loss: 0.0147, Val-Loss: 0.0144\n",
      "Epoch [40/700], Train-Loss: 0.0020, Val-Loss: 0.0025\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[2.0529e-04, 2.4384e-04, 1.7179e-04,  ..., 1.2497e-04, 4.7277e-05,\n",
      "         5.1579e-05],\n",
      "        [2.0405e-04, 1.5891e-04, 1.7893e-04,  ..., 9.4300e-05, 5.9002e-05,\n",
      "         7.4066e-05],\n",
      "        [4.4121e-04, 1.9211e-04, 3.3175e-04,  ..., 5.4791e-04, 6.9435e-04,\n",
      "         2.4794e-04],\n",
      "        ...,\n",
      "        [1.0111e-04, 8.8833e-05, 1.2679e-04,  ..., 1.4100e-04, 1.4011e-04,\n",
      "         9.5349e-05],\n",
      "        [2.0735e-04, 2.5074e-04, 9.4703e-05,  ..., 1.2304e-04, 6.1335e-05,\n",
      "         4.7589e-05],\n",
      "        [1.0714e-04, 8.3247e-05, 6.5016e-05,  ..., 1.5780e-04, 6.7870e-05,\n",
      "         5.2833e-05]], device='cuda:0')\n",
      "pred: tensor([0.8427, 0.8229, 0.7999, 0.8341, 0.7866, 0.8454, 0.8151, 0.8056, 0.8113,\n",
      "        0.8140, 0.7971, 0.8371, 0.7780, 0.8003, 0.8375, 0.8072, 0.8282, 0.8226,\n",
      "        0.7857, 0.7859, 0.8231, 0.8289, 0.8615, 0.8445, 0.8791, 0.7875, 0.8606,\n",
      "        0.8268, 0.8439, 0.8453, 0.8432, 0.8642, 0.8653, 0.8489, 0.8382, 0.8617,\n",
      "        0.8062, 0.8324, 0.8290, 0.8297, 0.8250, 0.8591, 0.7981, 0.8547, 0.8319,\n",
      "        0.8251, 0.8422, 0.8385, 0.8337, 0.8497, 0.7673, 0.7912, 0.7372, 0.7947,\n",
      "        0.7789, 0.7697, 0.8165, 0.8318, 0.7635, 0.8398, 0.7995, 0.8097, 0.7873,\n",
      "        0.7899, 0.8303, 0.8371, 0.7874, 0.7574, 0.8299, 0.8226, 0.8481, 0.7888,\n",
      "        0.8340, 0.7716, 0.7802, 0.8588, 0.8632, 0.8509, 0.8689, 0.8554, 0.8372,\n",
      "        0.8394, 0.8658, 0.8356, 0.8532, 0.8133, 0.7728, 0.8400, 0.7949, 0.8449,\n",
      "        0.8374, 0.8343, 0.7911, 0.8207, 0.8526, 0.8391, 0.7970, 0.8492, 0.8399,\n",
      "        0.8254], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9104830483048303, 0.9429342934293429, 0.9344854485448543, 0.9165916591659165, 0.9116471647164716]}\n",
      "Epoch [10/700], Train-Loss: 0.0590, Val-Loss: 0.0924\n",
      "Epoch [20/700], Train-Loss: 0.0195, Val-Loss: 0.0260\n",
      "Epoch [30/700], Train-Loss: 0.0015, Val-Loss: 0.0032\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0011\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[6.4227e-05, 1.0449e-04, 8.1786e-05,  ..., 7.2430e-05, 9.6154e-05,\n",
      "         2.5805e-04],\n",
      "        [1.7150e-04, 1.1390e-04, 1.9966e-04,  ..., 6.5784e-05, 1.9238e-04,\n",
      "         1.5187e-04],\n",
      "        [3.3419e-04, 1.2880e-04, 3.8174e-04,  ..., 1.6731e-04, 3.1582e-04,\n",
      "         3.1137e-04],\n",
      "        ...,\n",
      "        [1.0862e-04, 1.8253e-04, 1.1369e-04,  ..., 3.6205e-05, 4.4385e-05,\n",
      "         2.9656e-04],\n",
      "        [8.5215e-05, 8.0998e-05, 7.0373e-05,  ..., 1.1632e-04, 8.0555e-05,\n",
      "         9.7170e-05],\n",
      "        [7.0403e-05, 6.3193e-05, 7.7110e-05,  ..., 3.5797e-05, 6.1565e-05,\n",
      "         9.2969e-05]], device='cuda:0')\n",
      "pred: tensor([0.8408, 0.8297, 0.7973, 0.8158, 0.7752, 0.8315, 0.8140, 0.7933, 0.7879,\n",
      "        0.8307, 0.8038, 0.8210, 0.7868, 0.7994, 0.8268, 0.8165, 0.8332, 0.8341,\n",
      "        0.8043, 0.7927, 0.8183, 0.8348, 0.8592, 0.8527, 0.8826, 0.8081, 0.8702,\n",
      "        0.8143, 0.8457, 0.8434, 0.8492, 0.8669, 0.8692, 0.8702, 0.8516, 0.8664,\n",
      "        0.8221, 0.8473, 0.8276, 0.8296, 0.8364, 0.8713, 0.8069, 0.8634, 0.8267,\n",
      "        0.8158, 0.8414, 0.8031, 0.8282, 0.8304, 0.7597, 0.7762, 0.7502, 0.7691,\n",
      "        0.7817, 0.7733, 0.8360, 0.8296, 0.7604, 0.8421, 0.7955, 0.8023, 0.7865,\n",
      "        0.8027, 0.8317, 0.8505, 0.7955, 0.7811, 0.8048, 0.8359, 0.8340, 0.8096,\n",
      "        0.8335, 0.8101, 0.8021, 0.8615, 0.8549, 0.8396, 0.8597, 0.8583, 0.8230,\n",
      "        0.8399, 0.8527, 0.8315, 0.8615, 0.8144, 0.7746, 0.8317, 0.7707, 0.8424,\n",
      "        0.8390, 0.8435, 0.8011, 0.8272, 0.8626, 0.8448, 0.8018, 0.8682, 0.8424,\n",
      "        0.8352], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0148, Val-Loss: 0.0586\n",
      "Epoch [20/700], Train-Loss: 0.0080, Val-Loss: 0.0156\n",
      "Epoch [30/700], Train-Loss: 0.0012, Val-Loss: 0.0005\n",
      "Epoch [40/700], Train-Loss: 0.0012, Val-Loss: 0.0004\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[5.6172e-05, 1.8203e-04, 9.2820e-05,  ..., 6.2275e-05, 6.1325e-05,\n",
      "         1.0260e-04],\n",
      "        [1.0032e-04, 1.8116e-04, 1.2156e-04,  ..., 1.5166e-04, 2.4388e-04,\n",
      "         1.3513e-04],\n",
      "        [1.5401e-04, 7.3768e-04, 3.3132e-04,  ..., 3.4371e-04, 1.2533e-04,\n",
      "         2.6943e-04],\n",
      "        ...,\n",
      "        [1.0714e-04, 1.9828e-04, 1.0914e-04,  ..., 8.8953e-05, 7.0534e-05,\n",
      "         6.3537e-05],\n",
      "        [8.6374e-05, 7.9578e-05, 2.9012e-05,  ..., 8.9276e-05, 5.3964e-05,\n",
      "         1.0513e-04],\n",
      "        [1.9777e-05, 1.4505e-04, 1.4295e-04,  ..., 4.3015e-05, 1.1365e-04,\n",
      "         6.8597e-05]], device='cuda:0')\n",
      "pred: tensor([0.8470, 0.8326, 0.8092, 0.8102, 0.7847, 0.8312, 0.8082, 0.8192, 0.7950,\n",
      "        0.8270, 0.8001, 0.8263, 0.7973, 0.8188, 0.8339, 0.8021, 0.8311, 0.8287,\n",
      "        0.7884, 0.7909, 0.8267, 0.8366, 0.8615, 0.8529, 0.8734, 0.8004, 0.8610,\n",
      "        0.8013, 0.8463, 0.8454, 0.8492, 0.8704, 0.8660, 0.8571, 0.8546, 0.8717,\n",
      "        0.8278, 0.8437, 0.8334, 0.8228, 0.8348, 0.8702, 0.7977, 0.8695, 0.8035,\n",
      "        0.7965, 0.8294, 0.8207, 0.8113, 0.8330, 0.7508, 0.7778, 0.7744, 0.7711,\n",
      "        0.7854, 0.7672, 0.8349, 0.8273, 0.7483, 0.8477, 0.7820, 0.7896, 0.7719,\n",
      "        0.8153, 0.8353, 0.8398, 0.8052, 0.7815, 0.8007, 0.8363, 0.8317, 0.8079,\n",
      "        0.8328, 0.7980, 0.7997, 0.8598, 0.8432, 0.8325, 0.8593, 0.8509, 0.8327,\n",
      "        0.8416, 0.8558, 0.8374, 0.8667, 0.8163, 0.7757, 0.8376, 0.8113, 0.8286,\n",
      "        0.8307, 0.8239, 0.7988, 0.8187, 0.8598, 0.8448, 0.7941, 0.8625, 0.8510,\n",
      "        0.8466], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0491, Val-Loss: 0.0773\n",
      "Epoch [20/700], Train-Loss: 0.0161, Val-Loss: 0.0209\n",
      "Epoch [30/700], Train-Loss: 0.0012, Val-Loss: 0.0026\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0008\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.5426e-04, 1.2771e-04, 8.3053e-05,  ..., 1.1499e-04, 1.1722e-04,\n",
      "         1.2903e-04],\n",
      "        [1.3614e-04, 7.0120e-05, 5.5501e-05,  ..., 1.6315e-04, 2.7743e-04,\n",
      "         2.5615e-04],\n",
      "        [3.0487e-04, 2.9695e-04, 1.8608e-04,  ..., 1.2632e-04, 1.4397e-04,\n",
      "         2.0777e-04],\n",
      "        ...,\n",
      "        [1.5384e-04, 1.1572e-04, 1.2592e-04,  ..., 1.5142e-04, 1.0901e-04,\n",
      "         9.2147e-05],\n",
      "        [5.6418e-05, 1.6410e-04, 6.5580e-05,  ..., 7.6058e-05, 8.7864e-05,\n",
      "         6.4601e-05],\n",
      "        [6.8437e-05, 8.9623e-05, 3.3106e-05,  ..., 7.4653e-05, 7.0600e-05,\n",
      "         3.9220e-05]], device='cuda:0')\n",
      "pred: tensor([0.8244, 0.8259, 0.8032, 0.8108, 0.7801, 0.8254, 0.8131, 0.8085, 0.7967,\n",
      "        0.8255, 0.8008, 0.8422, 0.8001, 0.8049, 0.8372, 0.8142, 0.8317, 0.8270,\n",
      "        0.7948, 0.7989, 0.8241, 0.8330, 0.8556, 0.8533, 0.8691, 0.8016, 0.8601,\n",
      "        0.8175, 0.8454, 0.8447, 0.8605, 0.8810, 0.8716, 0.8618, 0.8559, 0.8615,\n",
      "        0.8201, 0.8391, 0.8342, 0.8194, 0.8346, 0.8637, 0.8032, 0.8730, 0.8242,\n",
      "        0.7967, 0.8478, 0.8186, 0.8252, 0.8349, 0.7726, 0.7855, 0.7602, 0.7840,\n",
      "        0.7845, 0.7770, 0.8316, 0.8183, 0.7747, 0.8406, 0.8043, 0.7892, 0.7914,\n",
      "        0.7990, 0.8362, 0.8435, 0.8034, 0.7632, 0.8123, 0.8384, 0.8381, 0.7880,\n",
      "        0.8222, 0.8040, 0.8036, 0.8622, 0.8563, 0.8492, 0.8542, 0.8514, 0.8310,\n",
      "        0.8345, 0.8667, 0.8444, 0.8681, 0.8181, 0.7831, 0.8382, 0.8019, 0.8384,\n",
      "        0.8391, 0.8448, 0.8077, 0.8410, 0.8521, 0.8383, 0.8016, 0.8500, 0.8422,\n",
      "        0.8342], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0173, Val-Loss: 0.0009\n",
      "Epoch [20/700], Train-Loss: 0.0027, Val-Loss: 0.0010\n",
      "Epoch [30/700], Train-Loss: 0.0116, Val-Loss: 0.0078\n",
      "Epoch [40/700], Train-Loss: 0.0061, Val-Loss: 0.0044\n",
      "Epoch [50/700], Train-Loss: 0.0023, Val-Loss: 0.0025\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.3820e-05, 2.1512e-04, 1.1360e-04,  ..., 9.8922e-05, 1.0876e-04,\n",
      "         6.9872e-05],\n",
      "        [1.0354e-04, 1.5721e-04, 1.1054e-04,  ..., 3.8925e-05, 7.0372e-05,\n",
      "         9.0361e-05],\n",
      "        [2.2710e-04, 1.3089e-04, 1.1775e-04,  ..., 1.4535e-04, 4.9339e-04,\n",
      "         5.6211e-05],\n",
      "        ...,\n",
      "        [9.2055e-05, 1.1351e-04, 1.7552e-04,  ..., 1.0289e-04, 2.1194e-04,\n",
      "         8.8751e-05],\n",
      "        [4.5605e-05, 7.8216e-05, 9.6455e-05,  ..., 3.6623e-05, 1.5293e-04,\n",
      "         1.3236e-04],\n",
      "        [5.0774e-05, 2.6553e-05, 5.2710e-05,  ..., 5.7546e-05, 1.0994e-04,\n",
      "         7.0506e-05]], device='cuda:0')\n",
      "pred: tensor([0.8346, 0.8242, 0.8001, 0.8195, 0.7829, 0.8280, 0.8033, 0.8092, 0.7972,\n",
      "        0.8208, 0.7875, 0.8372, 0.7962, 0.8076, 0.8329, 0.8118, 0.8174, 0.8374,\n",
      "        0.7879, 0.7986, 0.8223, 0.8368, 0.8649, 0.8485, 0.8702, 0.8115, 0.8597,\n",
      "        0.8204, 0.8473, 0.8349, 0.8493, 0.8698, 0.8640, 0.8644, 0.8464, 0.8691,\n",
      "        0.8179, 0.8471, 0.8253, 0.8407, 0.8297, 0.8646, 0.8056, 0.8677, 0.8273,\n",
      "        0.7995, 0.8273, 0.8210, 0.8315, 0.8350, 0.7530, 0.7781, 0.7639, 0.7684,\n",
      "        0.7864, 0.7745, 0.8328, 0.8277, 0.7591, 0.8435, 0.8029, 0.7918, 0.7872,\n",
      "        0.8045, 0.8366, 0.8539, 0.7779, 0.7821, 0.7986, 0.8260, 0.8313, 0.7937,\n",
      "        0.8255, 0.8110, 0.7948, 0.8613, 0.8406, 0.8529, 0.8634, 0.8543, 0.8305,\n",
      "        0.8317, 0.8520, 0.8332, 0.8571, 0.8096, 0.7753, 0.8349, 0.8041, 0.8400,\n",
      "        0.8331, 0.8377, 0.8192, 0.8299, 0.8557, 0.8416, 0.8076, 0.8658, 0.8459,\n",
      "        0.8390], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0053, Val-Loss: 0.0419\n",
      "Epoch [20/700], Train-Loss: 0.0086, Val-Loss: 0.0184\n",
      "Epoch [30/700], Train-Loss: 0.0006, Val-Loss: 0.0008\n",
      "Epoch [40/700], Train-Loss: 0.0006, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.9005e-05, 5.9811e-05, 1.0472e-04,  ..., 1.1034e-04, 1.7155e-04,\n",
      "         7.8129e-05],\n",
      "        [1.0938e-04, 7.1189e-05, 9.5585e-05,  ..., 5.6351e-05, 1.0714e-04,\n",
      "         3.6859e-05],\n",
      "        [5.6564e-05, 5.1811e-05, 1.4706e-04,  ..., 3.5627e-04, 3.2626e-04,\n",
      "         4.7327e-04],\n",
      "        ...,\n",
      "        [7.7252e-05, 1.8558e-04, 3.0515e-04,  ..., 1.4501e-04, 1.9135e-04,\n",
      "         2.1087e-04],\n",
      "        [6.9599e-05, 6.9850e-05, 5.2716e-05,  ..., 4.7071e-05, 9.2705e-05,\n",
      "         5.0109e-05],\n",
      "        [1.1640e-05, 3.3790e-05, 3.6249e-05,  ..., 2.8993e-05, 3.4033e-05,\n",
      "         5.3917e-05]], device='cuda:0')\n",
      "pred: tensor([0.8371, 0.8362, 0.7896, 0.8193, 0.7829, 0.8450, 0.8264, 0.8289, 0.7998,\n",
      "        0.8200, 0.7936, 0.8273, 0.7946, 0.8088, 0.8352, 0.8110, 0.8296, 0.8228,\n",
      "        0.7876, 0.7995, 0.8268, 0.8435, 0.8682, 0.8572, 0.8914, 0.8070, 0.8795,\n",
      "        0.8257, 0.8452, 0.8470, 0.8487, 0.8728, 0.8681, 0.8627, 0.8577, 0.8698,\n",
      "        0.8328, 0.8537, 0.8435, 0.8365, 0.8512, 0.8830, 0.8001, 0.8695, 0.8380,\n",
      "        0.8116, 0.8511, 0.8268, 0.8364, 0.8321, 0.7576, 0.7994, 0.7581, 0.7774,\n",
      "        0.7936, 0.7814, 0.8244, 0.8307, 0.7561, 0.8452, 0.7947, 0.8041, 0.7752,\n",
      "        0.8021, 0.8339, 0.8451, 0.7990, 0.7775, 0.8009, 0.8275, 0.8367, 0.7973,\n",
      "        0.8272, 0.8098, 0.8019, 0.8590, 0.8583, 0.8502, 0.8571, 0.8533, 0.8295,\n",
      "        0.8350, 0.8624, 0.8353, 0.8665, 0.8146, 0.7882, 0.8258, 0.7831, 0.8428,\n",
      "        0.8386, 0.8442, 0.8073, 0.8214, 0.8624, 0.8414, 0.8016, 0.8578, 0.8447,\n",
      "        0.8462], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9688808880888088, 0.9635643564356435, 0.96998499849985, 0.9528352835283527, 0.962160216021602]}\n",
      "Epoch [10/700], Train-Loss: 0.0746, Val-Loss: 0.0364\n",
      "Epoch [20/700], Train-Loss: 0.0142, Val-Loss: 0.0077\n",
      "Epoch [30/700], Train-Loss: 0.0025, Val-Loss: 0.0004\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.4252e-05, 1.6462e-04, 2.4620e-04,  ..., 5.7713e-04, 1.7963e-04,\n",
      "         1.1593e-04],\n",
      "        [8.5633e-05, 1.5001e-04, 1.7462e-04,  ..., 1.6975e-04, 4.7033e-05,\n",
      "         2.0955e-04],\n",
      "        [1.3927e-04, 2.0787e-04, 1.6320e-04,  ..., 1.6253e-04, 5.4078e-04,\n",
      "         6.4688e-04],\n",
      "        ...,\n",
      "        [2.0049e-04, 4.3473e-04, 1.7425e-04,  ..., 1.1997e-04, 1.2057e-04,\n",
      "         1.4038e-04],\n",
      "        [8.7169e-05, 7.5565e-05, 5.9672e-05,  ..., 2.6183e-04, 3.0611e-05,\n",
      "         3.4080e-04],\n",
      "        [5.5727e-05, 1.0274e-04, 1.1337e-04,  ..., 1.9186e-04, 1.0341e-04,\n",
      "         5.1338e-05]], device='cuda:0')\n",
      "pred: tensor([0.8386, 0.8374, 0.7975, 0.8089, 0.7810, 0.8313, 0.8057, 0.8009, 0.7827,\n",
      "        0.8173, 0.7917, 0.8250, 0.7877, 0.8096, 0.8330, 0.8167, 0.8264, 0.8355,\n",
      "        0.7979, 0.8050, 0.8337, 0.8434, 0.8559, 0.8541, 0.8746, 0.8113, 0.8707,\n",
      "        0.8232, 0.8514, 0.8462, 0.8517, 0.8683, 0.8657, 0.8610, 0.8569, 0.8660,\n",
      "        0.8396, 0.8374, 0.8282, 0.8149, 0.8437, 0.8653, 0.7858, 0.8743, 0.8403,\n",
      "        0.8064, 0.8496, 0.8403, 0.8428, 0.8419, 0.7599, 0.7826, 0.7501, 0.7745,\n",
      "        0.7756, 0.7635, 0.8356, 0.8295, 0.7581, 0.8476, 0.7994, 0.7926, 0.7850,\n",
      "        0.8048, 0.8316, 0.8443, 0.7864, 0.7675, 0.8028, 0.8360, 0.8301, 0.7947,\n",
      "        0.8252, 0.7928, 0.7888, 0.8552, 0.8513, 0.8371, 0.8574, 0.8551, 0.8318,\n",
      "        0.8301, 0.8591, 0.8350, 0.8678, 0.8156, 0.7907, 0.8407, 0.7879, 0.8341,\n",
      "        0.8423, 0.8372, 0.7965, 0.8317, 0.8642, 0.8416, 0.8076, 0.8636, 0.8521,\n",
      "        0.8442], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0881, Val-Loss: 0.0985\n",
      "Epoch [20/700], Train-Loss: 0.0248, Val-Loss: 0.0265\n",
      "Epoch [30/700], Train-Loss: 0.0062, Val-Loss: 0.0073\n",
      "Epoch [40/700], Train-Loss: 0.0025, Val-Loss: 0.0030\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0008\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.5789e-04, 1.3361e-04, 1.3152e-04,  ..., 9.0238e-05, 1.3133e-04,\n",
      "         1.1278e-04],\n",
      "        [1.3548e-04, 1.0341e-04, 3.9746e-05,  ..., 1.6344e-04, 1.5108e-04,\n",
      "         1.5125e-04],\n",
      "        [2.5789e-04, 2.6470e-04, 2.9300e-04,  ..., 2.0952e-04, 1.8679e-04,\n",
      "         1.7380e-04],\n",
      "        ...,\n",
      "        [1.3478e-04, 1.1740e-04, 5.6955e-05,  ..., 1.4858e-04, 1.2447e-04,\n",
      "         1.9670e-04],\n",
      "        [7.1535e-05, 5.0243e-05, 5.2297e-05,  ..., 9.1726e-05, 1.1720e-04,\n",
      "         8.5643e-05],\n",
      "        [1.2000e-04, 4.7971e-05, 4.1485e-05,  ..., 1.0509e-04, 5.6051e-05,\n",
      "         9.4052e-05]], device='cuda:0')\n",
      "pred: tensor([0.8408, 0.8374, 0.8090, 0.8145, 0.7871, 0.8260, 0.8117, 0.8101, 0.7918,\n",
      "        0.8226, 0.7947, 0.8289, 0.7957, 0.8077, 0.8310, 0.8059, 0.8286, 0.8302,\n",
      "        0.7888, 0.7953, 0.8235, 0.8390, 0.8678, 0.8607, 0.8847, 0.8016, 0.8771,\n",
      "        0.8165, 0.8564, 0.8523, 0.8534, 0.8601, 0.8728, 0.8778, 0.8574, 0.8713,\n",
      "        0.8237, 0.8489, 0.8407, 0.8179, 0.8445, 0.8724, 0.7956, 0.8685, 0.8301,\n",
      "        0.8001, 0.8462, 0.8176, 0.8307, 0.8369, 0.7502, 0.7868, 0.7568, 0.7705,\n",
      "        0.7719, 0.7716, 0.8336, 0.8274, 0.7526, 0.8439, 0.7946, 0.8074, 0.7745,\n",
      "        0.8017, 0.8284, 0.8459, 0.7965, 0.7739, 0.8013, 0.8308, 0.8380, 0.7924,\n",
      "        0.8194, 0.8072, 0.7978, 0.8663, 0.8650, 0.8505, 0.8723, 0.8687, 0.8267,\n",
      "        0.8326, 0.8595, 0.8458, 0.8595, 0.8209, 0.7748, 0.8414, 0.7978, 0.8387,\n",
      "        0.8441, 0.8328, 0.8047, 0.8333, 0.8553, 0.8497, 0.8045, 0.8661, 0.8505,\n",
      "        0.8391], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1427, Val-Loss: 0.1091\n",
      "Epoch [20/700], Train-Loss: 0.0387, Val-Loss: 0.0345\n",
      "Epoch [30/700], Train-Loss: 0.0137, Val-Loss: 0.0122\n",
      "Epoch [40/700], Train-Loss: 0.0052, Val-Loss: 0.0048\n",
      "Epoch [50/700], Train-Loss: 0.0020, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.1197e-04, 3.0493e-04, 1.9890e-04,  ..., 1.3386e-04, 8.8241e-05,\n",
      "         1.6434e-04],\n",
      "        [8.3033e-05, 1.2046e-04, 4.2337e-05,  ..., 8.6616e-05, 1.0711e-04,\n",
      "         1.6442e-04],\n",
      "        [6.1392e-05, 3.0196e-04, 4.9141e-04,  ..., 2.8349e-04, 2.3119e-04,\n",
      "         1.5952e-04],\n",
      "        ...,\n",
      "        [1.0537e-04, 1.1946e-04, 1.5793e-04,  ..., 1.1118e-04, 1.0687e-04,\n",
      "         1.2036e-04],\n",
      "        [2.3515e-04, 5.4454e-05, 1.1343e-04,  ..., 8.9361e-05, 5.5242e-05,\n",
      "         7.6860e-05],\n",
      "        [1.0830e-04, 7.5298e-05, 3.4383e-04,  ..., 8.5823e-05, 1.2703e-04,\n",
      "         8.8819e-05]], device='cuda:0')\n",
      "pred: tensor([0.8349, 0.8289, 0.8093, 0.8171, 0.7877, 0.8298, 0.8228, 0.8258, 0.7896,\n",
      "        0.8204, 0.8128, 0.8277, 0.7924, 0.8242, 0.8437, 0.8057, 0.8344, 0.8317,\n",
      "        0.7822, 0.7912, 0.8269, 0.8388, 0.8598, 0.8590, 0.8783, 0.7976, 0.8763,\n",
      "        0.8054, 0.8552, 0.8458, 0.8578, 0.8627, 0.8675, 0.8681, 0.8621, 0.8649,\n",
      "        0.8354, 0.8496, 0.8386, 0.8194, 0.8401, 0.8659, 0.7985, 0.8686, 0.8462,\n",
      "        0.7974, 0.8512, 0.8394, 0.8452, 0.8521, 0.7536, 0.7907, 0.7600, 0.7788,\n",
      "        0.7749, 0.7850, 0.8360, 0.8176, 0.7492, 0.8430, 0.8084, 0.7853, 0.7869,\n",
      "        0.8044, 0.8316, 0.8445, 0.8024, 0.7720, 0.8105, 0.8311, 0.8271, 0.7910,\n",
      "        0.8362, 0.8098, 0.8019, 0.8689, 0.8615, 0.8465, 0.8725, 0.8568, 0.8322,\n",
      "        0.8400, 0.8581, 0.8429, 0.8654, 0.8201, 0.7669, 0.8364, 0.7953, 0.8434,\n",
      "        0.8418, 0.8370, 0.8019, 0.8396, 0.8630, 0.8422, 0.7989, 0.8519, 0.8509,\n",
      "        0.8457], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0924, Val-Loss: 0.0541\n",
      "Epoch [20/700], Train-Loss: 0.0235, Val-Loss: 0.0160\n",
      "Epoch [30/700], Train-Loss: 0.0070, Val-Loss: 0.0031\n",
      "Epoch [40/700], Train-Loss: 0.0021, Val-Loss: 0.0010\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.8281e-05, 2.1881e-04, 1.3796e-04,  ..., 9.5571e-05, 1.1743e-03,\n",
      "         1.9238e-04],\n",
      "        [6.3613e-05, 1.1541e-04, 6.9315e-05,  ..., 1.5359e-04, 1.2711e-02,\n",
      "         1.9925e-04],\n",
      "        [5.8928e-04, 6.5603e-05, 3.5173e-04,  ..., 2.3216e-04, 4.7459e-03,\n",
      "         5.7110e-05],\n",
      "        ...,\n",
      "        [5.5799e-05, 9.3420e-05, 9.6115e-05,  ..., 1.4983e-04, 3.2270e-03,\n",
      "         2.8020e-04],\n",
      "        [1.4760e-05, 5.2601e-05, 4.5746e-05,  ..., 7.3332e-05, 1.4910e-03,\n",
      "         1.2381e-04],\n",
      "        [4.8505e-05, 8.2328e-05, 8.6104e-05,  ..., 4.6608e-05, 2.2613e-03,\n",
      "         1.8990e-04]], device='cuda:0')\n",
      "pred: tensor([0.8350, 0.8198, 0.8070, 0.8069, 0.7864, 0.8330, 0.8160, 0.8097, 0.7928,\n",
      "        0.8238, 0.7787, 0.8296, 0.7884, 0.8141, 0.8398, 0.8082, 0.8324, 0.8249,\n",
      "        0.7851, 0.7974, 0.8182, 0.8411, 0.8571, 0.8537, 0.8866, 0.8068, 0.8836,\n",
      "        0.8158, 0.8426, 0.8504, 0.8461, 0.8769, 0.8661, 0.8704, 0.8621, 0.8729,\n",
      "        0.8211, 0.8488, 0.8217, 0.8296, 0.8369, 0.8750, 0.7954, 0.8681, 0.8302,\n",
      "        0.8114, 0.8500, 0.8338, 0.8335, 0.8383, 0.7443, 0.7837, 0.7637, 0.7734,\n",
      "        0.7908, 0.7723, 0.8325, 0.8237, 0.7473, 0.8457, 0.7868, 0.7914, 0.7921,\n",
      "        0.8017, 0.8435, 0.8443, 0.7920, 0.7688, 0.8000, 0.8294, 0.8361, 0.7976,\n",
      "        0.8279, 0.8072, 0.7951, 0.8597, 0.8518, 0.8467, 0.8658, 0.8584, 0.8305,\n",
      "        0.8388, 0.8627, 0.8339, 0.8655, 0.8183, 0.7841, 0.8398, 0.7789, 0.8367,\n",
      "        0.8463, 0.8321, 0.8138, 0.8296, 0.8593, 0.8392, 0.8125, 0.8494, 0.8478,\n",
      "        0.8324], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1211, Val-Loss: 0.0731\n",
      "Epoch [20/700], Train-Loss: 0.0316, Val-Loss: 0.0231\n",
      "Epoch [30/700], Train-Loss: 0.0110, Val-Loss: 0.0066\n",
      "Epoch [40/700], Train-Loss: 0.0036, Val-Loss: 0.0024\n",
      "Epoch [50/700], Train-Loss: 0.0009, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.2639e-04, 9.3295e-05, 1.0516e-04,  ..., 6.4431e-05, 7.7184e-05,\n",
      "         1.1414e-04],\n",
      "        [6.1950e-05, 1.1234e-04, 1.1788e-04,  ..., 1.6155e-04, 8.8197e-05,\n",
      "         1.9873e-04],\n",
      "        [1.5458e-04, 2.3906e-04, 1.6581e-04,  ..., 1.5754e-04, 1.3973e-04,\n",
      "         1.3439e-04],\n",
      "        ...,\n",
      "        [1.7964e-04, 7.7715e-05, 1.0930e-04,  ..., 9.9146e-05, 1.2392e-04,\n",
      "         7.8340e-05],\n",
      "        [6.3426e-05, 8.5318e-05, 1.3213e-04,  ..., 4.6988e-05, 8.6657e-05,\n",
      "         4.8600e-05],\n",
      "        [6.2859e-05, 8.1350e-05, 1.3033e-04,  ..., 5.5970e-05, 5.4082e-05,\n",
      "         5.9636e-05]], device='cuda:0')\n",
      "pred: tensor([0.8366, 0.8246, 0.8028, 0.8127, 0.7863, 0.8304, 0.8072, 0.8088, 0.7856,\n",
      "        0.8291, 0.8054, 0.8282, 0.8024, 0.8193, 0.8348, 0.8080, 0.8386, 0.8330,\n",
      "        0.7787, 0.7950, 0.8231, 0.8344, 0.8616, 0.8552, 0.8857, 0.8129, 0.8689,\n",
      "        0.8247, 0.8498, 0.8436, 0.8546, 0.8674, 0.8689, 0.8687, 0.8577, 0.8763,\n",
      "        0.8375, 0.8466, 0.8329, 0.8322, 0.8497, 0.8718, 0.7971, 0.8704, 0.8286,\n",
      "        0.7930, 0.8412, 0.8222, 0.8344, 0.8372, 0.7484, 0.7890, 0.7457, 0.7598,\n",
      "        0.7788, 0.7766, 0.8310, 0.8283, 0.7545, 0.8448, 0.7993, 0.8050, 0.7855,\n",
      "        0.8083, 0.8343, 0.8433, 0.7985, 0.7829, 0.8023, 0.8329, 0.8344, 0.7974,\n",
      "        0.8350, 0.8027, 0.7914, 0.8721, 0.8566, 0.8398, 0.8573, 0.8606, 0.8333,\n",
      "        0.8384, 0.8657, 0.8425, 0.8666, 0.8203, 0.7739, 0.8426, 0.7886, 0.8550,\n",
      "        0.8407, 0.8409, 0.8152, 0.8364, 0.8504, 0.8502, 0.8070, 0.8698, 0.8497,\n",
      "        0.8398], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9537353735373536, 0.9820222022202219, 0.9632763276327632, 0.972133213321332, 0.9819861986198619]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9234329328137326, 0.9234329328137326, 0.9234329328137326, 0.9234329328137326, 0.9234329328137326]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9204656728021304, 0.9204656728021304, 0.9204656728021304, 0.9204656728021304, 0.9204656728021304]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9334657243490357, 0.9334657243490357, 0.9334657243490357, 0.9334657243490357, 0.9334657243490357]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.930287519283355, 0.930287519283355, 0.930287519283355, 0.930287519283355, 0.930287519283355]}\n",
      "Epoch [10/700], Train-Loss: 0.2472, Val-Loss: 0.2072\n",
      "Epoch [20/700], Train-Loss: 0.0234, Val-Loss: 0.0377\n",
      "Epoch [30/700], Train-Loss: 0.0009, Val-Loss: 0.0006\n",
      "Epoch [40/700], Train-Loss: 0.0087, Val-Loss: 0.0075\n",
      "Epoch [50/700], Train-Loss: 0.0021, Val-Loss: 0.0022\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.3395e-05, 7.7247e-05, 3.3591e-04,  ..., 3.0883e-05, 4.4152e-05,\n",
      "         2.7205e-04],\n",
      "        [1.3544e-04, 2.4516e-04, 3.8068e-04,  ..., 1.0640e-04, 7.9597e-05,\n",
      "         3.4775e-04],\n",
      "        [3.0509e-04, 9.4969e-05, 6.5941e-04,  ..., 9.1896e-05, 2.4632e-04,\n",
      "         4.0074e-04],\n",
      "        ...,\n",
      "        [5.1762e-04, 2.5003e-04, 1.4406e-03,  ..., 4.1130e-05, 4.6226e-04,\n",
      "         3.7822e-04],\n",
      "        [7.6096e-05, 1.3892e-04, 3.1970e-05,  ..., 1.2258e-04, 1.0363e-04,\n",
      "         3.3402e-03],\n",
      "        [7.1089e-05, 1.0006e-04, 6.2700e-04,  ..., 6.1716e-05, 5.5123e-05,\n",
      "         3.7125e-03]], device='cuda:0')\n",
      "pred: tensor([0.8369, 0.8152, 0.7979, 0.8126, 0.7971, 0.8317, 0.8093, 0.8085, 0.7948,\n",
      "        0.8192, 0.7868, 0.8335, 0.8111, 0.8063, 0.8358, 0.8076, 0.8214, 0.8135,\n",
      "        0.7733, 0.7876, 0.8296, 0.8488, 0.8629, 0.8570, 0.8845, 0.8153, 0.8609,\n",
      "        0.7826, 0.8544, 0.8420, 0.8447, 0.8581, 0.8720, 0.8722, 0.8564, 0.8759,\n",
      "        0.8372, 0.8489, 0.8368, 0.8272, 0.8324, 0.8731, 0.8056, 0.8627, 0.8289,\n",
      "        0.7822, 0.8316, 0.8091, 0.8385, 0.8281, 0.7792, 0.7854, 0.7584, 0.7836,\n",
      "        0.7747, 0.7653, 0.8225, 0.8196, 0.7583, 0.8353, 0.7938, 0.8015, 0.7848,\n",
      "        0.8049, 0.8292, 0.8452, 0.7882, 0.7849, 0.8243, 0.8366, 0.8375, 0.7786,\n",
      "        0.8234, 0.7811, 0.8054, 0.8584, 0.8367, 0.8521, 0.8553, 0.8734, 0.8380,\n",
      "        0.8344, 0.8584, 0.8498, 0.8623, 0.8150, 0.7874, 0.8411, 0.7862, 0.8306,\n",
      "        0.8452, 0.8325, 0.8126, 0.8285, 0.8703, 0.8428, 0.8157, 0.8607, 0.8566,\n",
      "        0.8505], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2162, Val-Loss: 0.1617\n",
      "Epoch [20/700], Train-Loss: 0.0749, Val-Loss: 0.0670\n",
      "Epoch [30/700], Train-Loss: 0.0100, Val-Loss: 0.0146\n",
      "Epoch [40/700], Train-Loss: 0.0012, Val-Loss: 0.0004\n",
      "Epoch [50/700], Train-Loss: 0.0024, Val-Loss: 0.0012\n",
      "Epoch [60/700], Train-Loss: 0.0013, Val-Loss: 0.0014\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.9602e-05, 2.3547e-04, 8.2658e-05,  ..., 4.3857e-05, 1.3532e-04,\n",
      "         1.8451e-04],\n",
      "        [6.8134e-05, 3.7578e-05, 1.1151e-04,  ..., 1.2728e-04, 2.5861e-04,\n",
      "         2.5697e-04],\n",
      "        [9.9501e-04, 2.9598e-04, 3.1450e-04,  ..., 1.5126e-04, 2.2207e-04,\n",
      "         3.1858e-04],\n",
      "        ...,\n",
      "        [1.0882e-04, 2.0178e-04, 1.0855e-04,  ..., 5.2830e-05, 5.1969e-04,\n",
      "         7.9775e-05],\n",
      "        [6.1059e-05, 5.2563e-05, 6.2422e-05,  ..., 6.5031e-05, 1.5108e-04,\n",
      "         4.8542e-05],\n",
      "        [1.3228e-04, 5.9281e-05, 1.3059e-04,  ..., 1.9030e-04, 1.4778e-04,\n",
      "         1.4802e-04]], device='cuda:0')\n",
      "pred: tensor([0.8372, 0.8174, 0.7987, 0.8108, 0.7989, 0.8289, 0.8079, 0.8050, 0.7930,\n",
      "        0.8193, 0.7885, 0.8367, 0.8127, 0.8067, 0.8366, 0.8060, 0.8227, 0.8150,\n",
      "        0.7724, 0.7886, 0.8282, 0.8510, 0.8624, 0.8568, 0.8889, 0.8144, 0.8617,\n",
      "        0.7820, 0.8496, 0.8427, 0.8472, 0.8559, 0.8759, 0.8734, 0.8582, 0.8777,\n",
      "        0.8381, 0.8530, 0.8380, 0.8241, 0.8355, 0.8739, 0.8069, 0.8632, 0.8318,\n",
      "        0.7859, 0.8307, 0.8120, 0.8397, 0.8285, 0.7771, 0.7856, 0.7567, 0.7832,\n",
      "        0.7721, 0.7650, 0.8206, 0.8183, 0.7576, 0.8346, 0.7936, 0.8021, 0.7833,\n",
      "        0.8053, 0.8292, 0.8470, 0.7892, 0.7823, 0.8235, 0.8340, 0.8386, 0.7737,\n",
      "        0.8235, 0.7836, 0.8052, 0.8555, 0.8385, 0.8548, 0.8599, 0.8774, 0.8388,\n",
      "        0.8345, 0.8594, 0.8505, 0.8631, 0.8133, 0.7864, 0.8390, 0.7863, 0.8313,\n",
      "        0.8442, 0.8313, 0.8111, 0.8252, 0.8634, 0.8415, 0.8147, 0.8601, 0.8601,\n",
      "        0.8495], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2925, Val-Loss: 0.2418\n",
      "Epoch [20/700], Train-Loss: 0.0423, Val-Loss: 0.0580\n",
      "Epoch [30/700], Train-Loss: 0.0006, Val-Loss: 0.0030\n",
      "Epoch [40/700], Train-Loss: 0.0079, Val-Loss: 0.0060\n",
      "Epoch [50/700], Train-Loss: 0.0035, Val-Loss: 0.0030\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.2260e-05, 5.0896e-05, 7.3541e-05,  ..., 2.0303e-04, 6.5455e-05,\n",
      "         9.6778e-05],\n",
      "        [2.0841e-04, 6.3537e-04, 3.6409e-04,  ..., 1.2543e-04, 6.6571e-05,\n",
      "         8.4966e-04],\n",
      "        [3.8869e-05, 1.1726e-04, 2.3310e-04,  ..., 3.1673e-04, 2.3537e-04,\n",
      "         1.0072e-03],\n",
      "        ...,\n",
      "        [1.3161e-04, 1.8604e-04, 1.0064e-04,  ..., 8.2555e-05, 1.6130e-04,\n",
      "         1.9433e-04],\n",
      "        [2.9653e-04, 2.0052e-04, 1.8755e-04,  ..., 7.2632e-05, 6.3280e-05,\n",
      "         2.5160e-04],\n",
      "        [1.0209e-04, 4.4529e-04, 1.5490e-04,  ..., 8.2706e-05, 4.6275e-05,\n",
      "         1.2244e-03]], device='cuda:0')\n",
      "pred: tensor([0.8393, 0.8174, 0.7979, 0.8127, 0.7983, 0.8300, 0.8089, 0.8079, 0.7945,\n",
      "        0.8189, 0.7896, 0.8323, 0.8125, 0.8086, 0.8352, 0.8081, 0.8222, 0.8144,\n",
      "        0.7758, 0.7876, 0.8312, 0.8490, 0.8645, 0.8563, 0.8849, 0.8179, 0.8608,\n",
      "        0.7838, 0.8547, 0.8409, 0.8445, 0.8562, 0.8733, 0.8724, 0.8561, 0.8748,\n",
      "        0.8391, 0.8513, 0.8372, 0.8273, 0.8361, 0.8743, 0.8073, 0.8612, 0.8283,\n",
      "        0.7844, 0.8310, 0.8107, 0.8352, 0.8295, 0.7773, 0.7841, 0.7551, 0.7818,\n",
      "        0.7734, 0.7630, 0.8226, 0.8192, 0.7566, 0.8369, 0.7912, 0.8032, 0.7833,\n",
      "        0.8048, 0.8285, 0.8447, 0.7875, 0.7815, 0.8229, 0.8353, 0.8391, 0.7754,\n",
      "        0.8254, 0.7832, 0.8035, 0.8605, 0.8365, 0.8517, 0.8564, 0.8746, 0.8374,\n",
      "        0.8343, 0.8596, 0.8511, 0.8627, 0.8169, 0.7865, 0.8414, 0.7879, 0.8316,\n",
      "        0.8449, 0.8329, 0.8120, 0.8290, 0.8690, 0.8427, 0.8140, 0.8616, 0.8576,\n",
      "        0.8523], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3066, Val-Loss: 0.2627\n",
      "Epoch [20/700], Train-Loss: 0.0224, Val-Loss: 0.0408\n",
      "Epoch [30/700], Train-Loss: 0.0010, Val-Loss: 0.0008\n",
      "Epoch [40/700], Train-Loss: 0.0101, Val-Loss: 0.0084\n",
      "Epoch [50/700], Train-Loss: 0.0027, Val-Loss: 0.0028\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.5016e-05, 3.0947e-04, 3.8154e-04,  ..., 3.5436e-05, 3.3013e-05,\n",
      "         9.0991e-05],\n",
      "        [1.3995e-04, 1.5510e-04, 4.1522e-04,  ..., 1.4638e-04, 1.3429e-04,\n",
      "         1.8340e-03],\n",
      "        [1.6353e-04, 2.8611e-04, 5.5558e-04,  ..., 8.0693e-05, 9.2526e-05,\n",
      "         2.0996e-03],\n",
      "        ...,\n",
      "        [2.1808e-04, 2.2232e-04, 2.4035e-04,  ..., 2.2149e-04, 1.2033e-04,\n",
      "         6.9648e-04],\n",
      "        [2.5975e-05, 5.7644e-05, 2.3882e-04,  ..., 3.0161e-04, 1.2055e-04,\n",
      "         3.7318e-04],\n",
      "        [9.2318e-05, 2.2830e-04, 2.6702e-04,  ..., 3.5435e-04, 2.6252e-04,\n",
      "         1.1024e-03]], device='cuda:0')\n",
      "pred: tensor([0.8363, 0.8170, 0.7979, 0.8123, 0.7972, 0.8306, 0.8081, 0.8092, 0.7941,\n",
      "        0.8196, 0.7878, 0.8340, 0.8115, 0.8106, 0.8368, 0.8077, 0.8221, 0.8144,\n",
      "        0.7742, 0.7875, 0.8303, 0.8492, 0.8652, 0.8564, 0.8858, 0.8170, 0.8603,\n",
      "        0.7824, 0.8547, 0.8433, 0.8464, 0.8571, 0.8726, 0.8745, 0.8568, 0.8780,\n",
      "        0.8374, 0.8511, 0.8385, 0.8233, 0.8335, 0.8729, 0.8064, 0.8626, 0.8270,\n",
      "        0.7839, 0.8309, 0.8099, 0.8367, 0.8283, 0.7793, 0.7841, 0.7563, 0.7836,\n",
      "        0.7754, 0.7647, 0.8208, 0.8190, 0.7587, 0.8336, 0.7947, 0.8001, 0.7834,\n",
      "        0.8044, 0.8294, 0.8457, 0.7863, 0.7813, 0.8236, 0.8341, 0.8381, 0.7774,\n",
      "        0.8238, 0.7813, 0.8054, 0.8581, 0.8365, 0.8572, 0.8550, 0.8766, 0.8375,\n",
      "        0.8333, 0.8603, 0.8508, 0.8629, 0.8157, 0.7865, 0.8417, 0.7882, 0.8321,\n",
      "        0.8442, 0.8306, 0.8124, 0.8268, 0.8636, 0.8434, 0.8137, 0.8595, 0.8573,\n",
      "        0.8491], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3183, Val-Loss: 0.2595\n",
      "Epoch [20/700], Train-Loss: 0.0546, Val-Loss: 0.0709\n",
      "Epoch [30/700], Train-Loss: 0.0010, Val-Loss: 0.0042\n",
      "Epoch [40/700], Train-Loss: 0.0088, Val-Loss: 0.0064\n",
      "Epoch [50/700], Train-Loss: 0.0042, Val-Loss: 0.0037\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0011\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.0273e-04, 2.6830e-04, 4.7556e-04,  ..., 5.3386e-05, 7.2017e-05,\n",
      "         1.4038e-04],\n",
      "        [8.2621e-05, 6.1752e-05, 1.8047e-04,  ..., 8.2495e-05, 3.1742e-05,\n",
      "         1.5791e-04],\n",
      "        [5.2461e-05, 1.9434e-04, 1.7942e-04,  ..., 3.2259e-04, 2.1900e-04,\n",
      "         2.9255e-03],\n",
      "        ...,\n",
      "        [1.4000e-04, 7.3260e-04, 1.0121e-03,  ..., 2.9995e-04, 3.8276e-04,\n",
      "         4.2015e-04],\n",
      "        [5.5227e-06, 1.9829e-05, 3.4907e-04,  ..., 2.3495e-04, 1.7573e-04,\n",
      "         1.0646e-03],\n",
      "        [1.8017e-05, 5.8786e-05, 1.6320e-04,  ..., 8.8065e-05, 8.1053e-05,\n",
      "         6.5657e-04]], device='cuda:0')\n",
      "pred: tensor([0.8390, 0.8191, 0.7985, 0.8093, 0.7987, 0.8320, 0.8104, 0.8088, 0.7951,\n",
      "        0.8192, 0.7896, 0.8355, 0.8107, 0.8055, 0.8334, 0.8073, 0.8211, 0.8125,\n",
      "        0.7740, 0.7871, 0.8307, 0.8499, 0.8655, 0.8552, 0.8846, 0.8159, 0.8609,\n",
      "        0.7831, 0.8572, 0.8425, 0.8464, 0.8600, 0.8705, 0.8726, 0.8548, 0.8747,\n",
      "        0.8394, 0.8537, 0.8370, 0.8274, 0.8365, 0.8716, 0.8064, 0.8609, 0.8287,\n",
      "        0.7808, 0.8303, 0.8098, 0.8376, 0.8271, 0.7785, 0.7849, 0.7586, 0.7827,\n",
      "        0.7721, 0.7653, 0.8251, 0.8213, 0.7594, 0.8337, 0.7932, 0.8028, 0.7831,\n",
      "        0.8045, 0.8294, 0.8439, 0.7889, 0.7810, 0.8228, 0.8376, 0.8376, 0.7773,\n",
      "        0.8205, 0.7826, 0.8066, 0.8580, 0.8361, 0.8515, 0.8565, 0.8740, 0.8378,\n",
      "        0.8342, 0.8593, 0.8491, 0.8635, 0.8166, 0.7879, 0.8405, 0.7850, 0.8313,\n",
      "        0.8450, 0.8319, 0.8108, 0.8300, 0.8656, 0.8430, 0.8141, 0.8580, 0.8605,\n",
      "        0.8518], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9446984698469847, 0.9473507350735072, 0.95013501350135, 0.9465706570657064, 0.9442424242424241]}\n",
      "Epoch [10/700], Train-Loss: 0.0925, Val-Loss: 0.0448\n",
      "Epoch [20/700], Train-Loss: 0.0347, Val-Loss: 0.0165\n",
      "Epoch [30/700], Train-Loss: 0.0191, Val-Loss: 0.0178\n",
      "Epoch [40/700], Train-Loss: 0.0033, Val-Loss: 0.0041\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.3866e-04, 9.5452e-05, 1.4732e-04,  ..., 5.4266e-05, 8.2409e-05,\n",
      "         8.6012e-05],\n",
      "        [1.8583e-04, 1.2035e-04, 2.8071e-04,  ..., 2.0874e-04, 2.2378e-04,\n",
      "         2.2120e-04],\n",
      "        [2.0998e-04, 1.7368e-04, 8.6776e-04,  ..., 1.7798e-04, 1.0765e-04,\n",
      "         1.3142e-04],\n",
      "        ...,\n",
      "        [8.0938e-05, 3.5263e-04, 2.1068e-04,  ..., 2.2338e-04, 5.4163e-04,\n",
      "         2.3545e-04],\n",
      "        [1.6066e-04, 3.0077e-04, 3.8355e-05,  ..., 1.4085e-04, 1.2571e-04,\n",
      "         4.0352e-05],\n",
      "        [3.4894e-04, 1.2781e-04, 7.5960e-05,  ..., 2.9285e-05, 8.2470e-05,\n",
      "         3.0753e-05]], device='cuda:0')\n",
      "pred: tensor([0.8375, 0.8223, 0.7923, 0.8142, 0.7908, 0.8347, 0.8057, 0.8038, 0.7923,\n",
      "        0.8187, 0.7974, 0.8193, 0.8061, 0.7996, 0.8329, 0.8000, 0.8361, 0.8191,\n",
      "        0.7811, 0.7864, 0.8259, 0.8373, 0.8652, 0.8571, 0.8928, 0.8106, 0.8675,\n",
      "        0.7892, 0.8481, 0.8397, 0.8496, 0.8699, 0.8700, 0.8756, 0.8577, 0.8756,\n",
      "        0.8355, 0.8443, 0.8378, 0.8314, 0.8317, 0.8735, 0.8095, 0.8692, 0.8167,\n",
      "        0.7792, 0.8189, 0.8118, 0.8239, 0.8257, 0.7741, 0.7935, 0.7616, 0.7779,\n",
      "        0.7768, 0.7718, 0.8166, 0.8236, 0.7610, 0.8392, 0.7998, 0.8078, 0.7873,\n",
      "        0.8046, 0.8305, 0.8447, 0.7886, 0.7739, 0.8161, 0.8313, 0.8396, 0.7824,\n",
      "        0.8275, 0.7956, 0.7964, 0.8585, 0.8459, 0.8433, 0.8672, 0.8733, 0.8271,\n",
      "        0.8341, 0.8621, 0.8470, 0.8655, 0.8118, 0.7768, 0.8404, 0.7852, 0.8315,\n",
      "        0.8436, 0.8416, 0.8107, 0.8284, 0.8729, 0.8510, 0.8072, 0.8497, 0.8460,\n",
      "        0.8442], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1736, Val-Loss: 0.1116\n",
      "Epoch [20/700], Train-Loss: 0.0758, Val-Loss: 0.0548\n",
      "Epoch [30/700], Train-Loss: 0.0181, Val-Loss: 0.0214\n",
      "Epoch [40/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0013, Val-Loss: 0.0011\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.5665e-05, 1.0552e-04, 2.8210e-04,  ..., 1.0696e-03, 1.9507e-04,\n",
      "         1.4899e-04],\n",
      "        [4.8961e-04, 2.4404e-04, 1.3460e-03,  ..., 2.2099e-03, 6.4851e-04,\n",
      "         3.1205e-05],\n",
      "        [1.7592e-04, 1.5520e-04, 2.4131e-04,  ..., 6.4350e-04, 2.6916e-04,\n",
      "         9.4290e-04],\n",
      "        ...,\n",
      "        [1.2202e-04, 1.2170e-04, 1.0457e-04,  ..., 4.2911e-04, 9.1634e-05,\n",
      "         2.4229e-04],\n",
      "        [1.5484e-04, 1.7156e-04, 9.3372e-05,  ..., 1.0098e-03, 1.7218e-04,\n",
      "         3.9451e-04],\n",
      "        [9.9801e-05, 1.0189e-04, 5.2472e-05,  ..., 5.5186e-05, 2.7891e-04,\n",
      "         2.5931e-05]], device='cuda:0')\n",
      "pred: tensor([0.8386, 0.8223, 0.7907, 0.8148, 0.7910, 0.8323, 0.8075, 0.8033, 0.7927,\n",
      "        0.8169, 0.7988, 0.8178, 0.8058, 0.7996, 0.8337, 0.8009, 0.8365, 0.8196,\n",
      "        0.7810, 0.7862, 0.8271, 0.8382, 0.8655, 0.8599, 0.8911, 0.8090, 0.8672,\n",
      "        0.7908, 0.8518, 0.8397, 0.8510, 0.8655, 0.8668, 0.8782, 0.8599, 0.8757,\n",
      "        0.8331, 0.8456, 0.8345, 0.8364, 0.8321, 0.8706, 0.8090, 0.8695, 0.8185,\n",
      "        0.7805, 0.8156, 0.8127, 0.8273, 0.8300, 0.7741, 0.7925, 0.7616, 0.7786,\n",
      "        0.7786, 0.7721, 0.8173, 0.8224, 0.7591, 0.8385, 0.8018, 0.8077, 0.7872,\n",
      "        0.8008, 0.8285, 0.8429, 0.7893, 0.7742, 0.8142, 0.8299, 0.8385, 0.7815,\n",
      "        0.8263, 0.7949, 0.7964, 0.8581, 0.8467, 0.8469, 0.8669, 0.8732, 0.8265,\n",
      "        0.8326, 0.8633, 0.8481, 0.8669, 0.8124, 0.7776, 0.8391, 0.7852, 0.8309,\n",
      "        0.8435, 0.8431, 0.8087, 0.8264, 0.8718, 0.8532, 0.8050, 0.8460, 0.8442,\n",
      "        0.8455], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1019, Val-Loss: 0.0579\n",
      "Epoch [20/700], Train-Loss: 0.0438, Val-Loss: 0.0263\n",
      "Epoch [30/700], Train-Loss: 0.0149, Val-Loss: 0.0160\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0014\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.5665e-05, 1.0552e-04, 2.8210e-04,  ..., 8.4559e-05, 5.3920e-05,\n",
      "         1.0599e-04],\n",
      "        [4.8961e-04, 2.4404e-04, 1.3460e-03,  ..., 9.6906e-05, 6.2437e-05,\n",
      "         1.9080e-04],\n",
      "        [1.7592e-04, 1.5520e-04, 2.4131e-04,  ..., 1.3021e-04, 2.2130e-04,\n",
      "         1.5089e-03],\n",
      "        ...,\n",
      "        [1.2202e-04, 1.2170e-04, 1.0457e-04,  ..., 8.2550e-05, 3.4358e-04,\n",
      "         9.0923e-05],\n",
      "        [1.5484e-04, 1.7156e-04, 9.3372e-05,  ..., 9.3876e-05, 3.4695e-04,\n",
      "         1.0983e-04],\n",
      "        [9.9801e-05, 1.0189e-04, 5.2472e-05,  ..., 5.2157e-05, 2.2785e-04,\n",
      "         1.2969e-04]], device='cuda:0')\n",
      "pred: tensor([0.8360, 0.8205, 0.7896, 0.8144, 0.7927, 0.8351, 0.8062, 0.8023, 0.7919,\n",
      "        0.8185, 0.7980, 0.8225, 0.8045, 0.7960, 0.8333, 0.8022, 0.8356, 0.8205,\n",
      "        0.7805, 0.7847, 0.8271, 0.8437, 0.8632, 0.8553, 0.8924, 0.8085, 0.8679,\n",
      "        0.7919, 0.8507, 0.8404, 0.8497, 0.8678, 0.8698, 0.8786, 0.8574, 0.8759,\n",
      "        0.8332, 0.8469, 0.8385, 0.8373, 0.8336, 0.8726, 0.8101, 0.8708, 0.8170,\n",
      "        0.7793, 0.8171, 0.8123, 0.8257, 0.8308, 0.7750, 0.7955, 0.7616, 0.7778,\n",
      "        0.7782, 0.7720, 0.8154, 0.8211, 0.7612, 0.8384, 0.8003, 0.8065, 0.7872,\n",
      "        0.8015, 0.8284, 0.8439, 0.7892, 0.7731, 0.8139, 0.8299, 0.8389, 0.7816,\n",
      "        0.8256, 0.7948, 0.7950, 0.8567, 0.8479, 0.8456, 0.8702, 0.8715, 0.8280,\n",
      "        0.8330, 0.8617, 0.8448, 0.8659, 0.8117, 0.7779, 0.8411, 0.7841, 0.8319,\n",
      "        0.8426, 0.8422, 0.8099, 0.8273, 0.8727, 0.8534, 0.8029, 0.8485, 0.8469,\n",
      "        0.8426], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2026, Val-Loss: 0.1466\n",
      "Epoch [20/700], Train-Loss: 0.0742, Val-Loss: 0.0609\n",
      "Epoch [30/700], Train-Loss: 0.0116, Val-Loss: 0.0154\n",
      "Epoch [40/700], Train-Loss: 0.0006, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0019, Val-Loss: 0.0008\n",
      "Epoch [60/700], Train-Loss: 0.0012, Val-Loss: 0.0012\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.8500e-05, 4.4944e-05, 1.3899e-04,  ..., 1.4671e-04, 1.1900e-04,\n",
      "         3.0264e-04],\n",
      "        [4.5407e-04, 7.1192e-05, 8.1143e-05,  ..., 2.8391e-04, 8.7848e-05,\n",
      "         3.7704e-04],\n",
      "        [2.0114e-04, 9.5427e-05, 1.1802e-04,  ..., 1.4277e-04, 2.5983e-04,\n",
      "         2.8514e-04],\n",
      "        ...,\n",
      "        [5.8785e-05, 7.2610e-05, 8.7589e-05,  ..., 1.9371e-04, 6.2090e-05,\n",
      "         2.5594e-05],\n",
      "        [1.2106e-04, 3.7363e-05, 2.1969e-05,  ..., 1.5761e-04, 3.1804e-05,\n",
      "         1.4812e-04],\n",
      "        [8.7468e-05, 1.7411e-04, 2.8976e-04,  ..., 5.7108e-05, 1.5458e-05,\n",
      "         1.4503e-05]], device='cuda:0')\n",
      "pred: tensor([0.8364, 0.8203, 0.7919, 0.8155, 0.7914, 0.8341, 0.8091, 0.8034, 0.7909,\n",
      "        0.8182, 0.7994, 0.8212, 0.8065, 0.7998, 0.8336, 0.8020, 0.8352, 0.8192,\n",
      "        0.7807, 0.7847, 0.8269, 0.8404, 0.8653, 0.8520, 0.8902, 0.8087, 0.8665,\n",
      "        0.7934, 0.8493, 0.8397, 0.8504, 0.8670, 0.8688, 0.8780, 0.8553, 0.8783,\n",
      "        0.8351, 0.8462, 0.8367, 0.8367, 0.8323, 0.8747, 0.8090, 0.8700, 0.8155,\n",
      "        0.7787, 0.8178, 0.8104, 0.8255, 0.8290, 0.7748, 0.7933, 0.7617, 0.7769,\n",
      "        0.7773, 0.7712, 0.8156, 0.8217, 0.7611, 0.8371, 0.8029, 0.8057, 0.7877,\n",
      "        0.8039, 0.8301, 0.8430, 0.7901, 0.7739, 0.8136, 0.8270, 0.8382, 0.7838,\n",
      "        0.8257, 0.7964, 0.7984, 0.8572, 0.8462, 0.8483, 0.8668, 0.8741, 0.8274,\n",
      "        0.8330, 0.8630, 0.8470, 0.8654, 0.8142, 0.7764, 0.8402, 0.7854, 0.8326,\n",
      "        0.8445, 0.8413, 0.8104, 0.8270, 0.8703, 0.8519, 0.8048, 0.8508, 0.8468,\n",
      "        0.8420], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0579, Val-Loss: 0.0198\n",
      "Epoch [20/700], Train-Loss: 0.0177, Val-Loss: 0.0057\n",
      "Epoch [30/700], Train-Loss: 0.0155, Val-Loss: 0.0135\n",
      "Epoch [40/700], Train-Loss: 0.0041, Val-Loss: 0.0044\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0013\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.5665e-05, 1.0552e-04, 2.8210e-04,  ..., 1.6923e-04, 5.4794e-04,\n",
      "         5.9729e-05],\n",
      "        [4.8961e-04, 2.4404e-04, 1.3460e-03,  ..., 1.7055e-04, 1.0689e-04,\n",
      "         1.9590e-04],\n",
      "        [1.7592e-04, 1.5520e-04, 2.4131e-04,  ..., 1.4423e-04, 1.1831e-04,\n",
      "         8.1545e-04],\n",
      "        ...,\n",
      "        [1.2202e-04, 1.2170e-04, 1.0457e-04,  ..., 2.5291e-04, 7.8674e-05,\n",
      "         4.5163e-04],\n",
      "        [1.5484e-04, 1.7156e-04, 9.3372e-05,  ..., 3.9452e-05, 2.4003e-04,\n",
      "         1.2997e-04],\n",
      "        [9.9801e-05, 1.0189e-04, 5.2472e-05,  ..., 1.2552e-04, 4.9780e-05,\n",
      "         5.7861e-05]], device='cuda:0')\n",
      "pred: tensor([0.8371, 0.8207, 0.7904, 0.8172, 0.7913, 0.8347, 0.8075, 0.8032, 0.7943,\n",
      "        0.8190, 0.7976, 0.8176, 0.8080, 0.8006, 0.8324, 0.8019, 0.8366, 0.8214,\n",
      "        0.7811, 0.7864, 0.8276, 0.8420, 0.8635, 0.8555, 0.8906, 0.8115, 0.8667,\n",
      "        0.7875, 0.8510, 0.8419, 0.8506, 0.8684, 0.8681, 0.8783, 0.8562, 0.8779,\n",
      "        0.8337, 0.8461, 0.8363, 0.8360, 0.8335, 0.8708, 0.8086, 0.8701, 0.8164,\n",
      "        0.7797, 0.8182, 0.8127, 0.8267, 0.8286, 0.7759, 0.7921, 0.7620, 0.7793,\n",
      "        0.7785, 0.7725, 0.8172, 0.8223, 0.7614, 0.8407, 0.8025, 0.8060, 0.7890,\n",
      "        0.8027, 0.8292, 0.8455, 0.7900, 0.7721, 0.8144, 0.8247, 0.8372, 0.7825,\n",
      "        0.8246, 0.7936, 0.7965, 0.8558, 0.8507, 0.8473, 0.8702, 0.8740, 0.8275,\n",
      "        0.8338, 0.8617, 0.8463, 0.8642, 0.8126, 0.7761, 0.8409, 0.7841, 0.8310,\n",
      "        0.8424, 0.8403, 0.8092, 0.8263, 0.8730, 0.8516, 0.8058, 0.8502, 0.8459,\n",
      "        0.8430], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9637203720372036, 0.9602040204020401, 0.9603720372037203, 0.9615601560156015, 0.9607440744074406]}\n",
      "Epoch [10/700], Train-Loss: 0.0016, Val-Loss: 0.0124\n",
      "Epoch [20/700], Train-Loss: 0.0010, Val-Loss: 0.0057\n",
      "Epoch [30/700], Train-Loss: 0.0044, Val-Loss: 0.0018\n",
      "Epoch [40/700], Train-Loss: 0.0032, Val-Loss: 0.0014\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0013\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.6924e-04, 3.6660e-04, 3.3653e-04,  ..., 5.8183e-05, 2.4540e-05,\n",
      "         1.9474e-04],\n",
      "        [5.4666e-05, 8.4389e-05, 5.0037e-05,  ..., 1.0645e-04, 9.6428e-05,\n",
      "         4.2713e-03],\n",
      "        [1.2121e-04, 1.4708e-04, 2.6816e-04,  ..., 2.5373e-04, 6.9918e-05,\n",
      "         1.0879e-02],\n",
      "        ...,\n",
      "        [2.1922e-04, 2.5819e-04, 1.1232e-04,  ..., 4.0626e-05, 1.1774e-04,\n",
      "         7.1639e-05],\n",
      "        [2.2988e-04, 2.0975e-04, 1.9825e-04,  ..., 1.1727e-04, 1.0589e-04,\n",
      "         2.0346e-04],\n",
      "        [3.9892e-05, 2.9849e-04, 7.1592e-04,  ..., 3.8763e-05, 6.3008e-06,\n",
      "         6.5369e-05]], device='cuda:0')\n",
      "pred: tensor([0.8368, 0.8226, 0.7964, 0.8173, 0.7929, 0.8247, 0.8132, 0.8133, 0.7933,\n",
      "        0.8182, 0.7961, 0.8272, 0.8023, 0.8052, 0.8417, 0.8014, 0.8250, 0.8178,\n",
      "        0.7752, 0.7836, 0.8223, 0.8415, 0.8681, 0.8638, 0.8877, 0.8128, 0.8684,\n",
      "        0.8199, 0.8590, 0.8438, 0.8525, 0.8710, 0.8719, 0.8705, 0.8565, 0.8770,\n",
      "        0.8407, 0.8500, 0.8426, 0.8350, 0.8369, 0.8708, 0.8079, 0.8668, 0.8269,\n",
      "        0.7907, 0.8342, 0.8124, 0.8304, 0.8382, 0.7699, 0.7867, 0.7614, 0.7778,\n",
      "        0.7778, 0.7669, 0.8233, 0.8240, 0.7592, 0.8423, 0.7989, 0.8051, 0.7879,\n",
      "        0.7992, 0.8306, 0.8487, 0.7929, 0.7739, 0.8136, 0.8263, 0.8351, 0.7895,\n",
      "        0.8258, 0.8013, 0.7977, 0.8619, 0.8511, 0.8501, 0.8726, 0.8674, 0.8311,\n",
      "        0.8327, 0.8602, 0.8428, 0.8628, 0.8163, 0.7786, 0.8443, 0.7974, 0.8375,\n",
      "        0.8360, 0.8350, 0.8034, 0.8304, 0.8657, 0.8429, 0.8029, 0.8535, 0.8536,\n",
      "        0.8455], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0313, Val-Loss: 0.0804\n",
      "Epoch [20/700], Train-Loss: 0.0122, Val-Loss: 0.0217\n",
      "Epoch [30/700], Train-Loss: 0.0001, Val-Loss: 0.0007\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.8500e-05, 4.4944e-05, 1.3899e-04,  ..., 1.0061e-04, 5.5266e-04,\n",
      "         5.4284e-04],\n",
      "        [4.5407e-04, 7.1192e-05, 8.1143e-05,  ..., 2.4638e-05, 5.9282e-05,\n",
      "         2.6701e-04],\n",
      "        [2.0114e-04, 9.5427e-05, 1.1802e-04,  ..., 4.9887e-04, 1.8840e-04,\n",
      "         7.0193e-04],\n",
      "        ...,\n",
      "        [5.8785e-05, 7.2610e-05, 8.7589e-05,  ..., 7.4329e-04, 7.3083e-05,\n",
      "         4.4796e-04],\n",
      "        [1.2106e-04, 3.7363e-05, 2.1969e-05,  ..., 1.3996e-04, 1.0183e-04,\n",
      "         2.1373e-04],\n",
      "        [8.7468e-05, 1.7411e-04, 2.8976e-04,  ..., 5.6112e-05, 6.3852e-05,\n",
      "         2.1804e-04]], device='cuda:0')\n",
      "pred: tensor([0.8391, 0.8237, 0.7951, 0.8183, 0.7932, 0.8286, 0.8115, 0.8100, 0.7932,\n",
      "        0.8193, 0.7978, 0.8319, 0.8033, 0.8063, 0.8420, 0.8036, 0.8237, 0.8165,\n",
      "        0.7759, 0.7847, 0.8207, 0.8418, 0.8663, 0.8623, 0.8888, 0.8129, 0.8709,\n",
      "        0.8131, 0.8580, 0.8418, 0.8531, 0.8725, 0.8712, 0.8719, 0.8551, 0.8744,\n",
      "        0.8381, 0.8494, 0.8402, 0.8337, 0.8382, 0.8718, 0.8073, 0.8669, 0.8293,\n",
      "        0.7894, 0.8335, 0.8139, 0.8335, 0.8351, 0.7706, 0.7871, 0.7604, 0.7784,\n",
      "        0.7760, 0.7701, 0.8245, 0.8223, 0.7622, 0.8437, 0.7987, 0.8031, 0.7898,\n",
      "        0.7994, 0.8311, 0.8489, 0.7914, 0.7728, 0.8122, 0.8262, 0.8352, 0.7881,\n",
      "        0.8248, 0.8033, 0.7957, 0.8609, 0.8540, 0.8532, 0.8699, 0.8659, 0.8321,\n",
      "        0.8297, 0.8615, 0.8450, 0.8633, 0.8158, 0.7784, 0.8443, 0.7968, 0.8354,\n",
      "        0.8353, 0.8349, 0.8058, 0.8297, 0.8657, 0.8474, 0.8048, 0.8515, 0.8541,\n",
      "        0.8445], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1038, Val-Loss: 0.0876\n",
      "Epoch [20/700], Train-Loss: 0.0314, Val-Loss: 0.0296\n",
      "Epoch [30/700], Train-Loss: 0.0103, Val-Loss: 0.0099\n",
      "Epoch [40/700], Train-Loss: 0.0039, Val-Loss: 0.0042\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0013\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.8366e-05, 5.9327e-05, 3.1506e-04,  ..., 4.1981e-05, 1.3269e-04,\n",
      "         1.6584e-04],\n",
      "        [1.6184e-04, 7.3350e-05, 1.5565e-03,  ..., 2.0251e-04, 3.1589e-04,\n",
      "         1.8461e-03],\n",
      "        [4.1545e-04, 2.6436e-04, 8.1506e-04,  ..., 1.9768e-04, 1.4233e-04,\n",
      "         6.8538e-04],\n",
      "        ...,\n",
      "        [5.9577e-05, 1.1225e-04, 6.6621e-04,  ..., 4.0029e-05, 2.8137e-04,\n",
      "         8.6146e-04],\n",
      "        [1.2117e-05, 1.9793e-04, 1.6577e-04,  ..., 8.1449e-05, 1.5275e-04,\n",
      "         1.7195e-04],\n",
      "        [2.4887e-05, 9.7258e-05, 1.8791e-03,  ..., 5.3253e-05, 1.1800e-04,\n",
      "         2.1952e-04]], device='cuda:0')\n",
      "pred: tensor([0.8381, 0.8225, 0.7968, 0.8188, 0.7929, 0.8260, 0.8118, 0.8098, 0.7934,\n",
      "        0.8197, 0.7968, 0.8276, 0.8009, 0.8042, 0.8435, 0.8015, 0.8254, 0.8195,\n",
      "        0.7767, 0.7838, 0.8208, 0.8391, 0.8689, 0.8630, 0.8895, 0.8124, 0.8697,\n",
      "        0.8158, 0.8568, 0.8420, 0.8509, 0.8706, 0.8699, 0.8747, 0.8551, 0.8778,\n",
      "        0.8394, 0.8484, 0.8427, 0.8370, 0.8392, 0.8684, 0.8095, 0.8675, 0.8259,\n",
      "        0.7869, 0.8313, 0.8103, 0.8301, 0.8354, 0.7683, 0.7891, 0.7610, 0.7786,\n",
      "        0.7766, 0.7697, 0.8232, 0.8227, 0.7597, 0.8425, 0.7984, 0.8019, 0.7894,\n",
      "        0.7992, 0.8298, 0.8482, 0.7926, 0.7726, 0.8125, 0.8257, 0.8334, 0.7893,\n",
      "        0.8255, 0.8016, 0.7983, 0.8636, 0.8516, 0.8516, 0.8709, 0.8680, 0.8313,\n",
      "        0.8316, 0.8617, 0.8442, 0.8626, 0.8143, 0.7762, 0.8429, 0.7973, 0.8370,\n",
      "        0.8367, 0.8351, 0.8029, 0.8325, 0.8664, 0.8473, 0.8030, 0.8519, 0.8535,\n",
      "        0.8443], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0111, Val-Loss: 0.0582\n",
      "Epoch [20/700], Train-Loss: 0.0099, Val-Loss: 0.0194\n",
      "Epoch [30/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [40/700], Train-Loss: 0.0006, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.8970e-04, 6.1615e-04, 8.9243e-04,  ..., 1.0104e-04, 4.4603e-04,\n",
      "         2.5135e-04],\n",
      "        [1.3141e-04, 7.5378e-05, 2.9316e-04,  ..., 3.2914e-04, 1.1867e-04,\n",
      "         3.1251e-04],\n",
      "        [5.1030e-05, 4.6316e-04, 4.3484e-04,  ..., 6.3458e-05, 4.0160e-04,\n",
      "         9.3401e-05],\n",
      "        ...,\n",
      "        [6.9521e-04, 2.4335e-04, 1.0089e-03,  ..., 8.2588e-05, 1.6573e-04,\n",
      "         8.2907e-05],\n",
      "        [6.8149e-05, 2.4937e-05, 2.1934e-05,  ..., 5.2385e-05, 9.9986e-05,\n",
      "         2.6330e-04],\n",
      "        [6.4223e-05, 7.7612e-05, 2.9414e-04,  ..., 1.1045e-04, 6.2842e-05,\n",
      "         2.0550e-04]], device='cuda:0')\n",
      "pred: tensor([0.8400, 0.8207, 0.7963, 0.8188, 0.7939, 0.8281, 0.8123, 0.8109, 0.7929,\n",
      "        0.8207, 0.7968, 0.8295, 0.8017, 0.8054, 0.8441, 0.8003, 0.8255, 0.8181,\n",
      "        0.7753, 0.7825, 0.8206, 0.8431, 0.8669, 0.8612, 0.8881, 0.8129, 0.8694,\n",
      "        0.8183, 0.8571, 0.8423, 0.8533, 0.8727, 0.8713, 0.8712, 0.8549, 0.8757,\n",
      "        0.8403, 0.8489, 0.8416, 0.8363, 0.8383, 0.8713, 0.8087, 0.8663, 0.8291,\n",
      "        0.7881, 0.8335, 0.8144, 0.8311, 0.8375, 0.7701, 0.7870, 0.7615, 0.7764,\n",
      "        0.7774, 0.7697, 0.8260, 0.8226, 0.7595, 0.8442, 0.7983, 0.8036, 0.7898,\n",
      "        0.8009, 0.8301, 0.8475, 0.7919, 0.7730, 0.8133, 0.8269, 0.8338, 0.7903,\n",
      "        0.8243, 0.8032, 0.7978, 0.8599, 0.8545, 0.8545, 0.8712, 0.8678, 0.8312,\n",
      "        0.8294, 0.8609, 0.8446, 0.8633, 0.8159, 0.7774, 0.8440, 0.7973, 0.8368,\n",
      "        0.8362, 0.8354, 0.8055, 0.8300, 0.8661, 0.8466, 0.8011, 0.8508, 0.8558,\n",
      "        0.8458], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0737, Val-Loss: 0.0965\n",
      "Epoch [20/700], Train-Loss: 0.0241, Val-Loss: 0.0295\n",
      "Epoch [30/700], Train-Loss: 0.0031, Val-Loss: 0.0058\n",
      "Epoch [40/700], Train-Loss: 0.0010, Val-Loss: 0.0023\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.3526e-04, 1.2365e-02, 1.6747e-04,  ..., 1.0061e-04, 5.5266e-04,\n",
      "         5.4284e-04],\n",
      "        [5.1582e-04, 6.8726e-03, 1.5591e-04,  ..., 2.4638e-05, 5.9282e-05,\n",
      "         2.6701e-04],\n",
      "        [7.1404e-04, 2.9864e-01, 1.7248e-04,  ..., 4.9887e-04, 1.8840e-04,\n",
      "         7.0193e-04],\n",
      "        ...,\n",
      "        [3.2763e-04, 8.1233e-02, 9.3760e-05,  ..., 7.4329e-04, 7.3083e-05,\n",
      "         4.4796e-04],\n",
      "        [3.5404e-04, 7.0754e-02, 3.3660e-05,  ..., 1.3996e-04, 1.0183e-04,\n",
      "         2.1373e-04],\n",
      "        [1.4974e-04, 3.9252e-02, 7.4263e-05,  ..., 5.6112e-05, 6.3852e-05,\n",
      "         2.1804e-04]], device='cuda:0')\n",
      "pred: tensor([0.8367, 0.8259, 0.7965, 0.8191, 0.7934, 0.8282, 0.8105, 0.8114, 0.7929,\n",
      "        0.8190, 0.7969, 0.8277, 0.8018, 0.8044, 0.8414, 0.8014, 0.8255, 0.8181,\n",
      "        0.7757, 0.7843, 0.8191, 0.8411, 0.8661, 0.8626, 0.8896, 0.8127, 0.8676,\n",
      "        0.8165, 0.8583, 0.8447, 0.8494, 0.8678, 0.8707, 0.8681, 0.8533, 0.8745,\n",
      "        0.8394, 0.8495, 0.8423, 0.8382, 0.8379, 0.8690, 0.8084, 0.8666, 0.8251,\n",
      "        0.7892, 0.8325, 0.8138, 0.8294, 0.8356, 0.7686, 0.7869, 0.7609, 0.7785,\n",
      "        0.7761, 0.7687, 0.8228, 0.8230, 0.7611, 0.8423, 0.7980, 0.8007, 0.7886,\n",
      "        0.7989, 0.8312, 0.8493, 0.7913, 0.7714, 0.8144, 0.8282, 0.8357, 0.7861,\n",
      "        0.8240, 0.8006, 0.7951, 0.8630, 0.8552, 0.8510, 0.8705, 0.8654, 0.8307,\n",
      "        0.8311, 0.8621, 0.8447, 0.8631, 0.8156, 0.7789, 0.8433, 0.7966, 0.8383,\n",
      "        0.8366, 0.8355, 0.8045, 0.8290, 0.8667, 0.8445, 0.8025, 0.8536, 0.8543,\n",
      "        0.8454], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9741854185418541, 0.9735973597359736, 0.9717011701170115, 0.9711971197119711, 0.9723132313231323]}\n",
      "Epoch [10/700], Train-Loss: 0.1117, Val-Loss: 0.0768\n",
      "Epoch [20/700], Train-Loss: 0.0307, Val-Loss: 0.0254\n",
      "Epoch [30/700], Train-Loss: 0.0110, Val-Loss: 0.0080\n",
      "Epoch [40/700], Train-Loss: 0.0040, Val-Loss: 0.0032\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.5821e-04, 1.8316e-04, 2.2194e-04,  ..., 1.2943e-05, 1.0704e-04,\n",
      "         5.5138e-05],\n",
      "        [1.0037e-04, 1.3351e-04, 2.9113e-04,  ..., 2.5372e-04, 1.2729e-04,\n",
      "         2.8321e-04],\n",
      "        [8.6671e-05, 7.8506e-05, 1.0427e-04,  ..., 3.6240e-05, 8.5112e-05,\n",
      "         9.8549e-05],\n",
      "        ...,\n",
      "        [3.8019e-04, 3.9253e-04, 1.8885e-03,  ..., 3.6990e-04, 1.0350e-03,\n",
      "         8.2839e-04],\n",
      "        [6.6903e-05, 1.0065e-04, 1.3239e-04,  ..., 2.0083e-05, 2.2437e-05,\n",
      "         1.6686e-04],\n",
      "        [8.4471e-05, 1.9006e-04, 1.0803e-04,  ..., 1.3117e-05, 5.1666e-05,\n",
      "         2.5297e-04]], device='cuda:0')\n",
      "pred: tensor([0.8373, 0.8239, 0.7963, 0.8182, 0.7902, 0.8247, 0.8115, 0.8063, 0.7915,\n",
      "        0.8166, 0.7998, 0.8260, 0.7978, 0.8152, 0.8389, 0.8018, 0.8259, 0.8204,\n",
      "        0.7805, 0.7879, 0.8232, 0.8377, 0.8655, 0.8600, 0.8842, 0.8086, 0.8680,\n",
      "        0.8139, 0.8573, 0.8472, 0.8555, 0.8710, 0.8736, 0.8714, 0.8584, 0.8728,\n",
      "        0.8300, 0.8481, 0.8403, 0.8402, 0.8360, 0.8718, 0.8050, 0.8661, 0.8255,\n",
      "        0.7921, 0.8377, 0.8103, 0.8303, 0.8328, 0.7677, 0.7849, 0.7600, 0.7730,\n",
      "        0.7820, 0.7690, 0.8250, 0.8220, 0.7644, 0.8429, 0.7969, 0.8012, 0.7875,\n",
      "        0.8017, 0.8308, 0.8489, 0.7910, 0.7744, 0.8116, 0.8284, 0.8324, 0.7927,\n",
      "        0.8257, 0.8028, 0.7936, 0.8619, 0.8551, 0.8421, 0.8663, 0.8630, 0.8295,\n",
      "        0.8323, 0.8607, 0.8422, 0.8630, 0.8186, 0.7757, 0.8376, 0.7921, 0.8404,\n",
      "        0.8388, 0.8394, 0.8069, 0.8318, 0.8623, 0.8486, 0.8024, 0.8563, 0.8514,\n",
      "        0.8384], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0828, Val-Loss: 0.0469\n",
      "Epoch [20/700], Train-Loss: 0.0201, Val-Loss: 0.0154\n",
      "Epoch [30/700], Train-Loss: 0.0073, Val-Loss: 0.0047\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0019\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.7470e-05, 5.4301e-04, 2.7019e-04,  ..., 3.4154e-03, 7.5337e-05,\n",
      "         1.4488e-04],\n",
      "        [1.0091e-04, 3.1863e-04, 1.4823e-04,  ..., 8.1677e-04, 1.2384e-03,\n",
      "         1.8221e-04],\n",
      "        [1.0342e-04, 2.3786e-04, 4.3533e-04,  ..., 1.5162e-04, 1.9263e-04,\n",
      "         1.0440e-04],\n",
      "        ...,\n",
      "        [4.1817e-05, 1.9826e-04, 5.9068e-05,  ..., 5.6278e-05, 1.8074e-05,\n",
      "         5.7676e-06],\n",
      "        [6.9905e-05, 1.0339e-04, 1.3196e-04,  ..., 3.9346e-04, 1.5371e-04,\n",
      "         1.2937e-04],\n",
      "        [6.3296e-05, 4.7583e-04, 1.8219e-04,  ..., 1.1926e-03, 5.5811e-04,\n",
      "         3.2006e-04]], device='cuda:0')\n",
      "pred: tensor([0.8372, 0.8220, 0.7966, 0.8178, 0.7908, 0.8264, 0.8134, 0.8072, 0.7914,\n",
      "        0.8173, 0.7984, 0.8269, 0.7976, 0.8157, 0.8375, 0.8020, 0.8271, 0.8209,\n",
      "        0.7816, 0.7885, 0.8237, 0.8384, 0.8652, 0.8595, 0.8853, 0.8070, 0.8691,\n",
      "        0.8159, 0.8586, 0.8457, 0.8547, 0.8720, 0.8723, 0.8721, 0.8573, 0.8741,\n",
      "        0.8317, 0.8506, 0.8426, 0.8411, 0.8354, 0.8709, 0.8050, 0.8650, 0.8253,\n",
      "        0.7920, 0.8393, 0.8105, 0.8293, 0.8335, 0.7664, 0.7852, 0.7612, 0.7722,\n",
      "        0.7819, 0.7692, 0.8239, 0.8241, 0.7643, 0.8410, 0.7969, 0.8025, 0.7879,\n",
      "        0.8024, 0.8318, 0.8478, 0.7914, 0.7751, 0.8121, 0.8276, 0.8312, 0.7933,\n",
      "        0.8256, 0.8029, 0.7946, 0.8611, 0.8527, 0.8425, 0.8702, 0.8658, 0.8309,\n",
      "        0.8317, 0.8600, 0.8418, 0.8622, 0.8180, 0.7761, 0.8377, 0.7930, 0.8400,\n",
      "        0.8387, 0.8395, 0.8060, 0.8324, 0.8606, 0.8468, 0.8027, 0.8545, 0.8524,\n",
      "        0.8381], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0561, Val-Loss: 0.0193\n",
      "Epoch [20/700], Train-Loss: 0.0094, Val-Loss: 0.0033\n",
      "Epoch [30/700], Train-Loss: 0.0006, Val-Loss: 0.0002\n",
      "Epoch [40/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0008\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.4091e-03, 8.9145e-05, 2.8702e-04,  ..., 1.1183e-04, 1.9595e-04,\n",
      "         3.8597e-04],\n",
      "        [4.4552e-04, 2.3369e-04, 2.6958e-04,  ..., 2.4787e-04, 2.3213e-04,\n",
      "         6.8412e-04],\n",
      "        [3.2840e-03, 1.0889e-03, 4.9185e-04,  ..., 3.0478e-04, 1.2752e-04,\n",
      "         2.5214e-03],\n",
      "        ...,\n",
      "        [3.4773e-01, 3.0269e-03, 1.8017e-04,  ..., 1.2266e-04, 1.1883e-04,\n",
      "         4.9141e-04],\n",
      "        [4.5284e-04, 1.2163e-04, 1.3206e-03,  ..., 7.3141e-05, 4.1570e-05,\n",
      "         8.9450e-05],\n",
      "        [2.7951e-04, 2.1713e-04, 8.9941e-04,  ..., 3.6378e-05, 1.3731e-04,\n",
      "         3.9408e-04]], device='cuda:0')\n",
      "pred: tensor([0.8382, 0.8239, 0.7972, 0.8163, 0.7899, 0.8244, 0.8121, 0.8073, 0.7926,\n",
      "        0.8184, 0.7989, 0.8258, 0.7983, 0.8163, 0.8376, 0.8032, 0.8269, 0.8221,\n",
      "        0.7806, 0.7889, 0.8246, 0.8375, 0.8645, 0.8603, 0.8848, 0.8073, 0.8678,\n",
      "        0.8142, 0.8576, 0.8474, 0.8547, 0.8708, 0.8730, 0.8712, 0.8580, 0.8725,\n",
      "        0.8302, 0.8501, 0.8405, 0.8396, 0.8348, 0.8743, 0.8053, 0.8649, 0.8222,\n",
      "        0.7918, 0.8378, 0.8105, 0.8267, 0.8328, 0.7656, 0.7850, 0.7602, 0.7734,\n",
      "        0.7804, 0.7695, 0.8237, 0.8228, 0.7630, 0.8415, 0.7977, 0.8029, 0.7870,\n",
      "        0.8029, 0.8310, 0.8477, 0.7921, 0.7749, 0.8116, 0.8283, 0.8322, 0.7922,\n",
      "        0.8254, 0.8016, 0.7939, 0.8611, 0.8544, 0.8420, 0.8688, 0.8635, 0.8295,\n",
      "        0.8322, 0.8597, 0.8405, 0.8627, 0.8178, 0.7757, 0.8382, 0.7914, 0.8400,\n",
      "        0.8391, 0.8399, 0.8053, 0.8320, 0.8599, 0.8462, 0.8033, 0.8567, 0.8515,\n",
      "        0.8387], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0298, Val-Loss: 0.0048\n",
      "Epoch [20/700], Train-Loss: 0.0046, Val-Loss: 0.0006\n",
      "Epoch [30/700], Train-Loss: 0.0001, Val-Loss: 0.0008\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0008\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.1706e-05, 4.8505e-05, 1.3014e-04,  ..., 7.0476e-05, 2.4792e-04,\n",
      "         1.3181e-03],\n",
      "        [7.6170e-05, 8.3384e-05, 1.2806e-04,  ..., 9.2484e-05, 1.7679e-04,\n",
      "         1.1627e-04],\n",
      "        [1.1706e-04, 8.6042e-05, 5.7700e-05,  ..., 8.6792e-05, 2.4956e-04,\n",
      "         2.0858e-03],\n",
      "        ...,\n",
      "        [1.6484e-04, 1.6969e-04, 2.2655e-04,  ..., 4.5550e-05, 3.0044e-04,\n",
      "         8.0150e-04],\n",
      "        [5.2340e-05, 1.3285e-04, 1.4469e-04,  ..., 2.2446e-05, 1.5931e-04,\n",
      "         9.8643e-05],\n",
      "        [2.4950e-05, 1.1347e-04, 1.5673e-04,  ..., 3.3289e-05, 5.5907e-04,\n",
      "         3.9848e-04]], device='cuda:0')\n",
      "pred: tensor([0.8367, 0.8219, 0.7953, 0.8159, 0.7913, 0.8257, 0.8128, 0.8066, 0.7915,\n",
      "        0.8175, 0.7990, 0.8290, 0.7976, 0.8188, 0.8392, 0.8020, 0.8269, 0.8208,\n",
      "        0.7801, 0.7885, 0.8248, 0.8387, 0.8629, 0.8600, 0.8857, 0.8086, 0.8673,\n",
      "        0.8120, 0.8563, 0.8459, 0.8543, 0.8711, 0.8710, 0.8699, 0.8579, 0.8733,\n",
      "        0.8320, 0.8511, 0.8400, 0.8397, 0.8352, 0.8724, 0.8057, 0.8662, 0.8228,\n",
      "        0.7925, 0.8366, 0.8101, 0.8278, 0.8348, 0.7667, 0.7838, 0.7593, 0.7719,\n",
      "        0.7824, 0.7693, 0.8232, 0.8230, 0.7634, 0.8431, 0.7981, 0.8010, 0.7872,\n",
      "        0.8027, 0.8312, 0.8492, 0.7913, 0.7752, 0.8129, 0.8284, 0.8330, 0.7925,\n",
      "        0.8259, 0.8026, 0.7934, 0.8602, 0.8522, 0.8415, 0.8692, 0.8644, 0.8297,\n",
      "        0.8319, 0.8605, 0.8424, 0.8628, 0.8193, 0.7759, 0.8404, 0.7927, 0.8400,\n",
      "        0.8368, 0.8395, 0.8074, 0.8321, 0.8645, 0.8479, 0.8028, 0.8563, 0.8513,\n",
      "        0.8389], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1126, Val-Loss: 0.0675\n",
      "Epoch [20/700], Train-Loss: 0.0250, Val-Loss: 0.0163\n",
      "Epoch [30/700], Train-Loss: 0.0067, Val-Loss: 0.0025\n",
      "Epoch [40/700], Train-Loss: 0.0016, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.2041e-05, 3.7201e-05, 1.0877e-04,  ..., 1.6768e-05, 1.1679e-05,\n",
      "         1.3379e-03],\n",
      "        [8.6396e-05, 1.0503e-04, 3.6188e-05,  ..., 2.3727e-05, 3.1663e-05,\n",
      "         8.3357e-05],\n",
      "        [6.5868e-04, 1.5575e-03, 1.4570e-04,  ..., 1.1815e-04, 6.3817e-05,\n",
      "         4.1151e-04],\n",
      "        ...,\n",
      "        [1.1778e-04, 9.8645e-05, 1.1183e-04,  ..., 1.0972e-04, 9.4606e-05,\n",
      "         1.2584e-04],\n",
      "        [2.4011e-04, 1.0245e-04, 8.1014e-05,  ..., 1.6663e-04, 9.8073e-05,\n",
      "         1.6071e-03],\n",
      "        [6.7268e-05, 5.0484e-05, 9.4141e-06,  ..., 3.2311e-05, 4.5591e-05,\n",
      "         4.2893e-04]], device='cuda:0')\n",
      "pred: tensor([0.8371, 0.8238, 0.7967, 0.8171, 0.7906, 0.8257, 0.8131, 0.8071, 0.7922,\n",
      "        0.8179, 0.7986, 0.8278, 0.7967, 0.8151, 0.8388, 0.8013, 0.8262, 0.8217,\n",
      "        0.7810, 0.7896, 0.8233, 0.8396, 0.8645, 0.8608, 0.8861, 0.8078, 0.8684,\n",
      "        0.8131, 0.8564, 0.8470, 0.8538, 0.8715, 0.8740, 0.8718, 0.8577, 0.8733,\n",
      "        0.8310, 0.8493, 0.8419, 0.8374, 0.8360, 0.8702, 0.8060, 0.8662, 0.8223,\n",
      "        0.7908, 0.8364, 0.8106, 0.8262, 0.8323, 0.7666, 0.7849, 0.7599, 0.7730,\n",
      "        0.7826, 0.7692, 0.8233, 0.8223, 0.7642, 0.8418, 0.7957, 0.8026, 0.7879,\n",
      "        0.8035, 0.8301, 0.8495, 0.7904, 0.7745, 0.8123, 0.8275, 0.8306, 0.7914,\n",
      "        0.8259, 0.8031, 0.7942, 0.8618, 0.8534, 0.8433, 0.8703, 0.8644, 0.8301,\n",
      "        0.8313, 0.8597, 0.8407, 0.8612, 0.8189, 0.7758, 0.8368, 0.7912, 0.8386,\n",
      "        0.8381, 0.8400, 0.8075, 0.8337, 0.8637, 0.8483, 0.8037, 0.8570, 0.8530,\n",
      "        0.8396], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9810741074107409, 0.9789498949894989, 0.9815181518151814, 0.9792499249924992, 0.9792259225922592]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.6351640681352405, 0.7458581896524129, 0.6818127354374178, 0.6825327257511652, 0.8012268021701995]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8192215185630004, 0.7856817391329869, 0.7528535087163538, 0.8007265136751636, 0.8359487551124415]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8752754299131001, 0.8612233101188762, 0.8395836099247994, 0.872286209957423, 0.8493501318775405]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8494723221676738, 0.8683744453431883, 0.8860067151087762, 0.808467896138662, 0.8432523121381661]}\n",
      "Epoch [10/700], Train-Loss: 0.4137, Val-Loss: 0.3689\n",
      "Epoch [20/700], Train-Loss: 0.0011, Val-Loss: 0.0087\n",
      "Epoch [30/700], Train-Loss: 0.0149, Val-Loss: 0.0058\n",
      "Epoch [40/700], Train-Loss: 0.0149, Val-Loss: 0.0143\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0018\n",
      "Epoch [60/700], Train-Loss: 0.0009, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0011, Val-Loss: 0.0009\n",
      "Epoch [80/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[7.7536e-05, 1.7449e-04, 8.4210e-05,  ..., 1.1039e-04, 2.0274e-04,\n",
      "         2.8049e-04],\n",
      "        [9.6615e-05, 2.3892e-04, 2.1216e-04,  ..., 2.2100e-04, 5.5923e-04,\n",
      "         4.9546e-04],\n",
      "        [5.0132e-05, 4.0069e-04, 9.8276e-05,  ..., 1.3185e-04, 4.2237e-04,\n",
      "         2.8672e-04],\n",
      "        ...,\n",
      "        [5.4232e-05, 8.7915e-05, 9.3422e-05,  ..., 1.0476e-04, 1.8282e-04,\n",
      "         8.1290e-04],\n",
      "        [1.3202e-04, 1.9307e-04, 1.4544e-04,  ..., 1.9697e-04, 1.5981e-04,\n",
      "         1.2057e-04],\n",
      "        [8.3887e-05, 9.3396e-05, 6.3709e-05,  ..., 4.0794e-05, 9.9417e-05,\n",
      "         3.5174e-04]], device='cuda:0')\n",
      "pred: tensor([0.8467, 0.8201, 0.8051, 0.7971, 0.7817, 0.8226, 0.8118, 0.8220, 0.7833,\n",
      "        0.8065, 0.8006, 0.8468, 0.7730, 0.8203, 0.8271, 0.8270, 0.8285, 0.8325,\n",
      "        0.7775, 0.8057, 0.8320, 0.8379, 0.8489, 0.8533, 0.8650, 0.8229, 0.8542,\n",
      "        0.8221, 0.8491, 0.8412, 0.8349, 0.8776, 0.8683, 0.8594, 0.8351, 0.8607,\n",
      "        0.8254, 0.8504, 0.8491, 0.8142, 0.8418, 0.8561, 0.8136, 0.8665, 0.8182,\n",
      "        0.8063, 0.8243, 0.8185, 0.8073, 0.8370, 0.7559, 0.8142, 0.7585, 0.7902,\n",
      "        0.7722, 0.7898, 0.8326, 0.8324, 0.7360, 0.8435, 0.8200, 0.8120, 0.7505,\n",
      "        0.8191, 0.8291, 0.8472, 0.8044, 0.7940, 0.7895, 0.8397, 0.8310, 0.8134,\n",
      "        0.8306, 0.8169, 0.8187, 0.8580, 0.8448, 0.8372, 0.8504, 0.8263, 0.8333,\n",
      "        0.8363, 0.8520, 0.8486, 0.8473, 0.8365, 0.7692, 0.8521, 0.8181, 0.8331,\n",
      "        0.8543, 0.8361, 0.8154, 0.8226, 0.8343, 0.8399, 0.8176, 0.8390, 0.8356,\n",
      "        0.8271], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3534, Val-Loss: 0.3006\n",
      "Epoch [20/700], Train-Loss: 0.0377, Val-Loss: 0.0600\n",
      "Epoch [30/700], Train-Loss: 0.0008, Val-Loss: 0.0028\n",
      "Epoch [40/700], Train-Loss: 0.0104, Val-Loss: 0.0075\n",
      "Epoch [50/700], Train-Loss: 0.0046, Val-Loss: 0.0043\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0012\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "x: tensor([[4.7790e-05, 7.5595e-05, 1.2500e-04,  ..., 2.0349e-04, 2.0553e-04,\n",
      "         5.6015e-05],\n",
      "        [1.0592e-04, 6.9877e-05, 1.2520e-04,  ..., 1.2030e-04, 1.2870e-04,\n",
      "         1.5800e-04],\n",
      "        [7.4704e-05, 8.2748e-05, 3.0598e-04,  ..., 1.3529e-04, 5.0422e-05,\n",
      "         5.9703e-05],\n",
      "        ...,\n",
      "        [6.9918e-05, 9.4593e-05, 1.9212e-04,  ..., 1.3966e-04, 1.8463e-04,\n",
      "         2.2603e-05],\n",
      "        [1.5886e-04, 1.3004e-04, 2.7502e-04,  ..., 2.4814e-04, 1.1046e-04,\n",
      "         1.8345e-05],\n",
      "        [1.1119e-04, 1.3694e-04, 7.0922e-04,  ..., 6.2903e-05, 1.0854e-04,\n",
      "         8.5856e-05]], device='cuda:0')\n",
      "pred: tensor([0.8359, 0.8174, 0.8078, 0.8155, 0.7698, 0.8372, 0.8160, 0.8180, 0.7959,\n",
      "        0.8265, 0.8040, 0.8256, 0.7898, 0.8290, 0.8205, 0.8050, 0.8151, 0.8203,\n",
      "        0.7933, 0.8037, 0.8162, 0.8343, 0.8716, 0.8402, 0.8891, 0.8140, 0.8512,\n",
      "        0.8120, 0.8384, 0.8353, 0.8581, 0.8661, 0.8533, 0.8740, 0.8492, 0.8687,\n",
      "        0.8353, 0.8413, 0.8298, 0.7885, 0.8354, 0.9186, 0.8079, 0.8655, 0.8222,\n",
      "        0.8235, 0.8560, 0.8457, 0.8241, 0.8295, 0.7868, 0.7982, 0.7478, 0.7723,\n",
      "        0.7780, 0.7655, 0.8341, 0.8237, 0.7379, 0.8240, 0.7689, 0.8041, 0.7895,\n",
      "        0.8270, 0.8162, 0.8205, 0.8003, 0.7756, 0.8196, 0.8316, 0.8309, 0.7926,\n",
      "        0.8234, 0.8220, 0.7957, 0.8614, 0.8421, 0.8474, 0.8474, 0.8538, 0.8331,\n",
      "        0.8370, 0.8533, 0.8414, 0.8577, 0.7971, 0.7820, 0.8189, 0.7873, 0.8450,\n",
      "        0.8633, 0.8280, 0.8033, 0.8373, 0.8608, 0.8792, 0.8073, 0.8457, 0.8244,\n",
      "        0.8528], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2693, Val-Loss: 0.2267\n",
      "Epoch [20/700], Train-Loss: 0.0338, Val-Loss: 0.0503\n",
      "Epoch [30/700], Train-Loss: 0.0011, Val-Loss: 0.0032\n",
      "Epoch [40/700], Train-Loss: 0.0078, Val-Loss: 0.0056\n",
      "Epoch [50/700], Train-Loss: 0.0039, Val-Loss: 0.0034\n",
      "Epoch [60/700], Train-Loss: 0.0009, Val-Loss: 0.0012\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "x: tensor([[7.4404e-05, 1.4120e-04, 1.1503e-04,  ..., 8.5317e-05, 2.4276e-04,\n",
      "         1.2977e-03],\n",
      "        [1.3081e-04, 1.9356e-04, 1.4779e-04,  ..., 1.3589e-04, 2.4431e-04,\n",
      "         1.0138e-03],\n",
      "        [1.2628e-04, 1.0544e-04, 1.8781e-04,  ..., 9.1167e-05, 1.8478e-04,\n",
      "         1.0265e-03],\n",
      "        ...,\n",
      "        [1.2156e-04, 1.4579e-04, 1.1261e-04,  ..., 5.8675e-05, 7.3902e-05,\n",
      "         1.8465e-04],\n",
      "        [1.0313e-04, 7.0421e-05, 4.8291e-05,  ..., 5.2255e-05, 3.9349e-05,\n",
      "         1.8416e-03],\n",
      "        [1.9680e-04, 8.0229e-05, 7.8823e-05,  ..., 9.1729e-05, 4.6434e-05,\n",
      "         5.7750e-03]], device='cuda:0')\n",
      "pred: tensor([0.8350, 0.8365, 0.7915, 0.8433, 0.7993, 0.8467, 0.8083, 0.8153, 0.7844,\n",
      "        0.8259, 0.7690, 0.8318, 0.7518, 0.8328, 0.8442, 0.8336, 0.8242, 0.8212,\n",
      "        0.8018, 0.8160, 0.8251, 0.8223, 0.8543, 0.8527, 0.8763, 0.8121, 0.8841,\n",
      "        0.7938, 0.8490, 0.8388, 0.8765, 0.8363, 0.8683, 0.8685, 0.8560, 0.8870,\n",
      "        0.8369, 0.8402, 0.8388, 0.8298, 0.8576, 0.8524, 0.8072, 0.8607, 0.8344,\n",
      "        0.8383, 0.8485, 0.8278, 0.8333, 0.8531, 0.7515, 0.8018, 0.7814, 0.7815,\n",
      "        0.7848, 0.7525, 0.8211, 0.8291, 0.7337, 0.8455, 0.7840, 0.8010, 0.7865,\n",
      "        0.7882, 0.8343, 0.8502, 0.8126, 0.7785, 0.8199, 0.8351, 0.8290, 0.8063,\n",
      "        0.8244, 0.7883, 0.7683, 0.8650, 0.8376, 0.8567, 0.8755, 0.8524, 0.8441,\n",
      "        0.8341, 0.8643, 0.8441, 0.8611, 0.8240, 0.7846, 0.8517, 0.7929, 0.8477,\n",
      "        0.8328, 0.8243, 0.8163, 0.8428, 0.8514, 0.8505, 0.8063, 0.8600, 0.8472,\n",
      "        0.8516], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.4118, Val-Loss: 0.3597\n",
      "Epoch [20/700], Train-Loss: 0.0068, Val-Loss: 0.0241\n",
      "Epoch [30/700], Train-Loss: 0.0072, Val-Loss: 0.0017\n",
      "Epoch [40/700], Train-Loss: 0.0152, Val-Loss: 0.0138\n",
      "Epoch [50/700], Train-Loss: 0.0021, Val-Loss: 0.0026\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0009, Val-Loss: 0.0007\n",
      "Epoch [80/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[1.4984e-04, 1.1360e-04, 9.9235e-05,  ..., 3.7351e-05, 5.8188e-05,\n",
      "         3.1389e-05],\n",
      "        [5.2100e-05, 6.2536e-05, 1.2676e-04,  ..., 2.9990e-04, 4.4438e-04,\n",
      "         4.7473e-04],\n",
      "        [2.9337e-04, 2.5667e-04, 2.8662e-04,  ..., 1.5833e-04, 1.6352e-04,\n",
      "         3.2909e-03],\n",
      "        ...,\n",
      "        [7.5108e-05, 8.0925e-05, 8.8605e-05,  ..., 5.6116e-05, 9.0007e-05,\n",
      "         7.0234e-05],\n",
      "        [7.1777e-05, 1.5535e-04, 9.5441e-05,  ..., 5.8415e-04, 3.1975e-04,\n",
      "         2.3826e-04],\n",
      "        [6.7086e-05, 1.2599e-04, 7.3582e-05,  ..., 8.3389e-04, 3.7862e-05,\n",
      "         1.2138e-04]], device='cuda:0')\n",
      "pred: tensor([0.8303, 0.8322, 0.7916, 0.8288, 0.7751, 0.8395, 0.7940, 0.8014, 0.7516,\n",
      "        0.8165, 0.7810, 0.8375, 0.8079, 0.8335, 0.8445, 0.8282, 0.8234, 0.8131,\n",
      "        0.7956, 0.8070, 0.8252, 0.8344, 0.8523, 0.8356, 0.8756, 0.8159, 0.8629,\n",
      "        0.8318, 0.8313, 0.8444, 0.8418, 0.8805, 0.8396, 0.8689, 0.8484, 0.8677,\n",
      "        0.8185, 0.8459, 0.8312, 0.8360, 0.8441, 0.8567, 0.8016, 0.8550, 0.8305,\n",
      "        0.8025, 0.8501, 0.8007, 0.8290, 0.8218, 0.7491, 0.7813, 0.7871, 0.7887,\n",
      "        0.7700, 0.7848, 0.8382, 0.8221, 0.7476, 0.8415, 0.7971, 0.8089, 0.7775,\n",
      "        0.8113, 0.8392, 0.8436, 0.7897, 0.7606, 0.8004, 0.8368, 0.8270, 0.8269,\n",
      "        0.8381, 0.7858, 0.7586, 0.8733, 0.8400, 0.8400, 0.8583, 0.8345, 0.8277,\n",
      "        0.8347, 0.8404, 0.8332, 0.8530, 0.8147, 0.7780, 0.8395, 0.8146, 0.8322,\n",
      "        0.8283, 0.8503, 0.8095, 0.8473, 0.8518, 0.8539, 0.8038, 0.8670, 0.8403,\n",
      "        0.8528], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3651, Val-Loss: 0.3081\n",
      "Epoch [20/700], Train-Loss: 0.0330, Val-Loss: 0.0548\n",
      "Epoch [30/700], Train-Loss: 0.0010, Val-Loss: 0.0014\n",
      "Epoch [40/700], Train-Loss: 0.0120, Val-Loss: 0.0094\n",
      "Epoch [50/700], Train-Loss: 0.0037, Val-Loss: 0.0039\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.3611e-04, 2.5846e-04, 1.9828e-04,  ..., 6.3805e-04, 3.4775e-04,\n",
      "         1.8281e-04],\n",
      "        [2.2663e-04, 1.1325e-04, 1.1288e-04,  ..., 8.0014e-04, 1.3907e-03,\n",
      "         1.0621e-04],\n",
      "        [1.9797e-04, 2.4235e-04, 4.8357e-04,  ..., 6.7546e-04, 5.6492e-04,\n",
      "         1.2487e-04],\n",
      "        ...,\n",
      "        [8.5191e-05, 9.2778e-05, 1.2693e-04,  ..., 1.9753e-04, 7.6215e-04,\n",
      "         6.8462e-05],\n",
      "        [1.5726e-04, 9.3989e-05, 1.3825e-04,  ..., 2.1594e-04, 1.5969e-04,\n",
      "         8.4575e-05],\n",
      "        [1.1453e-04, 1.2592e-04, 2.8376e-05,  ..., 2.2804e-04, 2.0321e-04,\n",
      "         1.4252e-04]], device='cuda:0')\n",
      "pred: tensor([0.8429, 0.8449, 0.7926, 0.8089, 0.7724, 0.8164, 0.8035, 0.8102, 0.7895,\n",
      "        0.8244, 0.7781, 0.8292, 0.7883, 0.8105, 0.8561, 0.8086, 0.8381, 0.8363,\n",
      "        0.7864, 0.7948, 0.8070, 0.8432, 0.8634, 0.8488, 0.8666, 0.8016, 0.8562,\n",
      "        0.8295, 0.8424, 0.8542, 0.8480, 0.8707, 0.8691, 0.8782, 0.8664, 0.8588,\n",
      "        0.8264, 0.8570, 0.8444, 0.8430, 0.8462, 0.8601, 0.8001, 0.8566, 0.8236,\n",
      "        0.7924, 0.8449, 0.8082, 0.8262, 0.8439, 0.7681, 0.7751, 0.7619, 0.7614,\n",
      "        0.8061, 0.7704, 0.8413, 0.8270, 0.7640, 0.8403, 0.7954, 0.8124, 0.7981,\n",
      "        0.8095, 0.8287, 0.8458, 0.7991, 0.7970, 0.8229, 0.8391, 0.8292, 0.7983,\n",
      "        0.8348, 0.7980, 0.7857, 0.8754, 0.8440, 0.8455, 0.8549, 0.8421, 0.8212,\n",
      "        0.8246, 0.8551, 0.8479, 0.8581, 0.8118, 0.7891, 0.8409, 0.7723, 0.8252,\n",
      "        0.8375, 0.8414, 0.8100, 0.8403, 0.8558, 0.8328, 0.8053, 0.8566, 0.8717,\n",
      "        0.8348], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9124272427242723, 0.9059945994599459, 0.9028262826282627, 0.9027662766276627, 0.9385658565856586]}\n",
      "Epoch [10/700], Train-Loss: 0.2891, Val-Loss: 0.2254\n",
      "Epoch [20/700], Train-Loss: 0.0849, Val-Loss: 0.0844\n",
      "Epoch [30/700], Train-Loss: 0.0095, Val-Loss: 0.0148\n",
      "Epoch [40/700], Train-Loss: 0.0029, Val-Loss: 0.0011\n",
      "Epoch [50/700], Train-Loss: 0.0037, Val-Loss: 0.0024\n",
      "Epoch [60/700], Train-Loss: 0.0017, Val-Loss: 0.0016\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0006\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.4782e-05, 2.6449e-05, 3.0628e-05,  ..., 1.8082e-04, 8.9897e-05,\n",
      "         1.0880e-04],\n",
      "        [3.7208e-05, 1.7636e-04, 1.2023e-04,  ..., 1.7311e-04, 1.0859e-04,\n",
      "         1.0998e-04],\n",
      "        [1.7311e-04, 1.6289e-04, 5.2727e-04,  ..., 3.0508e-04, 2.4021e-04,\n",
      "         2.3418e-04],\n",
      "        ...,\n",
      "        [1.2265e-04, 6.4776e-04, 1.2793e-04,  ..., 1.9803e-04, 1.3134e-04,\n",
      "         9.2462e-05],\n",
      "        [1.7268e-04, 1.1332e-04, 1.5651e-04,  ..., 4.9941e-05, 4.5763e-05,\n",
      "         9.5675e-05],\n",
      "        [7.4080e-05, 4.1333e-04, 1.7250e-04,  ..., 1.1289e-04, 1.2478e-04,\n",
      "         4.3891e-05]], device='cuda:0')\n",
      "pred: tensor([0.8430, 0.8298, 0.7945, 0.8207, 0.7796, 0.8391, 0.8035, 0.8172, 0.7933,\n",
      "        0.8269, 0.7938, 0.8195, 0.8060, 0.8304, 0.8423, 0.8140, 0.8244, 0.8200,\n",
      "        0.7842, 0.7925, 0.8414, 0.8322, 0.8557, 0.8518, 0.8738, 0.8100, 0.8610,\n",
      "        0.8397, 0.8432, 0.8439, 0.8372, 0.8687, 0.8644, 0.8665, 0.8507, 0.8753,\n",
      "        0.8198, 0.8394, 0.8302, 0.8408, 0.8392, 0.8680, 0.8030, 0.8708, 0.8137,\n",
      "        0.7888, 0.8425, 0.8370, 0.8235, 0.8242, 0.7748, 0.7933, 0.7589, 0.7864,\n",
      "        0.7814, 0.7782, 0.8249, 0.8216, 0.7533, 0.8380, 0.8055, 0.8065, 0.7771,\n",
      "        0.8007, 0.8429, 0.8525, 0.7887, 0.7810, 0.8140, 0.8415, 0.8331, 0.8182,\n",
      "        0.8412, 0.8060, 0.7929, 0.8549, 0.8516, 0.8366, 0.8553, 0.8564, 0.8305,\n",
      "        0.8408, 0.8547, 0.8448, 0.8694, 0.8191, 0.7924, 0.8463, 0.7872, 0.8362,\n",
      "        0.8440, 0.8245, 0.8083, 0.8352, 0.8504, 0.8463, 0.8068, 0.8574, 0.8417,\n",
      "        0.8349], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1496, Val-Loss: 0.1003\n",
      "Epoch [20/700], Train-Loss: 0.0612, Val-Loss: 0.0449\n",
      "Epoch [30/700], Train-Loss: 0.0144, Val-Loss: 0.0171\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0007\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.4166e-04, 1.0885e-04, 1.5922e-04,  ..., 8.0982e-05, 6.7689e-05,\n",
      "         1.5842e-04],\n",
      "        [3.4771e-04, 1.4249e-04, 2.3417e-04,  ..., 1.6672e-04, 1.3900e-04,\n",
      "         1.4446e-04],\n",
      "        [3.3879e-04, 2.7394e-04, 2.6139e-04,  ..., 1.4969e-04, 1.1156e-04,\n",
      "         3.5122e-04],\n",
      "        ...,\n",
      "        [6.4906e-05, 9.7404e-05, 1.5985e-04,  ..., 8.0745e-05, 7.0826e-05,\n",
      "         9.0964e-05],\n",
      "        [7.0555e-05, 4.3894e-05, 1.5816e-04,  ..., 1.3118e-04, 7.6829e-05,\n",
      "         1.1492e-04],\n",
      "        [1.1719e-04, 2.5795e-05, 1.3526e-04,  ..., 1.2458e-04, 1.0426e-04,\n",
      "         5.0110e-04]], device='cuda:0')\n",
      "pred: tensor([0.8488, 0.8365, 0.8062, 0.8074, 0.7777, 0.8246, 0.8231, 0.8008, 0.7858,\n",
      "        0.8150, 0.8042, 0.8209, 0.8038, 0.8241, 0.8299, 0.8213, 0.8222, 0.8234,\n",
      "        0.7787, 0.7885, 0.8190, 0.8375, 0.8782, 0.8396, 0.8794, 0.8093, 0.8676,\n",
      "        0.8312, 0.8455, 0.8337, 0.8370, 0.8640, 0.8600, 0.8800, 0.8348, 0.8814,\n",
      "        0.8269, 0.8503, 0.8294, 0.8276, 0.8352, 0.8637, 0.8012, 0.8680, 0.8216,\n",
      "        0.7990, 0.8492, 0.8302, 0.8272, 0.8439, 0.7686, 0.7895, 0.7741, 0.7792,\n",
      "        0.7893, 0.7964, 0.8438, 0.8239, 0.7440, 0.8398, 0.7987, 0.7953, 0.7985,\n",
      "        0.8186, 0.8290, 0.8391, 0.8079, 0.7718, 0.8202, 0.8309, 0.8317, 0.8125,\n",
      "        0.8375, 0.8075, 0.7819, 0.8699, 0.8740, 0.8516, 0.8536, 0.8482, 0.8382,\n",
      "        0.8418, 0.8655, 0.8570, 0.8592, 0.8193, 0.7610, 0.8365, 0.7948, 0.8359,\n",
      "        0.8260, 0.8371, 0.8115, 0.8409, 0.8690, 0.8531, 0.8032, 0.8457, 0.8517,\n",
      "        0.8525], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0701, Val-Loss: 0.0315\n",
      "Epoch [20/700], Train-Loss: 0.0299, Val-Loss: 0.0142\n",
      "Epoch [30/700], Train-Loss: 0.0165, Val-Loss: 0.0158\n",
      "Epoch [40/700], Train-Loss: 0.0025, Val-Loss: 0.0032\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.6730e-04, 1.4412e-04, 1.5701e-04,  ..., 9.3639e-05, 1.4863e-04,\n",
      "         3.0773e-04],\n",
      "        [1.4708e-04, 2.8964e-04, 1.1284e-04,  ..., 1.2751e-04, 1.0096e-04,\n",
      "         1.8042e-04],\n",
      "        [4.4121e-04, 3.1075e-04, 4.0610e-04,  ..., 1.7745e-04, 3.1768e-05,\n",
      "         5.8164e-04],\n",
      "        ...,\n",
      "        [1.4095e-04, 1.6108e-04, 1.8244e-04,  ..., 1.4013e-04, 3.7442e-04,\n",
      "         1.8009e-04],\n",
      "        [3.3818e-04, 3.4178e-04, 1.2276e-04,  ..., 2.2075e-04, 1.4850e-04,\n",
      "         1.5221e-04],\n",
      "        [1.2780e-04, 1.6843e-04, 3.1376e-04,  ..., 1.3052e-04, 1.0852e-04,\n",
      "         2.3692e-04]], device='cuda:0')\n",
      "pred: tensor([0.8395, 0.8257, 0.7944, 0.8149, 0.7817, 0.8338, 0.8105, 0.8125, 0.7947,\n",
      "        0.8131, 0.7824, 0.8307, 0.8030, 0.8083, 0.8411, 0.7979, 0.8181, 0.8186,\n",
      "        0.7952, 0.7839, 0.8215, 0.8364, 0.8763, 0.8502, 0.9060, 0.8066, 0.8676,\n",
      "        0.8156, 0.8424, 0.8425, 0.8453, 0.8682, 0.8731, 0.8589, 0.8529, 0.8949,\n",
      "        0.8396, 0.8366, 0.8379, 0.8270, 0.8434, 0.8752, 0.8035, 0.8623, 0.8322,\n",
      "        0.7903, 0.8330, 0.8212, 0.8267, 0.8394, 0.7759, 0.7739, 0.7614, 0.7753,\n",
      "        0.7821, 0.7837, 0.8312, 0.8210, 0.7583, 0.8326, 0.7986, 0.8041, 0.7778,\n",
      "        0.7973, 0.8279, 0.8354, 0.7972, 0.7576, 0.8184, 0.8252, 0.8303, 0.7833,\n",
      "        0.8285, 0.8135, 0.7896, 0.8605, 0.8369, 0.8374, 0.8429, 0.8391, 0.8264,\n",
      "        0.8338, 0.8663, 0.8482, 0.8630, 0.8103, 0.7781, 0.8327, 0.7891, 0.8331,\n",
      "        0.8365, 0.8393, 0.7962, 0.8390, 0.8468, 0.8455, 0.8005, 0.8527, 0.8467,\n",
      "        0.8362], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1102, Val-Loss: 0.0659\n",
      "Epoch [20/700], Train-Loss: 0.0492, Val-Loss: 0.0318\n",
      "Epoch [30/700], Train-Loss: 0.0153, Val-Loss: 0.0167\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0012\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0009, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.6697e-05, 1.0346e-04, 5.7398e-05,  ..., 1.9063e-04, 1.4804e-04,\n",
      "         3.3330e-04],\n",
      "        [9.5947e-05, 7.0162e-05, 5.9470e-05,  ..., 1.6487e-04, 1.5702e-04,\n",
      "         1.7140e-04],\n",
      "        [1.1330e-04, 1.6583e-04, 1.0087e-04,  ..., 1.9134e-04, 3.2392e-04,\n",
      "         1.6248e-04],\n",
      "        ...,\n",
      "        [7.7882e-05, 1.3999e-04, 8.9864e-05,  ..., 1.1099e-04, 1.7865e-04,\n",
      "         5.2718e-04],\n",
      "        [3.8005e-05, 4.4714e-05, 4.3005e-05,  ..., 9.9683e-05, 8.0921e-05,\n",
      "         1.4145e-04],\n",
      "        [8.9129e-05, 4.6005e-05, 9.8346e-05,  ..., 8.3891e-05, 4.7783e-05,\n",
      "         1.3319e-04]], device='cuda:0')\n",
      "pred: tensor([0.8333, 0.8406, 0.7899, 0.8203, 0.7995, 0.8252, 0.8230, 0.8004, 0.7824,\n",
      "        0.8261, 0.8096, 0.8273, 0.7763, 0.8169, 0.8277, 0.8077, 0.8210, 0.8198,\n",
      "        0.7974, 0.7909, 0.8283, 0.8353, 0.8577, 0.8558, 0.8837, 0.8116, 0.8622,\n",
      "        0.8249, 0.8419, 0.8575, 0.8495, 0.8675, 0.8536, 0.8685, 0.8471, 0.8848,\n",
      "        0.8233, 0.8499, 0.8406, 0.8151, 0.8402, 0.8653, 0.8025, 0.8859, 0.8314,\n",
      "        0.8133, 0.8521, 0.8291, 0.8344, 0.8418, 0.7491, 0.7950, 0.7756, 0.7419,\n",
      "        0.7720, 0.7579, 0.8285, 0.8201, 0.7431, 0.8491, 0.8209, 0.8081, 0.7708,\n",
      "        0.8008, 0.8438, 0.8421, 0.8038, 0.7629, 0.8147, 0.8245, 0.8310, 0.7806,\n",
      "        0.8313, 0.7929, 0.8003, 0.8715, 0.8435, 0.8402, 0.8529, 0.8411, 0.8298,\n",
      "        0.8390, 0.8683, 0.8416, 0.8590, 0.8205, 0.7742, 0.8416, 0.7964, 0.8369,\n",
      "        0.8378, 0.8433, 0.8114, 0.8404, 0.8678, 0.8475, 0.8234, 0.8611, 0.8404,\n",
      "        0.8345], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1658, Val-Loss: 0.1050\n",
      "Epoch [20/700], Train-Loss: 0.0686, Val-Loss: 0.0475\n",
      "Epoch [30/700], Train-Loss: 0.0186, Val-Loss: 0.0209\n",
      "Epoch [40/700], Train-Loss: 0.0008, Val-Loss: 0.0013\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0012, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[6.9789e-05, 8.2044e-05, 1.4438e-04,  ..., 5.6633e-05, 6.9630e-05,\n",
      "         6.5807e-05],\n",
      "        [9.8041e-05, 7.2328e-05, 5.5807e-05,  ..., 1.1032e-04, 1.4728e-04,\n",
      "         2.9175e-05],\n",
      "        [1.0945e-04, 1.1036e-04, 9.0538e-05,  ..., 8.1988e-04, 8.4811e-04,\n",
      "         1.2244e-04],\n",
      "        ...,\n",
      "        [3.7130e-05, 4.2337e-05, 1.1833e-04,  ..., 1.2252e-04, 2.5720e-04,\n",
      "         1.1491e-04],\n",
      "        [1.2877e-05, 5.2168e-05, 8.8177e-05,  ..., 7.7176e-05, 3.7598e-04,\n",
      "         9.3596e-05],\n",
      "        [2.7258e-05, 7.8408e-05, 7.6879e-05,  ..., 8.1590e-05, 1.8508e-04,\n",
      "         3.9842e-05]], device='cuda:0')\n",
      "pred: tensor([0.8410, 0.8376, 0.7798, 0.8178, 0.7822, 0.8509, 0.7993, 0.8284, 0.7879,\n",
      "        0.8456, 0.8127, 0.8342, 0.7989, 0.8239, 0.8544, 0.8160, 0.8392, 0.8373,\n",
      "        0.7854, 0.7790, 0.8269, 0.8360, 0.8608, 0.8620, 0.8881, 0.8142, 0.8615,\n",
      "        0.7904, 0.8463, 0.8525, 0.8444, 0.8615, 0.8614, 0.8579, 0.8458, 0.8552,\n",
      "        0.8379, 0.8400, 0.8433, 0.8249, 0.8441, 0.8739, 0.7919, 0.8659, 0.8277,\n",
      "        0.8066, 0.8240, 0.8151, 0.8351, 0.8238, 0.7611, 0.7898, 0.7620, 0.7901,\n",
      "        0.7975, 0.7788, 0.8311, 0.8271, 0.7377, 0.8374, 0.7813, 0.7963, 0.7797,\n",
      "        0.8008, 0.8323, 0.8405, 0.7899, 0.7756, 0.8245, 0.8332, 0.8242, 0.7795,\n",
      "        0.8136, 0.7974, 0.7754, 0.8661, 0.8356, 0.8473, 0.8559, 0.8602, 0.8345,\n",
      "        0.8279, 0.8608, 0.8411, 0.8591, 0.8137, 0.7692, 0.8180, 0.8029, 0.8397,\n",
      "        0.8411, 0.8399, 0.8081, 0.8320, 0.8537, 0.8551, 0.8150, 0.8537, 0.8424,\n",
      "        0.8462], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9471227122712271, 0.9457185718571856, 0.9578517851785178, 0.9554155415541552, 0.9155715571557155]}\n",
      "Epoch [10/700], Train-Loss: 0.0158, Val-Loss: 0.0589\n",
      "Epoch [20/700], Train-Loss: 0.0105, Val-Loss: 0.0185\n",
      "Epoch [30/700], Train-Loss: 0.0006, Val-Loss: 0.0007\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.5968e-04, 2.3348e-04, 2.2818e-04,  ..., 2.0197e-04, 1.4251e-04,\n",
      "         1.3743e-04],\n",
      "        [5.5620e-04, 1.3041e-04, 3.2220e-04,  ..., 1.1336e-04, 8.7155e-05,\n",
      "         4.5602e-05],\n",
      "        [1.0439e-03, 1.6836e-04, 3.1131e-04,  ..., 8.4057e-05, 1.1626e-04,\n",
      "         1.6802e-04],\n",
      "        ...,\n",
      "        [1.1164e-03, 1.2514e-04, 4.7787e-04,  ..., 2.0440e-04, 1.3243e-04,\n",
      "         2.3137e-04],\n",
      "        [1.2837e-03, 1.3470e-04, 1.1975e-03,  ..., 1.9980e-04, 1.3298e-04,\n",
      "         8.4421e-05],\n",
      "        [1.8382e-04, 3.1847e-04, 3.1107e-04,  ..., 2.2356e-04, 4.2216e-05,\n",
      "         2.4176e-05]], device='cuda:0')\n",
      "pred: tensor([0.8400, 0.8359, 0.7995, 0.8235, 0.7859, 0.8221, 0.8339, 0.8058, 0.7782,\n",
      "        0.8121, 0.8039, 0.8343, 0.8025, 0.8271, 0.8432, 0.8109, 0.8272, 0.8277,\n",
      "        0.7869, 0.7954, 0.8192, 0.8334, 0.8605, 0.8623, 0.8755, 0.8006, 0.8719,\n",
      "        0.8123, 0.8440, 0.8555, 0.8495, 0.8790, 0.8593, 0.8729, 0.8552, 0.8675,\n",
      "        0.8371, 0.8508, 0.8371, 0.8271, 0.8414, 0.8691, 0.7966, 0.8577, 0.8473,\n",
      "        0.8058, 0.8506, 0.8313, 0.8534, 0.8441, 0.7480, 0.7785, 0.7618, 0.7810,\n",
      "        0.7784, 0.7718, 0.8368, 0.8222, 0.7352, 0.8449, 0.8001, 0.8078, 0.7701,\n",
      "        0.8074, 0.8457, 0.8425, 0.8001, 0.7664, 0.8172, 0.8323, 0.8263, 0.7905,\n",
      "        0.8241, 0.8055, 0.7973, 0.8733, 0.8596, 0.8516, 0.8634, 0.8528, 0.8242,\n",
      "        0.8258, 0.8641, 0.8420, 0.8642, 0.8145, 0.7735, 0.8491, 0.8001, 0.8393,\n",
      "        0.8442, 0.8452, 0.8028, 0.8318, 0.8665, 0.8456, 0.7960, 0.8681, 0.8553,\n",
      "        0.8352], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0057, Val-Loss: 0.0432\n",
      "Epoch [20/700], Train-Loss: 0.0053, Val-Loss: 0.0132\n",
      "Epoch [30/700], Train-Loss: 0.0023, Val-Loss: 0.0004\n",
      "Epoch [40/700], Train-Loss: 0.0019, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0012, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.6503e-04, 1.7394e-04, 1.2782e-04,  ..., 9.9030e-05, 1.0430e-04,\n",
      "         1.3288e-04],\n",
      "        [5.0691e-05, 7.3233e-05, 7.6121e-05,  ..., 3.1796e-04, 1.5639e-04,\n",
      "         2.3935e-04],\n",
      "        [2.1450e-04, 1.6569e-04, 2.9832e-04,  ..., 5.6489e-04, 2.8519e-04,\n",
      "         1.1958e-04],\n",
      "        ...,\n",
      "        [1.3033e-04, 8.6619e-05, 1.3992e-04,  ..., 8.6855e-05, 1.4813e-04,\n",
      "         1.2909e-04],\n",
      "        [5.1009e-04, 3.1944e-04, 1.7932e-04,  ..., 1.2597e-04, 8.5092e-05,\n",
      "         8.5383e-05],\n",
      "        [1.1243e-04, 1.7998e-04, 7.4654e-05,  ..., 6.0471e-05, 4.9496e-05,\n",
      "         1.0358e-04]], device='cuda:0')\n",
      "pred: tensor([0.8376, 0.8233, 0.7991, 0.8125, 0.7797, 0.8172, 0.7894, 0.8126, 0.8016,\n",
      "        0.8328, 0.7969, 0.8397, 0.7901, 0.8230, 0.8353, 0.8044, 0.8287, 0.8271,\n",
      "        0.7801, 0.7885, 0.8166, 0.8181, 0.8593, 0.8640, 0.8766, 0.8093, 0.8691,\n",
      "        0.8210, 0.8512, 0.8497, 0.8515, 0.8715, 0.8697, 0.8712, 0.8610, 0.8657,\n",
      "        0.8418, 0.8446, 0.8418, 0.8343, 0.8429, 0.8722, 0.8062, 0.8683, 0.8348,\n",
      "        0.7996, 0.8372, 0.8329, 0.8383, 0.8297, 0.7489, 0.7843, 0.7664, 0.7713,\n",
      "        0.7889, 0.7832, 0.8322, 0.8154, 0.7532, 0.8414, 0.8092, 0.7995, 0.7924,\n",
      "        0.8002, 0.8398, 0.8488, 0.7971, 0.7728, 0.8230, 0.8353, 0.8276, 0.7942,\n",
      "        0.8237, 0.7979, 0.8029, 0.8523, 0.8547, 0.8396, 0.8677, 0.8544, 0.8355,\n",
      "        0.8308, 0.8590, 0.8404, 0.8611, 0.8201, 0.7743, 0.8336, 0.7840, 0.8627,\n",
      "        0.8436, 0.8549, 0.7969, 0.8492, 0.8699, 0.8560, 0.8087, 0.8698, 0.8517,\n",
      "        0.8433], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0135, Val-Loss: 0.0655\n",
      "Epoch [20/700], Train-Loss: 0.0124, Val-Loss: 0.0218\n",
      "Epoch [30/700], Train-Loss: 0.0004, Val-Loss: 0.0009\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.8231e-05, 7.1420e-05, 7.1940e-05,  ..., 1.7648e-04, 2.2423e-04,\n",
      "         9.1994e-05],\n",
      "        [1.4003e-04, 2.8352e-04, 1.5737e-04,  ..., 4.1336e-04, 9.6759e-05,\n",
      "         2.4309e-04],\n",
      "        [2.2256e-04, 4.1891e-04, 9.0112e-05,  ..., 8.1374e-05, 2.0557e-04,\n",
      "         8.9048e-05],\n",
      "        ...,\n",
      "        [7.4927e-05, 1.0954e-04, 9.4801e-05,  ..., 7.3870e-05, 9.3545e-05,\n",
      "         1.3240e-04],\n",
      "        [1.6763e-04, 3.2430e-05, 2.4151e-05,  ..., 1.1890e-04, 5.6128e-05,\n",
      "         3.5899e-05],\n",
      "        [2.0712e-04, 2.4940e-04, 7.4592e-05,  ..., 2.5780e-04, 1.1113e-04,\n",
      "         5.0965e-05]], device='cuda:0')\n",
      "pred: tensor([0.8365, 0.8367, 0.7967, 0.8185, 0.7882, 0.8225, 0.8179, 0.8010, 0.7928,\n",
      "        0.8215, 0.7854, 0.8170, 0.7792, 0.8098, 0.8367, 0.8038, 0.8280, 0.8134,\n",
      "        0.8006, 0.8078, 0.8214, 0.8431, 0.8710, 0.8554, 0.8831, 0.8111, 0.8718,\n",
      "        0.8145, 0.8508, 0.8524, 0.8507, 0.8695, 0.8713, 0.8770, 0.8579, 0.8758,\n",
      "        0.8425, 0.8473, 0.8398, 0.8257, 0.8493, 0.8728, 0.7974, 0.8741, 0.8321,\n",
      "        0.8123, 0.8410, 0.8239, 0.8410, 0.8386, 0.7639, 0.7977, 0.7604, 0.7786,\n",
      "        0.7883, 0.7772, 0.8213, 0.8210, 0.7533, 0.8406, 0.7982, 0.8113, 0.7808,\n",
      "        0.8007, 0.8379, 0.8512, 0.7893, 0.7826, 0.8131, 0.8275, 0.8329, 0.8004,\n",
      "        0.8180, 0.7949, 0.7943, 0.8684, 0.8575, 0.8470, 0.8708, 0.8649, 0.8335,\n",
      "        0.8312, 0.8617, 0.8331, 0.8591, 0.8222, 0.7771, 0.8365, 0.7970, 0.8453,\n",
      "        0.8372, 0.8419, 0.8100, 0.8291, 0.8617, 0.8483, 0.8132, 0.8569, 0.8518,\n",
      "        0.8346], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0227, Val-Loss: 0.0637\n",
      "Epoch [20/700], Train-Loss: 0.0142, Val-Loss: 0.0207\n",
      "Epoch [30/700], Train-Loss: 0.0005, Val-Loss: 0.0017\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.4153e-05, 1.7360e-04, 1.3698e-04,  ..., 4.5804e-05, 5.6544e-05,\n",
      "         7.3702e-05],\n",
      "        [8.4322e-05, 6.3730e-05, 1.1590e-04,  ..., 1.6059e-04, 2.0210e-04,\n",
      "         3.1084e-04],\n",
      "        [2.9818e-04, 4.2011e-04, 5.7544e-04,  ..., 6.2186e-05, 1.6065e-04,\n",
      "         3.2225e-04],\n",
      "        ...,\n",
      "        [9.9704e-05, 5.9999e-05, 6.8018e-05,  ..., 1.1336e-04, 8.2193e-05,\n",
      "         6.3940e-04],\n",
      "        [5.0978e-05, 8.9929e-05, 1.0343e-04,  ..., 6.8191e-05, 6.0568e-05,\n",
      "         6.6473e-04],\n",
      "        [2.0356e-05, 1.9523e-05, 2.0504e-05,  ..., 8.9101e-05, 5.1390e-05,\n",
      "         3.2191e-04]], device='cuda:0')\n",
      "pred: tensor([0.8452, 0.8242, 0.8035, 0.8282, 0.7896, 0.8177, 0.8106, 0.8109, 0.7971,\n",
      "        0.8140, 0.7945, 0.8385, 0.7998, 0.8099, 0.8389, 0.8032, 0.8348, 0.8204,\n",
      "        0.7865, 0.7803, 0.8243, 0.8403, 0.8666, 0.8459, 0.8758, 0.8168, 0.8737,\n",
      "        0.8210, 0.8434, 0.8424, 0.8497, 0.8672, 0.8612, 0.8765, 0.8514, 0.8663,\n",
      "        0.8405, 0.8571, 0.8427, 0.8352, 0.8418, 0.8744, 0.8061, 0.8732, 0.8321,\n",
      "        0.7972, 0.8361, 0.8256, 0.8251, 0.8272, 0.7865, 0.7904, 0.7635, 0.7785,\n",
      "        0.7915, 0.7812, 0.8400, 0.8214, 0.7619, 0.8504, 0.7927, 0.8020, 0.7718,\n",
      "        0.8056, 0.8284, 0.8432, 0.8063, 0.7845, 0.8215, 0.8305, 0.8416, 0.7925,\n",
      "        0.8319, 0.7981, 0.8020, 0.8716, 0.8503, 0.8427, 0.8543, 0.8559, 0.8308,\n",
      "        0.8328, 0.8614, 0.8429, 0.8642, 0.8198, 0.7887, 0.8354, 0.7951, 0.8349,\n",
      "        0.8311, 0.8318, 0.7966, 0.8278, 0.8564, 0.8442, 0.8047, 0.8576, 0.8390,\n",
      "        0.8388], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0041, Val-Loss: 0.0415\n",
      "Epoch [20/700], Train-Loss: 0.0054, Val-Loss: 0.0137\n",
      "Epoch [30/700], Train-Loss: 0.0028, Val-Loss: 0.0007\n",
      "Epoch [40/700], Train-Loss: 0.0024, Val-Loss: 0.0008\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.4656e-05, 6.2822e-05, 1.0034e-04,  ..., 6.2011e-05, 1.5300e-04,\n",
      "         1.0766e-04],\n",
      "        [1.9385e-04, 1.2257e-04, 1.6918e-04,  ..., 1.2165e-04, 1.3263e-04,\n",
      "         6.9148e-05],\n",
      "        [4.7036e-04, 1.9260e-04, 3.2510e-04,  ..., 2.0080e-04, 2.1824e-04,\n",
      "         1.3186e-04],\n",
      "        ...,\n",
      "        [3.9947e-05, 6.4942e-05, 5.2825e-05,  ..., 3.7673e-05, 1.8502e-04,\n",
      "         4.7977e-05],\n",
      "        [1.8657e-04, 6.0602e-05, 7.8507e-05,  ..., 6.0304e-05, 1.7088e-04,\n",
      "         7.8994e-05],\n",
      "        [4.1408e-04, 5.5877e-05, 4.3419e-05,  ..., 1.8002e-04, 8.0578e-05,\n",
      "         1.3554e-04]], device='cuda:0')\n",
      "pred: tensor([0.8435, 0.8228, 0.7929, 0.8193, 0.7878, 0.8360, 0.8081, 0.8118, 0.7936,\n",
      "        0.8157, 0.7991, 0.8249, 0.8122, 0.8214, 0.8381, 0.8108, 0.8281, 0.8201,\n",
      "        0.7922, 0.7940, 0.8226, 0.8263, 0.8755, 0.8417, 0.8844, 0.8045, 0.8678,\n",
      "        0.8263, 0.8339, 0.8365, 0.8455, 0.8590, 0.8674, 0.8802, 0.8569, 0.8695,\n",
      "        0.8267, 0.8452, 0.8293, 0.8241, 0.8350, 0.8683, 0.7980, 0.8704, 0.8244,\n",
      "        0.8066, 0.8430, 0.8314, 0.8316, 0.8520, 0.7611, 0.8020, 0.7659, 0.7880,\n",
      "        0.7924, 0.7854, 0.8301, 0.8167, 0.7517, 0.8408, 0.7953, 0.7758, 0.7795,\n",
      "        0.8047, 0.8243, 0.8527, 0.7965, 0.7794, 0.8145, 0.8258, 0.8274, 0.7906,\n",
      "        0.8198, 0.7953, 0.7886, 0.8623, 0.8465, 0.8484, 0.8546, 0.8580, 0.8252,\n",
      "        0.8272, 0.8612, 0.8400, 0.8613, 0.8252, 0.7758, 0.8387, 0.7913, 0.8429,\n",
      "        0.8267, 0.8411, 0.8102, 0.8272, 0.8556, 0.8447, 0.7997, 0.8508, 0.8407,\n",
      "        0.8405], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9523552355235522, 0.9535913591359134, 0.9645484548454845, 0.969228922892289, 0.9573117311731172]}\n",
      "Epoch [10/700], Train-Loss: 0.0731, Val-Loss: 0.1222\n",
      "Epoch [20/700], Train-Loss: 0.0264, Val-Loss: 0.0327\n",
      "Epoch [30/700], Train-Loss: 0.0040, Val-Loss: 0.0071\n",
      "Epoch [40/700], Train-Loss: 0.0015, Val-Loss: 0.0027\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[3.2455e-04, 5.0459e-04, 5.7825e-04,  ..., 8.2110e-05, 9.3913e-05,\n",
      "         6.9339e-05],\n",
      "        [1.6946e-04, 2.2503e-04, 2.2894e-04,  ..., 2.2692e-04, 2.5963e-04,\n",
      "         3.1371e-04],\n",
      "        [8.3518e-04, 3.7263e-03, 5.6904e-04,  ..., 3.2702e-04, 6.8350e-04,\n",
      "         1.0793e-03],\n",
      "        ...,\n",
      "        [4.6477e-05, 1.3298e-04, 2.9789e-05,  ..., 7.7512e-05, 1.4456e-04,\n",
      "         2.6682e-04],\n",
      "        [3.8394e-04, 2.3616e-04, 8.5967e-04,  ..., 7.7161e-05, 2.6905e-04,\n",
      "         1.5587e-04],\n",
      "        [2.1865e-04, 1.1311e-04, 2.5764e-04,  ..., 8.0334e-05, 7.9682e-05,\n",
      "         1.0663e-04]], device='cuda:0')\n",
      "pred: tensor([0.8438, 0.8334, 0.7955, 0.8179, 0.7889, 0.8330, 0.8184, 0.8132, 0.7898,\n",
      "        0.8186, 0.8025, 0.8292, 0.7959, 0.8143, 0.8357, 0.8085, 0.8277, 0.8196,\n",
      "        0.7814, 0.7874, 0.8323, 0.8335, 0.8652, 0.8511, 0.8800, 0.8067, 0.8662,\n",
      "        0.8221, 0.8445, 0.8499, 0.8535, 0.8802, 0.8582, 0.8640, 0.8613, 0.8768,\n",
      "        0.8267, 0.8490, 0.8436, 0.8348, 0.8450, 0.8683, 0.8004, 0.8671, 0.8340,\n",
      "        0.8102, 0.8384, 0.8287, 0.8360, 0.8412, 0.7661, 0.7957, 0.7697, 0.7717,\n",
      "        0.7916, 0.7843, 0.8253, 0.8303, 0.7576, 0.8434, 0.8025, 0.8072, 0.7835,\n",
      "        0.8088, 0.8324, 0.8457, 0.7944, 0.7750, 0.8198, 0.8253, 0.8306, 0.7955,\n",
      "        0.8285, 0.7952, 0.7939, 0.8651, 0.8515, 0.8539, 0.8665, 0.8535, 0.8297,\n",
      "        0.8275, 0.8562, 0.8457, 0.8613, 0.8222, 0.7842, 0.8374, 0.7915, 0.8409,\n",
      "        0.8364, 0.8457, 0.8086, 0.8307, 0.8563, 0.8430, 0.8024, 0.8621, 0.8493,\n",
      "        0.8420], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1317, Val-Loss: 0.1206\n",
      "Epoch [20/700], Train-Loss: 0.0393, Val-Loss: 0.0376\n",
      "Epoch [30/700], Train-Loss: 0.0124, Val-Loss: 0.0122\n",
      "Epoch [40/700], Train-Loss: 0.0047, Val-Loss: 0.0049\n",
      "Epoch [50/700], Train-Loss: 0.0019, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.4115e-05, 9.2896e-05, 1.1009e-04,  ..., 7.4614e-05, 1.0666e-04,\n",
      "         7.8924e-05],\n",
      "        [1.0850e-04, 1.1232e-04, 1.1430e-04,  ..., 9.0246e-05, 1.6135e-04,\n",
      "         1.8323e-04],\n",
      "        [1.1593e-04, 7.2147e-05, 6.3914e-05,  ..., 1.6718e-04, 7.7960e-05,\n",
      "         1.3696e-04],\n",
      "        ...,\n",
      "        [9.7871e-05, 8.1088e-05, 1.1014e-04,  ..., 1.7856e-04, 1.4433e-04,\n",
      "         6.2752e-05],\n",
      "        [5.6357e-05, 7.0343e-05, 6.3981e-05,  ..., 1.8313e-04, 1.6819e-04,\n",
      "         1.2848e-04],\n",
      "        [4.8275e-05, 5.6209e-05, 2.1707e-05,  ..., 1.3102e-04, 1.7858e-04,\n",
      "         2.6807e-04]], device='cuda:0')\n",
      "pred: tensor([0.8443, 0.8186, 0.7952, 0.8148, 0.7856, 0.8314, 0.8194, 0.8175, 0.7863,\n",
      "        0.8238, 0.7997, 0.8293, 0.8002, 0.8173, 0.8264, 0.8055, 0.8256, 0.8200,\n",
      "        0.7883, 0.7919, 0.8182, 0.8415, 0.8695, 0.8514, 0.8808, 0.8069, 0.8695,\n",
      "        0.8157, 0.8457, 0.8451, 0.8522, 0.8700, 0.8611, 0.8639, 0.8599, 0.8751,\n",
      "        0.8281, 0.8428, 0.8433, 0.8218, 0.8438, 0.8686, 0.7997, 0.8657, 0.8205,\n",
      "        0.7970, 0.8412, 0.8223, 0.8270, 0.8393, 0.7725, 0.7898, 0.7593, 0.7800,\n",
      "        0.7853, 0.7751, 0.8347, 0.8240, 0.7605, 0.8441, 0.8043, 0.8050, 0.7820,\n",
      "        0.8088, 0.8298, 0.8413, 0.7977, 0.7758, 0.8137, 0.8348, 0.8335, 0.8005,\n",
      "        0.8333, 0.7983, 0.7853, 0.8522, 0.8536, 0.8408, 0.8592, 0.8618, 0.8294,\n",
      "        0.8334, 0.8594, 0.8428, 0.8634, 0.8156, 0.7761, 0.8364, 0.7943, 0.8395,\n",
      "        0.8357, 0.8369, 0.8099, 0.8355, 0.8666, 0.8435, 0.7981, 0.8559, 0.8548,\n",
      "        0.8347], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0862, Val-Loss: 0.0515\n",
      "Epoch [20/700], Train-Loss: 0.0214, Val-Loss: 0.0147\n",
      "Epoch [30/700], Train-Loss: 0.0068, Val-Loss: 0.0037\n",
      "Epoch [40/700], Train-Loss: 0.0022, Val-Loss: 0.0012\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0166e-04, 5.7020e-05, 1.2341e-04,  ..., 1.5991e-05, 3.8311e-04,\n",
      "         1.9558e-05],\n",
      "        [1.6823e-04, 8.9839e-05, 8.2658e-05,  ..., 1.4023e-04, 8.4414e-04,\n",
      "         2.5117e-04],\n",
      "        [4.0984e-04, 5.2364e-04, 1.9776e-04,  ..., 3.0450e-04, 1.3516e-02,\n",
      "         6.2531e-04],\n",
      "        ...,\n",
      "        [1.0369e-04, 1.1207e-05, 2.4769e-04,  ..., 1.1165e-04, 7.5878e-04,\n",
      "         3.6547e-04],\n",
      "        [4.7021e-05, 2.2597e-06, 9.1251e-06,  ..., 9.2729e-05, 9.8811e-04,\n",
      "         4.1873e-05],\n",
      "        [7.8944e-05, 1.5180e-05, 3.7836e-05,  ..., 1.4476e-04, 4.0401e-04,\n",
      "         8.4496e-05]], device='cuda:0')\n",
      "pred: tensor([0.8419, 0.8271, 0.7949, 0.8196, 0.7947, 0.8260, 0.8191, 0.8158, 0.7862,\n",
      "        0.8268, 0.7997, 0.8295, 0.7919, 0.8210, 0.8330, 0.8174, 0.8365, 0.8219,\n",
      "        0.7937, 0.7949, 0.8225, 0.8359, 0.8651, 0.8485, 0.8873, 0.8079, 0.8678,\n",
      "        0.8196, 0.8413, 0.8501, 0.8511, 0.8739, 0.8615, 0.8677, 0.8539, 0.8817,\n",
      "        0.8284, 0.8464, 0.8370, 0.8313, 0.8361, 0.8666, 0.7947, 0.8632, 0.8221,\n",
      "        0.8014, 0.8464, 0.8299, 0.8281, 0.8400, 0.7677, 0.7934, 0.7681, 0.7805,\n",
      "        0.7770, 0.7801, 0.8270, 0.8238, 0.7543, 0.8434, 0.8001, 0.7997, 0.7853,\n",
      "        0.8031, 0.8320, 0.8512, 0.8026, 0.7771, 0.8090, 0.8337, 0.8301, 0.8033,\n",
      "        0.8231, 0.7980, 0.7933, 0.8626, 0.8597, 0.8486, 0.8650, 0.8664, 0.8249,\n",
      "        0.8307, 0.8562, 0.8418, 0.8634, 0.8171, 0.7755, 0.8415, 0.7842, 0.8362,\n",
      "        0.8389, 0.8347, 0.7964, 0.8273, 0.8651, 0.8474, 0.8090, 0.8603, 0.8543,\n",
      "        0.8416], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1065, Val-Loss: 0.0824\n",
      "Epoch [20/700], Train-Loss: 0.0298, Val-Loss: 0.0252\n",
      "Epoch [30/700], Train-Loss: 0.0106, Val-Loss: 0.0087\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0031\n",
      "Epoch [50/700], Train-Loss: 0.0012, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.4906e-05, 1.9658e-04, 2.6414e-04,  ..., 1.9290e-04, 1.3302e-04,\n",
      "         1.5161e-04],\n",
      "        [1.1607e-04, 1.1881e-04, 1.0072e-04,  ..., 1.1713e-04, 2.2294e-04,\n",
      "         8.1363e-05],\n",
      "        [1.5850e-04, 7.1848e-04, 1.3772e-04,  ..., 2.6244e-04, 2.3066e-04,\n",
      "         1.6219e-04],\n",
      "        ...,\n",
      "        [1.1558e-04, 1.5355e-04, 2.9291e-04,  ..., 1.6467e-04, 1.5093e-04,\n",
      "         1.0318e-04],\n",
      "        [1.5485e-04, 8.2832e-05, 1.1353e-04,  ..., 9.8228e-05, 1.1724e-04,\n",
      "         7.4527e-05],\n",
      "        [1.6572e-05, 3.6278e-05, 6.3105e-05,  ..., 8.0897e-05, 1.7848e-04,\n",
      "         1.1381e-04]], device='cuda:0')\n",
      "pred: tensor([0.8440, 0.8339, 0.7986, 0.8145, 0.7806, 0.8283, 0.8049, 0.8158, 0.7938,\n",
      "        0.8221, 0.7933, 0.8237, 0.7950, 0.8241, 0.8365, 0.8022, 0.8242, 0.8245,\n",
      "        0.7933, 0.7809, 0.8258, 0.8295, 0.8678, 0.8493, 0.8840, 0.8056, 0.8692,\n",
      "        0.8266, 0.8464, 0.8456, 0.8475, 0.8669, 0.8643, 0.8641, 0.8505, 0.8797,\n",
      "        0.8270, 0.8498, 0.8424, 0.8285, 0.8388, 0.8631, 0.7989, 0.8650, 0.8250,\n",
      "        0.8015, 0.8409, 0.8293, 0.8297, 0.8328, 0.7655, 0.7900, 0.7641, 0.7782,\n",
      "        0.7846, 0.7840, 0.8360, 0.8258, 0.7574, 0.8430, 0.8025, 0.8043, 0.7850,\n",
      "        0.8061, 0.8362, 0.8453, 0.7939, 0.7751, 0.8138, 0.8314, 0.8323, 0.7979,\n",
      "        0.8355, 0.8034, 0.7902, 0.8591, 0.8618, 0.8416, 0.8618, 0.8584, 0.8290,\n",
      "        0.8331, 0.8564, 0.8452, 0.8606, 0.8176, 0.7737, 0.8383, 0.7880, 0.8376,\n",
      "        0.8342, 0.8438, 0.8023, 0.8295, 0.8668, 0.8467, 0.8102, 0.8618, 0.8454,\n",
      "        0.8467], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1151, Val-Loss: 0.0887\n",
      "Epoch [20/700], Train-Loss: 0.0304, Val-Loss: 0.0252\n",
      "Epoch [30/700], Train-Loss: 0.0103, Val-Loss: 0.0080\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0032\n",
      "Epoch [50/700], Train-Loss: 0.0014, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.2136e-05, 1.9555e-04, 7.2988e-05,  ..., 5.5939e-05, 1.2571e-04,\n",
      "         7.3659e-05],\n",
      "        [1.0396e-04, 8.9972e-05, 1.3675e-04,  ..., 3.2392e-04, 1.7860e-04,\n",
      "         1.1802e-04],\n",
      "        [9.3446e-05, 1.6652e-04, 1.5648e-04,  ..., 7.9856e-05, 1.3017e-04,\n",
      "         1.0582e-04],\n",
      "        ...,\n",
      "        [9.3469e-05, 1.4249e-04, 1.6830e-04,  ..., 1.1711e-04, 1.3699e-04,\n",
      "         1.2103e-04],\n",
      "        [6.3252e-05, 4.0416e-05, 5.3577e-05,  ..., 1.3349e-04, 1.2286e-04,\n",
      "         7.7035e-05],\n",
      "        [2.0855e-04, 3.4665e-05, 1.2377e-04,  ..., 1.0001e-04, 2.1197e-04,\n",
      "         1.2825e-04]], device='cuda:0')\n",
      "pred: tensor([0.8402, 0.8291, 0.7952, 0.8198, 0.7786, 0.8190, 0.8164, 0.8146, 0.7947,\n",
      "        0.8287, 0.7965, 0.8322, 0.7968, 0.8151, 0.8418, 0.8127, 0.8263, 0.8288,\n",
      "        0.7916, 0.7960, 0.8197, 0.8291, 0.8772, 0.8512, 0.8850, 0.8121, 0.8684,\n",
      "        0.8218, 0.8429, 0.8411, 0.8471, 0.8709, 0.8617, 0.8678, 0.8569, 0.8824,\n",
      "        0.8208, 0.8518, 0.8381, 0.8229, 0.8327, 0.8620, 0.7999, 0.8741, 0.8237,\n",
      "        0.8058, 0.8487, 0.8298, 0.8315, 0.8307, 0.7662, 0.7900, 0.7581, 0.7714,\n",
      "        0.7818, 0.7743, 0.8360, 0.8298, 0.7519, 0.8388, 0.8002, 0.8053, 0.7866,\n",
      "        0.8118, 0.8307, 0.8467, 0.7908, 0.7666, 0.8139, 0.8292, 0.8273, 0.7934,\n",
      "        0.8262, 0.8012, 0.7898, 0.8543, 0.8529, 0.8484, 0.8545, 0.8553, 0.8302,\n",
      "        0.8338, 0.8584, 0.8422, 0.8563, 0.8248, 0.7775, 0.8420, 0.7947, 0.8396,\n",
      "        0.8347, 0.8359, 0.8043, 0.8326, 0.8609, 0.8425, 0.8082, 0.8571, 0.8398,\n",
      "        0.8404], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.967968796879688, 0.98012601260126, 0.9794539453945392, 0.9786138613861385, 0.9762856285628562]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_average_model_performance_over_sample_size(model_confidences_tensor_dict={'Random/kNN (Baseline)': timm_model_confidence_for_val_samples_tensor,\n",
    "                                                                               'Random/Linear (Baseline)': timm_model_confidence_for_val_samples_tensor,\n",
    "                                                                               'High PDS/kNN (DISCO)': timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                                                               'High PDS/Linear (DISCO)': timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                                                               'Synth PDS/kNN (SynthDISCO)': timm_model_confidence_for_main_set_tensor,\n",
    "                                                                               'Synth PDS/Linear (SynthDISCO)': timm_model_confidence_for_main_set_tensor\n",
    "                                                                               },\n",
    "                                                                               # 'cluster_representatives_l1': timm_model_confidence_for_val_samples_tensor[l1_dist_top_indices],\n",
    "                                                                               # 'cluster_representatives_l2': timm_model_confidence_for_val_samples_tensor[l2_dist_top_indices]},\n",
    "                                                model_catalog=timm_model_catalog,\n",
    "                                                model_classes_for_evaluation={'Random/kNN (Baseline)': NNModel,\n",
    "                                                                               'Random/Linear (Baseline)': MLPRegressor,\n",
    "                                                                               'High PDS/kNN (DISCO)': NNModel,\n",
    "                                                                               'High PDS/Linear (DISCO)': MLPRegressor,\n",
    "                                                                               'Synth PDS/kNN (SynthDISCO)': NNModel,\n",
    "                                                                               'Synth PDS/Linear (SynthDISCO)': MLPRegressor\n",
    "                                                                               },\n",
    "                                                                              # 'cluster_representatives_l1': SimpleNN,\n",
    "                                                                              # 'cluster_representatives_l2': SimpleNN,},\n",
    "                                                plot_for_datasets=['imagenet1k'],\n",
    "                                                sample_sizes=[10,20,50,100],\n",
    "                                                score_function='mae',\n",
    "                                                sampling_strategy={'Random/kNN (Baseline)': 'random',\n",
    "                                                                   'Random/Linear (Baseline)': 'random',\n",
    "                                                                   'High PDS/kNN (DISCO)': 'first',\n",
    "                                                                   'High PDS/Linear (DISCO)': 'first',\n",
    "                                                                   'Synth PDS/kNN (SynthDISCO)': 'random',\n",
    "                                                                   'Synth PDS/Linear (SynthDISCO)': 'random'\n",
    "                                                                   },\n",
    "                                                number_bootstraping_steps=5,\n",
    "                                                k_fold_splits=4,\n",
    "                                                x_log_scale=False,\n",
    "                                                model_init_kwargs={\n",
    "                                                    'hidden_channels': [128, 128, 1],\n",
    "                                                },\n",
    "                                                model_fitting_kwargs={\n",
    "                                                    'n_epochs': 700,\n",
    "                                                    'lr': 0.001,\n",
    "                                                },\n",
    "                                                verbose=False,\n",
    "                                                add_baseline_for_sizes=[10,20,50,100],\n",
    "                                                label_for_baseline='Random/Eval (Baseline)',\n",
    "                                                gt_accuracies=timm_result_performances,\n",
    "                                                train_model_index=timm_train_model_indices,\n",
    "                                                val_model_index=timm_val_model_indices,\n",
    "                                                save_path='/mnt/lustre/work/oh/owl813/repos/model-selection/figures/timm_complete_eval.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_rank_score_dict: {'imagenet1k': [0.7649399671090216, 0.765051135098807, 0.6456943045082633, 0.5691958791802197, 0.7410181790365121]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.7828214500106115, 0.7007854190854819, 0.7814532332136964, 0.7561914336492003, 0.7472664017340102]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8324705059049633, 0.8058390608669308, 0.7479120098668679, 0.8408674075495614, 0.8314060169590645]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8635856296402336, 0.848630645005486, 0.8844975173228559, 0.7808353561657634, 0.8598807733542632]}\n",
      "Epoch [10/700], Train-Loss: 0.2338, Val-Loss: 0.1923\n",
      "Epoch [20/700], Train-Loss: 0.0404, Val-Loss: 0.0520\n",
      "Epoch [30/700], Train-Loss: 0.0015, Val-Loss: 0.0040\n",
      "Epoch [40/700], Train-Loss: 0.0063, Val-Loss: 0.0041\n",
      "Epoch [50/700], Train-Loss: 0.0036, Val-Loss: 0.0032\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.0940e-05, 7.2051e-05, 6.2818e-05,  ..., 1.1751e-04, 1.1647e-04,\n",
      "         9.4335e-05],\n",
      "        [6.2094e-04, 3.9836e-04, 2.7123e-04,  ..., 1.8338e-04, 2.3006e-04,\n",
      "         1.0934e-04],\n",
      "        [1.7213e-04, 1.6966e-04, 2.5338e-04,  ..., 4.3827e-04, 6.6746e-05,\n",
      "         2.2078e-04],\n",
      "        ...,\n",
      "        [4.1671e-04, 2.0376e-04, 1.0404e-04,  ..., 2.4872e-04, 2.6600e-04,\n",
      "         1.8631e-04],\n",
      "        [7.6934e-05, 8.9025e-05, 9.2637e-05,  ..., 1.4455e-04, 7.4679e-05,\n",
      "         1.2009e-04],\n",
      "        [2.1247e-04, 6.1904e-05, 8.7224e-05,  ..., 7.0558e-05, 4.8127e-05,\n",
      "         6.7611e-05]], device='cuda:0')\n",
      "pred: tensor([0.8374, 0.8102, 0.7981, 0.8048, 0.7705, 0.8363, 0.8417, 0.8176, 0.7522,\n",
      "        0.8385, 0.8020, 0.8388, 0.7882, 0.7913, 0.8376, 0.8217, 0.8360, 0.8279,\n",
      "        0.8099, 0.8117, 0.8164, 0.8519, 0.8652, 0.8515, 0.8654, 0.8102, 0.8636,\n",
      "        0.8204, 0.8500, 0.8459, 0.8557, 0.8538, 0.8633, 0.8790, 0.8526, 0.8599,\n",
      "        0.8433, 0.8420, 0.8513, 0.8352, 0.8545, 0.8625, 0.7948, 0.8701, 0.8324,\n",
      "        0.8185, 0.8492, 0.8283, 0.8294, 0.8492, 0.7450, 0.7727, 0.7470, 0.7951,\n",
      "        0.8003, 0.7610, 0.8305, 0.8439, 0.7500, 0.8404, 0.8009, 0.8135, 0.7794,\n",
      "        0.7989, 0.8284, 0.8363, 0.8232, 0.7931, 0.8070, 0.8346, 0.8405, 0.8150,\n",
      "        0.8239, 0.8262, 0.8049, 0.8535, 0.8400, 0.8537, 0.8537, 0.8577, 0.8412,\n",
      "        0.8443, 0.8572, 0.8492, 0.8588, 0.8260, 0.7898, 0.8507, 0.7841, 0.8490,\n",
      "        0.8591, 0.8451, 0.7938, 0.8432, 0.8605, 0.8535, 0.7996, 0.8583, 0.8465,\n",
      "        0.8471], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2771, Val-Loss: 0.2212\n",
      "Epoch [20/700], Train-Loss: 0.0633, Val-Loss: 0.0726\n",
      "Epoch [30/700], Train-Loss: 0.0031, Val-Loss: 0.0069\n",
      "Epoch [40/700], Train-Loss: 0.0071, Val-Loss: 0.0048\n",
      "Epoch [50/700], Train-Loss: 0.0046, Val-Loss: 0.0037\n",
      "Epoch [60/700], Train-Loss: 0.0013, Val-Loss: 0.0016\n",
      "Epoch [70/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [90/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [100/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [110/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [120/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [150/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [160/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [170/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [180/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[8.2586e-05, 9.0078e-05, 6.9373e-05,  ..., 6.0138e-05, 3.9678e-05,\n",
      "         1.4693e-04],\n",
      "        [1.0828e-04, 1.1336e-04, 9.0803e-05,  ..., 2.4360e-04, 1.8397e-04,\n",
      "         2.8794e-04],\n",
      "        [2.6262e-04, 1.2469e-04, 2.5898e-04,  ..., 2.0657e-04, 6.1170e-05,\n",
      "         6.7284e-04],\n",
      "        ...,\n",
      "        [1.3982e-04, 1.4515e-04, 1.1271e-04,  ..., 9.1591e-05, 1.2900e-04,\n",
      "         8.4461e-04],\n",
      "        [4.6070e-05, 7.6145e-05, 4.7604e-05,  ..., 2.4739e-05, 2.1782e-05,\n",
      "         1.6121e-05],\n",
      "        [4.9032e-05, 7.1536e-05, 8.5047e-05,  ..., 9.8014e-05, 4.3451e-05,\n",
      "         5.9794e-05]], device='cuda:0')\n",
      "pred: tensor([0.8476, 0.8157, 0.8105, 0.8403, 0.8000, 0.8250, 0.8178, 0.7937, 0.8009,\n",
      "        0.8106, 0.8014, 0.8370, 0.8176, 0.8195, 0.8239, 0.8117, 0.8357, 0.8199,\n",
      "        0.8006, 0.8078, 0.8420, 0.8445, 0.8606, 0.8405, 0.8675, 0.8035, 0.8596,\n",
      "        0.8237, 0.8363, 0.8183, 0.8289, 0.8636, 0.8619, 0.8523, 0.8359, 0.8542,\n",
      "        0.8249, 0.8287, 0.8194, 0.8127, 0.8226, 0.8605, 0.8084, 0.8615, 0.8299,\n",
      "        0.8030, 0.8275, 0.8299, 0.8281, 0.8313, 0.7585, 0.8050, 0.7382, 0.7887,\n",
      "        0.7927, 0.7748, 0.8227, 0.8314, 0.7564, 0.8535, 0.8212, 0.7966, 0.8036,\n",
      "        0.7993, 0.8286, 0.8338, 0.8048, 0.8092, 0.7818, 0.8296, 0.8229, 0.8228,\n",
      "        0.8191, 0.8027, 0.8035, 0.8529, 0.8335, 0.8389, 0.8471, 0.8541, 0.8337,\n",
      "        0.8389, 0.8578, 0.8424, 0.8643, 0.8276, 0.7673, 0.8284, 0.7849, 0.8434,\n",
      "        0.8474, 0.8312, 0.8122, 0.8195, 0.8554, 0.8385, 0.7925, 0.8430, 0.8235,\n",
      "        0.8509], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2583, Val-Loss: 0.2170\n",
      "Epoch [20/700], Train-Loss: 0.0292, Val-Loss: 0.0451\n",
      "Epoch [30/700], Train-Loss: 0.0010, Val-Loss: 0.0021\n",
      "Epoch [40/700], Train-Loss: 0.0081, Val-Loss: 0.0062\n",
      "Epoch [50/700], Train-Loss: 0.0034, Val-Loss: 0.0032\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[1.1490e-04, 9.5455e-05, 1.1687e-04,  ..., 1.5819e-04, 1.1397e-04,\n",
      "         2.5131e-04],\n",
      "        [8.3066e-05, 1.5364e-04, 1.0899e-04,  ..., 2.3256e-04, 1.2670e-04,\n",
      "         1.8236e-04],\n",
      "        [6.6172e-05, 9.4149e-05, 2.0804e-04,  ..., 1.1692e-04, 8.5512e-05,\n",
      "         2.3220e-04],\n",
      "        ...,\n",
      "        [9.6704e-05, 1.0712e-04, 2.2010e-04,  ..., 9.7425e-05, 7.1732e-05,\n",
      "         1.8442e-04],\n",
      "        [8.7529e-05, 1.2185e-04, 5.3227e-05,  ..., 6.0528e-05, 4.6838e-05,\n",
      "         9.5984e-05],\n",
      "        [8.3206e-05, 8.2654e-05, 1.3023e-04,  ..., 2.3818e-04, 9.3022e-05,\n",
      "         2.0238e-04]], device='cuda:0')\n",
      "pred: tensor([0.8338, 0.8161, 0.8047, 0.7938, 0.7869, 0.8298, 0.8135, 0.8425, 0.8115,\n",
      "        0.8428, 0.7918, 0.8293, 0.8072, 0.8240, 0.8442, 0.8082, 0.8167, 0.8057,\n",
      "        0.7943, 0.8074, 0.8187, 0.8412, 0.8614, 0.8560, 0.8939, 0.8249, 0.8595,\n",
      "        0.8206, 0.8541, 0.8310, 0.8459, 0.8941, 0.9067, 0.8498, 0.8560, 0.8675,\n",
      "        0.8420, 0.8474, 0.8336, 0.8292, 0.8298, 0.8572, 0.7826, 0.8703, 0.8317,\n",
      "        0.8143, 0.8314, 0.8288, 0.8302, 0.8336, 0.7632, 0.7973, 0.7759, 0.7993,\n",
      "        0.8015, 0.7856, 0.8295, 0.8172, 0.7701, 0.8335, 0.7866, 0.8140, 0.7741,\n",
      "        0.8156, 0.8330, 0.8275, 0.7981, 0.7574, 0.8182, 0.8368, 0.8283, 0.7970,\n",
      "        0.8424, 0.8080, 0.7975, 0.8682, 0.8411, 0.8305, 0.8496, 0.8659, 0.8336,\n",
      "        0.8407, 0.8511, 0.8358, 0.8598, 0.8385, 0.7927, 0.8332, 0.7893, 0.8389,\n",
      "        0.8360, 0.8337, 0.8029, 0.8267, 0.8541, 0.8366, 0.8132, 0.8412, 0.8649,\n",
      "        0.8566], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3690, Val-Loss: 0.3116\n",
      "Epoch [20/700], Train-Loss: 0.0386, Val-Loss: 0.0620\n",
      "Epoch [30/700], Train-Loss: 0.0010, Val-Loss: 0.0028\n",
      "Epoch [40/700], Train-Loss: 0.0116, Val-Loss: 0.0089\n",
      "Epoch [50/700], Train-Loss: 0.0048, Val-Loss: 0.0043\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0014\n",
      "Epoch [70/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [100/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [140/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [150/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [160/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [170/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [180/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [190/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [200/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [210/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [320/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [330/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [340/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [350/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [360/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [370/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [380/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [390/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [400/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [410/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [420/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [430/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [440/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [450/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [460/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [470/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [480/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [490/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [500/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [510/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [520/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [530/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [540/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [550/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [560/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [570/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [580/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [590/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [600/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [610/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [620/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [630/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [640/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [650/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [660/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [670/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [680/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [690/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "x: tensor([[9.5964e-05, 1.1753e-04, 1.2330e-04,  ..., 6.0469e-05, 9.1703e-05,\n",
      "         6.2372e-05],\n",
      "        [1.5376e-04, 1.1441e-04, 2.0850e-04,  ..., 2.8632e-04, 3.8741e-04,\n",
      "         1.7322e-04],\n",
      "        [1.5615e-04, 1.4093e-04, 1.9925e-04,  ..., 1.0816e-04, 2.0097e-04,\n",
      "         2.2467e-04],\n",
      "        ...,\n",
      "        [1.4705e-04, 1.5668e-04, 1.7668e-04,  ..., 5.9927e-05, 1.0108e-04,\n",
      "         1.4084e-04],\n",
      "        [1.1863e-04, 1.5490e-04, 6.1461e-05,  ..., 1.4756e-04, 7.2181e-05,\n",
      "         6.1884e-05],\n",
      "        [1.0581e-04, 1.3216e-04, 1.5422e-04,  ..., 2.2388e-04, 2.5469e-04,\n",
      "         1.7717e-04]], device='cuda:0')\n",
      "pred: tensor([0.8338, 0.8227, 0.8059, 0.8064, 0.7820, 0.8205, 0.8205, 0.8129, 0.7973,\n",
      "        0.8295, 0.7804, 0.8272, 0.8038, 0.8303, 0.8396, 0.7936, 0.8224, 0.8187,\n",
      "        0.7710, 0.7726, 0.8146, 0.8360, 0.8426, 0.8517, 0.8809, 0.7748, 0.8499,\n",
      "        0.8204, 0.8454, 0.8573, 0.8594, 0.8400, 0.8473, 0.8447, 0.8585, 0.8565,\n",
      "        0.8056, 0.8474, 0.8215, 0.7966, 0.8650, 0.8816, 0.7871, 0.8434, 0.7677,\n",
      "        0.7634, 0.8471, 0.8267, 0.8144, 0.8228, 0.7684, 0.7613, 0.7648, 0.7984,\n",
      "        0.7658, 0.7712, 0.8319, 0.8201, 0.7578, 0.8239, 0.7869, 0.8026, 0.7557,\n",
      "        0.8123, 0.8354, 0.8511, 0.7703, 0.7813, 0.8082, 0.8318, 0.8409, 0.8034,\n",
      "        0.8075, 0.8209, 0.7935, 0.8442, 0.8517, 0.7579, 0.8904, 0.8810, 0.8439,\n",
      "        0.8367, 0.8535, 0.8178, 0.8397, 0.8199, 0.8042, 0.8419, 0.8061, 0.8332,\n",
      "        0.8191, 0.8498, 0.8067, 0.8206, 0.8494, 0.8355, 0.8151, 0.8426, 0.8415,\n",
      "        0.8212], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2900, Val-Loss: 0.2392\n",
      "Epoch [20/700], Train-Loss: 0.0449, Val-Loss: 0.0615\n",
      "Epoch [30/700], Train-Loss: 0.0014, Val-Loss: 0.0040\n",
      "Epoch [40/700], Train-Loss: 0.0087, Val-Loss: 0.0062\n",
      "Epoch [50/700], Train-Loss: 0.0043, Val-Loss: 0.0039\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0012\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[1.2710e-04, 1.5576e-04, 1.7449e-04,  ..., 8.7302e-05, 6.3843e-05,\n",
      "         2.2970e-04],\n",
      "        [1.9342e-04, 3.2598e-04, 3.3948e-04,  ..., 7.8128e-05, 8.1230e-05,\n",
      "         1.5020e-04],\n",
      "        [2.5699e-04, 3.5907e-04, 6.3550e-04,  ..., 1.6902e-04, 2.9784e-04,\n",
      "         8.2997e-04],\n",
      "        ...,\n",
      "        [1.6585e-04, 9.1493e-05, 1.5518e-04,  ..., 1.1907e-04, 7.5887e-05,\n",
      "         1.2945e-04],\n",
      "        [4.8910e-04, 3.0609e-04, 3.7271e-04,  ..., 9.7170e-05, 6.1558e-05,\n",
      "         2.4156e-04],\n",
      "        [1.0480e-04, 9.8626e-05, 1.7270e-04,  ..., 3.9603e-05, 4.6069e-05,\n",
      "         1.8428e-04]], device='cuda:0')\n",
      "pred: tensor([0.8240, 0.8130, 0.7828, 0.8084, 0.7882, 0.8501, 0.8110, 0.7806, 0.7893,\n",
      "        0.8373, 0.7955, 0.8349, 0.8107, 0.8098, 0.8504, 0.8250, 0.8407, 0.8447,\n",
      "        0.8050, 0.8147, 0.8139, 0.8341, 0.8596, 0.8501, 0.8792, 0.7915, 0.8688,\n",
      "        0.7936, 0.8523, 0.8406, 0.8537, 0.8775, 0.8719, 0.8552, 0.8563, 0.8566,\n",
      "        0.8180, 0.8353, 0.8297, 0.7907, 0.8337, 0.8644, 0.7927, 0.8618, 0.8165,\n",
      "        0.8026, 0.8441, 0.8311, 0.8109, 0.8398, 0.7617, 0.7944, 0.7653, 0.7887,\n",
      "        0.7739, 0.7775, 0.8218, 0.8266, 0.7595, 0.8347, 0.7865, 0.8087, 0.7987,\n",
      "        0.8034, 0.8261, 0.8350, 0.7800, 0.7654, 0.8101, 0.8194, 0.8229, 0.8043,\n",
      "        0.8233, 0.8111, 0.7936, 0.8311, 0.8459, 0.8443, 0.8551, 0.8554, 0.8266,\n",
      "        0.8450, 0.8546, 0.8280, 0.8519, 0.7956, 0.7819, 0.8520, 0.8016, 0.8413,\n",
      "        0.8258, 0.8288, 0.7981, 0.8071, 0.8492, 0.8495, 0.8026, 0.8759, 0.8519,\n",
      "        0.8304], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9345094509450944, 0.8834563456345633, 0.8969456945694567, 0.8686348634863486, 0.9112511251125112]}\n",
      "Epoch [10/700], Train-Loss: 0.0768, Val-Loss: 0.0381\n",
      "Epoch [20/700], Train-Loss: 0.0310, Val-Loss: 0.0158\n",
      "Epoch [30/700], Train-Loss: 0.0154, Val-Loss: 0.0149\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0032\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[9.1474e-05, 1.2156e-04, 3.4614e-04,  ..., 9.4443e-05, 6.1316e-05,\n",
      "         1.1657e-04],\n",
      "        [1.7301e-04, 1.8490e-04, 8.3471e-04,  ..., 2.1552e-04, 1.7380e-04,\n",
      "         1.5238e-04],\n",
      "        [3.3603e-04, 5.0464e-04, 5.0998e-04,  ..., 1.1269e-04, 1.7634e-04,\n",
      "         1.7275e-04],\n",
      "        ...,\n",
      "        [8.4277e-05, 9.6390e-05, 1.4771e-04,  ..., 1.4854e-04, 1.0501e-04,\n",
      "         1.0519e-04],\n",
      "        [6.6093e-05, 7.9015e-05, 7.9581e-05,  ..., 5.7186e-05, 7.1842e-05,\n",
      "         5.9225e-05],\n",
      "        [1.2077e-04, 1.4711e-04, 2.4451e-04,  ..., 1.4812e-04, 1.3089e-04,\n",
      "         1.6867e-04]], device='cuda:0')\n",
      "pred: tensor([0.8403, 0.8129, 0.7985, 0.8151, 0.7863, 0.8270, 0.8044, 0.8224, 0.7901,\n",
      "        0.8099, 0.8023, 0.8197, 0.7931, 0.8226, 0.8174, 0.8101, 0.8108, 0.8353,\n",
      "        0.8007, 0.7927, 0.8310, 0.8434, 0.8504, 0.8528, 0.8827, 0.8122, 0.8767,\n",
      "        0.8033, 0.8523, 0.8446, 0.8493, 0.8616, 0.8639, 0.8547, 0.8485, 0.8568,\n",
      "        0.8164, 0.8476, 0.8157, 0.8153, 0.8488, 0.8526, 0.8039, 0.8867, 0.8030,\n",
      "        0.7834, 0.8486, 0.8305, 0.8136, 0.8293, 0.7501, 0.7949, 0.7725, 0.7354,\n",
      "        0.7885, 0.7849, 0.8233, 0.8174, 0.7420, 0.8227, 0.8102, 0.8087, 0.7835,\n",
      "        0.8058, 0.8383, 0.8239, 0.8093, 0.7558, 0.7973, 0.8389, 0.8187, 0.8039,\n",
      "        0.8071, 0.8045, 0.8242, 0.8756, 0.8438, 0.8497, 0.8590, 0.8254, 0.8439,\n",
      "        0.8368, 0.8568, 0.8197, 0.8503, 0.8408, 0.7289, 0.8469, 0.7792, 0.8470,\n",
      "        0.8440, 0.8428, 0.8143, 0.8432, 0.8386, 0.8477, 0.8021, 0.8436, 0.8352,\n",
      "        0.8291], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1576, Val-Loss: 0.1105\n",
      "Epoch [20/700], Train-Loss: 0.0585, Val-Loss: 0.0421\n",
      "Epoch [30/700], Train-Loss: 0.0123, Val-Loss: 0.0144\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0009\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[1.1330e-04, 5.2230e-05, 1.9428e-04,  ..., 1.7474e-04, 1.5078e-04,\n",
      "         3.6606e-04],\n",
      "        [1.7629e-04, 2.3753e-04, 1.1928e-04,  ..., 2.2326e-04, 1.4855e-04,\n",
      "         7.0332e-04],\n",
      "        [2.0789e-04, 3.4347e-04, 4.7057e-04,  ..., 1.7374e-04, 9.5892e-04,\n",
      "         1.0233e-03],\n",
      "        ...,\n",
      "        [1.5248e-04, 1.5617e-04, 1.2953e-04,  ..., 1.5010e-04, 2.1440e-04,\n",
      "         6.0154e-05],\n",
      "        [2.2951e-04, 9.9130e-05, 1.3478e-04,  ..., 4.7820e-04, 1.3658e-04,\n",
      "         4.2647e-04],\n",
      "        [4.6951e-05, 6.9760e-05, 1.3756e-04,  ..., 1.9858e-05, 9.5601e-05,\n",
      "         1.8193e-03]], device='cuda:0')\n",
      "pred: tensor([0.8291, 0.8245, 0.8041, 0.8101, 0.7985, 0.8145, 0.8182, 0.7944, 0.7784,\n",
      "        0.8156, 0.8060, 0.8418, 0.8097, 0.8143, 0.8515, 0.8056, 0.8341, 0.8434,\n",
      "        0.7800, 0.7886, 0.8284, 0.8410, 0.8523, 0.8448, 0.8637, 0.8055, 0.8770,\n",
      "        0.8139, 0.8361, 0.8281, 0.8561, 0.8484, 0.8732, 0.8576, 0.8370, 0.8510,\n",
      "        0.8299, 0.8338, 0.7893, 0.8280, 0.8364, 0.8760, 0.7922, 0.8695, 0.8109,\n",
      "        0.8058, 0.8447, 0.8089, 0.8177, 0.8210, 0.7516, 0.7864, 0.7467, 0.7773,\n",
      "        0.7965, 0.7746, 0.8291, 0.8405, 0.7578, 0.8278, 0.7988, 0.8194, 0.8027,\n",
      "        0.8192, 0.8314, 0.8308, 0.7930, 0.7692, 0.8039, 0.8329, 0.8159, 0.7965,\n",
      "        0.8114, 0.7962, 0.8030, 0.8729, 0.8389, 0.8151, 0.8398, 0.8399, 0.8292,\n",
      "        0.8502, 0.8667, 0.8277, 0.8719, 0.8299, 0.7598, 0.8412, 0.7961, 0.8355,\n",
      "        0.8367, 0.8335, 0.8075, 0.8215, 0.8473, 0.8452, 0.8187, 0.8553, 0.8567,\n",
      "        0.8590], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0427, Val-Loss: 0.0108\n",
      "Epoch [20/700], Train-Loss: 0.0113, Val-Loss: 0.0026\n",
      "Epoch [30/700], Train-Loss: 0.0147, Val-Loss: 0.0116\n",
      "Epoch [40/700], Train-Loss: 0.0058, Val-Loss: 0.0051\n",
      "Epoch [50/700], Train-Loss: 0.0018, Val-Loss: 0.0022\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "x: tensor([[9.2607e-05, 1.3769e-04, 1.3266e-04,  ..., 1.2178e-04, 4.7293e-05,\n",
      "         8.6924e-05],\n",
      "        [8.0528e-05, 9.7781e-05, 1.0985e-04,  ..., 1.4976e-04, 1.6750e-04,\n",
      "         2.3071e-04],\n",
      "        [4.7324e-04, 4.5610e-04, 4.5001e-04,  ..., 2.2285e-04, 1.6418e-04,\n",
      "         1.9910e-04],\n",
      "        ...,\n",
      "        [1.0192e-04, 9.9984e-05, 8.7383e-05,  ..., 1.4631e-04, 6.2881e-05,\n",
      "         1.8169e-04],\n",
      "        [3.7970e-05, 8.0967e-05, 1.3087e-04,  ..., 7.3868e-05, 3.7749e-05,\n",
      "         8.2293e-05],\n",
      "        [8.9516e-05, 1.3285e-04, 6.8521e-05,  ..., 7.6762e-05, 6.1945e-05,\n",
      "         5.1946e-05]], device='cuda:0')\n",
      "pred: tensor([0.8333, 0.8410, 0.8076, 0.8052, 0.7669, 0.8363, 0.8139, 0.8248, 0.8002,\n",
      "        0.8387, 0.8015, 0.8378, 0.8069, 0.8032, 0.8345, 0.8087, 0.8432, 0.8371,\n",
      "        0.7876, 0.8070, 0.8278, 0.8232, 0.8585, 0.8509, 0.8686, 0.8106, 0.8676,\n",
      "        0.8290, 0.8478, 0.8377, 0.8441, 0.8529, 0.8677, 0.8531, 0.8453, 0.8651,\n",
      "        0.8375, 0.8398, 0.8266, 0.8287, 0.8383, 0.8543, 0.8014, 0.8747, 0.8241,\n",
      "        0.8151, 0.8436, 0.8305, 0.8344, 0.8346, 0.7531, 0.8035, 0.7475, 0.7733,\n",
      "        0.7891, 0.7814, 0.8265, 0.8352, 0.7637, 0.8335, 0.7925, 0.8167, 0.7922,\n",
      "        0.8061, 0.8218, 0.8311, 0.8014, 0.7657, 0.8161, 0.8130, 0.8276, 0.7987,\n",
      "        0.8273, 0.7893, 0.7844, 0.8635, 0.8547, 0.8317, 0.8336, 0.8504, 0.8278,\n",
      "        0.8437, 0.8611, 0.8363, 0.8591, 0.8213, 0.7879, 0.8423, 0.8020, 0.8334,\n",
      "        0.8324, 0.8401, 0.7792, 0.8351, 0.8562, 0.8459, 0.7717, 0.8558, 0.8471,\n",
      "        0.8375], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1170, Val-Loss: 0.0618\n",
      "Epoch [20/700], Train-Loss: 0.0422, Val-Loss: 0.0215\n",
      "Epoch [30/700], Train-Loss: 0.0200, Val-Loss: 0.0195\n",
      "Epoch [40/700], Train-Loss: 0.0030, Val-Loss: 0.0039\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.9194e-05, 3.1128e-04, 1.7451e-04,  ..., 1.3077e-05, 5.6333e-05,\n",
      "         3.3013e-04],\n",
      "        [4.5408e-05, 5.6721e-05, 7.5731e-05,  ..., 1.5031e-04, 2.5716e-04,\n",
      "         6.7078e-04],\n",
      "        [1.5889e-04, 2.3193e-04, 2.4600e-04,  ..., 1.9279e-04, 5.7802e-05,\n",
      "         1.8040e-03],\n",
      "        ...,\n",
      "        [5.0264e-05, 1.2630e-04, 4.5654e-04,  ..., 2.6214e-04, 1.2032e-04,\n",
      "         2.6949e-04],\n",
      "        [5.2495e-05, 1.3695e-05, 6.1471e-05,  ..., 1.3643e-04, 8.4855e-05,\n",
      "         5.6197e-04],\n",
      "        [4.3639e-05, 8.6547e-05, 8.1732e-05,  ..., 9.0848e-05, 1.1528e-04,\n",
      "         5.1489e-04]], device='cuda:0')\n",
      "pred: tensor([0.8164, 0.8194, 0.8006, 0.8157, 0.7815, 0.8267, 0.8079, 0.8380, 0.7864,\n",
      "        0.8304, 0.8001, 0.8238, 0.8013, 0.8145, 0.8241, 0.8046, 0.8340, 0.8344,\n",
      "        0.8022, 0.7992, 0.8116, 0.8424, 0.8552, 0.8485, 0.8691, 0.8182, 0.8776,\n",
      "        0.8036, 0.8522, 0.8361, 0.8459, 0.8564, 0.8693, 0.8599, 0.8482, 0.8640,\n",
      "        0.8312, 0.8413, 0.8252, 0.8249, 0.8223, 0.8703, 0.8042, 0.8758, 0.8220,\n",
      "        0.8147, 0.8487, 0.8408, 0.8270, 0.8269, 0.7705, 0.7928, 0.7461, 0.7801,\n",
      "        0.7888, 0.7699, 0.8369, 0.8456, 0.7640, 0.8377, 0.7917, 0.8263, 0.8064,\n",
      "        0.8079, 0.8259, 0.8519, 0.7799, 0.7692, 0.8083, 0.8294, 0.8116, 0.7900,\n",
      "        0.8294, 0.8082, 0.7932, 0.8734, 0.8434, 0.8395, 0.8442, 0.8425, 0.8204,\n",
      "        0.8443, 0.8569, 0.8338, 0.8582, 0.8076, 0.7679, 0.8384, 0.7632, 0.8425,\n",
      "        0.8329, 0.8460, 0.8094, 0.8232, 0.8643, 0.8486, 0.8084, 0.8602, 0.8593,\n",
      "        0.8422], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1448, Val-Loss: 0.0956\n",
      "Epoch [20/700], Train-Loss: 0.0621, Val-Loss: 0.0457\n",
      "Epoch [30/700], Train-Loss: 0.0141, Val-Loss: 0.0168\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0007\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0013, Val-Loss: 0.0011\n",
      "Epoch [70/700], Train-Loss: 0.0007, Val-Loss: 0.0006\n",
      "Epoch [80/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "x: tensor([[1.5291e-04, 8.5451e-05, 1.0294e-04,  ..., 9.3249e-05, 1.1603e-04,\n",
      "         5.2201e-05],\n",
      "        [1.7261e-04, 1.6877e-04, 1.0368e-04,  ..., 1.1125e-04, 1.7620e-04,\n",
      "         1.2981e-04],\n",
      "        [4.5635e-04, 2.2719e-04, 1.6841e-04,  ..., 1.5694e-04, 2.0325e-04,\n",
      "         1.6954e-04],\n",
      "        ...,\n",
      "        [3.0989e-04, 1.8444e-04, 1.5478e-04,  ..., 5.2981e-05, 1.4569e-04,\n",
      "         6.3402e-05],\n",
      "        [1.7867e-04, 1.6775e-04, 1.5042e-04,  ..., 2.1488e-04, 1.0511e-04,\n",
      "         1.3655e-05],\n",
      "        [1.7338e-04, 8.5239e-05, 9.5245e-05,  ..., 6.8101e-05, 1.6773e-04,\n",
      "         2.5949e-05]], device='cuda:0')\n",
      "pred: tensor([0.8458, 0.8321, 0.8096, 0.8068, 0.8028, 0.8231, 0.8247, 0.8425, 0.8118,\n",
      "        0.8260, 0.7980, 0.8326, 0.7847, 0.8043, 0.8346, 0.8204, 0.8199, 0.8253,\n",
      "        0.8027, 0.8011, 0.7928, 0.8360, 0.8534, 0.8522, 0.8570, 0.7839, 0.8626,\n",
      "        0.8191, 0.8421, 0.8512, 0.8344, 0.8737, 0.8568, 0.8564, 0.8526, 0.8416,\n",
      "        0.8262, 0.8418, 0.8223, 0.8203, 0.8387, 0.8592, 0.7934, 0.8684, 0.8292,\n",
      "        0.8260, 0.8412, 0.8132, 0.8415, 0.8361, 0.7905, 0.7855, 0.7628, 0.7523,\n",
      "        0.7646, 0.7708, 0.8495, 0.8338, 0.7522, 0.8519, 0.8076, 0.7953, 0.7852,\n",
      "        0.8137, 0.8291, 0.8369, 0.7943, 0.7747, 0.8025, 0.8405, 0.8368, 0.8212,\n",
      "        0.8177, 0.7988, 0.8063, 0.8512, 0.8503, 0.8521, 0.8468, 0.8500, 0.8367,\n",
      "        0.8412, 0.8490, 0.8341, 0.8542, 0.8069, 0.7634, 0.8401, 0.7812, 0.8361,\n",
      "        0.8359, 0.8340, 0.7882, 0.8441, 0.8515, 0.8375, 0.7988, 0.8551, 0.8457,\n",
      "        0.8419], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9155595559555956, 0.885100510051005, 0.9145874587458744, 0.9229402940294028, 0.918031803180318]}\n",
      "Epoch [10/700], Train-Loss: 0.0101, Val-Loss: 0.0023\n",
      "Epoch [20/700], Train-Loss: 0.0009, Val-Loss: 0.0031\n",
      "Epoch [30/700], Train-Loss: 0.0078, Val-Loss: 0.0040\n",
      "Epoch [40/700], Train-Loss: 0.0049, Val-Loss: 0.0029\n",
      "Epoch [50/700], Train-Loss: 0.0023, Val-Loss: 0.0019\n",
      "Epoch [60/700], Train-Loss: 0.0009, Val-Loss: 0.0008\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.3975e-05, 1.2411e-04, 5.9904e-05,  ..., 1.2242e-04, 1.6674e-04,\n",
      "         1.7151e-03],\n",
      "        [1.6023e-04, 2.6519e-04, 2.4940e-04,  ..., 5.1527e-05, 4.7582e-05,\n",
      "         1.1962e-04],\n",
      "        [9.3648e-05, 5.2454e-05, 2.2711e-05,  ..., 2.9444e-04, 1.1658e-04,\n",
      "         2.3083e-04],\n",
      "        ...,\n",
      "        [1.2818e-04, 1.1838e-04, 1.3858e-04,  ..., 7.4422e-05, 8.3468e-05,\n",
      "         2.2264e-04],\n",
      "        [5.8635e-05, 9.9387e-05, 8.3884e-05,  ..., 5.1887e-05, 3.5453e-05,\n",
      "         9.6652e-05],\n",
      "        [1.4398e-04, 7.6999e-05, 1.4213e-04,  ..., 3.2960e-05, 1.1133e-04,\n",
      "         2.5707e-04]], device='cuda:0')\n",
      "pred: tensor([0.8317, 0.8347, 0.8042, 0.8187, 0.7803, 0.8384, 0.8136, 0.8125, 0.7937,\n",
      "        0.8406, 0.8037, 0.8200, 0.7951, 0.8223, 0.8368, 0.8140, 0.8235, 0.8186,\n",
      "        0.7893, 0.8049, 0.8309, 0.8392, 0.8660, 0.8605, 0.8823, 0.8102, 0.8682,\n",
      "        0.8245, 0.8548, 0.8472, 0.8536, 0.8841, 0.8642, 0.8734, 0.8568, 0.8711,\n",
      "        0.8389, 0.8629, 0.8353, 0.8342, 0.8370, 0.8745, 0.7894, 0.8706, 0.8397,\n",
      "        0.7915, 0.8511, 0.8352, 0.8397, 0.8385, 0.7552, 0.7838, 0.7621, 0.7716,\n",
      "        0.7805, 0.7646, 0.8400, 0.8318, 0.7559, 0.8434, 0.8041, 0.8050, 0.7966,\n",
      "        0.8147, 0.8336, 0.8427, 0.7995, 0.7812, 0.8105, 0.8337, 0.8320, 0.8025,\n",
      "        0.8288, 0.8084, 0.8059, 0.8612, 0.8547, 0.8514, 0.8598, 0.8571, 0.8280,\n",
      "        0.8369, 0.8678, 0.8460, 0.8681, 0.8311, 0.7573, 0.8467, 0.7993, 0.8265,\n",
      "        0.8374, 0.8393, 0.8022, 0.8318, 0.8628, 0.8409, 0.8033, 0.8616, 0.8545,\n",
      "        0.8441], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0014, Val-Loss: 0.0164\n",
      "Epoch [20/700], Train-Loss: 0.0015, Val-Loss: 0.0063\n",
      "Epoch [30/700], Train-Loss: 0.0051, Val-Loss: 0.0021\n",
      "Epoch [40/700], Train-Loss: 0.0037, Val-Loss: 0.0017\n",
      "Epoch [50/700], Train-Loss: 0.0020, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0009, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.0131e-04, 8.1756e-05, 1.2140e-04,  ..., 1.8806e-04, 1.4841e-04,\n",
      "         9.5280e-05],\n",
      "        [3.1359e-04, 1.2448e-04, 8.6154e-05,  ..., 7.8994e-05, 4.0597e-05,\n",
      "         8.3211e-05],\n",
      "        [2.8087e-04, 3.6627e-04, 2.3709e-04,  ..., 5.1282e-04, 4.4608e-04,\n",
      "         1.5763e-04],\n",
      "        ...,\n",
      "        [1.0366e-04, 5.6782e-05, 1.1152e-04,  ..., 1.5803e-04, 1.3377e-04,\n",
      "         1.0553e-04],\n",
      "        [1.0981e-04, 8.3368e-05, 8.1349e-05,  ..., 6.2130e-05, 1.4126e-04,\n",
      "         6.8717e-05],\n",
      "        [2.1011e-04, 1.3142e-04, 8.6875e-05,  ..., 1.3282e-04, 7.9702e-05,\n",
      "         5.4484e-05]], device='cuda:0')\n",
      "pred: tensor([0.8412, 0.8278, 0.8112, 0.8263, 0.7899, 0.8299, 0.8163, 0.8293, 0.7998,\n",
      "        0.8243, 0.7932, 0.8325, 0.8059, 0.8150, 0.8395, 0.8063, 0.8277, 0.8209,\n",
      "        0.7789, 0.8021, 0.8282, 0.8347, 0.8726, 0.8462, 0.8847, 0.8078, 0.8700,\n",
      "        0.8311, 0.8600, 0.8380, 0.8582, 0.8608, 0.8687, 0.8789, 0.8494, 0.8769,\n",
      "        0.8364, 0.8504, 0.8378, 0.8419, 0.8410, 0.8697, 0.7981, 0.8777, 0.8402,\n",
      "        0.8086, 0.8395, 0.8282, 0.8390, 0.8471, 0.7551, 0.7962, 0.7610, 0.7870,\n",
      "        0.7731, 0.7661, 0.8340, 0.8304, 0.7483, 0.8466, 0.8232, 0.8135, 0.7685,\n",
      "        0.8117, 0.8435, 0.8495, 0.8089, 0.7761, 0.8015, 0.8358, 0.8331, 0.8121,\n",
      "        0.8350, 0.8070, 0.8206, 0.8621, 0.8589, 0.8395, 0.8635, 0.8553, 0.8290,\n",
      "        0.8403, 0.8634, 0.8479, 0.8657, 0.8206, 0.7805, 0.8431, 0.7953, 0.8411,\n",
      "        0.8447, 0.8388, 0.8117, 0.8327, 0.8553, 0.8414, 0.8004, 0.8493, 0.8518,\n",
      "        0.8209], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0282, Val-Loss: 0.0012\n",
      "Epoch [20/700], Train-Loss: 0.0034, Val-Loss: 0.0009\n",
      "Epoch [30/700], Train-Loss: 0.0136, Val-Loss: 0.0084\n",
      "Epoch [40/700], Train-Loss: 0.0073, Val-Loss: 0.0050\n",
      "Epoch [50/700], Train-Loss: 0.0028, Val-Loss: 0.0028\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0008\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[5.8739e-05, 1.5482e-04, 1.2006e-04,  ..., 1.7522e-04, 1.5220e-04,\n",
      "         1.7419e-04],\n",
      "        [1.2577e-04, 2.2341e-04, 8.3399e-05,  ..., 1.3173e-04, 3.8631e-04,\n",
      "         2.2460e-04],\n",
      "        [2.5024e-04, 1.8485e-04, 2.1175e-04,  ..., 4.7363e-04, 6.1889e-04,\n",
      "         1.2797e-04],\n",
      "        ...,\n",
      "        [9.9926e-05, 2.7640e-04, 1.8257e-04,  ..., 2.1090e-04, 2.5826e-04,\n",
      "         2.0064e-04],\n",
      "        [4.1604e-05, 8.1992e-05, 7.7159e-05,  ..., 1.4149e-04, 8.9294e-05,\n",
      "         1.4387e-04],\n",
      "        [4.2256e-05, 5.6719e-05, 8.8316e-05,  ..., 8.1848e-05, 1.6905e-04,\n",
      "         1.9047e-04]], device='cuda:0')\n",
      "pred: tensor([0.8289, 0.8273, 0.7968, 0.8100, 0.7802, 0.8448, 0.8234, 0.8239, 0.7971,\n",
      "        0.8142, 0.7875, 0.8289, 0.7801, 0.8117, 0.8404, 0.8043, 0.8307, 0.8334,\n",
      "        0.7950, 0.7894, 0.8265, 0.8307, 0.8596, 0.8564, 0.8706, 0.8115, 0.8656,\n",
      "        0.8124, 0.8433, 0.8457, 0.8574, 0.8773, 0.8621, 0.8686, 0.8601, 0.8690,\n",
      "        0.8195, 0.8468, 0.8259, 0.8262, 0.8389, 0.8767, 0.8062, 0.8653, 0.8341,\n",
      "        0.7969, 0.8500, 0.8182, 0.8300, 0.8297, 0.7512, 0.7825, 0.7435, 0.7598,\n",
      "        0.7885, 0.7539, 0.8348, 0.8338, 0.7681, 0.8429, 0.7837, 0.8041, 0.7873,\n",
      "        0.7986, 0.8253, 0.8426, 0.7972, 0.7717, 0.8072, 0.8343, 0.8194, 0.7973,\n",
      "        0.8263, 0.8141, 0.7853, 0.8527, 0.8521, 0.8544, 0.8533, 0.8583, 0.8308,\n",
      "        0.8425, 0.8562, 0.8438, 0.8556, 0.8131, 0.7818, 0.8356, 0.7995, 0.8432,\n",
      "        0.8373, 0.8390, 0.8052, 0.8438, 0.8622, 0.8436, 0.7910, 0.8585, 0.8662,\n",
      "        0.8447], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0314, Val-Loss: 0.0746\n",
      "Epoch [20/700], Train-Loss: 0.0150, Val-Loss: 0.0236\n",
      "Epoch [30/700], Train-Loss: 0.0009, Val-Loss: 0.0023\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0009\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.0535e-04, 1.6345e-04, 1.8056e-04,  ..., 1.3117e-04, 8.9139e-05,\n",
      "         8.4007e-05],\n",
      "        [2.1691e-05, 4.4391e-05, 1.1179e-05,  ..., 1.0003e-04, 1.3411e-04,\n",
      "         7.7787e-05],\n",
      "        [1.0800e-04, 1.3993e-04, 1.0659e-04,  ..., 2.0837e-04, 1.4287e-04,\n",
      "         2.3087e-04],\n",
      "        ...,\n",
      "        [7.7417e-05, 1.3071e-04, 2.0697e-04,  ..., 1.2456e-04, 1.4466e-04,\n",
      "         9.8860e-05],\n",
      "        [5.2980e-05, 7.4316e-05, 1.3895e-04,  ..., 7.5828e-05, 9.8951e-05,\n",
      "         2.1621e-04],\n",
      "        [3.8981e-05, 1.7577e-04, 1.7674e-04,  ..., 1.5360e-04, 9.2304e-05,\n",
      "         1.2860e-04]], device='cuda:0')\n",
      "pred: tensor([0.8365, 0.8417, 0.8001, 0.8074, 0.7838, 0.8312, 0.8314, 0.8283, 0.7979,\n",
      "        0.8328, 0.8115, 0.8389, 0.7869, 0.8223, 0.8237, 0.7789, 0.8306, 0.8225,\n",
      "        0.7817, 0.7938, 0.8147, 0.8242, 0.8528, 0.8570, 0.8782, 0.7984, 0.8725,\n",
      "        0.8242, 0.8590, 0.8461, 0.8533, 0.8823, 0.8738, 0.8594, 0.8547, 0.8696,\n",
      "        0.8182, 0.8450, 0.8257, 0.8092, 0.8336, 0.8645, 0.8005, 0.8728, 0.8325,\n",
      "        0.7840, 0.8481, 0.8243, 0.8274, 0.8494, 0.7393, 0.7689, 0.7770, 0.7803,\n",
      "        0.7687, 0.7836, 0.8437, 0.8413, 0.7453, 0.8548, 0.7992, 0.7996, 0.7594,\n",
      "        0.8155, 0.8381, 0.8480, 0.8093, 0.8062, 0.8225, 0.8324, 0.8411, 0.8027,\n",
      "        0.8395, 0.8139, 0.8086, 0.8608, 0.8566, 0.8536, 0.8737, 0.8523, 0.8356,\n",
      "        0.8435, 0.8686, 0.8407, 0.8693, 0.8190, 0.7802, 0.8415, 0.8133, 0.8466,\n",
      "        0.8441, 0.8450, 0.8017, 0.8468, 0.8606, 0.8457, 0.8103, 0.8706, 0.8453,\n",
      "        0.8394], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0768, Val-Loss: 0.0215\n",
      "Epoch [20/700], Train-Loss: 0.0123, Val-Loss: 0.0019\n",
      "Epoch [30/700], Train-Loss: 0.0160, Val-Loss: 0.0112\n",
      "Epoch [40/700], Train-Loss: 0.0071, Val-Loss: 0.0051\n",
      "Epoch [50/700], Train-Loss: 0.0027, Val-Loss: 0.0026\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[6.5811e-05, 1.1541e-04, 7.0805e-05,  ..., 8.4126e-05, 9.9549e-05,\n",
      "         6.0236e-05],\n",
      "        [1.6352e-04, 1.3204e-04, 1.0876e-04,  ..., 1.6841e-04, 2.8459e-04,\n",
      "         2.8708e-04],\n",
      "        [3.3219e-04, 2.0282e-04, 3.3600e-04,  ..., 1.4467e-04, 2.8675e-04,\n",
      "         3.4494e-04],\n",
      "        ...,\n",
      "        [1.0549e-04, 1.2069e-04, 1.5069e-04,  ..., 1.8814e-04, 2.3889e-04,\n",
      "         1.2983e-04],\n",
      "        [5.0481e-05, 1.1827e-04, 5.9436e-05,  ..., 7.6153e-05, 1.3446e-04,\n",
      "         1.5064e-04],\n",
      "        [1.1447e-04, 1.9421e-04, 1.6874e-04,  ..., 1.0009e-04, 3.5721e-05,\n",
      "         5.7371e-05]], device='cuda:0')\n",
      "pred: tensor([0.8425, 0.8203, 0.8124, 0.8284, 0.7813, 0.8428, 0.8059, 0.8284, 0.7945,\n",
      "        0.8095, 0.8063, 0.8366, 0.7915, 0.8267, 0.8467, 0.8114, 0.8287, 0.8341,\n",
      "        0.7891, 0.7931, 0.8141, 0.8357, 0.8709, 0.8604, 0.8705, 0.8030, 0.8670,\n",
      "        0.8030, 0.8516, 0.8534, 0.8451, 0.8789, 0.8682, 0.8741, 0.8498, 0.8639,\n",
      "        0.8380, 0.8500, 0.8488, 0.8331, 0.8547, 0.8695, 0.7981, 0.8603, 0.8266,\n",
      "        0.8011, 0.8412, 0.8240, 0.8295, 0.8488, 0.7522, 0.7944, 0.7665, 0.7684,\n",
      "        0.7870, 0.7794, 0.8288, 0.8247, 0.7477, 0.8557, 0.8079, 0.8007, 0.7703,\n",
      "        0.7936, 0.8313, 0.8466, 0.8012, 0.7801, 0.8147, 0.8273, 0.8326, 0.8086,\n",
      "        0.8304, 0.8065, 0.8017, 0.8578, 0.8532, 0.8492, 0.8653, 0.8638, 0.8316,\n",
      "        0.8384, 0.8562, 0.8420, 0.8587, 0.8247, 0.7641, 0.8402, 0.7953, 0.8345,\n",
      "        0.8416, 0.8498, 0.8054, 0.8326, 0.8688, 0.8446, 0.8060, 0.8582, 0.8501,\n",
      "        0.8453], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9551155115511549, 0.9564476447644762, 0.9616681668166817, 0.9496789678967896, 0.9624242424242424]}\n",
      "Epoch [10/700], Train-Loss: 0.0896, Val-Loss: 0.0471\n",
      "Epoch [20/700], Train-Loss: 0.0238, Val-Loss: 0.0165\n",
      "Epoch [30/700], Train-Loss: 0.0083, Val-Loss: 0.0044\n",
      "Epoch [40/700], Train-Loss: 0.0028, Val-Loss: 0.0017\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.1544e-05, 1.0362e-04, 8.0644e-05,  ..., 6.6826e-05, 5.8676e-05,\n",
      "         7.3128e-05],\n",
      "        [3.0205e-04, 2.9678e-04, 4.3386e-04,  ..., 4.8492e-05, 5.0600e-05,\n",
      "         8.1403e-05],\n",
      "        [8.9177e-05, 1.3298e-04, 9.8451e-05,  ..., 3.1599e-05, 5.7294e-05,\n",
      "         2.6736e-03],\n",
      "        ...,\n",
      "        [9.6870e-05, 8.2513e-05, 1.5951e-04,  ..., 1.2887e-04, 5.5359e-05,\n",
      "         1.0841e-04],\n",
      "        [5.4832e-05, 2.0702e-04, 1.3314e-04,  ..., 2.8794e-05, 3.8224e-05,\n",
      "         8.0444e-05],\n",
      "        [1.2050e-04, 1.0172e-04, 2.3194e-04,  ..., 1.6926e-05, 2.7285e-05,\n",
      "         6.2472e-05]], device='cuda:0')\n",
      "pred: tensor([0.8354, 0.8344, 0.7947, 0.8168, 0.7835, 0.8228, 0.8134, 0.8189, 0.7935,\n",
      "        0.8271, 0.7811, 0.8323, 0.7941, 0.8194, 0.8348, 0.8118, 0.8259, 0.8252,\n",
      "        0.7815, 0.7944, 0.8227, 0.8323, 0.8614, 0.8518, 0.8829, 0.8019, 0.8693,\n",
      "        0.8280, 0.8458, 0.8510, 0.8575, 0.8688, 0.8711, 0.8540, 0.8573, 0.8668,\n",
      "        0.8304, 0.8508, 0.8355, 0.8312, 0.8437, 0.8739, 0.8026, 0.8658, 0.8270,\n",
      "        0.7957, 0.8470, 0.8374, 0.8268, 0.8441, 0.7470, 0.7846, 0.7518, 0.7780,\n",
      "        0.7878, 0.7792, 0.8313, 0.8329, 0.7551, 0.8400, 0.8031, 0.8045, 0.7835,\n",
      "        0.8031, 0.8434, 0.8437, 0.8027, 0.7867, 0.8088, 0.8324, 0.8336, 0.8039,\n",
      "        0.8309, 0.8155, 0.7984, 0.8631, 0.8481, 0.8500, 0.8615, 0.8528, 0.8303,\n",
      "        0.8340, 0.8649, 0.8384, 0.8647, 0.8119, 0.7687, 0.8362, 0.7934, 0.8388,\n",
      "        0.8350, 0.8291, 0.8012, 0.8294, 0.8607, 0.8422, 0.8023, 0.8545, 0.8486,\n",
      "        0.8402], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1061, Val-Loss: 0.0628\n",
      "Epoch [20/700], Train-Loss: 0.0267, Val-Loss: 0.0183\n",
      "Epoch [30/700], Train-Loss: 0.0084, Val-Loss: 0.0041\n",
      "Epoch [40/700], Train-Loss: 0.0017, Val-Loss: 0.0007\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.6624e-05, 2.2588e-04, 1.1675e-04,  ..., 8.8501e-05, 6.5149e-05,\n",
      "         1.4029e-04],\n",
      "        [2.7848e-04, 2.3742e-04, 2.8808e-04,  ..., 1.9533e-04, 1.9917e-04,\n",
      "         2.8788e-04],\n",
      "        [1.0839e-04, 8.0221e-04, 1.0380e-04,  ..., 3.8150e-04, 2.7832e-04,\n",
      "         3.8643e-04],\n",
      "        ...,\n",
      "        [5.7665e-05, 2.0333e-04, 9.6153e-05,  ..., 1.2851e-04, 8.8121e-05,\n",
      "         1.5391e-04],\n",
      "        [2.8466e-05, 1.9760e-04, 1.2022e-04,  ..., 9.0088e-05, 5.5184e-05,\n",
      "         3.3685e-04],\n",
      "        [2.5250e-05, 1.2303e-04, 9.4751e-05,  ..., 7.6084e-05, 4.5021e-05,\n",
      "         1.2489e-04]], device='cuda:0')\n",
      "pred: tensor([0.8400, 0.8205, 0.8026, 0.8129, 0.7934, 0.8249, 0.8139, 0.8091, 0.7983,\n",
      "        0.8212, 0.7942, 0.8305, 0.8084, 0.8130, 0.8325, 0.8142, 0.8261, 0.8263,\n",
      "        0.7888, 0.7961, 0.8240, 0.8330, 0.8659, 0.8621, 0.8931, 0.8064, 0.8721,\n",
      "        0.8204, 0.8545, 0.8491, 0.8619, 0.8742, 0.8651, 0.8653, 0.8674, 0.8787,\n",
      "        0.8302, 0.8526, 0.8354, 0.8279, 0.8394, 0.8687, 0.7967, 0.8642, 0.8251,\n",
      "        0.8052, 0.8441, 0.8185, 0.8226, 0.8467, 0.7511, 0.7935, 0.7399, 0.7817,\n",
      "        0.7909, 0.7805, 0.8318, 0.8293, 0.7562, 0.8465, 0.7987, 0.8052, 0.7859,\n",
      "        0.8101, 0.8318, 0.8479, 0.7854, 0.7733, 0.8062, 0.8365, 0.8216, 0.7967,\n",
      "        0.8235, 0.7985, 0.7904, 0.8678, 0.8587, 0.8536, 0.8613, 0.8673, 0.8255,\n",
      "        0.8348, 0.8626, 0.8447, 0.8630, 0.8236, 0.7680, 0.8436, 0.7953, 0.8363,\n",
      "        0.8371, 0.8380, 0.8015, 0.8299, 0.8628, 0.8436, 0.8017, 0.8635, 0.8470,\n",
      "        0.8361], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1274, Val-Loss: 0.0842\n",
      "Epoch [20/700], Train-Loss: 0.0349, Val-Loss: 0.0284\n",
      "Epoch [30/700], Train-Loss: 0.0127, Val-Loss: 0.0091\n",
      "Epoch [40/700], Train-Loss: 0.0046, Val-Loss: 0.0037\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0008\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[5.4680e-05, 2.7282e-04, 9.1787e-05,  ..., 7.3993e-05, 9.6907e-05,\n",
      "         9.6043e-05],\n",
      "        [4.9812e-05, 1.6878e-04, 1.1726e-04,  ..., 4.0033e-05, 3.0337e-05,\n",
      "         1.5260e-04],\n",
      "        [6.0713e-05, 8.8717e-04, 2.4562e-04,  ..., 1.3811e-04, 2.3486e-04,\n",
      "         8.9819e-05],\n",
      "        ...,\n",
      "        [1.0105e-04, 3.2859e-04, 1.5775e-04,  ..., 7.6741e-05, 6.6714e-05,\n",
      "         7.8670e-05],\n",
      "        [1.3784e-04, 1.7228e-04, 1.1724e-04,  ..., 9.7402e-05, 1.0022e-04,\n",
      "         1.2114e-04],\n",
      "        [1.5140e-04, 8.6993e-04, 8.4203e-05,  ..., 5.1871e-05, 9.3341e-05,\n",
      "         2.8750e-04]], device='cuda:0')\n",
      "pred: tensor([0.8320, 0.8312, 0.7985, 0.8213, 0.7787, 0.8295, 0.8082, 0.8147, 0.7895,\n",
      "        0.8147, 0.7979, 0.8253, 0.7972, 0.8044, 0.8304, 0.8102, 0.8342, 0.8270,\n",
      "        0.7903, 0.7929, 0.8218, 0.8365, 0.8632, 0.8522, 0.8706, 0.8030, 0.8658,\n",
      "        0.8200, 0.8435, 0.8456, 0.8486, 0.8744, 0.8654, 0.8661, 0.8533, 0.8577,\n",
      "        0.8245, 0.8361, 0.8349, 0.8315, 0.8360, 0.8746, 0.7990, 0.8764, 0.8185,\n",
      "        0.7976, 0.8394, 0.8240, 0.8253, 0.8273, 0.7713, 0.7929, 0.7629, 0.7721,\n",
      "        0.7875, 0.7740, 0.8314, 0.8336, 0.7550, 0.8393, 0.7975, 0.8017, 0.7773,\n",
      "        0.8027, 0.8328, 0.8476, 0.7969, 0.7824, 0.8169, 0.8285, 0.8283, 0.7934,\n",
      "        0.8301, 0.8001, 0.7994, 0.8558, 0.8447, 0.8453, 0.8630, 0.8553, 0.8207,\n",
      "        0.8293, 0.8576, 0.8360, 0.8574, 0.8179, 0.7766, 0.8419, 0.7764, 0.8429,\n",
      "        0.8368, 0.8361, 0.7909, 0.8356, 0.8563, 0.8460, 0.8052, 0.8621, 0.8506,\n",
      "        0.8424], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1112, Val-Loss: 0.0733\n",
      "Epoch [20/700], Train-Loss: 0.0262, Val-Loss: 0.0191\n",
      "Epoch [30/700], Train-Loss: 0.0083, Val-Loss: 0.0043\n",
      "Epoch [40/700], Train-Loss: 0.0016, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.2364e-05, 2.9305e-04, 7.2998e-04,  ..., 8.4493e-05, 1.1193e-04,\n",
      "         9.5300e-05],\n",
      "        [1.2209e-04, 8.1249e-05, 1.0478e-04,  ..., 1.6859e-04, 1.7115e-04,\n",
      "         2.0406e-04],\n",
      "        [2.4780e-04, 2.4758e-04, 3.2569e-04,  ..., 1.1948e-04, 5.6881e-05,\n",
      "         7.8587e-05],\n",
      "        ...,\n",
      "        [1.6081e-04, 1.7644e-04, 1.7135e-04,  ..., 1.1909e-04, 7.3504e-05,\n",
      "         5.8524e-05],\n",
      "        [7.6212e-05, 9.9888e-05, 1.7636e-04,  ..., 7.5009e-05, 4.0893e-05,\n",
      "         2.8624e-04],\n",
      "        [1.3571e-04, 7.7124e-05, 1.2283e-04,  ..., 4.3150e-05, 5.3358e-05,\n",
      "         1.0473e-04]], device='cuda:0')\n",
      "pred: tensor([0.8395, 0.8315, 0.8067, 0.8058, 0.7836, 0.8157, 0.8191, 0.8062, 0.7981,\n",
      "        0.8205, 0.8032, 0.8344, 0.7923, 0.8095, 0.8373, 0.8013, 0.8393, 0.8308,\n",
      "        0.7970, 0.7949, 0.8299, 0.8417, 0.8607, 0.8563, 0.8759, 0.8086, 0.8672,\n",
      "        0.8199, 0.8518, 0.8499, 0.8496, 0.8613, 0.8628, 0.8693, 0.8564, 0.8628,\n",
      "        0.8293, 0.8458, 0.8350, 0.8256, 0.8397, 0.8698, 0.7928, 0.8741, 0.8323,\n",
      "        0.8232, 0.8359, 0.8375, 0.8309, 0.8431, 0.7449, 0.7819, 0.7553, 0.7624,\n",
      "        0.7742, 0.7686, 0.8261, 0.8327, 0.7439, 0.8445, 0.8093, 0.7970, 0.7863,\n",
      "        0.8057, 0.8400, 0.8477, 0.7952, 0.7922, 0.8048, 0.8295, 0.8348, 0.7989,\n",
      "        0.8334, 0.8037, 0.7936, 0.8584, 0.8604, 0.8495, 0.8617, 0.8556, 0.8285,\n",
      "        0.8338, 0.8613, 0.8337, 0.8622, 0.8114, 0.7666, 0.8433, 0.8009, 0.8392,\n",
      "        0.8410, 0.8319, 0.8097, 0.8307, 0.8561, 0.8377, 0.8098, 0.8640, 0.8533,\n",
      "        0.8440], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1124, Val-Loss: 0.0617\n",
      "Epoch [20/700], Train-Loss: 0.0295, Val-Loss: 0.0191\n",
      "Epoch [30/700], Train-Loss: 0.0089, Val-Loss: 0.0040\n",
      "Epoch [40/700], Train-Loss: 0.0028, Val-Loss: 0.0013\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.1813e-05, 6.7109e-05, 7.7403e-05,  ..., 2.3124e-04, 1.4477e-04,\n",
      "         1.4225e-04],\n",
      "        [2.0657e-04, 2.7468e-04, 2.2593e-04,  ..., 1.2308e-04, 9.9205e-05,\n",
      "         1.2582e-04],\n",
      "        [2.5769e-04, 2.5950e-04, 2.9424e-04,  ..., 5.0786e-04, 2.4000e-04,\n",
      "         1.2263e-04],\n",
      "        ...,\n",
      "        [1.2501e-04, 1.7008e-04, 1.6566e-04,  ..., 1.1217e-04, 1.2211e-04,\n",
      "         2.8465e-04],\n",
      "        [2.7336e-04, 1.7125e-04, 1.3841e-04,  ..., 4.9360e-05, 1.8966e-05,\n",
      "         8.0621e-05],\n",
      "        [3.2544e-04, 2.9496e-04, 2.4943e-04,  ..., 4.2874e-05, 3.5216e-05,\n",
      "         6.4071e-05]], device='cuda:0')\n",
      "pred: tensor([0.8437, 0.8358, 0.7976, 0.8159, 0.7799, 0.8133, 0.8157, 0.8146, 0.7969,\n",
      "        0.8302, 0.7929, 0.8342, 0.7914, 0.8152, 0.8442, 0.7962, 0.8330, 0.8217,\n",
      "        0.7728, 0.7938, 0.8186, 0.8248, 0.8620, 0.8524, 0.8769, 0.8040, 0.8658,\n",
      "        0.8124, 0.8431, 0.8505, 0.8499, 0.8746, 0.8674, 0.8653, 0.8506, 0.8657,\n",
      "        0.8288, 0.8559, 0.8396, 0.8335, 0.8439, 0.8693, 0.7934, 0.8700, 0.8280,\n",
      "        0.8022, 0.8480, 0.8260, 0.8274, 0.8461, 0.7480, 0.7813, 0.7483, 0.7764,\n",
      "        0.7843, 0.7734, 0.8320, 0.8335, 0.7526, 0.8460, 0.7892, 0.7948, 0.7767,\n",
      "        0.8109, 0.8406, 0.8363, 0.7802, 0.7625, 0.8109, 0.8358, 0.8357, 0.8011,\n",
      "        0.8300, 0.8025, 0.7939, 0.8563, 0.8425, 0.8451, 0.8670, 0.8647, 0.8359,\n",
      "        0.8352, 0.8603, 0.8432, 0.8627, 0.8141, 0.7716, 0.8435, 0.8105, 0.8404,\n",
      "        0.8291, 0.8309, 0.8039, 0.8337, 0.8648, 0.8450, 0.8061, 0.8577, 0.8535,\n",
      "        0.8499], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9758775877587758, 0.9762376237623761, 0.9789498949894989, 0.969108910891089, 0.965088508850885]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9234329328137326, 0.9234329328137326, 0.9234329328137326, 0.9234329328137326, 0.9234329328137326]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9204656728021304, 0.9204656728021304, 0.9204656728021304, 0.9204656728021304, 0.9204656728021304]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9334657243490357, 0.9334657243490357, 0.9334657243490357, 0.9334657243490357, 0.9334657243490357]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.930287519283355, 0.930287519283355, 0.930287519283355, 0.930287519283355, 0.930287519283355]}\n",
      "Epoch [10/700], Train-Loss: 0.2126, Val-Loss: 0.1713\n",
      "Epoch [20/700], Train-Loss: 0.0503, Val-Loss: 0.0552\n",
      "Epoch [30/700], Train-Loss: 0.0025, Val-Loss: 0.0059\n",
      "Epoch [40/700], Train-Loss: 0.0040, Val-Loss: 0.0026\n",
      "Epoch [50/700], Train-Loss: 0.0029, Val-Loss: 0.0021\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0011\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.0273e-04, 2.6830e-04, 4.7556e-04,  ..., 4.3857e-05, 1.3532e-04,\n",
      "         1.8451e-04],\n",
      "        [8.2621e-05, 6.1752e-05, 1.8047e-04,  ..., 1.2728e-04, 2.5861e-04,\n",
      "         2.5697e-04],\n",
      "        [5.2461e-05, 1.9434e-04, 1.7942e-04,  ..., 1.5126e-04, 2.2207e-04,\n",
      "         3.1858e-04],\n",
      "        ...,\n",
      "        [1.4000e-04, 7.3260e-04, 1.0121e-03,  ..., 5.2830e-05, 5.1969e-04,\n",
      "         7.9775e-05],\n",
      "        [5.5227e-06, 1.9829e-05, 3.4907e-04,  ..., 6.5031e-05, 1.5108e-04,\n",
      "         4.8542e-05],\n",
      "        [1.8017e-05, 5.8786e-05, 1.6320e-04,  ..., 1.9030e-04, 1.4778e-04,\n",
      "         1.4802e-04]], device='cuda:0')\n",
      "pred: tensor([0.8386, 0.8173, 0.7982, 0.8121, 0.7973, 0.8302, 0.8073, 0.8061, 0.7941,\n",
      "        0.8189, 0.7878, 0.8344, 0.8103, 0.8065, 0.8360, 0.8086, 0.8218, 0.8143,\n",
      "        0.7731, 0.7866, 0.8301, 0.8490, 0.8623, 0.8569, 0.8870, 0.8161, 0.8600,\n",
      "        0.7824, 0.8544, 0.8395, 0.8483, 0.8543, 0.8716, 0.8730, 0.8571, 0.8787,\n",
      "        0.8357, 0.8521, 0.8361, 0.8252, 0.8329, 0.8715, 0.8066, 0.8619, 0.8315,\n",
      "        0.7844, 0.8315, 0.8095, 0.8403, 0.8270, 0.7782, 0.7831, 0.7575, 0.7852,\n",
      "        0.7746, 0.7649, 0.8218, 0.8185, 0.7587, 0.8354, 0.7934, 0.8012, 0.7834,\n",
      "        0.8048, 0.8311, 0.8460, 0.7861, 0.7816, 0.8229, 0.8334, 0.8393, 0.7772,\n",
      "        0.8232, 0.7824, 0.8061, 0.8596, 0.8382, 0.8552, 0.8553, 0.8782, 0.8380,\n",
      "        0.8336, 0.8609, 0.8509, 0.8634, 0.8147, 0.7857, 0.8407, 0.7873, 0.8290,\n",
      "        0.8425, 0.8320, 0.8124, 0.8259, 0.8676, 0.8411, 0.8133, 0.8590, 0.8564,\n",
      "        0.8496], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2550, Val-Loss: 0.2064\n",
      "Epoch [20/700], Train-Loss: 0.0494, Val-Loss: 0.0608\n",
      "Epoch [30/700], Train-Loss: 0.0013, Val-Loss: 0.0044\n",
      "Epoch [40/700], Train-Loss: 0.0069, Val-Loss: 0.0049\n",
      "Epoch [50/700], Train-Loss: 0.0036, Val-Loss: 0.0030\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0010\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.2260e-05, 5.0896e-05, 7.3541e-05,  ..., 4.4473e-05, 8.0067e-05,\n",
      "         4.4452e-05],\n",
      "        [2.0841e-04, 6.3537e-04, 3.6409e-04,  ..., 1.9726e-04, 1.2250e-04,\n",
      "         6.9919e-04],\n",
      "        [3.8869e-05, 1.1726e-04, 2.3310e-04,  ..., 1.1121e-04, 2.7989e-04,\n",
      "         2.4452e-04],\n",
      "        ...,\n",
      "        [1.3161e-04, 1.8604e-04, 1.0064e-04,  ..., 2.7635e-05, 4.1292e-05,\n",
      "         8.7398e-05],\n",
      "        [2.9653e-04, 2.0052e-04, 1.8755e-04,  ..., 5.0002e-05, 4.3463e-05,\n",
      "         7.6249e-06],\n",
      "        [1.0209e-04, 4.4529e-04, 1.5490e-04,  ..., 9.9358e-05, 4.6574e-05,\n",
      "         5.0183e-05]], device='cuda:0')\n",
      "pred: tensor([0.8385, 0.8168, 0.7977, 0.8126, 0.7990, 0.8288, 0.8095, 0.8061, 0.7949,\n",
      "        0.8182, 0.7901, 0.8334, 0.8120, 0.8083, 0.8353, 0.8079, 0.8218, 0.8156,\n",
      "        0.7740, 0.7880, 0.8317, 0.8515, 0.8630, 0.8566, 0.8847, 0.8139, 0.8613,\n",
      "        0.7830, 0.8559, 0.8388, 0.8442, 0.8551, 0.8729, 0.8731, 0.8568, 0.8754,\n",
      "        0.8371, 0.8498, 0.8354, 0.8263, 0.8347, 0.8749, 0.8076, 0.8599, 0.8290,\n",
      "        0.7823, 0.8322, 0.8101, 0.8379, 0.8296, 0.7775, 0.7849, 0.7573, 0.7832,\n",
      "        0.7730, 0.7651, 0.8237, 0.8201, 0.7580, 0.8365, 0.7945, 0.8017, 0.7821,\n",
      "        0.8042, 0.8292, 0.8462, 0.7874, 0.7827, 0.8233, 0.8367, 0.8397, 0.7757,\n",
      "        0.8209, 0.7798, 0.8045, 0.8565, 0.8389, 0.8536, 0.8550, 0.8737, 0.8365,\n",
      "        0.8336, 0.8601, 0.8506, 0.8631, 0.8163, 0.7875, 0.8418, 0.7875, 0.8333,\n",
      "        0.8456, 0.8312, 0.8124, 0.8284, 0.8686, 0.8436, 0.8141, 0.8616, 0.8563,\n",
      "        0.8519], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2084, Val-Loss: 0.1617\n",
      "Epoch [20/700], Train-Loss: 0.0617, Val-Loss: 0.0616\n",
      "Epoch [30/700], Train-Loss: 0.0051, Val-Loss: 0.0093\n",
      "Epoch [40/700], Train-Loss: 0.0029, Val-Loss: 0.0015\n",
      "Epoch [50/700], Train-Loss: 0.0030, Val-Loss: 0.0019\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0014\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.3117e-03, 2.3821e-04, 2.0592e-04,  ..., 3.0883e-05, 4.4152e-05,\n",
      "         2.7205e-04],\n",
      "        [3.1670e-03, 2.7216e-03, 8.8035e-05,  ..., 1.0640e-04, 7.9597e-05,\n",
      "         3.4775e-04],\n",
      "        [7.0641e-04, 1.1214e-03, 6.4616e-04,  ..., 9.1896e-05, 2.4632e-04,\n",
      "         4.0074e-04],\n",
      "        ...,\n",
      "        [1.5659e-03, 2.2953e-04, 7.5764e-04,  ..., 4.1130e-05, 4.6226e-04,\n",
      "         3.7822e-04],\n",
      "        [2.3010e-03, 1.2094e-03, 4.9298e-04,  ..., 1.2258e-04, 1.0363e-04,\n",
      "         3.3402e-03],\n",
      "        [1.5300e-02, 2.7701e-03, 1.3931e-05,  ..., 6.1716e-05, 5.5123e-05,\n",
      "         3.7125e-03]], device='cuda:0')\n",
      "pred: tensor([0.8400, 0.8164, 0.7988, 0.8121, 0.7973, 0.8288, 0.8092, 0.8075, 0.7952,\n",
      "        0.8186, 0.7887, 0.8330, 0.8136, 0.8052, 0.8350, 0.8072, 0.8228, 0.8153,\n",
      "        0.7722, 0.7869, 0.8298, 0.8493, 0.8635, 0.8588, 0.8885, 0.8146, 0.8609,\n",
      "        0.7832, 0.8537, 0.8415, 0.8476, 0.8554, 0.8743, 0.8746, 0.8586, 0.8801,\n",
      "        0.8377, 0.8526, 0.8389, 0.8256, 0.8331, 0.8747, 0.8057, 0.8631, 0.8301,\n",
      "        0.7809, 0.8333, 0.8102, 0.8384, 0.8286, 0.7797, 0.7851, 0.7572, 0.7839,\n",
      "        0.7738, 0.7643, 0.8204, 0.8200, 0.7576, 0.8329, 0.7918, 0.8020, 0.7844,\n",
      "        0.8049, 0.8301, 0.8452, 0.7877, 0.7837, 0.8229, 0.8345, 0.8377, 0.7774,\n",
      "        0.8203, 0.7829, 0.8040, 0.8632, 0.8377, 0.8551, 0.8557, 0.8754, 0.8387,\n",
      "        0.8344, 0.8583, 0.8511, 0.8625, 0.8172, 0.7885, 0.8402, 0.7868, 0.8285,\n",
      "        0.8437, 0.8308, 0.8124, 0.8261, 0.8668, 0.8418, 0.8138, 0.8609, 0.8563,\n",
      "        0.8512], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2415, Val-Loss: 0.1924\n",
      "Epoch [20/700], Train-Loss: 0.0600, Val-Loss: 0.0645\n",
      "Epoch [30/700], Train-Loss: 0.0032, Val-Loss: 0.0071\n",
      "Epoch [40/700], Train-Loss: 0.0043, Val-Loss: 0.0027\n",
      "Epoch [50/700], Train-Loss: 0.0033, Val-Loss: 0.0024\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0012\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.0133e-05, 4.5945e-05, 2.2555e-04,  ..., 3.5436e-05, 3.3013e-05,\n",
      "         9.0991e-05],\n",
      "        [1.7005e-04, 1.1183e-04, 8.7188e-05,  ..., 1.4638e-04, 1.3429e-04,\n",
      "         1.8340e-03],\n",
      "        [1.7981e-04, 1.8190e-04, 1.7238e-04,  ..., 8.0693e-05, 9.2526e-05,\n",
      "         2.0996e-03],\n",
      "        ...,\n",
      "        [9.6612e-04, 9.4781e-05, 7.6921e-05,  ..., 2.2149e-04, 1.2033e-04,\n",
      "         6.9648e-04],\n",
      "        [3.1438e-04, 1.8797e-04, 2.0746e-04,  ..., 3.0161e-04, 1.2055e-04,\n",
      "         3.7318e-04],\n",
      "        [9.6409e-06, 4.5029e-05, 2.4909e-05,  ..., 3.5435e-04, 2.6252e-04,\n",
      "         1.1024e-03]], device='cuda:0')\n",
      "pred: tensor([0.8408, 0.8165, 0.7996, 0.8115, 0.7971, 0.8271, 0.8090, 0.8092, 0.7934,\n",
      "        0.8189, 0.7865, 0.8347, 0.8107, 0.8054, 0.8346, 0.8054, 0.8224, 0.8126,\n",
      "        0.7723, 0.7878, 0.8294, 0.8507, 0.8636, 0.8574, 0.8857, 0.8157, 0.8615,\n",
      "        0.7810, 0.8565, 0.8407, 0.8450, 0.8548, 0.8727, 0.8730, 0.8548, 0.8753,\n",
      "        0.8365, 0.8507, 0.8354, 0.8219, 0.8330, 0.8738, 0.8079, 0.8623, 0.8317,\n",
      "        0.7837, 0.8319, 0.8122, 0.8402, 0.8300, 0.7795, 0.7834, 0.7570, 0.7843,\n",
      "        0.7753, 0.7646, 0.8208, 0.8190, 0.7575, 0.8364, 0.7957, 0.8008, 0.7844,\n",
      "        0.8041, 0.8293, 0.8468, 0.7864, 0.7838, 0.8224, 0.8341, 0.8383, 0.7764,\n",
      "        0.8230, 0.7815, 0.8064, 0.8620, 0.8340, 0.8537, 0.8544, 0.8752, 0.8378,\n",
      "        0.8343, 0.8611, 0.8517, 0.8637, 0.8154, 0.7863, 0.8418, 0.7866, 0.8294,\n",
      "        0.8429, 0.8302, 0.8116, 0.8262, 0.8657, 0.8434, 0.8125, 0.8612, 0.8588,\n",
      "        0.8495], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3597, Val-Loss: 0.3102\n",
      "Epoch [20/700], Train-Loss: 0.0168, Val-Loss: 0.0372\n",
      "Epoch [30/700], Train-Loss: 0.0019, Val-Loss: 0.0004\n",
      "Epoch [40/700], Train-Loss: 0.0123, Val-Loss: 0.0105\n",
      "Epoch [50/700], Train-Loss: 0.0029, Val-Loss: 0.0031\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.3395e-05, 7.7247e-05, 3.3591e-04,  ..., 6.9479e-05, 1.5548e-04,\n",
      "         6.0007e-05],\n",
      "        [1.3544e-04, 2.4516e-04, 3.8068e-04,  ..., 1.7797e-04, 1.8462e-04,\n",
      "         2.7651e-04],\n",
      "        [3.0509e-04, 9.4969e-05, 6.5941e-04,  ..., 2.4501e-04, 2.7907e-04,\n",
      "         4.9721e-04],\n",
      "        ...,\n",
      "        [5.1762e-04, 2.5003e-04, 1.4406e-03,  ..., 4.0378e-04, 1.4148e-04,\n",
      "         2.5922e-04],\n",
      "        [7.6096e-05, 1.3892e-04, 3.1970e-05,  ..., 3.1735e-04, 1.4274e-04,\n",
      "         3.6967e-04],\n",
      "        [7.1089e-05, 1.0006e-04, 6.2700e-04,  ..., 2.1915e-04, 1.4045e-04,\n",
      "         4.0290e-04]], device='cuda:0')\n",
      "pred: tensor([0.8383, 0.8202, 0.7967, 0.8118, 0.7973, 0.8330, 0.8110, 0.8109, 0.7960,\n",
      "        0.8183, 0.7903, 0.8353, 0.8132, 0.8073, 0.8357, 0.8066, 0.8239, 0.8137,\n",
      "        0.7718, 0.7889, 0.8282, 0.8487, 0.8663, 0.8570, 0.8836, 0.8181, 0.8607,\n",
      "        0.7865, 0.8518, 0.8387, 0.8479, 0.8539, 0.8685, 0.8735, 0.8606, 0.8776,\n",
      "        0.8369, 0.8490, 0.8384, 0.8268, 0.8335, 0.8763, 0.8100, 0.8596, 0.8300,\n",
      "        0.7798, 0.8334, 0.8117, 0.8395, 0.8299, 0.7807, 0.7860, 0.7575, 0.7830,\n",
      "        0.7723, 0.7670, 0.8195, 0.8206, 0.7574, 0.8351, 0.7973, 0.8031, 0.7839,\n",
      "        0.8052, 0.8309, 0.8444, 0.7892, 0.7827, 0.8237, 0.8350, 0.8394, 0.7806,\n",
      "        0.8230, 0.7854, 0.8073, 0.8589, 0.8342, 0.8509, 0.8562, 0.8775, 0.8394,\n",
      "        0.8337, 0.8599, 0.8510, 0.8629, 0.8183, 0.7853, 0.8384, 0.7896, 0.8304,\n",
      "        0.8419, 0.8322, 0.8088, 0.8272, 0.8638, 0.8410, 0.8147, 0.8611, 0.8543,\n",
      "        0.8496], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9467866786678668, 0.9492949294929492, 0.9473267326732672, 0.9459105910591058, 0.9473507350735072]}\n",
      "Epoch [10/700], Train-Loss: 0.0250, Val-Loss: 0.0027\n",
      "Epoch [20/700], Train-Loss: 0.0060, Val-Loss: 0.0006\n",
      "Epoch [30/700], Train-Loss: 0.0121, Val-Loss: 0.0092\n",
      "Epoch [40/700], Train-Loss: 0.0049, Val-Loss: 0.0042\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0018\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.3866e-04, 9.5452e-05, 1.4732e-04,  ..., 1.3000e-04, 1.4922e-04,\n",
      "         8.6012e-04],\n",
      "        [1.8583e-04, 1.2035e-04, 2.8071e-04,  ..., 9.9775e-05, 1.0727e-04,\n",
      "         3.3298e-04],\n",
      "        [2.0998e-04, 1.7368e-04, 8.6776e-04,  ..., 3.0678e-04, 4.5288e-04,\n",
      "         2.9837e-03],\n",
      "        ...,\n",
      "        [8.0938e-05, 3.5263e-04, 2.1068e-04,  ..., 6.4281e-04, 7.8249e-05,\n",
      "         6.4877e-04],\n",
      "        [1.6066e-04, 3.0077e-04, 3.8355e-05,  ..., 3.6635e-04, 1.3375e-04,\n",
      "         7.5394e-03],\n",
      "        [3.4894e-04, 1.2781e-04, 7.5960e-05,  ..., 2.2293e-04, 1.7141e-05,\n",
      "         1.2685e-03]], device='cuda:0')\n",
      "pred: tensor([0.8386, 0.8185, 0.7911, 0.8144, 0.7912, 0.8309, 0.8052, 0.8025, 0.7920,\n",
      "        0.8159, 0.7981, 0.8196, 0.8072, 0.8013, 0.8334, 0.8023, 0.8378, 0.8215,\n",
      "        0.7785, 0.7847, 0.8276, 0.8429, 0.8675, 0.8571, 0.8909, 0.8070, 0.8668,\n",
      "        0.7873, 0.8516, 0.8378, 0.8507, 0.8696, 0.8692, 0.8793, 0.8582, 0.8787,\n",
      "        0.8340, 0.8442, 0.8371, 0.8340, 0.8307, 0.8739, 0.8102, 0.8689, 0.8173,\n",
      "        0.7788, 0.8186, 0.8112, 0.8265, 0.8316, 0.7748, 0.7944, 0.7624, 0.7796,\n",
      "        0.7763, 0.7727, 0.8157, 0.8229, 0.7605, 0.8377, 0.8032, 0.8074, 0.7867,\n",
      "        0.8014, 0.8289, 0.8437, 0.7893, 0.7739, 0.8164, 0.8271, 0.8411, 0.7825,\n",
      "        0.8281, 0.7940, 0.7987, 0.8575, 0.8500, 0.8482, 0.8646, 0.8707, 0.8263,\n",
      "        0.8330, 0.8630, 0.8476, 0.8667, 0.8107, 0.7769, 0.8440, 0.7832, 0.8319,\n",
      "        0.8427, 0.8441, 0.8112, 0.8264, 0.8707, 0.8552, 0.8031, 0.8479, 0.8464,\n",
      "        0.8425], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1813, Val-Loss: 0.1228\n",
      "Epoch [20/700], Train-Loss: 0.0703, Val-Loss: 0.0516\n",
      "Epoch [30/700], Train-Loss: 0.0156, Val-Loss: 0.0184\n",
      "Epoch [40/700], Train-Loss: 0.0001, Val-Loss: 0.0006\n",
      "Epoch [50/700], Train-Loss: 0.0008, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0008\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.9602e-05, 2.3547e-04, 8.2658e-05,  ..., 1.0696e-03, 1.9507e-04,\n",
      "         1.4899e-04],\n",
      "        [6.8134e-05, 3.7578e-05, 1.1151e-04,  ..., 2.2099e-03, 6.4851e-04,\n",
      "         3.1205e-05],\n",
      "        [9.9501e-04, 2.9598e-04, 3.1450e-04,  ..., 6.4350e-04, 2.6916e-04,\n",
      "         9.4290e-04],\n",
      "        ...,\n",
      "        [1.0882e-04, 2.0178e-04, 1.0855e-04,  ..., 4.2911e-04, 9.1634e-05,\n",
      "         2.4229e-04],\n",
      "        [6.1059e-05, 5.2563e-05, 6.2422e-05,  ..., 1.0098e-03, 1.7218e-04,\n",
      "         3.9451e-04],\n",
      "        [1.3228e-04, 5.9281e-05, 1.3059e-04,  ..., 5.5186e-05, 2.7891e-04,\n",
      "         2.5931e-05]], device='cuda:0')\n",
      "pred: tensor([0.8375, 0.8205, 0.7909, 0.8169, 0.7920, 0.8322, 0.8085, 0.8041, 0.7919,\n",
      "        0.8189, 0.8003, 0.8237, 0.8073, 0.8000, 0.8336, 0.8023, 0.8381, 0.8220,\n",
      "        0.7801, 0.7868, 0.8276, 0.8402, 0.8650, 0.8545, 0.8913, 0.8104, 0.8682,\n",
      "        0.7910, 0.8485, 0.8393, 0.8497, 0.8672, 0.8697, 0.8782, 0.8560, 0.8772,\n",
      "        0.8369, 0.8483, 0.8396, 0.8331, 0.8332, 0.8702, 0.8090, 0.8696, 0.8172,\n",
      "        0.7790, 0.8176, 0.8112, 0.8259, 0.8309, 0.7745, 0.7918, 0.7629, 0.7788,\n",
      "        0.7770, 0.7711, 0.8175, 0.8229, 0.7609, 0.8380, 0.8014, 0.8070, 0.7859,\n",
      "        0.8014, 0.8304, 0.8436, 0.7890, 0.7719, 0.8162, 0.8285, 0.8397, 0.7835,\n",
      "        0.8268, 0.7936, 0.7974, 0.8561, 0.8484, 0.8481, 0.8686, 0.8719, 0.8273,\n",
      "        0.8330, 0.8636, 0.8466, 0.8661, 0.8128, 0.7773, 0.8385, 0.7844, 0.8309,\n",
      "        0.8429, 0.8436, 0.8101, 0.8257, 0.8714, 0.8526, 0.8040, 0.8485, 0.8484,\n",
      "        0.8436], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3422, Val-Loss: 0.2784\n",
      "Epoch [20/700], Train-Loss: 0.0728, Val-Loss: 0.0865\n",
      "Epoch [30/700], Train-Loss: 0.0037, Val-Loss: 0.0090\n",
      "Epoch [40/700], Train-Loss: 0.0066, Val-Loss: 0.0039\n",
      "Epoch [50/700], Train-Loss: 0.0048, Val-Loss: 0.0036\n",
      "Epoch [60/700], Train-Loss: 0.0012, Val-Loss: 0.0017\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[5.8958e-05, 9.6725e-04, 1.8352e-02,  ..., 5.2609e-05, 8.6044e-05,\n",
      "         9.1533e-05],\n",
      "        [1.2237e-04, 1.4173e-04, 4.9286e-04,  ..., 3.1235e-04, 1.5835e-04,\n",
      "         1.1364e-03],\n",
      "        [2.3060e-04, 4.7979e-04, 4.6760e-03,  ..., 3.0795e-04, 6.3555e-05,\n",
      "         5.6044e-04],\n",
      "        ...,\n",
      "        [4.0965e-04, 5.7268e-04, 8.4891e-04,  ..., 8.9095e-05, 2.2242e-04,\n",
      "         3.7468e-04],\n",
      "        [4.9632e-05, 9.5709e-05, 3.1501e-03,  ..., 3.5623e-04, 7.2125e-05,\n",
      "         8.5479e-04],\n",
      "        [7.9451e-05, 1.5064e-04, 1.9529e-04,  ..., 1.1202e-04, 5.5775e-05,\n",
      "         1.1954e-03]], device='cuda:0')\n",
      "pred: tensor([0.8378, 0.8168, 0.7930, 0.8154, 0.7913, 0.8345, 0.8058, 0.8035, 0.7919,\n",
      "        0.8167, 0.7980, 0.8189, 0.8086, 0.8004, 0.8314, 0.8024, 0.8375, 0.8201,\n",
      "        0.7803, 0.7857, 0.8277, 0.8374, 0.8648, 0.8545, 0.8889, 0.8100, 0.8699,\n",
      "        0.7889, 0.8501, 0.8408, 0.8514, 0.8701, 0.8666, 0.8775, 0.8578, 0.8757,\n",
      "        0.8348, 0.8461, 0.8379, 0.8358, 0.8326, 0.8746, 0.8084, 0.8683, 0.8164,\n",
      "        0.7786, 0.8139, 0.8090, 0.8233, 0.8296, 0.7755, 0.7915, 0.7636, 0.7780,\n",
      "        0.7767, 0.7701, 0.8174, 0.8248, 0.7611, 0.8388, 0.8033, 0.8075, 0.7888,\n",
      "        0.8014, 0.8320, 0.8440, 0.7904, 0.7736, 0.8159, 0.8276, 0.8388, 0.7852,\n",
      "        0.8288, 0.7942, 0.7983, 0.8569, 0.8469, 0.8420, 0.8682, 0.8736, 0.8274,\n",
      "        0.8338, 0.8644, 0.8482, 0.8662, 0.8118, 0.7748, 0.8396, 0.7839, 0.8330,\n",
      "        0.8409, 0.8421, 0.8098, 0.8272, 0.8712, 0.8523, 0.8033, 0.8486, 0.8450,\n",
      "        0.8428], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0361, Val-Loss: 0.0085\n",
      "Epoch [20/700], Train-Loss: 0.0129, Val-Loss: 0.0032\n",
      "Epoch [30/700], Train-Loss: 0.0150, Val-Loss: 0.0128\n",
      "Epoch [40/700], Train-Loss: 0.0046, Val-Loss: 0.0045\n",
      "Epoch [50/700], Train-Loss: 0.0009, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[5.8958e-05, 9.6725e-04, 1.8352e-02,  ..., 3.5436e-05, 3.3013e-05,\n",
      "         9.0991e-05],\n",
      "        [1.2237e-04, 1.4173e-04, 4.9286e-04,  ..., 1.4638e-04, 1.3429e-04,\n",
      "         1.8340e-03],\n",
      "        [2.3060e-04, 4.7979e-04, 4.6760e-03,  ..., 8.0693e-05, 9.2526e-05,\n",
      "         2.0996e-03],\n",
      "        ...,\n",
      "        [4.0965e-04, 5.7268e-04, 8.4891e-04,  ..., 2.2149e-04, 1.2033e-04,\n",
      "         6.9648e-04],\n",
      "        [4.9632e-05, 9.5709e-05, 3.1501e-03,  ..., 3.0161e-04, 1.2055e-04,\n",
      "         3.7318e-04],\n",
      "        [7.9451e-05, 1.5064e-04, 1.9529e-04,  ..., 3.5435e-04, 2.6252e-04,\n",
      "         1.1024e-03]], device='cuda:0')\n",
      "pred: tensor([0.8393, 0.8206, 0.7914, 0.8156, 0.7926, 0.8324, 0.8050, 0.8049, 0.7903,\n",
      "        0.8179, 0.7980, 0.8191, 0.8071, 0.8001, 0.8322, 0.8016, 0.8356, 0.8194,\n",
      "        0.7815, 0.7841, 0.8271, 0.8372, 0.8651, 0.8541, 0.8895, 0.8106, 0.8677,\n",
      "        0.7944, 0.8485, 0.8405, 0.8485, 0.8699, 0.8676, 0.8737, 0.8555, 0.8767,\n",
      "        0.8350, 0.8461, 0.8378, 0.8376, 0.8311, 0.8764, 0.8103, 0.8690, 0.8169,\n",
      "        0.7783, 0.8161, 0.8117, 0.8249, 0.8281, 0.7774, 0.7929, 0.7621, 0.7792,\n",
      "        0.7783, 0.7727, 0.8139, 0.8225, 0.7585, 0.8404, 0.8043, 0.8064, 0.7846,\n",
      "        0.8024, 0.8304, 0.8426, 0.7928, 0.7753, 0.8171, 0.8283, 0.8388, 0.7843,\n",
      "        0.8290, 0.7950, 0.7999, 0.8569, 0.8499, 0.8455, 0.8681, 0.8739, 0.8274,\n",
      "        0.8347, 0.8633, 0.8474, 0.8658, 0.8134, 0.7773, 0.8401, 0.7846, 0.8317,\n",
      "        0.8453, 0.8432, 0.8092, 0.8271, 0.8685, 0.8539, 0.8041, 0.8488, 0.8454,\n",
      "        0.8415], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0569, Val-Loss: 0.0199\n",
      "Epoch [20/700], Train-Loss: 0.0211, Val-Loss: 0.0079\n",
      "Epoch [30/700], Train-Loss: 0.0165, Val-Loss: 0.0149\n",
      "Epoch [40/700], Train-Loss: 0.0034, Val-Loss: 0.0040\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.3117e-03, 2.3821e-04, 2.0592e-04,  ..., 3.5436e-05, 3.3013e-05,\n",
      "         9.0991e-05],\n",
      "        [3.1670e-03, 2.7216e-03, 8.8035e-05,  ..., 1.4638e-04, 1.3429e-04,\n",
      "         1.8340e-03],\n",
      "        [7.0641e-04, 1.1214e-03, 6.4616e-04,  ..., 8.0693e-05, 9.2526e-05,\n",
      "         2.0996e-03],\n",
      "        ...,\n",
      "        [1.5659e-03, 2.2953e-04, 7.5764e-04,  ..., 2.2149e-04, 1.2033e-04,\n",
      "         6.9648e-04],\n",
      "        [2.3010e-03, 1.2094e-03, 4.9298e-04,  ..., 3.0161e-04, 1.2055e-04,\n",
      "         3.7318e-04],\n",
      "        [1.5300e-02, 2.7701e-03, 1.3931e-05,  ..., 3.5435e-04, 2.6252e-04,\n",
      "         1.1024e-03]], device='cuda:0')\n",
      "pred: tensor([0.8371, 0.8195, 0.7916, 0.8145, 0.7925, 0.8324, 0.8073, 0.8042, 0.7908,\n",
      "        0.8187, 0.7976, 0.8220, 0.8084, 0.7984, 0.8336, 0.8022, 0.8368, 0.8193,\n",
      "        0.7802, 0.7832, 0.8266, 0.8417, 0.8635, 0.8549, 0.8908, 0.8101, 0.8662,\n",
      "        0.7930, 0.8542, 0.8396, 0.8509, 0.8692, 0.8693, 0.8773, 0.8573, 0.8802,\n",
      "        0.8361, 0.8473, 0.8379, 0.8390, 0.8321, 0.8751, 0.8088, 0.8682, 0.8180,\n",
      "        0.7814, 0.8158, 0.8090, 0.8269, 0.8329, 0.7763, 0.7935, 0.7643, 0.7772,\n",
      "        0.7774, 0.7715, 0.8128, 0.8211, 0.7621, 0.8366, 0.8034, 0.8069, 0.7884,\n",
      "        0.8015, 0.8290, 0.8436, 0.7899, 0.7730, 0.8146, 0.8272, 0.8404, 0.7841,\n",
      "        0.8275, 0.7971, 0.7958, 0.8563, 0.8505, 0.8481, 0.8695, 0.8727, 0.8268,\n",
      "        0.8320, 0.8620, 0.8455, 0.8663, 0.8128, 0.7751, 0.8401, 0.7841, 0.8316,\n",
      "        0.8445, 0.8431, 0.8107, 0.8274, 0.8740, 0.8517, 0.8032, 0.8506, 0.8448,\n",
      "        0.8445], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9592559255925591, 0.9616081608160815, 0.9606360636063606, 0.9620402040204019, 0.9587998799879988]}\n",
      "Epoch [10/700], Train-Loss: 0.0054, Val-Loss: 0.0373\n",
      "Epoch [20/700], Train-Loss: 0.0021, Val-Loss: 0.0083\n",
      "Epoch [30/700], Train-Loss: 0.0046, Val-Loss: 0.0017\n",
      "Epoch [40/700], Train-Loss: 0.0037, Val-Loss: 0.0017\n",
      "Epoch [50/700], Train-Loss: 0.0020, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[4.7104e-05, 8.3455e-05, 1.0683e-04,  ..., 5.3386e-05, 7.2017e-05,\n",
      "         1.4038e-04],\n",
      "        [7.8204e-05, 1.4499e-04, 1.0732e-04,  ..., 8.2495e-05, 3.1742e-05,\n",
      "         1.5791e-04],\n",
      "        [6.7509e-04, 3.2527e-04, 3.1498e-04,  ..., 3.2259e-04, 2.1900e-04,\n",
      "         2.9255e-03],\n",
      "        ...,\n",
      "        [1.5325e-04, 1.6982e-04, 3.1870e-04,  ..., 2.9995e-04, 3.8276e-04,\n",
      "         4.2015e-04],\n",
      "        [1.0312e-04, 1.8672e-04, 7.4125e-05,  ..., 2.3495e-04, 1.7573e-04,\n",
      "         1.0646e-03],\n",
      "        [4.6599e-05, 8.1814e-05, 3.1350e-04,  ..., 8.8065e-05, 8.1053e-05,\n",
      "         6.5657e-04]], device='cuda:0')\n",
      "pred: tensor([0.8374, 0.8212, 0.7961, 0.8192, 0.7930, 0.8251, 0.8101, 0.8090, 0.7933,\n",
      "        0.8197, 0.7967, 0.8280, 0.8014, 0.8049, 0.8409, 0.8015, 0.8243, 0.8182,\n",
      "        0.7761, 0.7831, 0.8195, 0.8402, 0.8682, 0.8622, 0.8895, 0.8131, 0.8693,\n",
      "        0.8174, 0.8569, 0.8422, 0.8516, 0.8699, 0.8720, 0.8737, 0.8543, 0.8780,\n",
      "        0.8390, 0.8497, 0.8435, 0.8344, 0.8370, 0.8689, 0.8082, 0.8686, 0.8259,\n",
      "        0.7897, 0.8298, 0.8131, 0.8303, 0.8349, 0.7712, 0.7871, 0.7613, 0.7786,\n",
      "        0.7782, 0.7685, 0.8234, 0.8216, 0.7618, 0.8412, 0.7958, 0.8015, 0.7880,\n",
      "        0.8002, 0.8305, 0.8491, 0.7928, 0.7730, 0.8128, 0.8277, 0.8351, 0.7874,\n",
      "        0.8250, 0.8020, 0.7969, 0.8609, 0.8528, 0.8517, 0.8713, 0.8648, 0.8322,\n",
      "        0.8322, 0.8608, 0.8440, 0.8631, 0.8142, 0.7779, 0.8420, 0.7969, 0.8365,\n",
      "        0.8363, 0.8345, 0.8049, 0.8302, 0.8648, 0.8452, 0.8021, 0.8524, 0.8553,\n",
      "        0.8459], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0216, Val-Loss: 0.0613\n",
      "Epoch [20/700], Train-Loss: 0.0089, Val-Loss: 0.0168\n",
      "Epoch [30/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.4091e-03, 8.9145e-05, 2.8702e-04,  ..., 1.6601e-04, 7.5406e-04,\n",
      "         1.6512e-03],\n",
      "        [4.4552e-04, 2.3369e-04, 2.6958e-04,  ..., 2.4416e-04, 4.7987e-04,\n",
      "         1.8780e-03],\n",
      "        [3.2840e-03, 1.0889e-03, 4.9185e-04,  ..., 8.9164e-05, 2.9917e-04,\n",
      "         1.3844e-03],\n",
      "        ...,\n",
      "        [3.4773e-01, 3.0269e-03, 1.8017e-04,  ..., 1.6232e-04, 3.2397e-04,\n",
      "         7.8537e-04],\n",
      "        [4.5284e-04, 1.2163e-04, 1.3206e-03,  ..., 2.1453e-04, 9.4413e-05,\n",
      "         1.3342e-04],\n",
      "        [2.7951e-04, 2.1713e-04, 8.9941e-04,  ..., 2.5563e-05, 8.9919e-05,\n",
      "         3.2177e-04]], device='cuda:0')\n",
      "pred: tensor([0.8360, 0.8179, 0.7959, 0.8187, 0.7917, 0.8267, 0.8114, 0.8094, 0.7925,\n",
      "        0.8185, 0.7980, 0.8275, 0.8015, 0.8044, 0.8408, 0.8008, 0.8252, 0.8175,\n",
      "        0.7775, 0.7821, 0.8213, 0.8395, 0.8666, 0.8619, 0.8870, 0.8135, 0.8686,\n",
      "        0.8138, 0.8547, 0.8435, 0.8495, 0.8706, 0.8701, 0.8690, 0.8563, 0.8750,\n",
      "        0.8400, 0.8477, 0.8425, 0.8326, 0.8379, 0.8706, 0.8088, 0.8667, 0.8271,\n",
      "        0.7886, 0.8311, 0.8133, 0.8306, 0.8334, 0.7705, 0.7856, 0.7600, 0.7777,\n",
      "        0.7777, 0.7686, 0.8230, 0.8229, 0.7586, 0.8428, 0.7986, 0.8016, 0.7873,\n",
      "        0.7993, 0.8288, 0.8479, 0.7920, 0.7733, 0.8130, 0.8270, 0.8353, 0.7882,\n",
      "        0.8272, 0.8021, 0.7962, 0.8618, 0.8537, 0.8507, 0.8729, 0.8662, 0.8314,\n",
      "        0.8313, 0.8608, 0.8451, 0.8632, 0.8175, 0.7790, 0.8450, 0.7974, 0.8375,\n",
      "        0.8364, 0.8350, 0.8041, 0.8314, 0.8626, 0.8475, 0.8043, 0.8531, 0.8529,\n",
      "        0.8453], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0316, Val-Loss: 0.0731\n",
      "Epoch [20/700], Train-Loss: 0.0139, Val-Loss: 0.0223\n",
      "Epoch [30/700], Train-Loss: 0.0001, Val-Loss: 0.0011\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.3395e-05, 7.7247e-05, 3.3591e-04,  ..., 2.0883e-04, 1.0783e-04,\n",
      "         4.3905e-04],\n",
      "        [1.3544e-04, 2.4516e-04, 3.8068e-04,  ..., 3.6942e-04, 9.4994e-04,\n",
      "         5.9667e-04],\n",
      "        [3.0509e-04, 9.4969e-05, 6.5941e-04,  ..., 1.1345e-03, 3.2104e-04,\n",
      "         2.2839e-04],\n",
      "        ...,\n",
      "        [5.1762e-04, 2.5003e-04, 1.4406e-03,  ..., 4.6223e-05, 1.9933e-04,\n",
      "         1.0474e-04],\n",
      "        [7.6096e-05, 1.3892e-04, 3.1970e-05,  ..., 3.1572e-04, 2.8120e-04,\n",
      "         1.9252e-04],\n",
      "        [7.1089e-05, 1.0006e-04, 6.2700e-04,  ..., 3.1191e-04, 3.5523e-05,\n",
      "         5.5132e-04]], device='cuda:0')\n",
      "pred: tensor([0.8376, 0.8226, 0.7950, 0.8178, 0.7913, 0.8254, 0.8106, 0.8129, 0.7929,\n",
      "        0.8185, 0.7984, 0.8301, 0.8028, 0.8061, 0.8444, 0.8043, 0.8273, 0.8185,\n",
      "        0.7775, 0.7855, 0.8197, 0.8429, 0.8675, 0.8644, 0.8887, 0.8128, 0.8689,\n",
      "        0.8159, 0.8591, 0.8417, 0.8528, 0.8693, 0.8714, 0.8705, 0.8551, 0.8777,\n",
      "        0.8394, 0.8499, 0.8437, 0.8352, 0.8370, 0.8712, 0.8084, 0.8673, 0.8274,\n",
      "        0.7869, 0.8349, 0.8151, 0.8323, 0.8372, 0.7692, 0.7881, 0.7608, 0.7793,\n",
      "        0.7757, 0.7696, 0.8221, 0.8242, 0.7591, 0.8409, 0.7985, 0.8041, 0.7877,\n",
      "        0.8000, 0.8287, 0.8496, 0.7929, 0.7713, 0.8136, 0.8272, 0.8348, 0.7892,\n",
      "        0.8244, 0.8028, 0.7959, 0.8604, 0.8538, 0.8524, 0.8703, 0.8674, 0.8322,\n",
      "        0.8312, 0.8607, 0.8440, 0.8616, 0.8151, 0.7778, 0.8432, 0.7975, 0.8356,\n",
      "        0.8371, 0.8366, 0.8046, 0.8295, 0.8663, 0.8461, 0.8039, 0.8551, 0.8549,\n",
      "        0.8448], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0332, Val-Loss: 0.0738\n",
      "Epoch [20/700], Train-Loss: 0.0142, Val-Loss: 0.0221\n",
      "Epoch [30/700], Train-Loss: 0.0005, Val-Loss: 0.0023\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0010\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.6924e-04, 3.6660e-04, 3.3653e-04,  ..., 5.2609e-05, 8.6044e-05,\n",
      "         9.1533e-05],\n",
      "        [5.4666e-05, 8.4389e-05, 5.0037e-05,  ..., 3.1235e-04, 1.5835e-04,\n",
      "         1.1364e-03],\n",
      "        [1.2121e-04, 1.4708e-04, 2.6816e-04,  ..., 3.0795e-04, 6.3555e-05,\n",
      "         5.6044e-04],\n",
      "        ...,\n",
      "        [2.1922e-04, 2.5819e-04, 1.1232e-04,  ..., 8.9095e-05, 2.2242e-04,\n",
      "         3.7468e-04],\n",
      "        [2.2988e-04, 2.0975e-04, 1.9825e-04,  ..., 3.5623e-04, 7.2125e-05,\n",
      "         8.5479e-04],\n",
      "        [3.9892e-05, 2.9849e-04, 7.1592e-04,  ..., 1.1202e-04, 5.5775e-05,\n",
      "         1.1954e-03]], device='cuda:0')\n",
      "pred: tensor([0.8348, 0.8209, 0.7959, 0.8183, 0.7918, 0.8254, 0.8121, 0.8105, 0.7936,\n",
      "        0.8196, 0.7977, 0.8305, 0.8008, 0.8044, 0.8419, 0.8014, 0.8248, 0.8182,\n",
      "        0.7743, 0.7828, 0.8219, 0.8408, 0.8670, 0.8621, 0.8891, 0.8134, 0.8683,\n",
      "        0.8141, 0.8563, 0.8413, 0.8511, 0.8719, 0.8708, 0.8696, 0.8544, 0.8763,\n",
      "        0.8395, 0.8496, 0.8403, 0.8352, 0.8364, 0.8711, 0.8083, 0.8655, 0.8280,\n",
      "        0.7890, 0.8349, 0.8136, 0.8334, 0.8345, 0.7715, 0.7874, 0.7606, 0.7782,\n",
      "        0.7776, 0.7713, 0.8240, 0.8225, 0.7612, 0.8445, 0.7962, 0.8030, 0.7867,\n",
      "        0.8013, 0.8299, 0.8489, 0.7910, 0.7732, 0.8134, 0.8263, 0.8355, 0.7890,\n",
      "        0.8248, 0.8019, 0.7979, 0.8592, 0.8532, 0.8516, 0.8694, 0.8645, 0.8314,\n",
      "        0.8321, 0.8615, 0.8445, 0.8635, 0.8145, 0.7774, 0.8442, 0.7969, 0.8332,\n",
      "        0.8353, 0.8360, 0.8041, 0.8297, 0.8663, 0.8451, 0.8029, 0.8524, 0.8554,\n",
      "        0.8465], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0036, Val-Loss: 0.0087\n",
      "Epoch [20/700], Train-Loss: 0.0008, Val-Loss: 0.0056\n",
      "Epoch [30/700], Train-Loss: 0.0053, Val-Loss: 0.0022\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0018\n",
      "Epoch [50/700], Train-Loss: 0.0019, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.7606e-05, 2.0566e-04, 3.0411e-04,  ..., 1.8713e-04, 1.2453e-04,\n",
      "         2.5518e-03],\n",
      "        [1.0344e-04, 1.9965e-04, 3.8782e-04,  ..., 7.7442e-05, 9.7667e-05,\n",
      "         7.1533e-04],\n",
      "        [6.7696e-05, 1.2650e-04, 1.5673e-04,  ..., 1.2459e-04, 3.1813e-04,\n",
      "         2.4232e-03],\n",
      "        ...,\n",
      "        [3.8917e-04, 6.7181e-05, 4.7203e-04,  ..., 4.1886e-04, 4.7224e-05,\n",
      "         4.9442e-03],\n",
      "        [7.0460e-05, 3.8565e-05, 9.7494e-05,  ..., 7.8674e-05, 4.2593e-05,\n",
      "         2.6494e-04],\n",
      "        [7.9722e-05, 2.5879e-05, 3.3854e-05,  ..., 1.0589e-04, 2.4470e-05,\n",
      "         1.6347e-04]], device='cuda:0')\n",
      "pred: tensor([0.8382, 0.8214, 0.7964, 0.8184, 0.7930, 0.8264, 0.8122, 0.8108, 0.7912,\n",
      "        0.8198, 0.7973, 0.8285, 0.8014, 0.8040, 0.8418, 0.8011, 0.8255, 0.8182,\n",
      "        0.7763, 0.7831, 0.8218, 0.8406, 0.8665, 0.8633, 0.8876, 0.8128, 0.8699,\n",
      "        0.8169, 0.8582, 0.8419, 0.8503, 0.8727, 0.8711, 0.8710, 0.8562, 0.8748,\n",
      "        0.8400, 0.8474, 0.8419, 0.8383, 0.8375, 0.8698, 0.8077, 0.8672, 0.8274,\n",
      "        0.7893, 0.8304, 0.8136, 0.8311, 0.8367, 0.7705, 0.7871, 0.7603, 0.7783,\n",
      "        0.7774, 0.7687, 0.8242, 0.8238, 0.7592, 0.8414, 0.7977, 0.8043, 0.7873,\n",
      "        0.7999, 0.8289, 0.8473, 0.7912, 0.7734, 0.8125, 0.8270, 0.8348, 0.7882,\n",
      "        0.8251, 0.8014, 0.7970, 0.8611, 0.8539, 0.8509, 0.8719, 0.8680, 0.8309,\n",
      "        0.8306, 0.8601, 0.8439, 0.8632, 0.8154, 0.7784, 0.8427, 0.7970, 0.8362,\n",
      "        0.8364, 0.8356, 0.8044, 0.8321, 0.8663, 0.8464, 0.8012, 0.8519, 0.8547,\n",
      "        0.8459], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9742574257425741, 0.9751815181518151, 0.9726492649264925, 0.9738973897389738, 0.9705250525052503]}\n",
      "Epoch [10/700], Train-Loss: 0.0700, Val-Loss: 0.0239\n",
      "Epoch [20/700], Train-Loss: 0.0167, Val-Loss: 0.0067\n",
      "Epoch [30/700], Train-Loss: 0.0018, Val-Loss: 0.0001\n",
      "Epoch [40/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.6924e-04, 3.6660e-04, 3.3653e-04,  ..., 4.8308e-05, 6.5634e-05,\n",
      "         2.4995e-04],\n",
      "        [5.4666e-05, 8.4389e-05, 5.0037e-05,  ..., 1.7225e-04, 2.8467e-04,\n",
      "         4.0416e-04],\n",
      "        [1.2121e-04, 1.4708e-04, 2.6816e-04,  ..., 2.5197e-04, 1.0540e-04,\n",
      "         2.6050e-04],\n",
      "        ...,\n",
      "        [2.1922e-04, 2.5819e-04, 1.1232e-04,  ..., 2.1208e-04, 1.6257e-04,\n",
      "         1.0109e-04],\n",
      "        [2.2988e-04, 2.0975e-04, 1.9825e-04,  ..., 6.3717e-05, 9.5933e-05,\n",
      "         1.9923e-05],\n",
      "        [3.9892e-05, 2.9849e-04, 7.1592e-04,  ..., 6.2782e-05, 1.1245e-05,\n",
      "         9.5908e-04]], device='cuda:0')\n",
      "pred: tensor([0.8373, 0.8213, 0.7956, 0.8175, 0.7917, 0.8249, 0.8131, 0.8085, 0.7908,\n",
      "        0.8181, 0.7984, 0.8272, 0.7971, 0.8166, 0.8374, 0.8018, 0.8259, 0.8180,\n",
      "        0.7802, 0.7904, 0.8238, 0.8402, 0.8669, 0.8619, 0.8847, 0.8093, 0.8684,\n",
      "        0.8156, 0.8582, 0.8478, 0.8540, 0.8707, 0.8741, 0.8715, 0.8570, 0.8729,\n",
      "        0.8317, 0.8490, 0.8413, 0.8355, 0.8359, 0.8709, 0.8063, 0.8648, 0.8236,\n",
      "        0.7919, 0.8364, 0.8099, 0.8272, 0.8314, 0.7657, 0.7835, 0.7596, 0.7718,\n",
      "        0.7821, 0.7701, 0.8255, 0.8241, 0.7633, 0.8419, 0.7972, 0.8041, 0.7880,\n",
      "        0.8041, 0.8301, 0.8493, 0.7918, 0.7758, 0.8131, 0.8277, 0.8319, 0.7926,\n",
      "        0.8247, 0.8039, 0.7970, 0.8628, 0.8534, 0.8448, 0.8703, 0.8662, 0.8296,\n",
      "        0.8315, 0.8596, 0.8418, 0.8628, 0.8208, 0.7759, 0.8365, 0.7882, 0.8395,\n",
      "        0.8360, 0.8393, 0.8085, 0.8313, 0.8649, 0.8457, 0.8045, 0.8565, 0.8526,\n",
      "        0.8378], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1161, Val-Loss: 0.0759\n",
      "Epoch [20/700], Train-Loss: 0.0322, Val-Loss: 0.0258\n",
      "Epoch [30/700], Train-Loss: 0.0115, Val-Loss: 0.0079\n",
      "Epoch [40/700], Train-Loss: 0.0035, Val-Loss: 0.0024\n",
      "Epoch [50/700], Train-Loss: 0.0009, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[5.2333e-05, 1.5889e-04, 7.1963e-05,  ..., 1.4671e-04, 1.1900e-04,\n",
      "         3.0264e-04],\n",
      "        [4.0778e-05, 1.1261e-04, 1.9490e-04,  ..., 2.8391e-04, 8.7848e-05,\n",
      "         3.7704e-04],\n",
      "        [3.2436e-05, 1.7914e-04, 7.7321e-05,  ..., 1.4277e-04, 2.5983e-04,\n",
      "         2.8514e-04],\n",
      "        ...,\n",
      "        [2.0799e-04, 4.7442e-05, 3.2258e-04,  ..., 1.9371e-04, 6.2090e-05,\n",
      "         2.5594e-05],\n",
      "        [1.0848e-04, 2.0688e-04, 1.6089e-04,  ..., 1.5761e-04, 3.1804e-05,\n",
      "         1.4812e-04],\n",
      "        [8.6779e-05, 1.0128e-04, 4.3450e-04,  ..., 5.7108e-05, 1.5458e-05,\n",
      "         1.4503e-05]], device='cuda:0')\n",
      "pred: tensor([0.8362, 0.8211, 0.7963, 0.8169, 0.7907, 0.8253, 0.8139, 0.8070, 0.7920,\n",
      "        0.8179, 0.7976, 0.8272, 0.7977, 0.8147, 0.8382, 0.8024, 0.8258, 0.8208,\n",
      "        0.7819, 0.7882, 0.8250, 0.8358, 0.8649, 0.8593, 0.8851, 0.8100, 0.8657,\n",
      "        0.8168, 0.8568, 0.8475, 0.8542, 0.8687, 0.8723, 0.8712, 0.8583, 0.8733,\n",
      "        0.8309, 0.8516, 0.8397, 0.8434, 0.8356, 0.8718, 0.8057, 0.8657, 0.8216,\n",
      "        0.7911, 0.8342, 0.8084, 0.8250, 0.8330, 0.7668, 0.7852, 0.7606, 0.7717,\n",
      "        0.7818, 0.7695, 0.8236, 0.8225, 0.7645, 0.8417, 0.7988, 0.8021, 0.7893,\n",
      "        0.8044, 0.8308, 0.8484, 0.7909, 0.7738, 0.8131, 0.8291, 0.8318, 0.7936,\n",
      "        0.8259, 0.8019, 0.7952, 0.8622, 0.8517, 0.8412, 0.8679, 0.8643, 0.8308,\n",
      "        0.8318, 0.8601, 0.8423, 0.8640, 0.8171, 0.7761, 0.8383, 0.7927, 0.8394,\n",
      "        0.8379, 0.8398, 0.8057, 0.8337, 0.8627, 0.8476, 0.8035, 0.8551, 0.8510,\n",
      "        0.8379], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1308, Val-Loss: 0.0897\n",
      "Epoch [20/700], Train-Loss: 0.0365, Val-Loss: 0.0300\n",
      "Epoch [30/700], Train-Loss: 0.0132, Val-Loss: 0.0098\n",
      "Epoch [40/700], Train-Loss: 0.0047, Val-Loss: 0.0039\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.9306e-05, 6.2480e-05, 1.4790e-04,  ..., 3.4154e-03, 7.5337e-05,\n",
      "         1.4488e-04],\n",
      "        [1.8351e-04, 6.8695e-05, 1.0657e-04,  ..., 8.1677e-04, 1.2384e-03,\n",
      "         1.8221e-04],\n",
      "        [6.2672e-05, 3.5119e-04, 2.2016e-04,  ..., 1.5162e-04, 1.9263e-04,\n",
      "         1.0440e-04],\n",
      "        ...,\n",
      "        [3.1973e-05, 1.1183e-04, 4.5029e-05,  ..., 5.6278e-05, 1.8074e-05,\n",
      "         5.7676e-06],\n",
      "        [7.0625e-05, 1.2442e-04, 1.2026e-04,  ..., 3.9346e-04, 1.5371e-04,\n",
      "         1.2937e-04],\n",
      "        [4.2333e-05, 9.4096e-05, 1.4673e-04,  ..., 1.1926e-03, 5.5811e-04,\n",
      "         3.2006e-04]], device='cuda:0')\n",
      "pred: tensor([0.8357, 0.8233, 0.7953, 0.8175, 0.7914, 0.8263, 0.8129, 0.8041, 0.7915,\n",
      "        0.8186, 0.7975, 0.8254, 0.7976, 0.8158, 0.8390, 0.8020, 0.8256, 0.8218,\n",
      "        0.7791, 0.7889, 0.8237, 0.8389, 0.8626, 0.8614, 0.8847, 0.8083, 0.8675,\n",
      "        0.8124, 0.8571, 0.8453, 0.8547, 0.8703, 0.8716, 0.8714, 0.8592, 0.8734,\n",
      "        0.8294, 0.8500, 0.8408, 0.8401, 0.8347, 0.8721, 0.8058, 0.8642, 0.8250,\n",
      "        0.7905, 0.8374, 0.8109, 0.8287, 0.8346, 0.7660, 0.7841, 0.7606, 0.7737,\n",
      "        0.7820, 0.7693, 0.8212, 0.8221, 0.7655, 0.8396, 0.7983, 0.8022, 0.7876,\n",
      "        0.8036, 0.8300, 0.8475, 0.7896, 0.7738, 0.8126, 0.8280, 0.8311, 0.7915,\n",
      "        0.8254, 0.8018, 0.7929, 0.8622, 0.8531, 0.8438, 0.8694, 0.8640, 0.8301,\n",
      "        0.8320, 0.8605, 0.8413, 0.8626, 0.8184, 0.7751, 0.8371, 0.7947, 0.8392,\n",
      "        0.8378, 0.8403, 0.8059, 0.8319, 0.8648, 0.8478, 0.8027, 0.8567, 0.8508,\n",
      "        0.8394], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1053, Val-Loss: 0.0537\n",
      "Epoch [20/700], Train-Loss: 0.0230, Val-Loss: 0.0145\n",
      "Epoch [30/700], Train-Loss: 0.0054, Val-Loss: 0.0018\n",
      "Epoch [40/700], Train-Loss: 0.0014, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.3172e-05, 2.4118e-05, 4.0757e-05,  ..., 4.4427e-04, 4.0578e-03,\n",
      "         1.2776e-04],\n",
      "        [3.4623e-05, 2.2887e-05, 1.3287e-04,  ..., 5.1084e-04, 8.8337e-04,\n",
      "         5.5153e-05],\n",
      "        [1.3047e-04, 9.2673e-05, 6.5799e-05,  ..., 5.7318e-03, 5.3636e-03,\n",
      "         2.1986e-04],\n",
      "        ...,\n",
      "        [7.8846e-05, 2.6499e-05, 4.5562e-05,  ..., 1.2805e-03, 1.3981e-02,\n",
      "         1.9610e-04],\n",
      "        [6.4081e-05, 7.1583e-05, 1.2422e-04,  ..., 1.5981e-04, 4.6434e-03,\n",
      "         1.2140e-05],\n",
      "        [7.5338e-05, 1.9360e-05, 4.1998e-05,  ..., 2.9638e-04, 5.5162e-03,\n",
      "         3.5560e-05]], device='cuda:0')\n",
      "pred: tensor([0.8369, 0.8251, 0.7969, 0.8181, 0.7924, 0.8270, 0.8113, 0.8085, 0.7916,\n",
      "        0.8175, 0.7989, 0.8284, 0.7975, 0.8150, 0.8377, 0.8026, 0.8264, 0.8193,\n",
      "        0.7809, 0.7894, 0.8232, 0.8391, 0.8645, 0.8617, 0.8849, 0.8091, 0.8673,\n",
      "        0.8158, 0.8568, 0.8455, 0.8524, 0.8714, 0.8716, 0.8720, 0.8586, 0.8730,\n",
      "        0.8312, 0.8506, 0.8402, 0.8396, 0.8346, 0.8718, 0.8052, 0.8652, 0.8240,\n",
      "        0.7916, 0.8359, 0.8120, 0.8294, 0.8334, 0.7671, 0.7849, 0.7607, 0.7725,\n",
      "        0.7826, 0.7688, 0.8238, 0.8225, 0.7632, 0.8448, 0.7987, 0.8016, 0.7876,\n",
      "        0.8028, 0.8305, 0.8482, 0.7906, 0.7753, 0.8133, 0.8286, 0.8330, 0.7946,\n",
      "        0.8264, 0.8019, 0.7941, 0.8614, 0.8505, 0.8436, 0.8687, 0.8636, 0.8300,\n",
      "        0.8313, 0.8593, 0.8409, 0.8623, 0.8186, 0.7769, 0.8358, 0.7906, 0.8375,\n",
      "        0.8395, 0.8400, 0.8085, 0.8334, 0.8646, 0.8452, 0.8022, 0.8569, 0.8528,\n",
      "        0.8425], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1160, Val-Loss: 0.0635\n",
      "Epoch [20/700], Train-Loss: 0.0319, Val-Loss: 0.0208\n",
      "Epoch [30/700], Train-Loss: 0.0094, Val-Loss: 0.0041\n",
      "Epoch [40/700], Train-Loss: 0.0028, Val-Loss: 0.0013\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[9.1508e-05, 1.7720e-04, 2.1993e-04,  ..., 6.9479e-05, 1.5548e-04,\n",
      "         6.0007e-05],\n",
      "        [1.2992e-04, 1.4064e-04, 2.0319e-04,  ..., 1.7797e-04, 1.8462e-04,\n",
      "         2.7651e-04],\n",
      "        [1.5698e-04, 2.4595e-04, 2.8927e-04,  ..., 2.4501e-04, 2.7907e-04,\n",
      "         4.9721e-04],\n",
      "        ...,\n",
      "        [1.4576e-04, 2.7715e-04, 1.9816e-04,  ..., 4.0378e-04, 1.4148e-04,\n",
      "         2.5922e-04],\n",
      "        [9.5050e-05, 8.7755e-05, 1.5738e-04,  ..., 3.1735e-04, 1.4274e-04,\n",
      "         3.6967e-04],\n",
      "        [7.7110e-05, 4.1890e-05, 7.8089e-05,  ..., 2.1915e-04, 1.4045e-04,\n",
      "         4.0290e-04]], device='cuda:0')\n",
      "pred: tensor([0.8380, 0.8223, 0.7968, 0.8163, 0.7904, 0.8245, 0.8134, 0.8070, 0.7920,\n",
      "        0.8180, 0.7984, 0.8288, 0.7975, 0.8147, 0.8394, 0.8007, 0.8246, 0.8222,\n",
      "        0.7786, 0.7892, 0.8244, 0.8384, 0.8662, 0.8638, 0.8862, 0.8103, 0.8666,\n",
      "        0.8169, 0.8586, 0.8465, 0.8549, 0.8710, 0.8729, 0.8713, 0.8593, 0.8739,\n",
      "        0.8344, 0.8509, 0.8416, 0.8415, 0.8365, 0.8723, 0.8062, 0.8644, 0.8219,\n",
      "        0.7913, 0.8389, 0.8110, 0.8255, 0.8323, 0.7651, 0.7853, 0.7604, 0.7721,\n",
      "        0.7809, 0.7697, 0.8234, 0.8228, 0.7632, 0.8417, 0.7980, 0.8028, 0.7892,\n",
      "        0.8031, 0.8311, 0.8480, 0.7916, 0.7747, 0.8120, 0.8274, 0.8325, 0.7931,\n",
      "        0.8274, 0.8019, 0.7948, 0.8634, 0.8518, 0.8446, 0.8699, 0.8637, 0.8320,\n",
      "        0.8320, 0.8604, 0.8424, 0.8628, 0.8196, 0.7759, 0.8379, 0.7936, 0.8387,\n",
      "        0.8367, 0.8389, 0.8069, 0.8318, 0.8643, 0.8452, 0.8037, 0.8550, 0.8519,\n",
      "        0.8386], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9815061506150613, 0.9777617761776177, 0.9768736873687368, 0.9794659465946594, 0.9786978697869786]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.7258224445746944, 0.6107193115643375, 0.6502236315814882, 0.737439815118894, 0.4301720596166143]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8755596795694641, 0.7556116500344322, 0.8486451363921058, 0.7446937892707194, 0.6431155251149807]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8132149797729749, 0.8140858112879238, 0.8535617049241815, 0.8526124229041236, 0.8769935962494563]}\n",
      "mean_rank_score_dict: {'imagenet1k': [0.8778102921032491, 0.9104082447184628, 0.8704107896426283, 0.8606711302325086, 0.8901050307732828]}\n",
      "Epoch [10/700], Train-Loss: 0.3502, Val-Loss: 0.3102\n",
      "Epoch [20/700], Train-Loss: 0.0062, Val-Loss: 0.0212\n",
      "Epoch [30/700], Train-Loss: 0.0046, Val-Loss: 0.0009\n",
      "Epoch [40/700], Train-Loss: 0.0129, Val-Loss: 0.0114\n",
      "Epoch [50/700], Train-Loss: 0.0026, Val-Loss: 0.0029\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [80/700], Train-Loss: 0.0005, Val-Loss: 0.0004\n",
      "Epoch [90/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [100/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "x: tensor([[7.5720e-05, 1.6405e-04, 5.2899e-05,  ..., 1.4762e-04, 2.3307e-05,\n",
      "         1.0672e-04],\n",
      "        [2.4927e-04, 1.1381e-04, 2.1071e-04,  ..., 2.1384e-04, 1.0652e-04,\n",
      "         3.4628e-04],\n",
      "        [1.4883e-04, 5.0811e-05, 1.9397e-04,  ..., 1.8418e-04, 2.8980e-04,\n",
      "         2.0489e-04],\n",
      "        ...,\n",
      "        [9.0271e-05, 6.3582e-05, 3.0270e-05,  ..., 5.5535e-04, 2.1129e-04,\n",
      "         1.7198e-04],\n",
      "        [1.2260e-05, 3.1705e-05, 3.3824e-05,  ..., 4.8290e-04, 3.6098e-04,\n",
      "         2.2156e-04],\n",
      "        [1.2677e-04, 7.6321e-05, 1.9021e-04,  ..., 1.5570e-04, 1.8898e-04,\n",
      "         5.3405e-05]], device='cuda:0')\n",
      "pred: tensor([0.8446, 0.8223, 0.7915, 0.8154, 0.7851, 0.8481, 0.8401, 0.7975, 0.8048,\n",
      "        0.8211, 0.8258, 0.8206, 0.8065, 0.8017, 0.8437, 0.7911, 0.8126, 0.8083,\n",
      "        0.7969, 0.8049, 0.8198, 0.8385, 0.8631, 0.8707, 0.8803, 0.8148, 0.8527,\n",
      "        0.8178, 0.8690, 0.8392, 0.8500, 0.8629, 0.8626, 0.8785, 0.8342, 0.8544,\n",
      "        0.8294, 0.8566, 0.8527, 0.8337, 0.8463, 0.9014, 0.8023, 0.8732, 0.8247,\n",
      "        0.7956, 0.8328, 0.8101, 0.8253, 0.8166, 0.7740, 0.7951, 0.7508, 0.7809,\n",
      "        0.7874, 0.7794, 0.8138, 0.8353, 0.7802, 0.8342, 0.7978, 0.8159, 0.7822,\n",
      "        0.7897, 0.8300, 0.8327, 0.8034, 0.7839, 0.8272, 0.8371, 0.8180, 0.8114,\n",
      "        0.8212, 0.8076, 0.7789, 0.8596, 0.8407, 0.8223, 0.8557, 0.8518, 0.8372,\n",
      "        0.8266, 0.8644, 0.8574, 0.8522, 0.8014, 0.7821, 0.8362, 0.8012, 0.8525,\n",
      "        0.8340, 0.8611, 0.8116, 0.8348, 0.8625, 0.8288, 0.8145, 0.9377, 0.8590,\n",
      "        0.8468], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.3181, Val-Loss: 0.2665\n",
      "Epoch [20/700], Train-Loss: 0.0355, Val-Loss: 0.0555\n",
      "Epoch [30/700], Train-Loss: 0.0007, Val-Loss: 0.0019\n",
      "Epoch [40/700], Train-Loss: 0.0097, Val-Loss: 0.0073\n",
      "Epoch [50/700], Train-Loss: 0.0035, Val-Loss: 0.0036\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.9611e-04, 8.8032e-05, 1.2114e-04,  ..., 6.3927e-05, 1.2247e-04,\n",
      "         1.0545e-04],\n",
      "        [3.4916e-04, 1.6832e-04, 1.5207e-04,  ..., 2.1516e-04, 5.3306e-04,\n",
      "         2.9426e-04],\n",
      "        [3.3688e-04, 3.2212e-04, 1.8921e-04,  ..., 1.3251e-04, 8.7110e-05,\n",
      "         4.1837e-04],\n",
      "        ...,\n",
      "        [2.2729e-04, 1.3027e-04, 9.5433e-05,  ..., 1.2385e-04, 2.4683e-04,\n",
      "         3.3312e-04],\n",
      "        [1.9568e-04, 1.8985e-04, 1.0720e-04,  ..., 4.6209e-05, 2.5142e-05,\n",
      "         2.0323e-04],\n",
      "        [7.7545e-05, 1.4119e-04, 1.1613e-04,  ..., 8.8900e-05, 6.0372e-05,\n",
      "         1.0309e-03]], device='cuda:0')\n",
      "pred: tensor([0.8326, 0.8318, 0.8064, 0.8256, 0.7770, 0.8327, 0.8200, 0.8321, 0.7855,\n",
      "        0.8262, 0.7982, 0.8264, 0.7969, 0.8218, 0.8321, 0.8229, 0.8279, 0.8156,\n",
      "        0.7932, 0.8034, 0.8275, 0.8393, 0.8628, 0.8435, 0.8688, 0.7965, 0.8689,\n",
      "        0.8483, 0.8365, 0.8332, 0.8418, 0.8590, 0.8777, 0.8734, 0.8438, 0.8578,\n",
      "        0.8422, 0.8495, 0.8314, 0.8150, 0.8449, 0.8714, 0.7991, 0.8686, 0.8291,\n",
      "        0.8159, 0.8532, 0.8349, 0.8354, 0.8317, 0.7490, 0.7966, 0.7549, 0.7699,\n",
      "        0.7594, 0.7686, 0.8316, 0.8253, 0.7655, 0.8399, 0.8163, 0.8126, 0.7843,\n",
      "        0.8113, 0.8261, 0.8370, 0.7879, 0.7970, 0.8087, 0.8288, 0.8361, 0.8078,\n",
      "        0.8169, 0.8251, 0.8059, 0.8727, 0.8517, 0.8369, 0.8451, 0.8578, 0.8262,\n",
      "        0.8325, 0.8520, 0.8445, 0.8559, 0.8200, 0.7706, 0.8298, 0.8069, 0.8358,\n",
      "        0.8348, 0.8362, 0.8023, 0.8272, 0.8602, 0.8483, 0.7946, 0.8587, 0.8426,\n",
      "        0.8481], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2291, Val-Loss: 0.1768\n",
      "Epoch [20/700], Train-Loss: 0.0654, Val-Loss: 0.0671\n",
      "Epoch [30/700], Train-Loss: 0.0042, Val-Loss: 0.0078\n",
      "Epoch [40/700], Train-Loss: 0.0043, Val-Loss: 0.0023\n",
      "Epoch [50/700], Train-Loss: 0.0035, Val-Loss: 0.0029\n",
      "Epoch [60/700], Train-Loss: 0.0010, Val-Loss: 0.0011\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "x: tensor([[4.1242e-05, 1.7091e-04, 7.0884e-05,  ..., 2.2532e-04, 2.1226e-04,\n",
      "         3.1608e-04],\n",
      "        [1.0101e-04, 1.7780e-04, 1.0864e-04,  ..., 5.7579e-05, 7.0928e-05,\n",
      "         8.3491e-05],\n",
      "        [8.9100e-05, 2.6616e-04, 2.9204e-04,  ..., 6.9584e-04, 2.9212e-04,\n",
      "         1.0211e-03],\n",
      "        ...,\n",
      "        [7.3752e-05, 6.4596e-05, 4.2832e-05,  ..., 6.7136e-04, 1.7950e-04,\n",
      "         1.8901e-04],\n",
      "        [9.3805e-05, 8.9160e-05, 3.8005e-05,  ..., 1.2339e-03, 2.1041e-04,\n",
      "         3.2388e-04],\n",
      "        [5.2086e-05, 7.5330e-05, 3.3403e-05,  ..., 1.6140e-04, 1.4213e-04,\n",
      "         9.1289e-04]], device='cuda:0')\n",
      "pred: tensor([0.8466, 0.7974, 0.7990, 0.7885, 0.7797, 0.8248, 0.8264, 0.8175, 0.8046,\n",
      "        0.8261, 0.8164, 0.8352, 0.8066, 0.8201, 0.8436, 0.8030, 0.8161, 0.8212,\n",
      "        0.7969, 0.7696, 0.8287, 0.8341, 0.8795, 0.8610, 0.9076, 0.8036, 0.8785,\n",
      "        0.8039, 0.8680, 0.8385, 0.8493, 0.8829, 0.8779, 0.8665, 0.8510, 0.8775,\n",
      "        0.8360, 0.8485, 0.8362, 0.8349, 0.8427, 0.8688, 0.7977, 0.8676, 0.8104,\n",
      "        0.7846, 0.8455, 0.8105, 0.8185, 0.8363, 0.7454, 0.7705, 0.7849, 0.7669,\n",
      "        0.7989, 0.7901, 0.8448, 0.8245, 0.7553, 0.8420, 0.8021, 0.8092, 0.7947,\n",
      "        0.8157, 0.8361, 0.8192, 0.8190, 0.8045, 0.8185, 0.8348, 0.8493, 0.8206,\n",
      "        0.8698, 0.8056, 0.8058, 0.8624, 0.8532, 0.8397, 0.8461, 0.8573, 0.8350,\n",
      "        0.8344, 0.8563, 0.8429, 0.8655, 0.8237, 0.7985, 0.8297, 0.7840, 0.8383,\n",
      "        0.8415, 0.8366, 0.8214, 0.8345, 0.8621, 0.8419, 0.7962, 0.8719, 0.8514,\n",
      "        0.8439], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.2816, Val-Loss: 0.2363\n",
      "Epoch [20/700], Train-Loss: 0.0272, Val-Loss: 0.0440\n",
      "Epoch [30/700], Train-Loss: 0.0011, Val-Loss: 0.0010\n",
      "Epoch [40/700], Train-Loss: 0.0097, Val-Loss: 0.0078\n",
      "Epoch [50/700], Train-Loss: 0.0027, Val-Loss: 0.0029\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "x: tensor([[2.6539e-05, 8.4078e-05, 2.9461e-05,  ..., 5.6989e-05, 1.1907e-04,\n",
      "         1.2252e-04],\n",
      "        [1.1801e-04, 7.4769e-05, 1.2247e-04,  ..., 3.2744e-04, 1.6896e-04,\n",
      "         1.0432e-03],\n",
      "        [8.4186e-05, 2.1154e-04, 1.1958e-04,  ..., 1.8195e-04, 4.3684e-04,\n",
      "         1.4757e-03],\n",
      "        ...,\n",
      "        [6.1522e-05, 1.3506e-04, 4.0127e-05,  ..., 3.2423e-04, 1.1177e-04,\n",
      "         1.0261e-03],\n",
      "        [1.0486e-04, 3.5663e-04, 2.4499e-04,  ..., 1.9372e-05, 1.6493e-05,\n",
      "         2.6818e-05],\n",
      "        [2.4176e-06, 2.6175e-05, 8.2245e-05,  ..., 1.4093e-04, 2.7804e-05,\n",
      "         6.1643e-04]], device='cuda:0')\n",
      "pred: tensor([0.8564, 0.8153, 0.7874, 0.8173, 0.7645, 0.8318, 0.8040, 0.8142, 0.7767,\n",
      "        0.8218, 0.7958, 0.8332, 0.7890, 0.8188, 0.8492, 0.7957, 0.8363, 0.8040,\n",
      "        0.7829, 0.7966, 0.8325, 0.8299, 0.8793, 0.8538, 0.8677, 0.8115, 0.8701,\n",
      "        0.8038, 0.8529, 0.8479, 0.8528, 0.8713, 0.8579, 0.8567, 0.8550, 0.8697,\n",
      "        0.8354, 0.8391, 0.8436, 0.8219, 0.8580, 0.8759, 0.8126, 0.8588, 0.8587,\n",
      "        0.8135, 0.8529, 0.8175, 0.8506, 0.8391, 0.7836, 0.8106, 0.7698, 0.7934,\n",
      "        0.7930, 0.7716, 0.8289, 0.8279, 0.7381, 0.8498, 0.8170, 0.8054, 0.7938,\n",
      "        0.8238, 0.8254, 0.8471, 0.7806, 0.7588, 0.8292, 0.8327, 0.8117, 0.7982,\n",
      "        0.8251, 0.7981, 0.7947, 0.8698, 0.8562, 0.8528, 0.8679, 0.8663, 0.8284,\n",
      "        0.8356, 0.8480, 0.8391, 0.8584, 0.8041, 0.7951, 0.8103, 0.7973, 0.8345,\n",
      "        0.8331, 0.8278, 0.7837, 0.8314, 0.8677, 0.8340, 0.7916, 0.8564, 0.8396,\n",
      "        0.8401], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1986, Val-Loss: 0.1448\n",
      "Epoch [20/700], Train-Loss: 0.0696, Val-Loss: 0.0647\n",
      "Epoch [30/700], Train-Loss: 0.0074, Val-Loss: 0.0115\n",
      "Epoch [40/700], Train-Loss: 0.0028, Val-Loss: 0.0012\n",
      "Epoch [50/700], Train-Loss: 0.0033, Val-Loss: 0.0022\n",
      "Epoch [60/700], Train-Loss: 0.0013, Val-Loss: 0.0013\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [370/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [380/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [390/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [400/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [410/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [420/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [430/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [440/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [450/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [460/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "x: tensor([[9.6445e-05, 2.1400e-04, 2.7250e-04,  ..., 9.3479e-05, 1.7385e-04,\n",
      "         2.4494e-04],\n",
      "        [1.6570e-04, 5.3047e-04, 1.9858e-04,  ..., 1.9058e-05, 5.0288e-05,\n",
      "         4.0332e-04],\n",
      "        [7.5739e-05, 1.6422e-03, 5.5590e-04,  ..., 2.8150e-04, 5.1708e-04,\n",
      "         2.2513e-04],\n",
      "        ...,\n",
      "        [1.1964e-04, 1.8272e-04, 3.4338e-04,  ..., 9.0344e-05, 9.5954e-05,\n",
      "         4.0201e-04],\n",
      "        [2.1383e-04, 5.7130e-04, 2.5895e-04,  ..., 3.5451e-05, 5.0104e-05,\n",
      "         6.9126e-04],\n",
      "        [2.4535e-04, 2.4102e-04, 5.9655e-05,  ..., 1.6908e-05, 2.4087e-05,\n",
      "         1.8587e-04]], device='cuda:0')\n",
      "pred: tensor([0.8393, 0.8227, 0.8104, 0.8223, 0.8082, 0.8034, 0.8064, 0.8224, 0.7711,\n",
      "        0.8191, 0.8012, 0.8371, 0.7442, 0.8138, 0.8539, 0.7942, 0.8362, 0.8291,\n",
      "        0.7773, 0.7963, 0.8199, 0.8452, 0.8557, 0.8843, 0.8900, 0.8052, 0.8636,\n",
      "        0.8300, 0.8543, 0.8640, 0.8436, 0.8734, 0.8561, 0.8616, 0.8755, 0.8668,\n",
      "        0.8549, 0.8558, 0.8211, 0.8397, 0.8580, 0.8947, 0.8062, 0.8548, 0.8421,\n",
      "        0.8015, 0.8486, 0.8425, 0.8452, 0.8537, 0.7577, 0.7804, 0.7419, 0.7852,\n",
      "        0.7779, 0.7850, 0.8096, 0.8157, 0.7448, 0.8388, 0.7799, 0.7974, 0.7821,\n",
      "        0.8021, 0.8360, 0.8698, 0.7711, 0.7976, 0.8099, 0.8339, 0.8379, 0.7936,\n",
      "        0.8216, 0.7911, 0.7849, 0.8626, 0.8802, 0.8471, 0.8665, 0.8633, 0.8337,\n",
      "        0.8375, 0.8583, 0.8451, 0.8566, 0.8254, 0.7875, 0.8547, 0.8073, 0.8437,\n",
      "        0.8465, 0.8417, 0.8075, 0.8441, 0.8581, 0.8551, 0.8057, 0.8556, 0.8364,\n",
      "        0.8386], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.892085208520852, 0.9368736873687368, 0.9223762376237622, 0.9172397239723971, 0.9248124812481248]}\n",
      "Epoch [10/700], Train-Loss: 0.1157, Val-Loss: 0.0676\n",
      "Epoch [20/700], Train-Loss: 0.0492, Val-Loss: 0.0307\n",
      "Epoch [30/700], Train-Loss: 0.0169, Val-Loss: 0.0177\n",
      "Epoch [40/700], Train-Loss: 0.0010, Val-Loss: 0.0019\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[3.4129e-05, 4.6669e-05, 1.0080e-04,  ..., 1.0379e-04, 1.4734e-04,\n",
      "         2.7394e-04],\n",
      "        [1.3737e-04, 2.6611e-04, 2.4091e-04,  ..., 1.6792e-04, 1.7349e-04,\n",
      "         1.2998e-04],\n",
      "        [5.6128e-05, 8.5935e-05, 4.5172e-05,  ..., 1.6710e-04, 1.4881e-04,\n",
      "         3.4133e-04],\n",
      "        ...,\n",
      "        [9.0731e-05, 1.0817e-04, 3.3308e-04,  ..., 1.2917e-04, 9.0360e-05,\n",
      "         1.9892e-04],\n",
      "        [7.9334e-05, 1.9487e-04, 9.8981e-05,  ..., 9.0025e-05, 1.7487e-04,\n",
      "         1.2949e-04],\n",
      "        [2.9269e-05, 1.1880e-04, 1.3157e-04,  ..., 2.3630e-04, 8.5414e-05,\n",
      "         2.1997e-04]], device='cuda:0')\n",
      "pred: tensor([0.8433, 0.8286, 0.7923, 0.8179, 0.7795, 0.8382, 0.8113, 0.7974, 0.7881,\n",
      "        0.8238, 0.7954, 0.8431, 0.7973, 0.8304, 0.8563, 0.8148, 0.8182, 0.8214,\n",
      "        0.7809, 0.7889, 0.8180, 0.8243, 0.8611, 0.8336, 0.8710, 0.7982, 0.8643,\n",
      "        0.8008, 0.8492, 0.8482, 0.8464, 0.8751, 0.8598, 0.8625, 0.8392, 0.8742,\n",
      "        0.8236, 0.8475, 0.8385, 0.8132, 0.8370, 0.8783, 0.7931, 0.8614, 0.8202,\n",
      "        0.7953, 0.8376, 0.8217, 0.8227, 0.8178, 0.7687, 0.7798, 0.7581, 0.7866,\n",
      "        0.7817, 0.7868, 0.8444, 0.8247, 0.7624, 0.8458, 0.7835, 0.8074, 0.7878,\n",
      "        0.8040, 0.8215, 0.8539, 0.7901, 0.7670, 0.8159, 0.8188, 0.8297, 0.7868,\n",
      "        0.8226, 0.8094, 0.7824, 0.8562, 0.8507, 0.8328, 0.8532, 0.8518, 0.8311,\n",
      "        0.8277, 0.8517, 0.8426, 0.8548, 0.8155, 0.7739, 0.8489, 0.7890, 0.8484,\n",
      "        0.8247, 0.8352, 0.8125, 0.8286, 0.8710, 0.8549, 0.8004, 0.8436, 0.8504,\n",
      "        0.8192], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0787, Val-Loss: 0.0377\n",
      "Epoch [20/700], Train-Loss: 0.0309, Val-Loss: 0.0150\n",
      "Epoch [30/700], Train-Loss: 0.0164, Val-Loss: 0.0155\n",
      "Epoch [40/700], Train-Loss: 0.0025, Val-Loss: 0.0035\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.0518e-04, 1.7102e-04, 1.6635e-04,  ..., 8.3476e-05, 1.5794e-04,\n",
      "         6.0849e-05],\n",
      "        [2.6951e-04, 1.5675e-04, 7.2063e-05,  ..., 6.5638e-05, 1.3337e-04,\n",
      "         1.7106e-04],\n",
      "        [2.2589e-04, 1.3056e-04, 1.8219e-04,  ..., 3.7183e-05, 9.1148e-05,\n",
      "         9.0662e-05],\n",
      "        ...,\n",
      "        [5.5017e-05, 1.3897e-04, 1.9017e-04,  ..., 2.5575e-05, 2.1600e-04,\n",
      "         5.4922e-05],\n",
      "        [1.6074e-04, 9.9899e-05, 1.1084e-04,  ..., 6.6215e-05, 2.1655e-05,\n",
      "         4.7614e-05],\n",
      "        [6.8362e-05, 1.2046e-04, 3.2665e-04,  ..., 3.0229e-05, 6.6397e-05,\n",
      "         1.8216e-04]], device='cuda:0')\n",
      "pred: tensor([0.8272, 0.8400, 0.8108, 0.8031, 0.7815, 0.8395, 0.7980, 0.8177, 0.7999,\n",
      "        0.8292, 0.8011, 0.8363, 0.8012, 0.8246, 0.8497, 0.8189, 0.8315, 0.8297,\n",
      "        0.7823, 0.7856, 0.8326, 0.8334, 0.8549, 0.8555, 0.8858, 0.8224, 0.8681,\n",
      "        0.8136, 0.8438, 0.8463, 0.8483, 0.8720, 0.8641, 0.8637, 0.8591, 0.8797,\n",
      "        0.8524, 0.8446, 0.8439, 0.8300, 0.8379, 0.8934, 0.7977, 0.8581, 0.8431,\n",
      "        0.7990, 0.8483, 0.8281, 0.8448, 0.8463, 0.7624, 0.7806, 0.7563, 0.7768,\n",
      "        0.7962, 0.7667, 0.8379, 0.8305, 0.7551, 0.8410, 0.8076, 0.8121, 0.7968,\n",
      "        0.8081, 0.8276, 0.8483, 0.7993, 0.7806, 0.8187, 0.8245, 0.8286, 0.7995,\n",
      "        0.8271, 0.8124, 0.7950, 0.8630, 0.8587, 0.8561, 0.8635, 0.8626, 0.8341,\n",
      "        0.8323, 0.8627, 0.8426, 0.8613, 0.8212, 0.7815, 0.8365, 0.7962, 0.8483,\n",
      "        0.8651, 0.8417, 0.8017, 0.8378, 0.8461, 0.8529, 0.7854, 0.8556, 0.8399,\n",
      "        0.8404], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1909, Val-Loss: 0.1284\n",
      "Epoch [20/700], Train-Loss: 0.0770, Val-Loss: 0.0568\n",
      "Epoch [30/700], Train-Loss: 0.0182, Val-Loss: 0.0208\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0011\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0011, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0006, Val-Loss: 0.0006\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[3.6241e-05, 3.5767e-05, 4.0069e-05,  ..., 1.2924e-04, 1.0963e-04,\n",
      "         1.1316e-04],\n",
      "        [9.1161e-05, 5.6129e-05, 6.7866e-05,  ..., 9.4284e-05, 1.5440e-04,\n",
      "         1.3254e-04],\n",
      "        [2.2841e-04, 1.6225e-04, 2.8290e-04,  ..., 3.5652e-04, 2.1815e-04,\n",
      "         3.3045e-04],\n",
      "        ...,\n",
      "        [1.0163e-04, 9.6272e-05, 1.3316e-04,  ..., 2.1363e-04, 1.3034e-04,\n",
      "         1.6734e-04],\n",
      "        [4.6244e-05, 4.9381e-05, 1.0067e-04,  ..., 8.0383e-05, 9.8582e-05,\n",
      "         1.3350e-04],\n",
      "        [1.4146e-04, 4.0530e-05, 6.9525e-05,  ..., 9.5714e-05, 8.7610e-05,\n",
      "         7.8964e-05]], device='cuda:0')\n",
      "pred: tensor([0.8482, 0.8361, 0.7983, 0.7974, 0.7980, 0.8203, 0.7930, 0.8289, 0.7902,\n",
      "        0.8207, 0.8060, 0.8312, 0.8057, 0.8254, 0.8343, 0.8101, 0.8304, 0.8374,\n",
      "        0.7948, 0.7967, 0.8183, 0.8455, 0.8668, 0.8534, 0.8871, 0.8075, 0.8700,\n",
      "        0.8356, 0.8464, 0.8491, 0.8385, 0.8762, 0.8681, 0.8779, 0.8424, 0.8809,\n",
      "        0.8302, 0.8604, 0.8485, 0.8539, 0.8353, 0.8689, 0.8065, 0.8682, 0.8265,\n",
      "        0.8006, 0.8356, 0.8306, 0.8245, 0.8413, 0.7648, 0.7892, 0.7612, 0.7698,\n",
      "        0.8025, 0.7832, 0.8414, 0.8235, 0.7444, 0.8422, 0.8068, 0.8060, 0.7736,\n",
      "        0.8115, 0.8286, 0.8454, 0.7941, 0.7910, 0.8110, 0.8284, 0.8330, 0.8004,\n",
      "        0.8335, 0.7861, 0.7801, 0.8547, 0.8580, 0.8592, 0.8458, 0.8531, 0.8232,\n",
      "        0.8253, 0.8681, 0.8470, 0.8618, 0.8176, 0.7900, 0.8613, 0.8041, 0.8422,\n",
      "        0.8397, 0.8346, 0.8102, 0.8359, 0.8565, 0.8542, 0.8006, 0.8475, 0.8411,\n",
      "        0.8393], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1322, Val-Loss: 0.0760\n",
      "Epoch [20/700], Train-Loss: 0.0552, Val-Loss: 0.0332\n",
      "Epoch [30/700], Train-Loss: 0.0203, Val-Loss: 0.0210\n",
      "Epoch [40/700], Train-Loss: 0.0014, Val-Loss: 0.0026\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.7727e-04, 6.0806e-05, 8.2172e-05,  ..., 1.3181e-04, 7.1741e-05,\n",
      "         2.2138e-04],\n",
      "        [2.2655e-04, 8.3063e-05, 6.3449e-05,  ..., 7.6677e-05, 9.7526e-05,\n",
      "         2.3684e-04],\n",
      "        [1.4414e-04, 2.9353e-05, 1.4872e-04,  ..., 2.6546e-04, 1.6819e-04,\n",
      "         4.1787e-04],\n",
      "        ...,\n",
      "        [2.0324e-04, 1.6038e-04, 7.1095e-05,  ..., 5.8834e-05, 1.4737e-04,\n",
      "         1.8976e-04],\n",
      "        [1.8597e-04, 1.1732e-04, 5.0032e-05,  ..., 1.2978e-04, 8.2785e-05,\n",
      "         2.4846e-04],\n",
      "        [3.8397e-04, 6.4346e-05, 1.6615e-04,  ..., 2.0007e-04, 1.4259e-04,\n",
      "         4.2596e-04]], device='cuda:0')\n",
      "pred: tensor([0.8417, 0.8127, 0.7862, 0.8180, 0.7902, 0.8295, 0.8273, 0.8243, 0.7783,\n",
      "        0.8204, 0.7906, 0.8337, 0.7962, 0.8107, 0.8511, 0.8011, 0.8267, 0.8210,\n",
      "        0.7844, 0.7926, 0.8219, 0.8341, 0.8671, 0.8490, 0.8963, 0.8007, 0.8679,\n",
      "        0.8305, 0.8354, 0.8286, 0.8543, 0.8779, 0.8723, 0.8705, 0.8583, 0.8811,\n",
      "        0.8288, 0.8474, 0.8355, 0.8381, 0.8383, 0.8854, 0.7957, 0.8789, 0.8390,\n",
      "        0.7922, 0.8308, 0.8164, 0.8311, 0.8392, 0.7595, 0.8123, 0.7648, 0.7732,\n",
      "        0.7687, 0.7773, 0.8187, 0.8289, 0.7494, 0.8387, 0.8002, 0.8002, 0.7918,\n",
      "        0.8139, 0.8334, 0.8519, 0.8001, 0.7738, 0.8104, 0.8298, 0.8302, 0.8078,\n",
      "        0.8198, 0.7904, 0.8040, 0.8528, 0.8343, 0.8353, 0.8565, 0.8424, 0.8306,\n",
      "        0.8323, 0.8690, 0.8411, 0.8651, 0.8037, 0.7828, 0.8383, 0.8003, 0.8303,\n",
      "        0.8355, 0.8351, 0.8083, 0.8252, 0.8771, 0.8416, 0.8015, 0.8452, 0.8665,\n",
      "        0.8419], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1254, Val-Loss: 0.0718\n",
      "Epoch [20/700], Train-Loss: 0.0495, Val-Loss: 0.0295\n",
      "Epoch [30/700], Train-Loss: 0.0180, Val-Loss: 0.0185\n",
      "Epoch [40/700], Train-Loss: 0.0015, Val-Loss: 0.0026\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [90/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [100/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [110/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.9628e-04, 9.4289e-05, 7.5468e-05,  ..., 1.1617e-04, 1.2600e-04,\n",
      "         1.1934e-04],\n",
      "        [1.4265e-03, 3.2084e-04, 4.9873e-04,  ..., 7.5685e-05, 1.7268e-04,\n",
      "         2.5480e-04],\n",
      "        [1.2651e-03, 8.9580e-04, 2.1759e-02,  ..., 1.1974e-04, 2.9832e-04,\n",
      "         1.9809e-04],\n",
      "        ...,\n",
      "        [4.3193e-04, 3.2215e-04, 1.4392e-04,  ..., 1.3608e-04, 1.2588e-04,\n",
      "         7.3359e-04],\n",
      "        [3.6738e-04, 7.8720e-05, 1.0465e-04,  ..., 4.4285e-05, 4.9655e-05,\n",
      "         1.1498e-04],\n",
      "        [4.9653e-04, 9.8249e-05, 5.7752e-04,  ..., 6.0907e-05, 1.5023e-04,\n",
      "         8.3889e-05]], device='cuda:0')\n",
      "pred: tensor([0.8503, 0.8263, 0.7961, 0.8203, 0.7868, 0.8249, 0.8315, 0.8189, 0.7996,\n",
      "        0.8181, 0.7958, 0.8378, 0.7987, 0.8241, 0.8439, 0.8160, 0.8298, 0.8243,\n",
      "        0.7920, 0.8065, 0.8140, 0.8374, 0.8775, 0.8469, 0.8692, 0.8058, 0.8667,\n",
      "        0.8147, 0.8446, 0.8461, 0.8439, 0.8808, 0.8737, 0.8755, 0.8401, 0.8645,\n",
      "        0.8310, 0.8402, 0.8311, 0.8069, 0.8425, 0.8569, 0.7975, 0.8683, 0.8315,\n",
      "        0.7988, 0.8450, 0.8199, 0.8323, 0.8356, 0.7784, 0.7955, 0.7433, 0.7762,\n",
      "        0.8026, 0.7890, 0.8225, 0.8255, 0.7582, 0.8405, 0.8018, 0.8134, 0.7861,\n",
      "        0.8087, 0.8368, 0.8402, 0.7928, 0.7805, 0.8255, 0.8312, 0.8245, 0.7793,\n",
      "        0.8222, 0.8025, 0.7970, 0.8603, 0.8603, 0.8443, 0.8659, 0.8583, 0.8328,\n",
      "        0.8316, 0.8530, 0.8359, 0.8723, 0.8219, 0.7858, 0.8463, 0.7748, 0.8299,\n",
      "        0.8312, 0.8385, 0.8167, 0.8399, 0.8561, 0.8269, 0.8057, 0.8611, 0.8439,\n",
      "        0.8456], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9413861386138612, 0.9382298229822982, 0.933153315331533, 0.9417221722172215, 0.9484668466846683]}\n",
      "Epoch [10/700], Train-Loss: 0.0522, Val-Loss: 0.0898\n",
      "Epoch [20/700], Train-Loss: 0.0188, Val-Loss: 0.0256\n",
      "Epoch [30/700], Train-Loss: 0.0015, Val-Loss: 0.0039\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0015\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.6142e-04, 6.0552e-04, 2.5388e-04,  ..., 1.1355e-04, 6.9917e-05,\n",
      "         3.2801e-05],\n",
      "        [6.0500e-05, 2.7606e-04, 2.5487e-04,  ..., 1.9917e-04, 4.5101e-04,\n",
      "         1.3186e-04],\n",
      "        [2.2025e-04, 2.8151e-04, 1.7249e-04,  ..., 1.5503e-04, 1.8221e-04,\n",
      "         3.0997e-04],\n",
      "        ...,\n",
      "        [4.6487e-05, 5.2671e-04, 1.5936e-04,  ..., 1.2107e-04, 1.6499e-04,\n",
      "         1.1456e-04],\n",
      "        [8.6989e-05, 1.3230e-04, 4.0702e-04,  ..., 6.1663e-05, 3.2118e-04,\n",
      "         1.1048e-04],\n",
      "        [6.1371e-05, 1.2759e-04, 7.2251e-05,  ..., 2.0984e-04, 3.5687e-05,\n",
      "         3.2249e-05]], device='cuda:0')\n",
      "pred: tensor([0.8446, 0.8144, 0.7977, 0.8126, 0.7862, 0.8305, 0.8162, 0.8294, 0.7862,\n",
      "        0.8130, 0.7948, 0.8238, 0.7946, 0.8188, 0.8335, 0.8006, 0.8205, 0.8232,\n",
      "        0.7973, 0.7938, 0.8208, 0.8325, 0.8679, 0.8549, 0.8871, 0.8051, 0.8624,\n",
      "        0.8203, 0.8473, 0.8469, 0.8466, 0.8739, 0.8584, 0.8796, 0.8557, 0.8785,\n",
      "        0.8346, 0.8544, 0.8325, 0.8270, 0.8317, 0.8778, 0.8015, 0.8774, 0.8266,\n",
      "        0.8040, 0.8509, 0.8189, 0.8205, 0.8433, 0.7629, 0.7755, 0.7641, 0.7776,\n",
      "        0.7833, 0.7860, 0.8309, 0.8302, 0.7677, 0.8388, 0.8042, 0.8088, 0.7750,\n",
      "        0.8032, 0.8322, 0.8516, 0.7948, 0.7821, 0.8126, 0.8302, 0.8361, 0.7945,\n",
      "        0.8280, 0.7993, 0.7906, 0.8538, 0.8470, 0.8450, 0.8583, 0.8576, 0.8262,\n",
      "        0.8224, 0.8586, 0.8331, 0.8577, 0.8073, 0.7779, 0.8477, 0.7915, 0.8424,\n",
      "        0.8376, 0.8482, 0.8030, 0.8411, 0.8708, 0.8513, 0.8112, 0.8593, 0.8503,\n",
      "        0.8418], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0424, Val-Loss: 0.0904\n",
      "Epoch [20/700], Train-Loss: 0.0182, Val-Loss: 0.0268\n",
      "Epoch [30/700], Train-Loss: 0.0007, Val-Loss: 0.0029\n",
      "Epoch [40/700], Train-Loss: 0.0003, Val-Loss: 0.0009\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.8978e-04, 2.9328e-04, 3.0109e-04,  ..., 6.3809e-05, 5.0256e-05,\n",
      "         1.1286e-04],\n",
      "        [1.2114e-04, 4.3829e-04, 1.1018e-04,  ..., 3.2386e-04, 3.4880e-04,\n",
      "         9.8726e-04],\n",
      "        [1.6025e-04, 5.4889e-04, 7.7021e-05,  ..., 1.6902e-04, 9.1095e-05,\n",
      "         9.0704e-03],\n",
      "        ...,\n",
      "        [4.5152e-04, 2.3922e-04, 3.2200e-04,  ..., 7.1779e-05, 1.0915e-04,\n",
      "         2.1076e-04],\n",
      "        [1.2817e-04, 2.8295e-04, 8.3063e-05,  ..., 1.6875e-04, 1.3330e-04,\n",
      "         3.7877e-04],\n",
      "        [7.6059e-05, 3.5446e-04, 2.8805e-04,  ..., 5.3473e-05, 3.3544e-05,\n",
      "         4.6271e-04]], device='cuda:0')\n",
      "pred: tensor([0.8436, 0.8365, 0.7911, 0.8115, 0.7868, 0.8274, 0.8046, 0.8219, 0.7948,\n",
      "        0.8242, 0.7927, 0.8279, 0.7896, 0.8088, 0.8426, 0.8115, 0.8352, 0.8214,\n",
      "        0.7984, 0.7960, 0.8320, 0.8329, 0.8658, 0.8588, 0.8910, 0.8005, 0.8787,\n",
      "        0.8115, 0.8452, 0.8457, 0.8529, 0.8677, 0.8630, 0.8651, 0.8601, 0.8798,\n",
      "        0.8383, 0.8473, 0.8482, 0.8241, 0.8321, 0.8900, 0.7943, 0.8586, 0.8369,\n",
      "        0.8074, 0.8464, 0.8360, 0.8447, 0.8389, 0.7685, 0.7862, 0.7678, 0.7851,\n",
      "        0.8024, 0.7783, 0.8361, 0.8292, 0.7476, 0.8416, 0.7951, 0.8080, 0.7791,\n",
      "        0.8024, 0.8303, 0.8566, 0.7901, 0.7772, 0.8254, 0.8253, 0.8355, 0.8109,\n",
      "        0.8340, 0.8104, 0.8010, 0.8639, 0.8548, 0.8443, 0.8634, 0.8539, 0.8302,\n",
      "        0.8318, 0.8541, 0.8432, 0.8596, 0.8224, 0.7649, 0.8427, 0.7983, 0.8376,\n",
      "        0.8381, 0.8464, 0.8050, 0.8374, 0.8683, 0.8465, 0.8084, 0.8515, 0.8530,\n",
      "        0.8368], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0088, Val-Loss: 0.0406\n",
      "Epoch [20/700], Train-Loss: 0.0040, Val-Loss: 0.0099\n",
      "Epoch [30/700], Train-Loss: 0.0018, Val-Loss: 0.0005\n",
      "Epoch [40/700], Train-Loss: 0.0016, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0011, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.2353e-04, 4.2411e-04, 1.7499e-04,  ..., 6.0362e-05, 8.7518e-05,\n",
      "         9.2198e-05],\n",
      "        [6.5751e-05, 9.5356e-05, 5.1011e-05,  ..., 2.4086e-04, 2.0392e-04,\n",
      "         2.0224e-04],\n",
      "        [1.1745e-04, 2.0531e-04, 4.3191e-04,  ..., 2.4395e-04, 6.9098e-05,\n",
      "         6.2381e-04],\n",
      "        ...,\n",
      "        [7.6483e-05, 8.2832e-05, 1.4968e-04,  ..., 3.5442e-04, 9.0281e-05,\n",
      "         5.2238e-05],\n",
      "        [6.5890e-05, 1.9063e-04, 3.5642e-05,  ..., 1.2105e-04, 5.4469e-05,\n",
      "         2.5957e-04],\n",
      "        [3.5726e-05, 5.3646e-05, 1.0378e-04,  ..., 9.7939e-05, 8.4917e-05,\n",
      "         6.2098e-05]], device='cuda:0')\n",
      "pred: tensor([0.8454, 0.8260, 0.8019, 0.8185, 0.7864, 0.8318, 0.8050, 0.8199, 0.7829,\n",
      "        0.8299, 0.7887, 0.8371, 0.7988, 0.8203, 0.8311, 0.8082, 0.8355, 0.8247,\n",
      "        0.7934, 0.7957, 0.8170, 0.8384, 0.8680, 0.8508, 0.8898, 0.8059, 0.8713,\n",
      "        0.8243, 0.8468, 0.8445, 0.8448, 0.8709, 0.8684, 0.8709, 0.8519, 0.8818,\n",
      "        0.8396, 0.8577, 0.8462, 0.8243, 0.8425, 0.8739, 0.7951, 0.8636, 0.8395,\n",
      "        0.8029, 0.8311, 0.8220, 0.8380, 0.8285, 0.7720, 0.7878, 0.7469, 0.7712,\n",
      "        0.8039, 0.7663, 0.8223, 0.8167, 0.7651, 0.8331, 0.7946, 0.7978, 0.7880,\n",
      "        0.8005, 0.8254, 0.8455, 0.7771, 0.7703, 0.8142, 0.8257, 0.8444, 0.8065,\n",
      "        0.8307, 0.7924, 0.7920, 0.8722, 0.8628, 0.8483, 0.8703, 0.8585, 0.8271,\n",
      "        0.8303, 0.8548, 0.8464, 0.8657, 0.8215, 0.7800, 0.8459, 0.7794, 0.8486,\n",
      "        0.8472, 0.8388, 0.8082, 0.8281, 0.8723, 0.8349, 0.8080, 0.8530, 0.8405,\n",
      "        0.8267], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0017, Val-Loss: 0.0294\n",
      "Epoch [20/700], Train-Loss: 0.0034, Val-Loss: 0.0117\n",
      "Epoch [30/700], Train-Loss: 0.0045, Val-Loss: 0.0012\n",
      "Epoch [40/700], Train-Loss: 0.0033, Val-Loss: 0.0012\n",
      "Epoch [50/700], Train-Loss: 0.0020, Val-Loss: 0.0012\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[9.1518e-05, 9.4527e-05, 1.7355e-04,  ..., 5.7219e-05, 9.4567e-05,\n",
      "         6.8570e-05],\n",
      "        [8.5545e-05, 1.0460e-04, 1.4323e-04,  ..., 1.2162e-04, 1.1648e-04,\n",
      "         1.9440e-04],\n",
      "        [1.5257e-04, 1.3720e-04, 9.9843e-05,  ..., 7.2584e-05, 7.7482e-04,\n",
      "         4.6194e-04],\n",
      "        ...,\n",
      "        [1.0274e-04, 2.0558e-04, 1.1855e-04,  ..., 1.8257e-04, 1.4472e-04,\n",
      "         1.5528e-04],\n",
      "        [5.0458e-05, 5.5383e-05, 8.8561e-05,  ..., 9.7169e-05, 9.4775e-05,\n",
      "         9.4582e-05],\n",
      "        [1.3503e-04, 1.0770e-04, 1.8057e-04,  ..., 8.5891e-05, 9.1755e-05,\n",
      "         1.6551e-04]], device='cuda:0')\n",
      "pred: tensor([0.8356, 0.8337, 0.7888, 0.8164, 0.7892, 0.8363, 0.8040, 0.8000, 0.7865,\n",
      "        0.8130, 0.7984, 0.8362, 0.7945, 0.8135, 0.8332, 0.8084, 0.8234, 0.8152,\n",
      "        0.7909, 0.7832, 0.8188, 0.8335, 0.8652, 0.8520, 0.8745, 0.8095, 0.8649,\n",
      "        0.8292, 0.8445, 0.8475, 0.8373, 0.8642, 0.8565, 0.8619, 0.8443, 0.8683,\n",
      "        0.8281, 0.8550, 0.8437, 0.8255, 0.8387, 0.8699, 0.7926, 0.8755, 0.8201,\n",
      "        0.7891, 0.8362, 0.8251, 0.8263, 0.8397, 0.7482, 0.7952, 0.7686, 0.7762,\n",
      "        0.7888, 0.7804, 0.8337, 0.8261, 0.7449, 0.8355, 0.8038, 0.8009, 0.7761,\n",
      "        0.8051, 0.8377, 0.8486, 0.8001, 0.7735, 0.8133, 0.8207, 0.8330, 0.8039,\n",
      "        0.8316, 0.7982, 0.8002, 0.8682, 0.8484, 0.8488, 0.8580, 0.8483, 0.8256,\n",
      "        0.8342, 0.8669, 0.8470, 0.8591, 0.8280, 0.7670, 0.8365, 0.7841, 0.8439,\n",
      "        0.8411, 0.8408, 0.8075, 0.8394, 0.8615, 0.8424, 0.8113, 0.8643, 0.8521,\n",
      "        0.8318], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0022, Val-Loss: 0.0236\n",
      "Epoch [20/700], Train-Loss: 0.0013, Val-Loss: 0.0052\n",
      "Epoch [30/700], Train-Loss: 0.0046, Val-Loss: 0.0021\n",
      "Epoch [40/700], Train-Loss: 0.0034, Val-Loss: 0.0018\n",
      "Epoch [50/700], Train-Loss: 0.0017, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.7286e-05, 5.6590e-05, 7.1733e-05,  ..., 1.5386e-04, 2.0832e-05,\n",
      "         3.2708e-04],\n",
      "        [7.8353e-05, 1.6361e-04, 5.9288e-05,  ..., 5.4640e-05, 5.6725e-05,\n",
      "         2.7293e-04],\n",
      "        [2.7867e-05, 2.7673e-04, 4.7260e-05,  ..., 9.5449e-05, 5.8147e-05,\n",
      "         5.6628e-04],\n",
      "        ...,\n",
      "        [1.1633e-04, 1.0234e-04, 8.1382e-05,  ..., 2.4658e-04, 1.6723e-05,\n",
      "         9.9327e-04],\n",
      "        [9.7585e-05, 8.9247e-05, 8.1671e-05,  ..., 1.3089e-04, 7.2208e-05,\n",
      "         1.6809e-03],\n",
      "        [7.2710e-05, 1.8198e-04, 1.3154e-04,  ..., 1.0234e-04, 7.4516e-06,\n",
      "         1.6194e-03]], device='cuda:0')\n",
      "pred: tensor([0.8417, 0.8320, 0.8020, 0.8063, 0.7883, 0.8158, 0.8120, 0.8060, 0.7846,\n",
      "        0.8133, 0.7874, 0.8230, 0.7978, 0.8188, 0.8349, 0.8202, 0.8354, 0.8284,\n",
      "        0.7819, 0.7795, 0.8191, 0.8336, 0.8611, 0.8591, 0.8810, 0.8084, 0.8607,\n",
      "        0.8141, 0.8495, 0.8385, 0.8386, 0.8613, 0.8688, 0.8633, 0.8495, 0.8710,\n",
      "        0.8303, 0.8392, 0.8355, 0.8284, 0.8256, 0.8609, 0.8010, 0.8593, 0.8392,\n",
      "        0.7908, 0.8419, 0.8177, 0.8369, 0.8398, 0.7572, 0.7810, 0.7491, 0.7591,\n",
      "        0.7822, 0.7584, 0.8333, 0.8188, 0.7443, 0.8370, 0.8069, 0.7968, 0.7809,\n",
      "        0.8042, 0.8314, 0.8505, 0.7879, 0.7780, 0.8084, 0.8235, 0.8223, 0.7958,\n",
      "        0.8195, 0.7941, 0.7899, 0.8556, 0.8560, 0.8407, 0.8659, 0.8585, 0.8282,\n",
      "        0.8310, 0.8563, 0.8385, 0.8581, 0.8167, 0.7780, 0.8379, 0.7899, 0.8418,\n",
      "        0.8281, 0.8386, 0.8116, 0.8293, 0.8542, 0.8383, 0.7975, 0.8572, 0.8553,\n",
      "        0.8389], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9658685868586857, 0.9645964596459645, 0.9588478847884787, 0.9654245424542452, 0.9627842784278428]}\n",
      "Epoch [10/700], Train-Loss: 0.0720, Val-Loss: 0.0315\n",
      "Epoch [20/700], Train-Loss: 0.0160, Val-Loss: 0.0085\n",
      "Epoch [30/700], Train-Loss: 0.0032, Val-Loss: 0.0009\n",
      "Epoch [40/700], Train-Loss: 0.0008, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[1.1710e-04, 2.9222e-04, 9.7602e-05,  ..., 2.2684e-04, 2.1636e-04,\n",
      "         1.4173e-04],\n",
      "        [2.6035e-04, 2.1465e-04, 4.5673e-04,  ..., 1.3196e-04, 1.5066e-04,\n",
      "         6.3962e-05],\n",
      "        [6.9145e-05, 7.4682e-04, 7.4467e-05,  ..., 6.2479e-05, 1.4413e-04,\n",
      "         2.6015e-04],\n",
      "        ...,\n",
      "        [1.7654e-04, 2.3945e-04, 9.4721e-05,  ..., 1.4015e-04, 8.4696e-04,\n",
      "         4.8301e-04],\n",
      "        [1.1786e-04, 3.3065e-04, 1.8081e-04,  ..., 1.2666e-04, 1.3965e-03,\n",
      "         9.0238e-05],\n",
      "        [7.4881e-05, 2.0795e-04, 1.7299e-04,  ..., 1.2724e-04, 1.5452e-04,\n",
      "         2.5706e-04]], device='cuda:0')\n",
      "pred: tensor([0.8338, 0.8276, 0.8035, 0.8125, 0.7894, 0.8221, 0.8154, 0.8215, 0.7871,\n",
      "        0.8252, 0.7927, 0.8287, 0.7893, 0.8163, 0.8390, 0.8123, 0.8284, 0.8204,\n",
      "        0.7960, 0.7959, 0.8214, 0.8360, 0.8654, 0.8552, 0.8799, 0.8011, 0.8599,\n",
      "        0.8210, 0.8495, 0.8516, 0.8488, 0.8485, 0.8684, 0.8642, 0.8551, 0.8778,\n",
      "        0.8351, 0.8455, 0.8345, 0.8308, 0.8410, 0.8612, 0.8014, 0.8612, 0.8304,\n",
      "        0.7974, 0.8394, 0.8313, 0.8299, 0.8401, 0.7686, 0.7871, 0.7615, 0.7838,\n",
      "        0.7834, 0.7800, 0.8312, 0.8312, 0.7561, 0.8396, 0.8026, 0.7969, 0.7860,\n",
      "        0.8057, 0.8321, 0.8382, 0.7961, 0.7734, 0.8230, 0.8321, 0.8291, 0.8014,\n",
      "        0.8244, 0.8039, 0.7975, 0.8588, 0.8488, 0.8478, 0.8558, 0.8562, 0.8263,\n",
      "        0.8336, 0.8578, 0.8425, 0.8620, 0.8195, 0.7827, 0.8419, 0.7974, 0.8418,\n",
      "        0.8304, 0.8429, 0.8002, 0.8304, 0.8626, 0.8471, 0.8074, 0.8543, 0.8481,\n",
      "        0.8382], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1226, Val-Loss: 0.0816\n",
      "Epoch [20/700], Train-Loss: 0.0352, Val-Loss: 0.0284\n",
      "Epoch [30/700], Train-Loss: 0.0127, Val-Loss: 0.0098\n",
      "Epoch [40/700], Train-Loss: 0.0046, Val-Loss: 0.0039\n",
      "Epoch [50/700], Train-Loss: 0.0017, Val-Loss: 0.0011\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.1909e-04, 1.5429e-04, 1.7361e-04,  ..., 3.2169e-05, 4.7388e-05,\n",
      "         1.0067e-04],\n",
      "        [1.3381e-04, 3.0891e-04, 1.1998e-04,  ..., 5.8685e-05, 6.4427e-05,\n",
      "         2.8073e-04],\n",
      "        [2.5120e-04, 1.6146e-04, 1.1258e-04,  ..., 9.2692e-05, 3.3628e-04,\n",
      "         1.5851e-03],\n",
      "        ...,\n",
      "        [2.5972e-04, 2.7642e-04, 3.8690e-04,  ..., 1.0613e-04, 1.1751e-04,\n",
      "         9.0678e-05],\n",
      "        [1.7773e-04, 1.5287e-04, 2.2459e-04,  ..., 1.4004e-04, 1.4838e-04,\n",
      "         5.7092e-04],\n",
      "        [1.4935e-04, 2.6943e-04, 1.9096e-04,  ..., 4.4945e-05, 4.6506e-05,\n",
      "         4.2741e-04]], device='cuda:0')\n",
      "pred: tensor([0.8393, 0.8271, 0.7909, 0.8148, 0.7891, 0.8206, 0.8071, 0.8172, 0.7867,\n",
      "        0.8151, 0.7975, 0.8262, 0.7910, 0.8195, 0.8370, 0.8051, 0.8264, 0.8179,\n",
      "        0.7885, 0.7892, 0.8235, 0.8310, 0.8639, 0.8641, 0.8861, 0.8049, 0.8636,\n",
      "        0.8181, 0.8487, 0.8456, 0.8448, 0.8684, 0.8592, 0.8692, 0.8570, 0.8751,\n",
      "        0.8286, 0.8455, 0.8423, 0.8197, 0.8401, 0.8805, 0.8027, 0.8623, 0.8279,\n",
      "        0.7973, 0.8442, 0.8230, 0.8258, 0.8368, 0.7624, 0.7884, 0.7611, 0.7796,\n",
      "        0.7914, 0.7788, 0.8227, 0.8178, 0.7582, 0.8376, 0.7983, 0.7982, 0.7844,\n",
      "        0.8051, 0.8325, 0.8481, 0.7967, 0.7775, 0.8125, 0.8287, 0.8321, 0.7917,\n",
      "        0.8234, 0.8011, 0.7874, 0.8573, 0.8522, 0.8409, 0.8600, 0.8499, 0.8318,\n",
      "        0.8275, 0.8595, 0.8430, 0.8573, 0.8192, 0.7786, 0.8423, 0.7929, 0.8452,\n",
      "        0.8347, 0.8399, 0.8060, 0.8378, 0.8573, 0.8416, 0.8059, 0.8575, 0.8464,\n",
      "        0.8362], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0693, Val-Loss: 0.0918\n",
      "Epoch [20/700], Train-Loss: 0.0246, Val-Loss: 0.0276\n",
      "Epoch [30/700], Train-Loss: 0.0055, Val-Loss: 0.0076\n",
      "Epoch [40/700], Train-Loss: 0.0020, Val-Loss: 0.0029\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[9.9380e-05, 4.3052e-05, 7.2732e-05,  ..., 1.4430e-04, 1.6278e-04,\n",
      "         6.7208e-05],\n",
      "        [2.2518e-04, 1.5683e-04, 2.0876e-04,  ..., 1.4269e-04, 1.1709e-04,\n",
      "         2.3019e-04],\n",
      "        [2.9051e-04, 1.0460e-04, 1.7647e-04,  ..., 1.7905e-04, 4.5473e-04,\n",
      "         7.9870e-05],\n",
      "        ...,\n",
      "        [7.4512e-05, 6.8038e-05, 9.0259e-05,  ..., 5.6940e-05, 2.3770e-04,\n",
      "         1.4690e-04],\n",
      "        [8.3180e-05, 9.1281e-05, 6.1410e-05,  ..., 4.4290e-05, 2.9092e-05,\n",
      "         4.4414e-05],\n",
      "        [8.0853e-05, 4.6984e-05, 7.9914e-05,  ..., 9.4164e-05, 5.9287e-05,\n",
      "         3.7861e-05]], device='cuda:0')\n",
      "pred: tensor([0.8439, 0.8279, 0.7975, 0.8080, 0.7882, 0.8280, 0.8134, 0.8126, 0.7891,\n",
      "        0.8245, 0.7935, 0.8302, 0.7994, 0.8208, 0.8435, 0.8003, 0.8278, 0.8243,\n",
      "        0.7877, 0.7930, 0.8240, 0.8353, 0.8734, 0.8558, 0.8885, 0.8100, 0.8671,\n",
      "        0.8301, 0.8488, 0.8474, 0.8499, 0.8672, 0.8637, 0.8752, 0.8634, 0.8718,\n",
      "        0.8375, 0.8438, 0.8431, 0.8460, 0.8390, 0.8706, 0.7993, 0.8661, 0.8173,\n",
      "        0.8058, 0.8435, 0.8229, 0.8225, 0.8435, 0.7686, 0.7969, 0.7609, 0.7751,\n",
      "        0.7880, 0.7868, 0.8333, 0.8225, 0.7550, 0.8429, 0.7971, 0.8053, 0.7873,\n",
      "        0.8111, 0.8341, 0.8473, 0.7966, 0.7792, 0.8127, 0.8310, 0.8382, 0.7991,\n",
      "        0.8356, 0.8109, 0.7879, 0.8655, 0.8561, 0.8461, 0.8655, 0.8530, 0.8321,\n",
      "        0.8310, 0.8602, 0.8478, 0.8624, 0.8182, 0.7679, 0.8411, 0.7872, 0.8387,\n",
      "        0.8392, 0.8395, 0.8057, 0.8275, 0.8572, 0.8443, 0.8010, 0.8567, 0.8430,\n",
      "        0.8361], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0974, Val-Loss: 0.0843\n",
      "Epoch [20/700], Train-Loss: 0.0283, Val-Loss: 0.0279\n",
      "Epoch [30/700], Train-Loss: 0.0082, Val-Loss: 0.0092\n",
      "Epoch [40/700], Train-Loss: 0.0030, Val-Loss: 0.0035\n",
      "Epoch [50/700], Train-Loss: 0.0012, Val-Loss: 0.0013\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.6246e-05, 1.0541e-04, 9.8089e-05,  ..., 2.0658e-02, 3.0495e-04,\n",
      "         1.2725e-04],\n",
      "        [2.0942e-04, 2.1294e-04, 1.4739e-04,  ..., 4.4410e-04, 6.2795e-05,\n",
      "         1.9495e-04],\n",
      "        [5.6786e-04, 1.2329e-03, 1.0881e-04,  ..., 5.5392e-03, 1.5692e-04,\n",
      "         2.2377e-04],\n",
      "        ...,\n",
      "        [2.6169e-05, 5.7871e-05, 6.4954e-05,  ..., 1.5260e-02, 7.0103e-05,\n",
      "         8.5667e-05],\n",
      "        [1.3829e-04, 1.0146e-04, 4.0295e-05,  ..., 6.9613e-02, 1.6322e-04,\n",
      "         6.0601e-05],\n",
      "        [7.8282e-05, 3.2961e-04, 4.1675e-05,  ..., 3.1224e-02, 1.2706e-04,\n",
      "         1.7032e-04]], device='cuda:0')\n",
      "pred: tensor([0.8504, 0.8260, 0.8007, 0.8138, 0.7891, 0.8322, 0.8164, 0.8113, 0.7992,\n",
      "        0.8239, 0.7982, 0.8321, 0.7830, 0.8139, 0.8391, 0.8136, 0.8166, 0.8193,\n",
      "        0.7898, 0.7995, 0.8244, 0.8314, 0.8660, 0.8535, 0.8872, 0.7990, 0.8690,\n",
      "        0.8211, 0.8451, 0.8335, 0.8438, 0.8784, 0.8728, 0.8763, 0.8550, 0.8797,\n",
      "        0.8217, 0.8483, 0.8379, 0.8278, 0.8269, 0.8753, 0.8012, 0.8628, 0.8421,\n",
      "        0.7993, 0.8462, 0.8258, 0.8405, 0.8398, 0.7650, 0.7846, 0.7633, 0.7782,\n",
      "        0.7877, 0.7736, 0.8288, 0.8268, 0.7434, 0.8377, 0.8081, 0.8040, 0.7831,\n",
      "        0.8048, 0.8299, 0.8434, 0.7895, 0.7849, 0.8190, 0.8412, 0.8305, 0.8014,\n",
      "        0.8310, 0.8051, 0.8006, 0.8589, 0.8559, 0.8508, 0.8578, 0.8521, 0.8288,\n",
      "        0.8319, 0.8592, 0.8464, 0.8639, 0.8157, 0.7640, 0.8326, 0.7834, 0.8476,\n",
      "        0.8359, 0.8425, 0.7984, 0.8329, 0.8687, 0.8370, 0.8079, 0.8544, 0.8454,\n",
      "        0.8385], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0822, Val-Loss: 0.0401\n",
      "Epoch [20/700], Train-Loss: 0.0209, Val-Loss: 0.0122\n",
      "Epoch [30/700], Train-Loss: 0.0052, Val-Loss: 0.0017\n",
      "Epoch [40/700], Train-Loss: 0.0012, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.8435e-05, 1.4288e-04, 1.8699e-04,  ..., 2.9268e-04, 4.4215e-05,\n",
      "         6.8743e-05],\n",
      "        [3.0713e-04, 1.7734e-04, 4.1619e-04,  ..., 3.0361e-04, 1.6998e-04,\n",
      "         5.1040e-05],\n",
      "        [3.6240e-04, 9.3300e-05, 2.7088e-04,  ..., 3.2447e-04, 2.3488e-04,\n",
      "         2.7677e-04],\n",
      "        ...,\n",
      "        [1.5093e-04, 2.2949e-04, 1.5733e-04,  ..., 4.1468e-04, 5.0451e-04,\n",
      "         1.6782e-04],\n",
      "        [5.4272e-05, 9.7918e-05, 3.1970e-05,  ..., 2.5630e-04, 1.1223e-04,\n",
      "         6.3600e-05],\n",
      "        [3.9614e-05, 1.1336e-04, 5.6743e-05,  ..., 2.4037e-04, 2.0353e-05,\n",
      "         2.0914e-05]], device='cuda:0')\n",
      "pred: tensor([0.8353, 0.8281, 0.7912, 0.8114, 0.7833, 0.8270, 0.8148, 0.8124, 0.7895,\n",
      "        0.8295, 0.7966, 0.8375, 0.7936, 0.8163, 0.8351, 0.8058, 0.8250, 0.8180,\n",
      "        0.7845, 0.7830, 0.8252, 0.8368, 0.8677, 0.8690, 0.8908, 0.8068, 0.8702,\n",
      "        0.8110, 0.8531, 0.8493, 0.8553, 0.8669, 0.8727, 0.8683, 0.8718, 0.8691,\n",
      "        0.8372, 0.8447, 0.8408, 0.8281, 0.8464, 0.8753, 0.7957, 0.8651, 0.8198,\n",
      "        0.7974, 0.8505, 0.8245, 0.8219, 0.8336, 0.7548, 0.7860, 0.7546, 0.7685,\n",
      "        0.7813, 0.7764, 0.8285, 0.8230, 0.7587, 0.8452, 0.7948, 0.8029, 0.7898,\n",
      "        0.8022, 0.8325, 0.8468, 0.7960, 0.7781, 0.8209, 0.8351, 0.8384, 0.8044,\n",
      "        0.8352, 0.7981, 0.7903, 0.8624, 0.8605, 0.8433, 0.8596, 0.8553, 0.8236,\n",
      "        0.8288, 0.8612, 0.8397, 0.8570, 0.8135, 0.7746, 0.8258, 0.7875, 0.8414,\n",
      "        0.8325, 0.8407, 0.8045, 0.8371, 0.8645, 0.8543, 0.8059, 0.8509, 0.8535,\n",
      "        0.8361], device='cuda:0')\n",
      "mean_rank_score_dict: {'imagenet1k': [0.9726732673267325, 0.9814701470147014, 0.9731893189318931, 0.9606120612061206, 0.9738133813381337]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_average_model_performance_over_sample_size(model_confidences_tensor_dict={'Random/kNN (Baseline)': timm_model_confidence_for_val_samples_tensor,\n",
    "                                                                               'Random/Linear (Baseline)': timm_model_confidence_for_val_samples_tensor,\n",
    "                                                                               'High PDS/kNN (DISCO)': timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                                                               'High PDS/Linear (DISCO)': timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                                                               'Synth PDS/kNN (SynthDISCO)': timm_model_confidence_for_main_set_tensor,\n",
    "                                                                               'Synth PDS/Linear (SynthDISCO)': timm_model_confidence_for_main_set_tensor\n",
    "                                                                               },\n",
    "                                                                               # 'cluster_representatives_l1': timm_model_confidence_for_val_samples_tensor[l1_dist_top_indices],\n",
    "                                                                               # 'cluster_representatives_l2': timm_model_confidence_for_val_samples_tensor[l2_dist_top_indices]},\n",
    "                                                model_catalog=timm_model_catalog,\n",
    "                                                score_function='rank',\n",
    "                                                model_classes_for_evaluation={'Random/kNN (Baseline)': NNModel,\n",
    "                                                                               'Random/Linear (Baseline)': MLPRegressor,\n",
    "                                                                               'High PDS/kNN (DISCO)': NNModel,\n",
    "                                                                               'High PDS/Linear (DISCO)': MLPRegressor,\n",
    "                                                                               'Synth PDS/kNN (SynthDISCO)': NNModel,\n",
    "                                                                               'Synth PDS/Linear (SynthDISCO)': MLPRegressor\n",
    "                                                                               },\n",
    "                                                                              # 'cluster_representatives_l1': SimpleNN,\n",
    "                                                                              # 'cluster_representatives_l2': SimpleNN,},\n",
    "                                                plot_for_datasets=['imagenet1k'],\n",
    "                                                sample_sizes=[10,20,50,100],\n",
    "                                                sampling_strategy={'Random/kNN (Baseline)': 'random',\n",
    "                                                                   'Random/Linear (Baseline)': 'random',\n",
    "                                                                   'High PDS/kNN (DISCO)': 'first',\n",
    "                                                                   'High PDS/Linear (DISCO)': 'first',\n",
    "                                                                   'Synth PDS/kNN (SynthDISCO)': 'random',\n",
    "                                                                   'Synth PDS/Linear (SynthDISCO)': 'random'\n",
    "                                                                   },\n",
    "                                                number_bootstraping_steps=5,\n",
    "                                                k_fold_splits=4,\n",
    "                                                x_log_scale=False,\n",
    "                                                model_init_kwargs={\n",
    "                                                    'hidden_channels': [128, 128, 1],\n",
    "                                                },\n",
    "                                                model_fitting_kwargs={\n",
    "                                                    'n_epochs': 700,\n",
    "                                                    'lr': 0.001,\n",
    "                                                },\n",
    "                                                verbose=False,\n",
    "                                                add_baseline_for_sizes=[10,20,50,100],\n",
    "                                                label_for_baseline='Random/Eval (Baseline)',\n",
    "                                                gt_accuracies=timm_result_performances,\n",
    "                                                train_model_index=timm_train_model_indices,\n",
    "                                                val_model_index=timm_val_model_indices,\n",
    "                                                save_path='/mnt/lustre/work/oh/owl813/repos/model-selection/figures/timm_complete_eval_rank.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_and_gt_performance_lists(model_confidences_tensor: torch.tensor,\n",
    "                                           model_catalog: Dict[str, Any],\n",
    "                                           sample_size: int,\n",
    "                                           k_fold_splits: int,\n",
    "                                           evaluation_model_class: type,\n",
    "                                           model_fitting_kwargs: Optional[Dict[str,Any]],\n",
    "                                           verbose: bool=True,\n",
    "                                           random_seed: int=1):\n",
    "    gt_performances_initial_order = torch.tensor([model_info['results']['imagenet1k'] for model_info in model_catalog.values()], dtype=torch.float)\n",
    "    kf = KFold(n_splits=k_fold_splits, shuffle=True, random_state=42)\n",
    "    gt_preformances = []\n",
    "    predicted_performances = []\n",
    "    device=get_device(use_gpu=True)\n",
    "    model_confidence_tensor_transposed = model_confidences_tensor.transpose(0, 1).to(device)\n",
    "    random.seed = random_seed\n",
    "    sample_indices = np.random.choice(model_confidence_tensor_transposed.shape[1], size=sample_size, replace=False)\n",
    "    # indices = torch.tensor(np.random.choice(model_confidences_array.shape[1], size=sample_size, replace=replace))\n",
    "    model_confidence_tensor_transposed_subsampled = model_confidence_tensor_transposed[:, sample_indices, :].reshape(model_confidence_tensor_transposed.shape[0], -1)\n",
    "    shuffled_model_indices = torch.randperm(model_confidence_tensor_transposed_subsampled.size(0))\n",
    "    shuffled_model_confidence_tensor_transposed_subsampled = model_confidence_tensor_transposed_subsampled[shuffled_model_indices]\n",
    "    for train_index, val_index in kf.split(shuffled_model_confidence_tensor_transposed_subsampled):\n",
    "        model_confidences_train, model_confidences_val = shuffled_model_confidence_tensor_transposed_subsampled[train_index], shuffled_model_confidence_tensor_transposed_subsampled[val_index]\n",
    "        shuffled_performances = gt_performances_initial_order[shuffled_model_indices].to(device)\n",
    "        perfomances_train, perfomances_val = shuffled_performances[train_index], shuffled_performances[val_index]\n",
    "        pred_func = create_model_of_class(model_class=evaluation_model_class, input_size=model_confidences_train.shape[1]).to(device)\n",
    "        if model_fitting_kwargs is not None:\n",
    "            pred_func.fit(x=model_confidences_train, y=perfomances_train, x_val=model_confidences_val, y_val=perfomances_val, verbose=verbose, **model_fitting_kwargs)\n",
    "        else:\n",
    "            pred_func.fit(x=model_confidences_train, y=perfomances_train, x_val=model_confidences_val, y_val=perfomances_val, verbose=verbose)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            performances_val_pred = pred_func(x=model_confidences_val)\n",
    "        gt_preformances += list(perfomances_val.cpu())\n",
    "        predicted_performances += list(performances_val_pred.cpu())\n",
    "    return predicted_performances, gt_preformances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_968094/1724587674.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  open_clip_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"open_clip_model_confidences_for_main_set.tensor\"))\n"
     ]
    }
   ],
   "source": [
    "path_to_save_folder = \"/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints\"\n",
    "open_clip_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"open_clip_model_confidences_for_main_set.tensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4775003970>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x=[float(x) for x in gt_preformances_timm], y=[float(x) for x in predicted_performances_timm], s=0.2, label='Timm', c='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f4b40550880>\n",
      "Epoch [10/1000], Train-Loss: 0.1042, Val-Loss: 0.1037\n",
      "Epoch [20/1000], Train-Loss: 0.0547, Val-Loss: 0.0514\n",
      "Epoch [30/1000], Train-Loss: 0.0111, Val-Loss: 0.0137\n",
      "Epoch [40/1000], Train-Loss: 0.0084, Val-Loss: 0.0136\n",
      "Epoch [50/1000], Train-Loss: 0.0048, Val-Loss: 0.0074\n",
      "Epoch [60/1000], Train-Loss: 0.0049, Val-Loss: 0.0052\n",
      "Epoch [70/1000], Train-Loss: 0.0028, Val-Loss: 0.0053\n",
      "Epoch [80/1000], Train-Loss: 0.0012, Val-Loss: 0.0057\n",
      "Epoch [90/1000], Train-Loss: 0.0013, Val-Loss: 0.0057\n",
      "Epoch [100/1000], Train-Loss: 0.0008, Val-Loss: 0.0050\n",
      "Epoch [110/1000], Train-Loss: 0.0026, Val-Loss: 0.0074\n",
      "Epoch [120/1000], Train-Loss: 0.0058, Val-Loss: 0.0079\n",
      "Epoch [130/1000], Train-Loss: 0.0029, Val-Loss: 0.0051\n",
      "Epoch [140/1000], Train-Loss: 0.0021, Val-Loss: 0.0062\n",
      "Epoch [150/1000], Train-Loss: 0.0025, Val-Loss: 0.0054\n",
      "Epoch [160/1000], Train-Loss: 0.0008, Val-Loss: 0.0066\n",
      "Epoch [170/1000], Train-Loss: 0.0032, Val-Loss: 0.0050\n",
      "Epoch [180/1000], Train-Loss: 0.0029, Val-Loss: 0.0054\n",
      "Epoch [190/1000], Train-Loss: 0.0027, Val-Loss: 0.0051\n",
      "Epoch [200/1000], Train-Loss: 0.0009, Val-Loss: 0.0061\n",
      "Epoch [210/1000], Train-Loss: 0.0012, Val-Loss: 0.0052\n",
      "Epoch [220/1000], Train-Loss: 0.0005, Val-Loss: 0.0056\n",
      "Epoch [230/1000], Train-Loss: 0.0054, Val-Loss: 0.0054\n",
      "Epoch [240/1000], Train-Loss: 0.0038, Val-Loss: 0.0049\n",
      "Epoch [250/1000], Train-Loss: 0.0022, Val-Loss: 0.0052\n",
      "Epoch [260/1000], Train-Loss: 0.0027, Val-Loss: 0.0050\n",
      "Epoch [270/1000], Train-Loss: 0.0002, Val-Loss: 0.0051\n",
      "Epoch [280/1000], Train-Loss: 0.0004, Val-Loss: 0.0059\n",
      "Epoch [290/1000], Train-Loss: 0.0011, Val-Loss: 0.0050\n",
      "Epoch [300/1000], Train-Loss: 0.0017, Val-Loss: 0.0052\n",
      "Epoch [310/1000], Train-Loss: 0.0018, Val-Loss: 0.0049\n",
      "Epoch [320/1000], Train-Loss: 0.0016, Val-Loss: 0.0052\n",
      "Epoch [330/1000], Train-Loss: 0.0016, Val-Loss: 0.0049\n",
      "Epoch [340/1000], Train-Loss: 0.0010, Val-Loss: 0.0053\n",
      "Epoch [350/1000], Train-Loss: 0.0009, Val-Loss: 0.0050\n",
      "Epoch [360/1000], Train-Loss: 0.0006, Val-Loss: 0.0054\n",
      "Epoch [370/1000], Train-Loss: 0.0005, Val-Loss: 0.0050\n",
      "Epoch [380/1000], Train-Loss: 0.0009, Val-Loss: 0.0052\n",
      "Epoch [390/1000], Train-Loss: 0.0009, Val-Loss: 0.0049\n",
      "Epoch [400/1000], Train-Loss: 0.0004, Val-Loss: 0.0053\n",
      "Epoch [410/1000], Train-Loss: 0.0012, Val-Loss: 0.0050\n",
      "Epoch [420/1000], Train-Loss: 0.0004, Val-Loss: 0.0053\n",
      "Epoch [430/1000], Train-Loss: 0.0008, Val-Loss: 0.0050\n",
      "Epoch [440/1000], Train-Loss: 0.0003, Val-Loss: 0.0052\n",
      "Epoch [450/1000], Train-Loss: 0.0008, Val-Loss: 0.0049\n",
      "Epoch [460/1000], Train-Loss: 0.0004, Val-Loss: 0.0052\n",
      "Epoch [470/1000], Train-Loss: 0.0009, Val-Loss: 0.0050\n",
      "Epoch [480/1000], Train-Loss: 0.0003, Val-Loss: 0.0052\n",
      "Epoch [490/1000], Train-Loss: 0.0010, Val-Loss: 0.0050\n",
      "Epoch [500/1000], Train-Loss: 0.0004, Val-Loss: 0.0052\n",
      "Epoch [510/1000], Train-Loss: 0.0018, Val-Loss: 0.0051\n",
      "Epoch [520/1000], Train-Loss: 0.0017, Val-Loss: 0.0049\n",
      "Epoch [530/1000], Train-Loss: 0.0002, Val-Loss: 0.0052\n",
      "Epoch [540/1000], Train-Loss: 0.0003, Val-Loss: 0.0050\n",
      "Epoch [550/1000], Train-Loss: 0.0005, Val-Loss: 0.0051\n",
      "Epoch [560/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [570/1000], Train-Loss: 0.0005, Val-Loss: 0.0051\n",
      "Epoch [580/1000], Train-Loss: 0.0015, Val-Loss: 0.0052\n",
      "Epoch [590/1000], Train-Loss: 0.0003, Val-Loss: 0.0052\n",
      "Epoch [600/1000], Train-Loss: 0.0005, Val-Loss: 0.0049\n",
      "Epoch [610/1000], Train-Loss: 0.0012, Val-Loss: 0.0052\n",
      "Epoch [620/1000], Train-Loss: 0.0011, Val-Loss: 0.0050\n",
      "Epoch [630/1000], Train-Loss: 0.0005, Val-Loss: 0.0049\n",
      "Epoch [640/1000], Train-Loss: 0.0005, Val-Loss: 0.0052\n",
      "Epoch [650/1000], Train-Loss: 0.0007, Val-Loss: 0.0050\n",
      "Epoch [660/1000], Train-Loss: 0.0005, Val-Loss: 0.0049\n",
      "Epoch [670/1000], Train-Loss: 0.0006, Val-Loss: 0.0052\n",
      "Epoch [680/1000], Train-Loss: 0.0006, Val-Loss: 0.0050\n",
      "Epoch [690/1000], Train-Loss: 0.0005, Val-Loss: 0.0050\n",
      "Epoch [700/1000], Train-Loss: 0.0006, Val-Loss: 0.0052\n",
      "Epoch [710/1000], Train-Loss: 0.0007, Val-Loss: 0.0050\n",
      "Epoch [720/1000], Train-Loss: 0.0004, Val-Loss: 0.0050\n",
      "Epoch [730/1000], Train-Loss: 0.0005, Val-Loss: 0.0051\n",
      "Epoch [740/1000], Train-Loss: 0.0005, Val-Loss: 0.0050\n",
      "Epoch [750/1000], Train-Loss: 0.0005, Val-Loss: 0.0050\n",
      "Epoch [760/1000], Train-Loss: 0.0003, Val-Loss: 0.0051\n",
      "Epoch [770/1000], Train-Loss: 0.0005, Val-Loss: 0.0050\n",
      "Epoch [780/1000], Train-Loss: 0.0004, Val-Loss: 0.0050\n",
      "Epoch [790/1000], Train-Loss: 0.0004, Val-Loss: 0.0051\n",
      "Epoch [800/1000], Train-Loss: 0.0003, Val-Loss: 0.0050\n",
      "Epoch [810/1000], Train-Loss: 0.0003, Val-Loss: 0.0050\n",
      "Epoch [820/1000], Train-Loss: 0.0004, Val-Loss: 0.0051\n",
      "Epoch [830/1000], Train-Loss: 0.0005, Val-Loss: 0.0050\n",
      "Epoch [840/1000], Train-Loss: 0.0003, Val-Loss: 0.0050\n",
      "Epoch [850/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [860/1000], Train-Loss: 0.0002, Val-Loss: 0.0050\n",
      "Epoch [870/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [880/1000], Train-Loss: 0.0003, Val-Loss: 0.0051\n",
      "Epoch [890/1000], Train-Loss: 0.0003, Val-Loss: 0.0050\n",
      "Epoch [900/1000], Train-Loss: 0.0005, Val-Loss: 0.0051\n",
      "Epoch [910/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [920/1000], Train-Loss: 0.0002, Val-Loss: 0.0050\n",
      "Epoch [930/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [940/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [960/1000], Train-Loss: 0.0002, Val-Loss: 0.0050\n",
      "Epoch [970/1000], Train-Loss: 0.0002, Val-Loss: 0.0050\n",
      "Epoch [980/1000], Train-Loss: 0.0000, Val-Loss: 0.0050\n",
      "Epoch [990/1000], Train-Loss: 0.0001, Val-Loss: 0.0050\n",
      "Epoch [1000/1000], Train-Loss: 0.0004, Val-Loss: 0.0050\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f4469fd05e0>\n",
      "Epoch [10/1000], Train-Loss: 0.1038, Val-Loss: 0.0922\n",
      "Epoch [20/1000], Train-Loss: 0.0569, Val-Loss: 0.0537\n",
      "Epoch [30/1000], Train-Loss: 0.0089, Val-Loss: 0.0210\n",
      "Epoch [40/1000], Train-Loss: 0.0110, Val-Loss: 0.0142\n",
      "Epoch [50/1000], Train-Loss: 0.0042, Val-Loss: 0.0067\n",
      "Epoch [60/1000], Train-Loss: 0.0050, Val-Loss: 0.0048\n",
      "Epoch [70/1000], Train-Loss: 0.0040, Val-Loss: 0.0045\n",
      "Epoch [80/1000], Train-Loss: 0.0041, Val-Loss: 0.0050\n",
      "Epoch [90/1000], Train-Loss: 0.0057, Val-Loss: 0.0062\n",
      "Epoch [100/1000], Train-Loss: 0.0013, Val-Loss: 0.0044\n",
      "Epoch [110/1000], Train-Loss: 0.0011, Val-Loss: 0.0045\n",
      "Epoch [120/1000], Train-Loss: 0.0008, Val-Loss: 0.0045\n",
      "Epoch [130/1000], Train-Loss: 0.0011, Val-Loss: 0.0046\n",
      "Epoch [140/1000], Train-Loss: 0.0009, Val-Loss: 0.0045\n",
      "Epoch [150/1000], Train-Loss: 0.0015, Val-Loss: 0.0044\n",
      "Epoch [160/1000], Train-Loss: 0.0004, Val-Loss: 0.0048\n",
      "Epoch [170/1000], Train-Loss: 0.0087, Val-Loss: 0.0132\n",
      "Epoch [180/1000], Train-Loss: 0.0083, Val-Loss: 0.0049\n",
      "Epoch [190/1000], Train-Loss: 0.0004, Val-Loss: 0.0066\n",
      "Epoch [200/1000], Train-Loss: 0.0090, Val-Loss: 0.0084\n",
      "Epoch [210/1000], Train-Loss: 0.0045, Val-Loss: 0.0044\n",
      "Epoch [220/1000], Train-Loss: 0.0044, Val-Loss: 0.0074\n",
      "Epoch [230/1000], Train-Loss: 0.0026, Val-Loss: 0.0056\n",
      "Epoch [240/1000], Train-Loss: 0.0034, Val-Loss: 0.0059\n",
      "Epoch [250/1000], Train-Loss: 0.0048, Val-Loss: 0.0052\n",
      "Epoch [260/1000], Train-Loss: 0.0023, Val-Loss: 0.0050\n",
      "Epoch [270/1000], Train-Loss: 0.0005, Val-Loss: 0.0048\n",
      "Epoch [280/1000], Train-Loss: 0.0022, Val-Loss: 0.0044\n",
      "Epoch [290/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [300/1000], Train-Loss: 0.0005, Val-Loss: 0.0059\n",
      "Epoch [310/1000], Train-Loss: 0.0026, Val-Loss: 0.0052\n",
      "Epoch [320/1000], Train-Loss: 0.0007, Val-Loss: 0.0047\n",
      "Epoch [330/1000], Train-Loss: 0.0005, Val-Loss: 0.0048\n",
      "Epoch [340/1000], Train-Loss: 0.0004, Val-Loss: 0.0046\n",
      "Epoch [350/1000], Train-Loss: 0.0009, Val-Loss: 0.0046\n",
      "Epoch [360/1000], Train-Loss: 0.0010, Val-Loss: 0.0045\n",
      "Epoch [370/1000], Train-Loss: 0.0014, Val-Loss: 0.0045\n",
      "Epoch [380/1000], Train-Loss: 0.0008, Val-Loss: 0.0045\n",
      "Epoch [390/1000], Train-Loss: 0.0013, Val-Loss: 0.0045\n",
      "Epoch [400/1000], Train-Loss: 0.0010, Val-Loss: 0.0045\n",
      "Epoch [410/1000], Train-Loss: 0.0012, Val-Loss: 0.0045\n",
      "Epoch [420/1000], Train-Loss: 0.0006, Val-Loss: 0.0045\n",
      "Epoch [430/1000], Train-Loss: 0.0009, Val-Loss: 0.0045\n",
      "Epoch [440/1000], Train-Loss: 0.0003, Val-Loss: 0.0045\n",
      "Epoch [450/1000], Train-Loss: 0.0011, Val-Loss: 0.0044\n",
      "Epoch [460/1000], Train-Loss: 0.0008, Val-Loss: 0.0045\n",
      "Epoch [470/1000], Train-Loss: 0.0008, Val-Loss: 0.0044\n",
      "Epoch [480/1000], Train-Loss: 0.0035, Val-Loss: 0.0055\n",
      "Epoch [490/1000], Train-Loss: 0.0012, Val-Loss: 0.0045\n",
      "Epoch [500/1000], Train-Loss: 0.0031, Val-Loss: 0.0048\n",
      "Epoch [510/1000], Train-Loss: 0.0010, Val-Loss: 0.0045\n",
      "Epoch [520/1000], Train-Loss: 0.0001, Val-Loss: 0.0045\n",
      "Epoch [530/1000], Train-Loss: 0.0006, Val-Loss: 0.0044\n",
      "Epoch [540/1000], Train-Loss: 0.0013, Val-Loss: 0.0044\n",
      "Epoch [550/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [560/1000], Train-Loss: 0.0005, Val-Loss: 0.0044\n",
      "Epoch [570/1000], Train-Loss: 0.0004, Val-Loss: 0.0044\n",
      "Epoch [580/1000], Train-Loss: 0.0004, Val-Loss: 0.0044\n",
      "Epoch [590/1000], Train-Loss: 0.0005, Val-Loss: 0.0044\n",
      "Epoch [600/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [610/1000], Train-Loss: 0.0005, Val-Loss: 0.0044\n",
      "Epoch [620/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [630/1000], Train-Loss: 0.0005, Val-Loss: 0.0044\n",
      "Epoch [640/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [650/1000], Train-Loss: 0.0004, Val-Loss: 0.0044\n",
      "Epoch [660/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [670/1000], Train-Loss: 0.0004, Val-Loss: 0.0044\n",
      "Epoch [680/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [690/1000], Train-Loss: 0.0004, Val-Loss: 0.0044\n",
      "Epoch [700/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [710/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [720/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [730/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [740/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [750/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [760/1000], Train-Loss: 0.0005, Val-Loss: 0.0044\n",
      "Epoch [770/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [780/1000], Train-Loss: 0.0011, Val-Loss: 0.0045\n",
      "Epoch [790/1000], Train-Loss: 0.0013, Val-Loss: 0.0045\n",
      "Epoch [800/1000], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [810/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [820/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [830/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [840/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [850/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [860/1000], Train-Loss: 0.0002, Val-Loss: 0.0044\n",
      "Epoch [870/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [880/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [890/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [900/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [910/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [920/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [930/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [940/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [960/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [970/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [980/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [990/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "Epoch [1000/1000], Train-Loss: 0.0001, Val-Loss: 0.0044\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f44681979d0>\n",
      "Epoch [10/1000], Train-Loss: 0.1052, Val-Loss: 0.1089\n",
      "Epoch [20/1000], Train-Loss: 0.0518, Val-Loss: 0.0520\n",
      "Epoch [30/1000], Train-Loss: 0.0154, Val-Loss: 0.0117\n",
      "Epoch [40/1000], Train-Loss: 0.0064, Val-Loss: 0.0128\n",
      "Epoch [50/1000], Train-Loss: 0.0072, Val-Loss: 0.0072\n",
      "Epoch [60/1000], Train-Loss: 0.0043, Val-Loss: 0.0065\n",
      "Epoch [70/1000], Train-Loss: 0.0019, Val-Loss: 0.0072\n",
      "Epoch [80/1000], Train-Loss: 0.0023, Val-Loss: 0.0059\n",
      "Epoch [90/1000], Train-Loss: 0.0046, Val-Loss: 0.0077\n",
      "Epoch [100/1000], Train-Loss: 0.0034, Val-Loss: 0.0059\n",
      "Epoch [110/1000], Train-Loss: 0.0037, Val-Loss: 0.0075\n",
      "Epoch [120/1000], Train-Loss: 0.0015, Val-Loss: 0.0059\n",
      "Epoch [130/1000], Train-Loss: 0.0037, Val-Loss: 0.0072\n",
      "Epoch [140/1000], Train-Loss: 0.0015, Val-Loss: 0.0058\n",
      "Epoch [150/1000], Train-Loss: 0.0022, Val-Loss: 0.0062\n",
      "Epoch [160/1000], Train-Loss: 0.0066, Val-Loss: 0.0116\n",
      "Epoch [170/1000], Train-Loss: 0.0139, Val-Loss: 0.0128\n",
      "Epoch [180/1000], Train-Loss: 0.0175, Val-Loss: 0.0180\n",
      "Epoch [190/1000], Train-Loss: 0.0093, Val-Loss: 0.0131\n",
      "Epoch [200/1000], Train-Loss: 0.0070, Val-Loss: 0.0078\n",
      "Epoch [210/1000], Train-Loss: 0.0010, Val-Loss: 0.0080\n",
      "Epoch [220/1000], Train-Loss: 0.0024, Val-Loss: 0.0060\n",
      "Epoch [230/1000], Train-Loss: 0.0022, Val-Loss: 0.0058\n",
      "Epoch [240/1000], Train-Loss: 0.0023, Val-Loss: 0.0058\n",
      "Epoch [250/1000], Train-Loss: 0.0012, Val-Loss: 0.0060\n",
      "Epoch [260/1000], Train-Loss: 0.0028, Val-Loss: 0.0070\n",
      "Epoch [270/1000], Train-Loss: 0.0013, Val-Loss: 0.0061\n",
      "Epoch [280/1000], Train-Loss: 0.0016, Val-Loss: 0.0057\n",
      "Epoch [290/1000], Train-Loss: 0.0027, Val-Loss: 0.0062\n",
      "Epoch [300/1000], Train-Loss: 0.0026, Val-Loss: 0.0060\n",
      "Epoch [310/1000], Train-Loss: 0.0027, Val-Loss: 0.0058\n",
      "Epoch [320/1000], Train-Loss: 0.0011, Val-Loss: 0.0064\n",
      "Epoch [330/1000], Train-Loss: 0.0004, Val-Loss: 0.0058\n",
      "Epoch [340/1000], Train-Loss: 0.0005, Val-Loss: 0.0064\n",
      "Epoch [350/1000], Train-Loss: 0.0002, Val-Loss: 0.0058\n",
      "Epoch [360/1000], Train-Loss: 0.0002, Val-Loss: 0.0059\n",
      "Epoch [370/1000], Train-Loss: 0.0012, Val-Loss: 0.0058\n",
      "Epoch [380/1000], Train-Loss: 0.0013, Val-Loss: 0.0062\n",
      "Epoch [390/1000], Train-Loss: 0.0010, Val-Loss: 0.0058\n",
      "Epoch [400/1000], Train-Loss: 0.0012, Val-Loss: 0.0062\n",
      "Epoch [410/1000], Train-Loss: 0.0012, Val-Loss: 0.0058\n",
      "Epoch [420/1000], Train-Loss: 0.0010, Val-Loss: 0.0061\n",
      "Epoch [430/1000], Train-Loss: 0.0006, Val-Loss: 0.0058\n",
      "Epoch [440/1000], Train-Loss: 0.0011, Val-Loss: 0.0062\n",
      "Epoch [450/1000], Train-Loss: 0.0006, Val-Loss: 0.0058\n",
      "Epoch [460/1000], Train-Loss: 0.0010, Val-Loss: 0.0061\n",
      "Epoch [470/1000], Train-Loss: 0.0004, Val-Loss: 0.0058\n",
      "Epoch [480/1000], Train-Loss: 0.0005, Val-Loss: 0.0060\n",
      "Epoch [490/1000], Train-Loss: 0.0011, Val-Loss: 0.0058\n",
      "Epoch [500/1000], Train-Loss: 0.0023, Val-Loss: 0.0074\n",
      "Epoch [510/1000], Train-Loss: 0.0012, Val-Loss: 0.0069\n",
      "Epoch [520/1000], Train-Loss: 0.0021, Val-Loss: 0.0074\n",
      "Epoch [530/1000], Train-Loss: 0.0013, Val-Loss: 0.0061\n",
      "Epoch [540/1000], Train-Loss: 0.0001, Val-Loss: 0.0058\n",
      "Epoch [550/1000], Train-Loss: 0.0011, Val-Loss: 0.0060\n",
      "Epoch [560/1000], Train-Loss: 0.0002, Val-Loss: 0.0061\n",
      "Epoch [570/1000], Train-Loss: 0.0011, Val-Loss: 0.0058\n",
      "Epoch [580/1000], Train-Loss: 0.0015, Val-Loss: 0.0062\n",
      "Epoch [590/1000], Train-Loss: 0.0001, Val-Loss: 0.0058\n",
      "Epoch [600/1000], Train-Loss: 0.0005, Val-Loss: 0.0061\n",
      "Epoch [610/1000], Train-Loss: 0.0007, Val-Loss: 0.0059\n",
      "Epoch [620/1000], Train-Loss: 0.0007, Val-Loss: 0.0058\n",
      "Epoch [630/1000], Train-Loss: 0.0008, Val-Loss: 0.0062\n",
      "Epoch [640/1000], Train-Loss: 0.0006, Val-Loss: 0.0059\n",
      "Epoch [650/1000], Train-Loss: 0.0007, Val-Loss: 0.0059\n",
      "Epoch [660/1000], Train-Loss: 0.0007, Val-Loss: 0.0061\n",
      "Epoch [670/1000], Train-Loss: 0.0006, Val-Loss: 0.0059\n",
      "Epoch [680/1000], Train-Loss: 0.0004, Val-Loss: 0.0058\n",
      "Epoch [690/1000], Train-Loss: 0.0005, Val-Loss: 0.0061\n",
      "Epoch [700/1000], Train-Loss: 0.0006, Val-Loss: 0.0059\n",
      "Epoch [710/1000], Train-Loss: 0.0005, Val-Loss: 0.0059\n",
      "Epoch [720/1000], Train-Loss: 0.0005, Val-Loss: 0.0061\n",
      "Epoch [730/1000], Train-Loss: 0.0004, Val-Loss: 0.0059\n",
      "Epoch [740/1000], Train-Loss: 0.0004, Val-Loss: 0.0059\n",
      "Epoch [750/1000], Train-Loss: 0.0005, Val-Loss: 0.0060\n",
      "Epoch [760/1000], Train-Loss: 0.0005, Val-Loss: 0.0059\n",
      "Epoch [770/1000], Train-Loss: 0.0004, Val-Loss: 0.0059\n",
      "Epoch [780/1000], Train-Loss: 0.0003, Val-Loss: 0.0060\n",
      "Epoch [790/1000], Train-Loss: 0.0004, Val-Loss: 0.0059\n",
      "Epoch [800/1000], Train-Loss: 0.0003, Val-Loss: 0.0059\n",
      "Epoch [810/1000], Train-Loss: 0.0003, Val-Loss: 0.0060\n",
      "Epoch [820/1000], Train-Loss: 0.0004, Val-Loss: 0.0059\n",
      "Epoch [830/1000], Train-Loss: 0.0003, Val-Loss: 0.0059\n",
      "Epoch [840/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [850/1000], Train-Loss: 0.0002, Val-Loss: 0.0059\n",
      "Epoch [860/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [870/1000], Train-Loss: 0.0002, Val-Loss: 0.0059\n",
      "Epoch [880/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [890/1000], Train-Loss: 0.0002, Val-Loss: 0.0059\n",
      "Epoch [900/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [910/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [920/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [930/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [940/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "Epoch [960/1000], Train-Loss: 0.0000, Val-Loss: 0.0059\n",
      "Epoch [970/1000], Train-Loss: 0.0000, Val-Loss: 0.0059\n",
      "Epoch [980/1000], Train-Loss: 0.0000, Val-Loss: 0.0059\n",
      "Epoch [990/1000], Train-Loss: 0.0002, Val-Loss: 0.0059\n",
      "Epoch [1000/1000], Train-Loss: 0.0001, Val-Loss: 0.0059\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f44362ac2e0>\n",
      "Epoch [10/1000], Train-Loss: 0.1043, Val-Loss: 0.1039\n",
      "Epoch [20/1000], Train-Loss: 0.0542, Val-Loss: 0.0524\n",
      "Epoch [30/1000], Train-Loss: 0.0118, Val-Loss: 0.0137\n",
      "Epoch [40/1000], Train-Loss: 0.0091, Val-Loss: 0.0137\n",
      "Epoch [50/1000], Train-Loss: 0.0049, Val-Loss: 0.0082\n",
      "Epoch [60/1000], Train-Loss: 0.0034, Val-Loss: 0.0061\n",
      "Epoch [70/1000], Train-Loss: 0.0022, Val-Loss: 0.0060\n",
      "Epoch [80/1000], Train-Loss: 0.0036, Val-Loss: 0.0055\n",
      "Epoch [90/1000], Train-Loss: 0.0048, Val-Loss: 0.0060\n",
      "Epoch [100/1000], Train-Loss: 0.0019, Val-Loss: 0.0056\n",
      "Epoch [110/1000], Train-Loss: 0.0043, Val-Loss: 0.0067\n",
      "Epoch [120/1000], Train-Loss: 0.0029, Val-Loss: 0.0055\n",
      "Epoch [130/1000], Train-Loss: 0.0037, Val-Loss: 0.0058\n",
      "Epoch [140/1000], Train-Loss: 0.0008, Val-Loss: 0.0065\n",
      "Epoch [150/1000], Train-Loss: 0.0034, Val-Loss: 0.0057\n",
      "Epoch [160/1000], Train-Loss: 0.0006, Val-Loss: 0.0064\n",
      "Epoch [170/1000], Train-Loss: 0.0036, Val-Loss: 0.0056\n",
      "Epoch [180/1000], Train-Loss: 0.0006, Val-Loss: 0.0058\n",
      "Epoch [190/1000], Train-Loss: 0.0047, Val-Loss: 0.0056\n",
      "Epoch [200/1000], Train-Loss: 0.0023, Val-Loss: 0.0066\n",
      "Epoch [210/1000], Train-Loss: 0.0044, Val-Loss: 0.0066\n",
      "Epoch [220/1000], Train-Loss: 0.0039, Val-Loss: 0.0067\n",
      "Epoch [230/1000], Train-Loss: 0.0061, Val-Loss: 0.0066\n",
      "Epoch [240/1000], Train-Loss: 0.0019, Val-Loss: 0.0062\n",
      "Epoch [250/1000], Train-Loss: 0.0031, Val-Loss: 0.0066\n",
      "Epoch [260/1000], Train-Loss: 0.0053, Val-Loss: 0.0064\n",
      "Epoch [270/1000], Train-Loss: 0.0007, Val-Loss: 0.0059\n",
      "Epoch [280/1000], Train-Loss: 0.0008, Val-Loss: 0.0058\n",
      "Epoch [290/1000], Train-Loss: 0.0030, Val-Loss: 0.0058\n",
      "Epoch [300/1000], Train-Loss: 0.0013, Val-Loss: 0.0058\n",
      "Epoch [310/1000], Train-Loss: 0.0024, Val-Loss: 0.0056\n",
      "Epoch [320/1000], Train-Loss: 0.0019, Val-Loss: 0.0058\n",
      "Epoch [330/1000], Train-Loss: 0.0018, Val-Loss: 0.0059\n",
      "Epoch [340/1000], Train-Loss: 0.0035, Val-Loss: 0.0058\n",
      "Epoch [350/1000], Train-Loss: 0.0012, Val-Loss: 0.0059\n",
      "Epoch [360/1000], Train-Loss: 0.0007, Val-Loss: 0.0057\n",
      "Epoch [370/1000], Train-Loss: 0.0014, Val-Loss: 0.0057\n",
      "Epoch [380/1000], Train-Loss: 0.0010, Val-Loss: 0.0058\n",
      "Epoch [390/1000], Train-Loss: 0.0018, Val-Loss: 0.0056\n",
      "Epoch [400/1000], Train-Loss: 0.0005, Val-Loss: 0.0056\n",
      "Epoch [410/1000], Train-Loss: 0.0011, Val-Loss: 0.0057\n",
      "Epoch [420/1000], Train-Loss: 0.0012, Val-Loss: 0.0056\n",
      "Epoch [430/1000], Train-Loss: 0.0012, Val-Loss: 0.0057\n",
      "Epoch [440/1000], Train-Loss: 0.0007, Val-Loss: 0.0056\n",
      "Epoch [450/1000], Train-Loss: 0.0011, Val-Loss: 0.0057\n",
      "Epoch [460/1000], Train-Loss: 0.0006, Val-Loss: 0.0056\n",
      "Epoch [470/1000], Train-Loss: 0.0009, Val-Loss: 0.0057\n",
      "Epoch [480/1000], Train-Loss: 0.0004, Val-Loss: 0.0056\n",
      "Epoch [490/1000], Train-Loss: 0.0008, Val-Loss: 0.0057\n",
      "Epoch [500/1000], Train-Loss: 0.0007, Val-Loss: 0.0056\n",
      "Epoch [510/1000], Train-Loss: 0.0001, Val-Loss: 0.0057\n",
      "Epoch [520/1000], Train-Loss: 0.0020, Val-Loss: 0.0060\n",
      "Epoch [530/1000], Train-Loss: 0.0018, Val-Loss: 0.0056\n",
      "Epoch [540/1000], Train-Loss: 0.0007, Val-Loss: 0.0056\n",
      "Epoch [550/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [560/1000], Train-Loss: 0.0003, Val-Loss: 0.0056\n",
      "Epoch [570/1000], Train-Loss: 0.0009, Val-Loss: 0.0056\n",
      "Epoch [580/1000], Train-Loss: 0.0002, Val-Loss: 0.0057\n",
      "Epoch [590/1000], Train-Loss: 0.0013, Val-Loss: 0.0057\n",
      "Epoch [600/1000], Train-Loss: 0.0024, Val-Loss: 0.0058\n",
      "Epoch [610/1000], Train-Loss: 0.0008, Val-Loss: 0.0056\n",
      "Epoch [620/1000], Train-Loss: 0.0004, Val-Loss: 0.0056\n",
      "Epoch [630/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [640/1000], Train-Loss: 0.0008, Val-Loss: 0.0056\n",
      "Epoch [650/1000], Train-Loss: 0.0006, Val-Loss: 0.0057\n",
      "Epoch [660/1000], Train-Loss: 0.0008, Val-Loss: 0.0056\n",
      "Epoch [670/1000], Train-Loss: 0.0005, Val-Loss: 0.0056\n",
      "Epoch [680/1000], Train-Loss: 0.0006, Val-Loss: 0.0057\n",
      "Epoch [690/1000], Train-Loss: 0.0006, Val-Loss: 0.0056\n",
      "Epoch [700/1000], Train-Loss: 0.0004, Val-Loss: 0.0056\n",
      "Epoch [710/1000], Train-Loss: 0.0006, Val-Loss: 0.0057\n",
      "Epoch [720/1000], Train-Loss: 0.0008, Val-Loss: 0.0056\n",
      "Epoch [730/1000], Train-Loss: 0.0006, Val-Loss: 0.0056\n",
      "Epoch [740/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [750/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [760/1000], Train-Loss: 0.0003, Val-Loss: 0.0056\n",
      "Epoch [770/1000], Train-Loss: 0.0011, Val-Loss: 0.0057\n",
      "Epoch [780/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [790/1000], Train-Loss: 0.0004, Val-Loss: 0.0056\n",
      "Epoch [800/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [810/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [820/1000], Train-Loss: 0.0005, Val-Loss: 0.0056\n",
      "Epoch [830/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [840/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [850/1000], Train-Loss: 0.0004, Val-Loss: 0.0056\n",
      "Epoch [860/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [870/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [880/1000], Train-Loss: 0.0004, Val-Loss: 0.0056\n",
      "Epoch [890/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [900/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [910/1000], Train-Loss: 0.0003, Val-Loss: 0.0056\n",
      "Epoch [920/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [930/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "Epoch [940/1000], Train-Loss: 0.0003, Val-Loss: 0.0056\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [960/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [970/1000], Train-Loss: 0.0003, Val-Loss: 0.0056\n",
      "Epoch [980/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [990/1000], Train-Loss: 0.0001, Val-Loss: 0.0056\n",
      "Epoch [1000/1000], Train-Loss: 0.0002, Val-Loss: 0.0056\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f443633f490>\n",
      "Epoch [10/1000], Train-Loss: 0.0300, Val-Loss: 0.0377\n",
      "Epoch [20/1000], Train-Loss: 0.0186, Val-Loss: 0.0180\n",
      "Epoch [30/1000], Train-Loss: 0.0138, Val-Loss: 0.0215\n",
      "Epoch [40/1000], Train-Loss: 0.0075, Val-Loss: 0.0150\n",
      "Epoch [50/1000], Train-Loss: 0.0028, Val-Loss: 0.0178\n",
      "Epoch [60/1000], Train-Loss: 0.0070, Val-Loss: 0.0153\n",
      "Epoch [70/1000], Train-Loss: 0.0017, Val-Loss: 0.0149\n",
      "Epoch [80/1000], Train-Loss: 0.0031, Val-Loss: 0.0156\n",
      "Epoch [90/1000], Train-Loss: 0.0049, Val-Loss: 0.0164\n",
      "Epoch [100/1000], Train-Loss: 0.0033, Val-Loss: 0.0162\n",
      "Epoch [110/1000], Train-Loss: 0.0024, Val-Loss: 0.0163\n",
      "Epoch [120/1000], Train-Loss: 0.0061, Val-Loss: 0.0161\n",
      "Epoch [130/1000], Train-Loss: 0.0044, Val-Loss: 0.0146\n",
      "Epoch [140/1000], Train-Loss: 0.0036, Val-Loss: 0.0155\n",
      "Epoch [150/1000], Train-Loss: 0.0031, Val-Loss: 0.0152\n",
      "Epoch [160/1000], Train-Loss: 0.0066, Val-Loss: 0.0150\n",
      "Epoch [170/1000], Train-Loss: 0.0105, Val-Loss: 0.0170\n",
      "Epoch [180/1000], Train-Loss: 0.0050, Val-Loss: 0.0172\n",
      "Epoch [190/1000], Train-Loss: 0.0015, Val-Loss: 0.0152\n",
      "Epoch [200/1000], Train-Loss: 0.0048, Val-Loss: 0.0152\n",
      "Epoch [210/1000], Train-Loss: 0.0047, Val-Loss: 0.0153\n",
      "Epoch [220/1000], Train-Loss: 0.0018, Val-Loss: 0.0154\n",
      "Epoch [230/1000], Train-Loss: 0.0013, Val-Loss: 0.0158\n",
      "Epoch [240/1000], Train-Loss: 0.0028, Val-Loss: 0.0152\n",
      "Epoch [250/1000], Train-Loss: 0.0053, Val-Loss: 0.0157\n",
      "Epoch [260/1000], Train-Loss: 0.0021, Val-Loss: 0.0160\n",
      "Epoch [270/1000], Train-Loss: 0.0041, Val-Loss: 0.0154\n",
      "Epoch [280/1000], Train-Loss: 0.0034, Val-Loss: 0.0154\n",
      "Epoch [290/1000], Train-Loss: 0.0019, Val-Loss: 0.0152\n",
      "Epoch [300/1000], Train-Loss: 0.0010, Val-Loss: 0.0154\n",
      "Epoch [310/1000], Train-Loss: 0.0008, Val-Loss: 0.0156\n",
      "Epoch [320/1000], Train-Loss: 0.0012, Val-Loss: 0.0150\n",
      "Epoch [330/1000], Train-Loss: 0.0018, Val-Loss: 0.0149\n",
      "Epoch [340/1000], Train-Loss: 0.0010, Val-Loss: 0.0156\n",
      "Epoch [350/1000], Train-Loss: 0.0008, Val-Loss: 0.0152\n",
      "Epoch [360/1000], Train-Loss: 0.0013, Val-Loss: 0.0155\n",
      "Epoch [370/1000], Train-Loss: 0.0018, Val-Loss: 0.0151\n",
      "Epoch [380/1000], Train-Loss: 0.0020, Val-Loss: 0.0156\n",
      "Epoch [390/1000], Train-Loss: 0.0010, Val-Loss: 0.0152\n",
      "Epoch [400/1000], Train-Loss: 0.0008, Val-Loss: 0.0153\n",
      "Epoch [410/1000], Train-Loss: 0.0005, Val-Loss: 0.0153\n",
      "Epoch [420/1000], Train-Loss: 0.0005, Val-Loss: 0.0153\n",
      "Epoch [430/1000], Train-Loss: 0.0011, Val-Loss: 0.0153\n",
      "Epoch [440/1000], Train-Loss: 0.0012, Val-Loss: 0.0153\n",
      "Epoch [450/1000], Train-Loss: 0.0005, Val-Loss: 0.0150\n",
      "Epoch [460/1000], Train-Loss: 0.0013, Val-Loss: 0.0150\n",
      "Epoch [470/1000], Train-Loss: 0.0011, Val-Loss: 0.0151\n",
      "Epoch [480/1000], Train-Loss: 0.0004, Val-Loss: 0.0154\n",
      "Epoch [490/1000], Train-Loss: 0.0004, Val-Loss: 0.0151\n",
      "Epoch [500/1000], Train-Loss: 0.0016, Val-Loss: 0.0153\n",
      "Epoch [510/1000], Train-Loss: 0.0003, Val-Loss: 0.0153\n",
      "Epoch [520/1000], Train-Loss: 0.0005, Val-Loss: 0.0152\n",
      "Epoch [530/1000], Train-Loss: 0.0014, Val-Loss: 0.0155\n",
      "Epoch [540/1000], Train-Loss: 0.0022, Val-Loss: 0.0150\n",
      "Epoch [550/1000], Train-Loss: 0.0010, Val-Loss: 0.0150\n",
      "Epoch [560/1000], Train-Loss: 0.0012, Val-Loss: 0.0155\n",
      "Epoch [570/1000], Train-Loss: 0.0021, Val-Loss: 0.0151\n",
      "Epoch [580/1000], Train-Loss: 0.0003, Val-Loss: 0.0151\n",
      "Epoch [590/1000], Train-Loss: 0.0016, Val-Loss: 0.0154\n",
      "Epoch [600/1000], Train-Loss: 0.0003, Val-Loss: 0.0153\n",
      "Epoch [610/1000], Train-Loss: 0.0011, Val-Loss: 0.0152\n",
      "Epoch [620/1000], Train-Loss: 0.0005, Val-Loss: 0.0151\n",
      "Epoch [630/1000], Train-Loss: 0.0003, Val-Loss: 0.0152\n",
      "Epoch [640/1000], Train-Loss: 0.0006, Val-Loss: 0.0152\n",
      "Epoch [650/1000], Train-Loss: 0.0003, Val-Loss: 0.0153\n",
      "Epoch [660/1000], Train-Loss: 0.0005, Val-Loss: 0.0152\n",
      "Epoch [670/1000], Train-Loss: 0.0003, Val-Loss: 0.0151\n",
      "Epoch [680/1000], Train-Loss: 0.0005, Val-Loss: 0.0151\n",
      "Epoch [690/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [700/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [710/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [720/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [730/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [740/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [750/1000], Train-Loss: 0.0004, Val-Loss: 0.0152\n",
      "Epoch [760/1000], Train-Loss: 0.0002, Val-Loss: 0.0153\n",
      "Epoch [770/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [780/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [790/1000], Train-Loss: 0.0003, Val-Loss: 0.0152\n",
      "Epoch [800/1000], Train-Loss: 0.0002, Val-Loss: 0.0153\n",
      "Epoch [810/1000], Train-Loss: 0.0003, Val-Loss: 0.0152\n",
      "Epoch [820/1000], Train-Loss: 0.0003, Val-Loss: 0.0152\n",
      "Epoch [830/1000], Train-Loss: 0.0003, Val-Loss: 0.0152\n",
      "Epoch [840/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [850/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [860/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [870/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [880/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [890/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [900/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [910/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [920/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [930/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [940/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [960/1000], Train-Loss: 0.0000, Val-Loss: 0.0152\n",
      "Epoch [970/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [980/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "Epoch [990/1000], Train-Loss: 0.0002, Val-Loss: 0.0152\n",
      "Epoch [1000/1000], Train-Loss: 0.0001, Val-Loss: 0.0152\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f443633f250>\n",
      "Epoch [10/1000], Train-Loss: 0.0340, Val-Loss: 0.0284\n",
      "Epoch [20/1000], Train-Loss: 0.0109, Val-Loss: 0.0168\n",
      "Epoch [30/1000], Train-Loss: 0.0155, Val-Loss: 0.0112\n",
      "Epoch [40/1000], Train-Loss: 0.0102, Val-Loss: 0.0137\n",
      "Epoch [50/1000], Train-Loss: 0.0057, Val-Loss: 0.0126\n",
      "Epoch [60/1000], Train-Loss: 0.0021, Val-Loss: 0.0122\n",
      "Epoch [70/1000], Train-Loss: 0.0025, Val-Loss: 0.0135\n",
      "Epoch [80/1000], Train-Loss: 0.0085, Val-Loss: 0.0133\n",
      "Epoch [90/1000], Train-Loss: 0.0034, Val-Loss: 0.0111\n",
      "Epoch [100/1000], Train-Loss: 0.0031, Val-Loss: 0.0129\n",
      "Epoch [110/1000], Train-Loss: 0.0043, Val-Loss: 0.0111\n",
      "Epoch [120/1000], Train-Loss: 0.0025, Val-Loss: 0.0115\n",
      "Epoch [130/1000], Train-Loss: 0.0025, Val-Loss: 0.0120\n",
      "Epoch [140/1000], Train-Loss: 0.0014, Val-Loss: 0.0116\n",
      "Epoch [150/1000], Train-Loss: 0.0025, Val-Loss: 0.0142\n",
      "Epoch [160/1000], Train-Loss: 0.0012, Val-Loss: 0.0116\n",
      "Epoch [170/1000], Train-Loss: 0.0020, Val-Loss: 0.0118\n",
      "Epoch [180/1000], Train-Loss: 0.0044, Val-Loss: 0.0114\n",
      "Epoch [190/1000], Train-Loss: 0.0028, Val-Loss: 0.0129\n",
      "Epoch [200/1000], Train-Loss: 0.0040, Val-Loss: 0.0112\n",
      "Epoch [210/1000], Train-Loss: 0.0011, Val-Loss: 0.0132\n",
      "Epoch [220/1000], Train-Loss: 0.0010, Val-Loss: 0.0111\n",
      "Epoch [230/1000], Train-Loss: 0.0020, Val-Loss: 0.0110\n",
      "Epoch [240/1000], Train-Loss: 0.0028, Val-Loss: 0.0128\n",
      "Epoch [250/1000], Train-Loss: 0.0021, Val-Loss: 0.0128\n",
      "Epoch [260/1000], Train-Loss: 0.0035, Val-Loss: 0.0128\n",
      "Epoch [270/1000], Train-Loss: 0.0031, Val-Loss: 0.0110\n",
      "Epoch [280/1000], Train-Loss: 0.0034, Val-Loss: 0.0122\n",
      "Epoch [290/1000], Train-Loss: 0.0011, Val-Loss: 0.0110\n",
      "Epoch [300/1000], Train-Loss: 0.0021, Val-Loss: 0.0113\n",
      "Epoch [310/1000], Train-Loss: 0.0013, Val-Loss: 0.0111\n",
      "Epoch [320/1000], Train-Loss: 0.0007, Val-Loss: 0.0109\n",
      "Epoch [330/1000], Train-Loss: 0.0007, Val-Loss: 0.0107\n",
      "Epoch [340/1000], Train-Loss: 0.0018, Val-Loss: 0.0109\n",
      "Epoch [350/1000], Train-Loss: 0.0020, Val-Loss: 0.0118\n",
      "Epoch [360/1000], Train-Loss: 0.0014, Val-Loss: 0.0111\n",
      "Epoch [370/1000], Train-Loss: 0.0019, Val-Loss: 0.0115\n",
      "Epoch [380/1000], Train-Loss: 0.0020, Val-Loss: 0.0111\n",
      "Epoch [390/1000], Train-Loss: 0.0022, Val-Loss: 0.0115\n",
      "Epoch [400/1000], Train-Loss: 0.0009, Val-Loss: 0.0120\n",
      "Epoch [410/1000], Train-Loss: 0.0013, Val-Loss: 0.0108\n",
      "Epoch [420/1000], Train-Loss: 0.0007, Val-Loss: 0.0112\n",
      "Epoch [430/1000], Train-Loss: 0.0009, Val-Loss: 0.0118\n",
      "Epoch [440/1000], Train-Loss: 0.0006, Val-Loss: 0.0111\n",
      "Epoch [450/1000], Train-Loss: 0.0004, Val-Loss: 0.0116\n",
      "Epoch [460/1000], Train-Loss: 0.0004, Val-Loss: 0.0116\n",
      "Epoch [470/1000], Train-Loss: 0.0003, Val-Loss: 0.0111\n",
      "Epoch [480/1000], Train-Loss: 0.0005, Val-Loss: 0.0113\n",
      "Epoch [490/1000], Train-Loss: 0.0007, Val-Loss: 0.0116\n",
      "Epoch [500/1000], Train-Loss: 0.0007, Val-Loss: 0.0115\n",
      "Epoch [510/1000], Train-Loss: 0.0002, Val-Loss: 0.0113\n",
      "Epoch [520/1000], Train-Loss: 0.0002, Val-Loss: 0.0117\n",
      "Epoch [530/1000], Train-Loss: 0.0006, Val-Loss: 0.0112\n",
      "Epoch [540/1000], Train-Loss: 0.0006, Val-Loss: 0.0116\n",
      "Epoch [550/1000], Train-Loss: 0.0008, Val-Loss: 0.0112\n",
      "Epoch [560/1000], Train-Loss: 0.0014, Val-Loss: 0.0111\n",
      "Epoch [570/1000], Train-Loss: 0.0003, Val-Loss: 0.0112\n",
      "Epoch [580/1000], Train-Loss: 0.0003, Val-Loss: 0.0113\n",
      "Epoch [590/1000], Train-Loss: 0.0006, Val-Loss: 0.0115\n",
      "Epoch [600/1000], Train-Loss: 0.0007, Val-Loss: 0.0113\n",
      "Epoch [610/1000], Train-Loss: 0.0002, Val-Loss: 0.0114\n",
      "Epoch [620/1000], Train-Loss: 0.0009, Val-Loss: 0.0114\n",
      "Epoch [630/1000], Train-Loss: 0.0002, Val-Loss: 0.0115\n",
      "Epoch [640/1000], Train-Loss: 0.0002, Val-Loss: 0.0112\n",
      "Epoch [650/1000], Train-Loss: 0.0005, Val-Loss: 0.0115\n",
      "Epoch [660/1000], Train-Loss: 0.0003, Val-Loss: 0.0113\n",
      "Epoch [670/1000], Train-Loss: 0.0003, Val-Loss: 0.0115\n",
      "Epoch [680/1000], Train-Loss: 0.0006, Val-Loss: 0.0112\n",
      "Epoch [690/1000], Train-Loss: 0.0003, Val-Loss: 0.0113\n",
      "Epoch [700/1000], Train-Loss: 0.0002, Val-Loss: 0.0113\n",
      "Epoch [710/1000], Train-Loss: 0.0002, Val-Loss: 0.0115\n",
      "Epoch [720/1000], Train-Loss: 0.0002, Val-Loss: 0.0115\n",
      "Epoch [730/1000], Train-Loss: 0.0004, Val-Loss: 0.0113\n",
      "Epoch [740/1000], Train-Loss: 0.0006, Val-Loss: 0.0114\n",
      "Epoch [750/1000], Train-Loss: 0.0008, Val-Loss: 0.0111\n",
      "Epoch [760/1000], Train-Loss: 0.0005, Val-Loss: 0.0114\n",
      "Epoch [770/1000], Train-Loss: 0.0003, Val-Loss: 0.0115\n",
      "Epoch [780/1000], Train-Loss: 0.0002, Val-Loss: 0.0113\n",
      "Epoch [790/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [800/1000], Train-Loss: 0.0002, Val-Loss: 0.0114\n",
      "Epoch [810/1000], Train-Loss: 0.0002, Val-Loss: 0.0114\n",
      "Epoch [820/1000], Train-Loss: 0.0003, Val-Loss: 0.0114\n",
      "Epoch [830/1000], Train-Loss: 0.0006, Val-Loss: 0.0116\n",
      "Epoch [840/1000], Train-Loss: 0.0005, Val-Loss: 0.0114\n",
      "Epoch [850/1000], Train-Loss: 0.0006, Val-Loss: 0.0112\n",
      "Epoch [860/1000], Train-Loss: 0.0004, Val-Loss: 0.0114\n",
      "Epoch [870/1000], Train-Loss: 0.0002, Val-Loss: 0.0114\n",
      "Epoch [880/1000], Train-Loss: 0.0003, Val-Loss: 0.0113\n",
      "Epoch [890/1000], Train-Loss: 0.0001, Val-Loss: 0.0115\n",
      "Epoch [900/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [910/1000], Train-Loss: 0.0001, Val-Loss: 0.0113\n",
      "Epoch [920/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [930/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [940/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0113\n",
      "Epoch [960/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [970/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [980/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [990/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "Epoch [1000/1000], Train-Loss: 0.0001, Val-Loss: 0.0114\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f443633f280>\n",
      "Epoch [10/1000], Train-Loss: 0.0354, Val-Loss: 0.0255\n",
      "Epoch [20/1000], Train-Loss: 0.0121, Val-Loss: 0.0131\n",
      "Epoch [30/1000], Train-Loss: 0.0152, Val-Loss: 0.0155\n",
      "Epoch [40/1000], Train-Loss: 0.0056, Val-Loss: 0.0095\n",
      "Epoch [50/1000], Train-Loss: 0.0048, Val-Loss: 0.0121\n",
      "Epoch [60/1000], Train-Loss: 0.0064, Val-Loss: 0.0095\n",
      "Epoch [70/1000], Train-Loss: 0.0066, Val-Loss: 0.0101\n",
      "Epoch [80/1000], Train-Loss: 0.0035, Val-Loss: 0.0092\n",
      "Epoch [90/1000], Train-Loss: 0.0032, Val-Loss: 0.0108\n",
      "Epoch [100/1000], Train-Loss: 0.0039, Val-Loss: 0.0094\n",
      "Epoch [110/1000], Train-Loss: 0.0020, Val-Loss: 0.0092\n",
      "Epoch [120/1000], Train-Loss: 0.0014, Val-Loss: 0.0090\n",
      "Epoch [130/1000], Train-Loss: 0.0039, Val-Loss: 0.0088\n",
      "Epoch [140/1000], Train-Loss: 0.0045, Val-Loss: 0.0096\n",
      "Epoch [150/1000], Train-Loss: 0.0035, Val-Loss: 0.0085\n",
      "Epoch [160/1000], Train-Loss: 0.0030, Val-Loss: 0.0089\n",
      "Epoch [170/1000], Train-Loss: 0.0035, Val-Loss: 0.0089\n",
      "Epoch [180/1000], Train-Loss: 0.0049, Val-Loss: 0.0090\n",
      "Epoch [190/1000], Train-Loss: 0.0028, Val-Loss: 0.0099\n",
      "Epoch [200/1000], Train-Loss: 0.0029, Val-Loss: 0.0089\n",
      "Epoch [210/1000], Train-Loss: 0.0029, Val-Loss: 0.0100\n",
      "Epoch [220/1000], Train-Loss: 0.0034, Val-Loss: 0.0089\n",
      "Epoch [230/1000], Train-Loss: 0.0022, Val-Loss: 0.0095\n",
      "Epoch [240/1000], Train-Loss: 0.0025, Val-Loss: 0.0087\n",
      "Epoch [250/1000], Train-Loss: 0.0016, Val-Loss: 0.0094\n",
      "Epoch [260/1000], Train-Loss: 0.0013, Val-Loss: 0.0091\n",
      "Epoch [270/1000], Train-Loss: 0.0030, Val-Loss: 0.0100\n",
      "Epoch [280/1000], Train-Loss: 0.0008, Val-Loss: 0.0097\n",
      "Epoch [290/1000], Train-Loss: 0.0012, Val-Loss: 0.0089\n",
      "Epoch [300/1000], Train-Loss: 0.0006, Val-Loss: 0.0092\n",
      "Epoch [310/1000], Train-Loss: 0.0007, Val-Loss: 0.0093\n",
      "Epoch [320/1000], Train-Loss: 0.0008, Val-Loss: 0.0093\n",
      "Epoch [330/1000], Train-Loss: 0.0011, Val-Loss: 0.0090\n",
      "Epoch [340/1000], Train-Loss: 0.0009, Val-Loss: 0.0090\n",
      "Epoch [350/1000], Train-Loss: 0.0007, Val-Loss: 0.0096\n",
      "Epoch [360/1000], Train-Loss: 0.0018, Val-Loss: 0.0086\n",
      "Epoch [370/1000], Train-Loss: 0.0012, Val-Loss: 0.0096\n",
      "Epoch [380/1000], Train-Loss: 0.0006, Val-Loss: 0.0089\n",
      "Epoch [390/1000], Train-Loss: 0.0006, Val-Loss: 0.0087\n",
      "Epoch [400/1000], Train-Loss: 0.0005, Val-Loss: 0.0092\n",
      "Epoch [410/1000], Train-Loss: 0.0005, Val-Loss: 0.0091\n",
      "Epoch [420/1000], Train-Loss: 0.0005, Val-Loss: 0.0088\n",
      "Epoch [430/1000], Train-Loss: 0.0013, Val-Loss: 0.0093\n",
      "Epoch [440/1000], Train-Loss: 0.0009, Val-Loss: 0.0087\n",
      "Epoch [450/1000], Train-Loss: 0.0021, Val-Loss: 0.0087\n",
      "Epoch [460/1000], Train-Loss: 0.0020, Val-Loss: 0.0088\n",
      "Epoch [470/1000], Train-Loss: 0.0022, Val-Loss: 0.0093\n",
      "Epoch [480/1000], Train-Loss: 0.0022, Val-Loss: 0.0101\n",
      "Epoch [490/1000], Train-Loss: 0.0017, Val-Loss: 0.0090\n",
      "Epoch [500/1000], Train-Loss: 0.0007, Val-Loss: 0.0088\n",
      "Epoch [510/1000], Train-Loss: 0.0008, Val-Loss: 0.0091\n",
      "Epoch [520/1000], Train-Loss: 0.0003, Val-Loss: 0.0087\n",
      "Epoch [530/1000], Train-Loss: 0.0011, Val-Loss: 0.0089\n",
      "Epoch [540/1000], Train-Loss: 0.0014, Val-Loss: 0.0088\n",
      "Epoch [550/1000], Train-Loss: 0.0007, Val-Loss: 0.0092\n",
      "Epoch [560/1000], Train-Loss: 0.0003, Val-Loss: 0.0091\n",
      "Epoch [570/1000], Train-Loss: 0.0008, Val-Loss: 0.0088\n",
      "Epoch [580/1000], Train-Loss: 0.0006, Val-Loss: 0.0089\n",
      "Epoch [590/1000], Train-Loss: 0.0003, Val-Loss: 0.0088\n",
      "Epoch [600/1000], Train-Loss: 0.0011, Val-Loss: 0.0094\n",
      "Epoch [610/1000], Train-Loss: 0.0004, Val-Loss: 0.0092\n",
      "Epoch [620/1000], Train-Loss: 0.0003, Val-Loss: 0.0088\n",
      "Epoch [630/1000], Train-Loss: 0.0005, Val-Loss: 0.0090\n",
      "Epoch [640/1000], Train-Loss: 0.0004, Val-Loss: 0.0089\n",
      "Epoch [650/1000], Train-Loss: 0.0003, Val-Loss: 0.0089\n",
      "Epoch [660/1000], Train-Loss: 0.0002, Val-Loss: 0.0090\n",
      "Epoch [670/1000], Train-Loss: 0.0007, Val-Loss: 0.0092\n",
      "Epoch [680/1000], Train-Loss: 0.0001, Val-Loss: 0.0088\n",
      "Epoch [690/1000], Train-Loss: 0.0003, Val-Loss: 0.0090\n",
      "Epoch [700/1000], Train-Loss: 0.0008, Val-Loss: 0.0091\n",
      "Epoch [710/1000], Train-Loss: 0.0008, Val-Loss: 0.0087\n",
      "Epoch [720/1000], Train-Loss: 0.0005, Val-Loss: 0.0087\n",
      "Epoch [730/1000], Train-Loss: 0.0006, Val-Loss: 0.0091\n",
      "Epoch [740/1000], Train-Loss: 0.0002, Val-Loss: 0.0088\n",
      "Epoch [750/1000], Train-Loss: 0.0001, Val-Loss: 0.0090\n",
      "Epoch [760/1000], Train-Loss: 0.0001, Val-Loss: 0.0088\n",
      "Epoch [770/1000], Train-Loss: 0.0002, Val-Loss: 0.0087\n",
      "Epoch [780/1000], Train-Loss: 0.0001, Val-Loss: 0.0090\n",
      "Epoch [790/1000], Train-Loss: 0.0004, Val-Loss: 0.0090\n",
      "Epoch [800/1000], Train-Loss: 0.0004, Val-Loss: 0.0089\n",
      "Epoch [810/1000], Train-Loss: 0.0002, Val-Loss: 0.0088\n",
      "Epoch [820/1000], Train-Loss: 0.0003, Val-Loss: 0.0088\n",
      "Epoch [830/1000], Train-Loss: 0.0002, Val-Loss: 0.0088\n",
      "Epoch [840/1000], Train-Loss: 0.0004, Val-Loss: 0.0089\n",
      "Epoch [850/1000], Train-Loss: 0.0002, Val-Loss: 0.0089\n",
      "Epoch [860/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [870/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [880/1000], Train-Loss: 0.0001, Val-Loss: 0.0088\n",
      "Epoch [890/1000], Train-Loss: 0.0001, Val-Loss: 0.0088\n",
      "Epoch [900/1000], Train-Loss: 0.0001, Val-Loss: 0.0088\n",
      "Epoch [910/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [920/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [930/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [940/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0088\n",
      "Epoch [960/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [970/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "Epoch [980/1000], Train-Loss: 0.0000, Val-Loss: 0.0088\n",
      "Epoch [990/1000], Train-Loss: 0.0002, Val-Loss: 0.0089\n",
      "Epoch [1000/1000], Train-Loss: 0.0001, Val-Loss: 0.0089\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f443633f040>\n",
      "Epoch [10/1000], Train-Loss: 0.0294, Val-Loss: 0.0418\n",
      "Epoch [20/1000], Train-Loss: 0.0107, Val-Loss: 0.0216\n",
      "Epoch [30/1000], Train-Loss: 0.0147, Val-Loss: 0.0181\n",
      "Epoch [40/1000], Train-Loss: 0.0048, Val-Loss: 0.0194\n",
      "Epoch [50/1000], Train-Loss: 0.0039, Val-Loss: 0.0214\n",
      "Epoch [60/1000], Train-Loss: 0.0061, Val-Loss: 0.0188\n",
      "Epoch [70/1000], Train-Loss: 0.0079, Val-Loss: 0.0189\n",
      "Epoch [80/1000], Train-Loss: 0.0050, Val-Loss: 0.0204\n",
      "Epoch [90/1000], Train-Loss: 0.0044, Val-Loss: 0.0196\n",
      "Epoch [100/1000], Train-Loss: 0.0044, Val-Loss: 0.0200\n",
      "Epoch [110/1000], Train-Loss: 0.0026, Val-Loss: 0.0190\n",
      "Epoch [120/1000], Train-Loss: 0.0013, Val-Loss: 0.0193\n",
      "Epoch [130/1000], Train-Loss: 0.0017, Val-Loss: 0.0200\n",
      "Epoch [140/1000], Train-Loss: 0.0017, Val-Loss: 0.0191\n",
      "Epoch [150/1000], Train-Loss: 0.0017, Val-Loss: 0.0189\n",
      "Epoch [160/1000], Train-Loss: 0.0019, Val-Loss: 0.0183\n",
      "Epoch [170/1000], Train-Loss: 0.0031, Val-Loss: 0.0186\n",
      "Epoch [180/1000], Train-Loss: 0.0052, Val-Loss: 0.0210\n",
      "Epoch [190/1000], Train-Loss: 0.0034, Val-Loss: 0.0195\n",
      "Epoch [200/1000], Train-Loss: 0.0052, Val-Loss: 0.0187\n",
      "Epoch [210/1000], Train-Loss: 0.0045, Val-Loss: 0.0189\n",
      "Epoch [220/1000], Train-Loss: 0.0050, Val-Loss: 0.0210\n",
      "Epoch [230/1000], Train-Loss: 0.0012, Val-Loss: 0.0193\n",
      "Epoch [240/1000], Train-Loss: 0.0010, Val-Loss: 0.0192\n",
      "Epoch [250/1000], Train-Loss: 0.0007, Val-Loss: 0.0196\n",
      "Epoch [260/1000], Train-Loss: 0.0030, Val-Loss: 0.0200\n",
      "Epoch [270/1000], Train-Loss: 0.0023, Val-Loss: 0.0188\n",
      "Epoch [280/1000], Train-Loss: 0.0019, Val-Loss: 0.0200\n",
      "Epoch [290/1000], Train-Loss: 0.0022, Val-Loss: 0.0190\n",
      "Epoch [300/1000], Train-Loss: 0.0009, Val-Loss: 0.0189\n",
      "Epoch [310/1000], Train-Loss: 0.0008, Val-Loss: 0.0194\n",
      "Epoch [320/1000], Train-Loss: 0.0015, Val-Loss: 0.0193\n",
      "Epoch [330/1000], Train-Loss: 0.0016, Val-Loss: 0.0187\n",
      "Epoch [340/1000], Train-Loss: 0.0036, Val-Loss: 0.0207\n",
      "Epoch [350/1000], Train-Loss: 0.0033, Val-Loss: 0.0194\n",
      "Epoch [360/1000], Train-Loss: 0.0015, Val-Loss: 0.0197\n",
      "Epoch [370/1000], Train-Loss: 0.0021, Val-Loss: 0.0191\n",
      "Epoch [380/1000], Train-Loss: 0.0014, Val-Loss: 0.0196\n",
      "Epoch [390/1000], Train-Loss: 0.0011, Val-Loss: 0.0192\n",
      "Epoch [400/1000], Train-Loss: 0.0019, Val-Loss: 0.0195\n",
      "Epoch [410/1000], Train-Loss: 0.0010, Val-Loss: 0.0195\n",
      "Epoch [420/1000], Train-Loss: 0.0007, Val-Loss: 0.0191\n",
      "Epoch [430/1000], Train-Loss: 0.0005, Val-Loss: 0.0190\n",
      "Epoch [440/1000], Train-Loss: 0.0005, Val-Loss: 0.0195\n",
      "Epoch [450/1000], Train-Loss: 0.0004, Val-Loss: 0.0195\n",
      "Epoch [460/1000], Train-Loss: 0.0004, Val-Loss: 0.0193\n",
      "Epoch [470/1000], Train-Loss: 0.0004, Val-Loss: 0.0194\n",
      "Epoch [480/1000], Train-Loss: 0.0004, Val-Loss: 0.0192\n",
      "Epoch [490/1000], Train-Loss: 0.0007, Val-Loss: 0.0192\n",
      "Epoch [500/1000], Train-Loss: 0.0003, Val-Loss: 0.0190\n",
      "Epoch [510/1000], Train-Loss: 0.0028, Val-Loss: 0.0197\n",
      "Epoch [520/1000], Train-Loss: 0.0004, Val-Loss: 0.0198\n",
      "Epoch [530/1000], Train-Loss: 0.0011, Val-Loss: 0.0189\n",
      "Epoch [540/1000], Train-Loss: 0.0008, Val-Loss: 0.0193\n",
      "Epoch [550/1000], Train-Loss: 0.0010, Val-Loss: 0.0194\n",
      "Epoch [560/1000], Train-Loss: 0.0006, Val-Loss: 0.0192\n",
      "Epoch [570/1000], Train-Loss: 0.0004, Val-Loss: 0.0195\n",
      "Epoch [580/1000], Train-Loss: 0.0009, Val-Loss: 0.0196\n",
      "Epoch [590/1000], Train-Loss: 0.0008, Val-Loss: 0.0193\n",
      "Epoch [600/1000], Train-Loss: 0.0002, Val-Loss: 0.0192\n",
      "Epoch [610/1000], Train-Loss: 0.0005, Val-Loss: 0.0195\n",
      "Epoch [620/1000], Train-Loss: 0.0002, Val-Loss: 0.0191\n",
      "Epoch [630/1000], Train-Loss: 0.0004, Val-Loss: 0.0195\n",
      "Epoch [640/1000], Train-Loss: 0.0002, Val-Loss: 0.0192\n",
      "Epoch [650/1000], Train-Loss: 0.0003, Val-Loss: 0.0191\n",
      "Epoch [660/1000], Train-Loss: 0.0006, Val-Loss: 0.0192\n",
      "Epoch [670/1000], Train-Loss: 0.0008, Val-Loss: 0.0192\n",
      "Epoch [680/1000], Train-Loss: 0.0010, Val-Loss: 0.0196\n",
      "Epoch [690/1000], Train-Loss: 0.0002, Val-Loss: 0.0190\n",
      "Epoch [700/1000], Train-Loss: 0.0003, Val-Loss: 0.0193\n",
      "Epoch [710/1000], Train-Loss: 0.0003, Val-Loss: 0.0191\n",
      "Epoch [720/1000], Train-Loss: 0.0001, Val-Loss: 0.0193\n",
      "Epoch [730/1000], Train-Loss: 0.0002, Val-Loss: 0.0194\n",
      "Epoch [740/1000], Train-Loss: 0.0003, Val-Loss: 0.0191\n",
      "Epoch [750/1000], Train-Loss: 0.0008, Val-Loss: 0.0194\n",
      "Epoch [760/1000], Train-Loss: 0.0005, Val-Loss: 0.0194\n",
      "Epoch [770/1000], Train-Loss: 0.0004, Val-Loss: 0.0191\n",
      "Epoch [780/1000], Train-Loss: 0.0010, Val-Loss: 0.0195\n",
      "Epoch [790/1000], Train-Loss: 0.0001, Val-Loss: 0.0191\n",
      "Epoch [800/1000], Train-Loss: 0.0001, Val-Loss: 0.0193\n",
      "Epoch [810/1000], Train-Loss: 0.0005, Val-Loss: 0.0191\n",
      "Epoch [820/1000], Train-Loss: 0.0007, Val-Loss: 0.0194\n",
      "Epoch [830/1000], Train-Loss: 0.0001, Val-Loss: 0.0194\n",
      "Epoch [840/1000], Train-Loss: 0.0001, Val-Loss: 0.0192\n",
      "Epoch [850/1000], Train-Loss: 0.0002, Val-Loss: 0.0192\n",
      "Epoch [860/1000], Train-Loss: 0.0003, Val-Loss: 0.0194\n",
      "Epoch [870/1000], Train-Loss: 0.0003, Val-Loss: 0.0193\n",
      "Epoch [880/1000], Train-Loss: 0.0001, Val-Loss: 0.0193\n",
      "Epoch [890/1000], Train-Loss: 0.0001, Val-Loss: 0.0192\n",
      "Epoch [900/1000], Train-Loss: 0.0002, Val-Loss: 0.0192\n",
      "Epoch [910/1000], Train-Loss: 0.0001, Val-Loss: 0.0192\n",
      "Epoch [920/1000], Train-Loss: 0.0001, Val-Loss: 0.0193\n",
      "Epoch [930/1000], Train-Loss: 0.0001, Val-Loss: 0.0192\n",
      "Epoch [940/1000], Train-Loss: 0.0002, Val-Loss: 0.0193\n",
      "Epoch [950/1000], Train-Loss: 0.0001, Val-Loss: 0.0193\n",
      "Epoch [960/1000], Train-Loss: 0.0001, Val-Loss: 0.0192\n",
      "Epoch [970/1000], Train-Loss: 0.0002, Val-Loss: 0.0192\n",
      "Epoch [980/1000], Train-Loss: 0.0001, Val-Loss: 0.0193\n",
      "Epoch [990/1000], Train-Loss: 0.0002, Val-Loss: 0.0193\n",
      "Epoch [1000/1000], Train-Loss: 0.0002, Val-Loss: 0.0193\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot for introduction\n",
    "\n",
    "k_fold_splits = 4\n",
    "sample_size = 100\n",
    "model_fitting_kwargs = {\n",
    "    'start_lr': 0.005,\n",
    "    'epochs': 1000,\n",
    "    'scheduler_class': torch.optim.lr_scheduler.ExponentialLR,\n",
    "    'scheduler_args': {'gamma': float(np.power(1e-4/0.005, 1/100))},\n",
    "    }\n",
    "evaluation_model_class = SimpleNN\n",
    "\n",
    "predicted_performances_timm, gt_preformances_timm = get_predicted_and_gt_performance_lists(model_confidences_tensor=timm_model_confidence_for_main_set_tensor,\n",
    "                                                                                           model_catalog=timm_model_catalog,\n",
    "                                                                                           sample_size=sample_size,\n",
    "                                                                                           k_fold_splits=k_fold_splits,\n",
    "                                                                                           evaluation_model_class=SimpleNN,\n",
    "                                                                                           model_fitting_kwargs=model_fitting_kwargs)\n",
    "\n",
    "predicted_performances_open_clip, gt_preformances_open_clip = get_predicted_and_gt_performance_lists(model_confidences_tensor=open_clip_model_confidence_for_main_set_tensor,\n",
    "                                                                                           model_catalog=open_clip_model_catalog,\n",
    "                                                                                           sample_size=sample_size,\n",
    "                                                                                           k_fold_splits=k_fold_splits,\n",
    "                                                                                           evaluation_model_class=SimpleNN,\n",
    "                                                                                           model_fitting_kwargs=model_fitting_kwargs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=[float(x) for x in gt_preformances_timm], y=[float(x) for x in predicted_performances_timm], s=0.2, label='Timm', c='blue')\n",
    "ax.scatter(x=[float(x) for x in gt_preformances_open_clip], y=[float(x) for x in predicted_performances_open_clip], s=0.2, label='Open-Clip', c='red')\n",
    "ax.set_xlabel('Ground Truth Performance', fontsize=18)\n",
    "ax.set_ylabel('Predicted Performance', fontsize=18)\n",
    "\n",
    "ax.legend(scatterpoints=1, markerscale=10, fontsize='large')\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "plt.savefig('/mnt/lustre/work/oh/owl813/repos/model-selection/figures/pred_perf_vs_get_perf.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val Sample Selection - ImageNet (Timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_result_performances = []\n",
    "for model_info in timm_model_catalog.values():\n",
    "    timm_result_performances.append(model_info['results']['imagenet1k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030274022825760017"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_mae(100, timm_result_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6528228970272456"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_empirical_rank_score(100, timm_result_performances, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {'imagenet1k'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  tensor(0.0141) ; rank:  0.8724969633821443\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor,\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=NNModel,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='random',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['imagenet1k'])/len(mae['imagenet1k']), '; rank: ', sum([x.correlation for x in rank['imagenet1k']])/len(rank['imagenet1k']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  tensor(0.0095) ; rank:  0.930287519283355\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=NNModel,\n",
    "                                            number_bootstraping_steps=1,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='first',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['imagenet1k'])/len(mae['imagenet1k']), '; rank: ', sum([x.correlation for x in rank['imagenet1k']])/len(rank['imagenet1k']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/700], Train-Loss: 0.0626, Val-Loss: 0.0964\n",
      "Epoch [20/700], Train-Loss: 0.0205, Val-Loss: 0.0264\n",
      "Epoch [30/700], Train-Loss: 0.0027, Val-Loss: 0.0051\n",
      "Epoch [40/700], Train-Loss: 0.0010, Val-Loss: 0.0021\n",
      "Epoch [50/700], Train-Loss: 0.0003, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.2405e-04, 6.8295e-05, 1.1762e-04,  ..., 7.8988e-05, 6.4136e-05,\n",
      "         5.6969e-05],\n",
      "        [3.3481e-04, 1.3777e-04, 9.8533e-05,  ..., 7.6427e-05, 4.2704e-05,\n",
      "         1.2015e-04],\n",
      "        [1.3980e-04, 1.9814e-04, 1.4147e-04,  ..., 7.8148e-05, 2.5250e-04,\n",
      "         3.0559e-04],\n",
      "        ...,\n",
      "        [9.2345e-05, 6.8127e-05, 1.6408e-04,  ..., 7.5656e-05, 1.0442e-04,\n",
      "         1.0649e-04],\n",
      "        [1.5910e-04, 4.8881e-05, 9.4536e-05,  ..., 5.4535e-05, 7.3459e-05,\n",
      "         6.4197e-05],\n",
      "        [3.7648e-04, 2.2924e-05, 8.3270e-05,  ..., 6.4708e-05, 1.6080e-04,\n",
      "         5.6245e-05]], device='cuda:0')\n",
      "pred: tensor([0.8403, 0.8311, 0.7901, 0.8039, 0.7831, 0.8271, 0.8168, 0.7997, 0.7953,\n",
      "        0.8183, 0.7904, 0.8301, 0.7970, 0.8121, 0.8418, 0.8070, 0.8278, 0.8271,\n",
      "        0.7802, 0.7927, 0.8175, 0.8427, 0.8594, 0.8589, 0.8824, 0.8031, 0.8736,\n",
      "        0.8186, 0.8581, 0.8512, 0.8551, 0.8658, 0.8703, 0.8659, 0.8550, 0.8664,\n",
      "        0.8308, 0.8487, 0.8286, 0.8277, 0.8438, 0.8713, 0.8051, 0.8625, 0.8277,\n",
      "        0.7974, 0.8370, 0.8290, 0.8333, 0.8364, 0.7502, 0.7866, 0.7441, 0.7746,\n",
      "        0.7858, 0.7747, 0.8309, 0.8319, 0.7619, 0.8401, 0.7925, 0.8037, 0.7819,\n",
      "        0.8112, 0.8281, 0.8541, 0.8030, 0.7774, 0.8102, 0.8313, 0.8510, 0.8055,\n",
      "        0.8335, 0.8062, 0.7931, 0.8583, 0.8560, 0.8468, 0.8608, 0.8549, 0.8357,\n",
      "        0.8315, 0.8599, 0.8411, 0.8607, 0.8170, 0.7693, 0.8520, 0.7871, 0.8344,\n",
      "        0.8388, 0.8404, 0.8088, 0.8377, 0.8637, 0.8539, 0.8117, 0.8661, 0.8556,\n",
      "        0.8454], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1382, Val-Loss: 0.0931\n",
      "Epoch [20/700], Train-Loss: 0.0380, Val-Loss: 0.0301\n",
      "Epoch [30/700], Train-Loss: 0.0133, Val-Loss: 0.0089\n",
      "Epoch [40/700], Train-Loss: 0.0046, Val-Loss: 0.0033\n",
      "Epoch [50/700], Train-Loss: 0.0014, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.3448e-04, 1.9590e-04, 2.9411e-04,  ..., 1.9217e-05, 3.9508e-05,\n",
      "         3.0775e-05],\n",
      "        [1.4931e-04, 1.8192e-04, 1.3911e-04,  ..., 8.5582e-05, 1.5317e-04,\n",
      "         1.9771e-04],\n",
      "        [3.4473e-04, 1.6516e-04, 2.9696e-04,  ..., 2.4611e-04, 7.8323e-04,\n",
      "         1.0974e-04],\n",
      "        ...,\n",
      "        [2.4187e-04, 2.2312e-04, 3.3668e-04,  ..., 4.3712e-05, 8.9119e-05,\n",
      "         7.4080e-05],\n",
      "        [2.7569e-04, 2.2379e-04, 2.7347e-04,  ..., 1.3315e-05, 2.1716e-05,\n",
      "         4.9494e-05],\n",
      "        [9.8853e-05, 2.6434e-04, 1.8563e-04,  ..., 5.2100e-05, 2.9107e-05,\n",
      "         2.4090e-05]], device='cuda:0')\n",
      "pred: tensor([0.8393, 0.8268, 0.7996, 0.8155, 0.7838, 0.8271, 0.8154, 0.8066, 0.7951,\n",
      "        0.8259, 0.7878, 0.8299, 0.7966, 0.8180, 0.8288, 0.8168, 0.8330, 0.8362,\n",
      "        0.7895, 0.7931, 0.8214, 0.8349, 0.8592, 0.8605, 0.8724, 0.8095, 0.8642,\n",
      "        0.8103, 0.8480, 0.8386, 0.8449, 0.8640, 0.8623, 0.8667, 0.8548, 0.8673,\n",
      "        0.8239, 0.8498, 0.8380, 0.8191, 0.8392, 0.8653, 0.8009, 0.8712, 0.8328,\n",
      "        0.7935, 0.8446, 0.8244, 0.8358, 0.8481, 0.7516, 0.7779, 0.7505, 0.7610,\n",
      "        0.7694, 0.7732, 0.8244, 0.8239, 0.7540, 0.8444, 0.8099, 0.7858, 0.7879,\n",
      "        0.8076, 0.8378, 0.8404, 0.7975, 0.7794, 0.8161, 0.8367, 0.8330, 0.8066,\n",
      "        0.8256, 0.8115, 0.8019, 0.8611, 0.8493, 0.8386, 0.8495, 0.8443, 0.8333,\n",
      "        0.8366, 0.8567, 0.8326, 0.8561, 0.8101, 0.7796, 0.8379, 0.8070, 0.8409,\n",
      "        0.8348, 0.8420, 0.8047, 0.8313, 0.8586, 0.8424, 0.7998, 0.8645, 0.8434,\n",
      "        0.8419], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0858, Val-Loss: 0.0323\n",
      "Epoch [20/700], Train-Loss: 0.0186, Val-Loss: 0.0077\n",
      "Epoch [30/700], Train-Loss: 0.0020, Val-Loss: 0.0002\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[7.8882e-05, 4.4033e-04, 1.7832e-04,  ..., 1.4948e-04, 7.0115e-05,\n",
      "         1.1308e-04],\n",
      "        [9.7339e-05, 2.1794e-04, 2.1439e-04,  ..., 1.3969e-04, 7.3526e-05,\n",
      "         3.6766e-04],\n",
      "        [3.5637e-04, 4.1847e-04, 1.9592e-04,  ..., 1.7807e-04, 3.7318e-04,\n",
      "         2.4041e-04],\n",
      "        ...,\n",
      "        [1.8437e-04, 1.5575e-03, 3.8221e-04,  ..., 1.1346e-04, 1.1782e-04,\n",
      "         1.5118e-04],\n",
      "        [5.9554e-05, 9.1577e-05, 2.1219e-04,  ..., 7.5124e-05, 7.4038e-05,\n",
      "         1.0181e-04],\n",
      "        [4.7928e-05, 1.4753e-04, 7.9685e-05,  ..., 8.7096e-05, 8.8507e-05,\n",
      "         9.3401e-05]], device='cuda:0')\n",
      "pred: tensor([0.8397, 0.8320, 0.8018, 0.8151, 0.7752, 0.8332, 0.8044, 0.8132, 0.7807,\n",
      "        0.8277, 0.7931, 0.8184, 0.7950, 0.8147, 0.8356, 0.8069, 0.8291, 0.8271,\n",
      "        0.7856, 0.8059, 0.8288, 0.8477, 0.8578, 0.8589, 0.8769, 0.8060, 0.8742,\n",
      "        0.8178, 0.8569, 0.8424, 0.8492, 0.8701, 0.8692, 0.8647, 0.8563, 0.8668,\n",
      "        0.8139, 0.8489, 0.8251, 0.8249, 0.8375, 0.8699, 0.7927, 0.8667, 0.8243,\n",
      "        0.8058, 0.8402, 0.8387, 0.8297, 0.8333, 0.7631, 0.7762, 0.7608, 0.7762,\n",
      "        0.7892, 0.7719, 0.8246, 0.8225, 0.7599, 0.8422, 0.8086, 0.7989, 0.7646,\n",
      "        0.8040, 0.8320, 0.8487, 0.7978, 0.7666, 0.8176, 0.8348, 0.8326, 0.8010,\n",
      "        0.8208, 0.7974, 0.8019, 0.8575, 0.8529, 0.8515, 0.8571, 0.8533, 0.8365,\n",
      "        0.8363, 0.8663, 0.8503, 0.8665, 0.8247, 0.7768, 0.8409, 0.7892, 0.8436,\n",
      "        0.8499, 0.8347, 0.8010, 0.8269, 0.8653, 0.8462, 0.8074, 0.8584, 0.8446,\n",
      "        0.8393], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1076, Val-Loss: 0.0548\n",
      "Epoch [20/700], Train-Loss: 0.0246, Val-Loss: 0.0143\n",
      "Epoch [30/700], Train-Loss: 0.0059, Val-Loss: 0.0019\n",
      "Epoch [40/700], Train-Loss: 0.0014, Val-Loss: 0.0004\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[8.8578e-05, 1.2327e-04, 7.6622e-05,  ..., 1.3906e-04, 7.9905e-05,\n",
      "         8.8010e-05],\n",
      "        [8.8093e-05, 1.9290e-04, 7.0881e-05,  ..., 7.5561e-05, 1.8043e-05,\n",
      "         7.6180e-05],\n",
      "        [1.7234e-04, 2.6863e-04, 1.6092e-04,  ..., 3.4585e-04, 2.0899e-04,\n",
      "         1.9099e-04],\n",
      "        ...,\n",
      "        [8.5290e-05, 9.1485e-05, 1.0307e-04,  ..., 1.3003e-04, 9.8168e-05,\n",
      "         2.2185e-04],\n",
      "        [4.8553e-05, 1.2049e-04, 1.3675e-04,  ..., 5.1761e-05, 6.0617e-05,\n",
      "         1.0378e-04],\n",
      "        [5.6402e-05, 1.3375e-04, 7.6206e-05,  ..., 1.4067e-04, 6.0469e-05,\n",
      "         1.0853e-04]], device='cuda:0')\n",
      "pred: tensor([0.8398, 0.8312, 0.8051, 0.8101, 0.7763, 0.8230, 0.8128, 0.8166, 0.8053,\n",
      "        0.8188, 0.7947, 0.8244, 0.7983, 0.8092, 0.8406, 0.8099, 0.8274, 0.8201,\n",
      "        0.7760, 0.7856, 0.8218, 0.8301, 0.8584, 0.8529, 0.8791, 0.7966, 0.8679,\n",
      "        0.8102, 0.8620, 0.8489, 0.8562, 0.8700, 0.8652, 0.8602, 0.8588, 0.8758,\n",
      "        0.8236, 0.8364, 0.8296, 0.8204, 0.8344, 0.8728, 0.7959, 0.8651, 0.8264,\n",
      "        0.8000, 0.8383, 0.8256, 0.8351, 0.8332, 0.7631, 0.7975, 0.7690, 0.7831,\n",
      "        0.7919, 0.7826, 0.8287, 0.8244, 0.7583, 0.8364, 0.7962, 0.8063, 0.7787,\n",
      "        0.8097, 0.8326, 0.8440, 0.7937, 0.7799, 0.7989, 0.8364, 0.8313, 0.7950,\n",
      "        0.8130, 0.8122, 0.7955, 0.8678, 0.8597, 0.8466, 0.8581, 0.8559, 0.8312,\n",
      "        0.8406, 0.8558, 0.8345, 0.8597, 0.8164, 0.7858, 0.8413, 0.7870, 0.8333,\n",
      "        0.8258, 0.8360, 0.8048, 0.8249, 0.8626, 0.8450, 0.8034, 0.8651, 0.8551,\n",
      "        0.8399], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1156, Val-Loss: 0.0613\n",
      "Epoch [20/700], Train-Loss: 0.0284, Val-Loss: 0.0175\n",
      "Epoch [30/700], Train-Loss: 0.0085, Val-Loss: 0.0039\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0012\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0161e-04, 1.2178e-04, 1.7209e-04,  ..., 1.7159e-04, 3.0825e-04,\n",
      "         4.4706e-04],\n",
      "        [5.9201e-05, 1.1934e-04, 4.4009e-05,  ..., 1.3333e-04, 2.6876e-04,\n",
      "         9.2810e-05],\n",
      "        [8.0567e-04, 3.0838e-04, 1.4500e-03,  ..., 2.3596e-04, 1.5716e-04,\n",
      "         2.3376e-04],\n",
      "        ...,\n",
      "        [1.3999e-04, 1.5250e-04, 1.3108e-04,  ..., 8.8987e-05, 9.0777e-05,\n",
      "         7.8741e-05],\n",
      "        [5.6358e-05, 1.0558e-04, 3.0688e-04,  ..., 1.3343e-04, 7.0756e-05,\n",
      "         1.7485e-04],\n",
      "        [5.6255e-05, 7.5637e-05, 6.6433e-05,  ..., 7.4338e-05, 1.1904e-04,\n",
      "         1.0346e-04]], device='cuda:0')\n",
      "pred: tensor([0.8447, 0.8213, 0.8046, 0.8198, 0.7810, 0.8346, 0.8227, 0.8159, 0.7997,\n",
      "        0.8280, 0.8044, 0.8388, 0.7989, 0.8132, 0.8381, 0.8074, 0.8134, 0.8269,\n",
      "        0.7871, 0.7916, 0.8297, 0.8372, 0.8605, 0.8608, 0.8782, 0.8006, 0.8620,\n",
      "        0.8215, 0.8547, 0.8494, 0.8492, 0.8675, 0.8683, 0.8662, 0.8545, 0.8714,\n",
      "        0.8317, 0.8475, 0.8345, 0.8341, 0.8419, 0.8715, 0.8029, 0.8744, 0.8299,\n",
      "        0.7946, 0.8438, 0.8197, 0.8384, 0.8344, 0.7521, 0.7836, 0.7472, 0.7722,\n",
      "        0.7840, 0.7694, 0.8263, 0.8246, 0.7405, 0.8505, 0.7934, 0.8051, 0.7750,\n",
      "        0.8089, 0.8366, 0.8493, 0.7928, 0.7690, 0.8070, 0.8356, 0.8444, 0.8035,\n",
      "        0.8328, 0.8094, 0.8016, 0.8686, 0.8557, 0.8477, 0.8623, 0.8656, 0.8313,\n",
      "        0.8364, 0.8656, 0.8434, 0.8641, 0.8200, 0.7629, 0.8387, 0.7977, 0.8346,\n",
      "        0.8452, 0.8355, 0.8004, 0.8312, 0.8651, 0.8483, 0.8040, 0.8575, 0.8539,\n",
      "        0.8401], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0868, Val-Loss: 0.0983\n",
      "Epoch [20/700], Train-Loss: 0.0280, Val-Loss: 0.0304\n",
      "Epoch [30/700], Train-Loss: 0.0068, Val-Loss: 0.0084\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0036\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0011\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[6.1163e-05, 7.3457e-05, 7.3244e-05,  ..., 7.3457e-05, 5.9418e-05,\n",
      "         2.1474e-04],\n",
      "        [8.9234e-05, 9.8034e-05, 1.2205e-04,  ..., 8.0922e-05, 4.4419e-05,\n",
      "         1.7513e-04],\n",
      "        [4.4412e-04, 3.2900e-04, 3.6232e-04,  ..., 3.4097e-04, 1.2320e-04,\n",
      "         2.4667e-04],\n",
      "        ...,\n",
      "        [7.3152e-05, 9.8338e-05, 9.7785e-05,  ..., 1.0723e-04, 1.0518e-04,\n",
      "         2.8022e-04],\n",
      "        [9.5635e-05, 1.1917e-04, 1.3753e-04,  ..., 6.1090e-05, 9.0726e-05,\n",
      "         1.2428e-04],\n",
      "        [1.4452e-04, 7.9194e-05, 7.2973e-05,  ..., 1.2178e-04, 4.6032e-05,\n",
      "         1.9433e-04]], device='cuda:0')\n",
      "pred: tensor([0.8424, 0.8299, 0.7955, 0.8135, 0.7852, 0.8277, 0.8227, 0.8156, 0.7964,\n",
      "        0.8364, 0.7980, 0.8208, 0.8101, 0.7986, 0.8394, 0.8123, 0.8304, 0.8277,\n",
      "        0.7839, 0.8088, 0.8283, 0.8369, 0.8653, 0.8583, 0.8890, 0.8018, 0.8702,\n",
      "        0.8165, 0.8549, 0.8469, 0.8516, 0.8694, 0.8629, 0.8667, 0.8645, 0.8720,\n",
      "        0.8293, 0.8478, 0.8371, 0.8237, 0.8423, 0.8709, 0.7973, 0.8742, 0.8295,\n",
      "        0.8036, 0.8451, 0.8170, 0.8308, 0.8350, 0.7513, 0.7817, 0.7518, 0.7802,\n",
      "        0.7717, 0.7682, 0.8387, 0.8341, 0.7537, 0.8539, 0.7987, 0.8110, 0.7787,\n",
      "        0.8028, 0.8402, 0.8478, 0.7992, 0.7746, 0.8114, 0.8336, 0.8361, 0.7943,\n",
      "        0.8288, 0.8058, 0.7961, 0.8664, 0.8517, 0.8416, 0.8618, 0.8592, 0.8337,\n",
      "        0.8430, 0.8614, 0.8364, 0.8654, 0.8160, 0.7759, 0.8424, 0.7921, 0.8399,\n",
      "        0.8404, 0.8344, 0.8040, 0.8319, 0.8634, 0.8463, 0.8080, 0.8553, 0.8452,\n",
      "        0.8306], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0819, Val-Loss: 0.0470\n",
      "Epoch [20/700], Train-Loss: 0.0200, Val-Loss: 0.0132\n",
      "Epoch [30/700], Train-Loss: 0.0055, Val-Loss: 0.0023\n",
      "Epoch [40/700], Train-Loss: 0.0015, Val-Loss: 0.0007\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.2026e-05, 1.2496e-04, 6.9240e-05,  ..., 1.5204e-04, 1.4438e-04,\n",
      "         1.0781e-04],\n",
      "        [1.0763e-04, 1.7192e-04, 1.1562e-04,  ..., 8.6569e-05, 6.3616e-05,\n",
      "         1.8011e-04],\n",
      "        [3.0448e-04, 1.9405e-04, 2.3220e-04,  ..., 9.6840e-05, 1.7631e-04,\n",
      "         1.8000e-04],\n",
      "        ...,\n",
      "        [1.4679e-04, 1.8170e-04, 1.6565e-04,  ..., 2.5413e-04, 1.3223e-04,\n",
      "         1.2319e-04],\n",
      "        [8.3785e-05, 1.0745e-04, 1.7263e-04,  ..., 6.1042e-05, 5.5013e-05,\n",
      "         7.5126e-05],\n",
      "        [7.0128e-05, 5.3016e-05, 6.3448e-05,  ..., 5.1600e-05, 7.0457e-05,\n",
      "         8.3459e-05]], device='cuda:0')\n",
      "pred: tensor([0.8325, 0.8148, 0.8039, 0.8151, 0.7785, 0.8201, 0.8267, 0.8190, 0.7997,\n",
      "        0.8316, 0.7955, 0.8329, 0.8018, 0.8113, 0.8281, 0.8046, 0.8303, 0.8275,\n",
      "        0.7919, 0.7884, 0.8261, 0.8368, 0.8573, 0.8506, 0.8807, 0.8030, 0.8670,\n",
      "        0.8152, 0.8506, 0.8497, 0.8551, 0.8809, 0.8642, 0.8630, 0.8538, 0.8719,\n",
      "        0.8287, 0.8429, 0.8194, 0.8224, 0.8372, 0.8652, 0.8008, 0.8652, 0.8190,\n",
      "        0.8018, 0.8469, 0.8231, 0.8308, 0.8442, 0.7414, 0.7826, 0.7515, 0.7686,\n",
      "        0.7801, 0.7694, 0.8290, 0.8273, 0.7577, 0.8435, 0.7893, 0.7966, 0.7820,\n",
      "        0.8028, 0.8308, 0.8435, 0.8053, 0.7869, 0.7960, 0.8354, 0.8303, 0.7970,\n",
      "        0.8384, 0.8021, 0.7944, 0.8748, 0.8533, 0.8521, 0.8573, 0.8576, 0.8246,\n",
      "        0.8340, 0.8594, 0.8416, 0.8632, 0.8194, 0.7668, 0.8397, 0.8044, 0.8335,\n",
      "        0.8313, 0.8352, 0.8068, 0.8303, 0.8570, 0.8410, 0.8074, 0.8527, 0.8521,\n",
      "        0.8391], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0991, Val-Loss: 0.0605\n",
      "Epoch [20/700], Train-Loss: 0.0277, Val-Loss: 0.0195\n",
      "Epoch [30/700], Train-Loss: 0.0085, Val-Loss: 0.0041\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0013\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.9294e-05, 9.9078e-05, 1.4983e-04,  ..., 7.5973e-05, 3.1427e-05,\n",
      "         6.4417e-05],\n",
      "        [1.7303e-04, 2.4059e-04, 2.7253e-04,  ..., 2.2125e-04, 1.0588e-04,\n",
      "         3.0093e-04],\n",
      "        [4.2088e-04, 2.9421e-04, 3.2045e-04,  ..., 1.5443e-04, 3.0750e-05,\n",
      "         1.1322e-04],\n",
      "        ...,\n",
      "        [1.6773e-04, 2.2536e-04, 2.7009e-04,  ..., 9.4313e-05, 2.7309e-05,\n",
      "         1.7892e-04],\n",
      "        [8.5762e-05, 8.3318e-05, 8.8782e-05,  ..., 9.3489e-05, 4.3657e-05,\n",
      "         7.0811e-05],\n",
      "        [6.6221e-05, 1.1145e-04, 5.5320e-05,  ..., 5.5473e-05, 3.0192e-05,\n",
      "         3.4846e-05]], device='cuda:0')\n",
      "pred: tensor([0.8341, 0.8238, 0.8054, 0.8158, 0.7821, 0.8241, 0.8066, 0.8086, 0.7991,\n",
      "        0.8153, 0.7928, 0.8313, 0.7978, 0.8238, 0.8344, 0.8075, 0.8271, 0.8325,\n",
      "        0.7911, 0.7946, 0.8331, 0.8401, 0.8613, 0.8605, 0.8762, 0.8088, 0.8721,\n",
      "        0.8083, 0.8525, 0.8453, 0.8543, 0.8694, 0.8658, 0.8562, 0.8621, 0.8675,\n",
      "        0.8390, 0.8469, 0.8326, 0.8041, 0.8427, 0.8780, 0.8028, 0.8646, 0.8304,\n",
      "        0.8034, 0.8428, 0.8201, 0.8316, 0.8303, 0.7608, 0.7821, 0.7528, 0.7660,\n",
      "        0.7749, 0.7759, 0.8314, 0.8343, 0.7592, 0.8381, 0.7993, 0.8112, 0.7871,\n",
      "        0.7988, 0.8340, 0.8449, 0.7905, 0.7706, 0.8081, 0.8326, 0.8332, 0.7969,\n",
      "        0.8265, 0.8071, 0.7940, 0.8630, 0.8381, 0.8501, 0.8542, 0.8540, 0.8306,\n",
      "        0.8370, 0.8677, 0.8359, 0.8654, 0.8179, 0.7767, 0.8393, 0.7982, 0.8466,\n",
      "        0.8372, 0.8381, 0.7948, 0.8264, 0.8642, 0.8413, 0.8009, 0.8563, 0.8467,\n",
      "        0.8399], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1389, Val-Loss: 0.1205\n",
      "Epoch [20/700], Train-Loss: 0.0347, Val-Loss: 0.0303\n",
      "Epoch [30/700], Train-Loss: 0.0119, Val-Loss: 0.0103\n",
      "Epoch [40/700], Train-Loss: 0.0045, Val-Loss: 0.0041\n",
      "Epoch [50/700], Train-Loss: 0.0017, Val-Loss: 0.0014\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.1945e-04, 2.4729e-04, 2.6424e-04,  ..., 7.9214e-05, 1.7181e-04,\n",
      "         3.3913e-04],\n",
      "        [2.4580e-04, 1.7401e-04, 2.2742e-04,  ..., 6.5596e-05, 1.6558e-04,\n",
      "         7.5816e-05],\n",
      "        [3.5181e-04, 6.5487e-04, 4.1689e-04,  ..., 2.0208e-04, 1.2720e-04,\n",
      "         2.4733e-04],\n",
      "        ...,\n",
      "        [1.2459e-04, 1.4525e-04, 5.5675e-05,  ..., 1.2696e-04, 1.1058e-04,\n",
      "         1.0950e-04],\n",
      "        [3.7071e-05, 5.5557e-05, 1.1592e-04,  ..., 1.6137e-05, 1.7476e-05,\n",
      "         6.0088e-05],\n",
      "        [6.3866e-05, 5.1958e-05, 1.4906e-04,  ..., 5.4129e-05, 2.5301e-05,\n",
      "         5.5243e-04]], device='cuda:0')\n",
      "pred: tensor([0.8352, 0.8356, 0.8159, 0.8227, 0.7935, 0.8348, 0.8260, 0.8152, 0.7975,\n",
      "        0.8132, 0.8001, 0.8243, 0.7965, 0.8176, 0.8392, 0.8035, 0.8268, 0.8251,\n",
      "        0.7892, 0.7860, 0.8295, 0.8402, 0.8598, 0.8587, 0.8732, 0.7997, 0.8664,\n",
      "        0.8194, 0.8536, 0.8512, 0.8553, 0.8729, 0.8669, 0.8747, 0.8563, 0.8744,\n",
      "        0.8313, 0.8519, 0.8436, 0.8384, 0.8428, 0.8789, 0.7998, 0.8638, 0.8306,\n",
      "        0.8087, 0.8505, 0.8289, 0.8375, 0.8366, 0.7620, 0.7867, 0.7618, 0.7812,\n",
      "        0.7872, 0.7797, 0.8344, 0.8277, 0.7481, 0.8456, 0.7936, 0.8008, 0.7784,\n",
      "        0.8061, 0.8301, 0.8428, 0.8064, 0.7825, 0.8106, 0.8302, 0.8367, 0.8038,\n",
      "        0.8289, 0.8016, 0.8000, 0.8705, 0.8550, 0.8475, 0.8681, 0.8541, 0.8328,\n",
      "        0.8349, 0.8614, 0.8415, 0.8603, 0.8195, 0.7664, 0.8409, 0.7859, 0.8351,\n",
      "        0.8370, 0.8332, 0.8017, 0.8316, 0.8652, 0.8480, 0.8052, 0.8594, 0.8550,\n",
      "        0.8415], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0937, Val-Loss: 0.0527\n",
      "Epoch [20/700], Train-Loss: 0.0270, Val-Loss: 0.0193\n",
      "Epoch [30/700], Train-Loss: 0.0083, Val-Loss: 0.0043\n",
      "Epoch [40/700], Train-Loss: 0.0017, Val-Loss: 0.0006\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[4.5125e-05, 1.2382e-04, 1.2738e-04,  ..., 8.4850e-05, 1.2174e-04,\n",
      "         9.8584e-05],\n",
      "        [1.1760e-04, 4.8439e-05, 4.0409e-05,  ..., 1.3475e-04, 1.7927e-04,\n",
      "         1.2196e-04],\n",
      "        [2.0482e-05, 3.1342e-05, 4.9355e-05,  ..., 1.2147e-04, 1.7382e-04,\n",
      "         1.8307e-04],\n",
      "        ...,\n",
      "        [8.1832e-05, 1.4344e-04, 1.1746e-04,  ..., 1.5837e-04, 1.3271e-04,\n",
      "         5.4556e-05],\n",
      "        [1.3558e-04, 1.3441e-04, 1.8141e-04,  ..., 5.6157e-05, 8.8325e-05,\n",
      "         9.7490e-05],\n",
      "        [1.4110e-04, 2.4188e-04, 6.4663e-05,  ..., 9.4011e-05, 1.0440e-04,\n",
      "         8.9785e-05]], device='cuda:0')\n",
      "pred: tensor([0.8350, 0.8310, 0.8053, 0.8166, 0.7819, 0.8290, 0.8147, 0.8279, 0.7920,\n",
      "        0.8242, 0.7937, 0.8264, 0.7952, 0.8091, 0.8426, 0.8098, 0.8338, 0.8330,\n",
      "        0.7896, 0.7949, 0.8237, 0.8355, 0.8604, 0.8480, 0.8768, 0.8008, 0.8694,\n",
      "        0.8192, 0.8502, 0.8463, 0.8541, 0.8731, 0.8651, 0.8599, 0.8570, 0.8581,\n",
      "        0.8301, 0.8492, 0.8208, 0.8161, 0.8327, 0.8692, 0.8022, 0.8612, 0.8324,\n",
      "        0.7992, 0.8477, 0.8283, 0.8385, 0.8400, 0.7497, 0.7882, 0.7551, 0.7719,\n",
      "        0.7835, 0.7749, 0.8303, 0.8207, 0.7584, 0.8385, 0.7953, 0.8118, 0.7880,\n",
      "        0.8098, 0.8288, 0.8379, 0.7869, 0.7807, 0.8147, 0.8353, 0.8332, 0.7978,\n",
      "        0.8262, 0.7992, 0.7990, 0.8623, 0.8464, 0.8523, 0.8677, 0.8622, 0.8302,\n",
      "        0.8344, 0.8622, 0.8350, 0.8661, 0.8175, 0.7807, 0.8459, 0.7829, 0.8427,\n",
      "        0.8303, 0.8398, 0.8126, 0.8332, 0.8623, 0.8399, 0.8020, 0.8589, 0.8448,\n",
      "        0.8445], device='cuda:0')\n",
      "mae:  tensor(0.0053) ; rank:  0.9733657365736572\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor,\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='random',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['imagenet1k'])/len(mae['imagenet1k']), '; rank: ', sum([x.correlation for x in rank['imagenet1k']])/len(rank['imagenet1k']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/700], Train-Loss: 0.0747, Val-Loss: 0.0356\n",
      "Epoch [20/700], Train-Loss: 0.0192, Val-Loss: 0.0118\n",
      "Epoch [30/700], Train-Loss: 0.0054, Val-Loss: 0.0019\n",
      "Epoch [40/700], Train-Loss: 0.0015, Val-Loss: 0.0006\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[5.2333e-05, 1.5889e-04, 7.1963e-05,  ..., 2.0303e-04, 6.5455e-05,\n",
      "         9.6778e-05],\n",
      "        [4.0778e-05, 1.1261e-04, 1.9490e-04,  ..., 1.2543e-04, 6.6571e-05,\n",
      "         8.4966e-04],\n",
      "        [3.2436e-05, 1.7914e-04, 7.7321e-05,  ..., 3.1673e-04, 2.3537e-04,\n",
      "         1.0072e-03],\n",
      "        ...,\n",
      "        [2.0799e-04, 4.7442e-05, 3.2258e-04,  ..., 8.2555e-05, 1.6130e-04,\n",
      "         1.9433e-04],\n",
      "        [1.0848e-04, 2.0688e-04, 1.6089e-04,  ..., 7.2632e-05, 6.3280e-05,\n",
      "         2.5160e-04],\n",
      "        [8.6779e-05, 1.0128e-04, 4.3450e-04,  ..., 8.2706e-05, 4.6275e-05,\n",
      "         1.2244e-03]], device='cuda:0')\n",
      "pred: tensor([0.8365, 0.8230, 0.7964, 0.8165, 0.7916, 0.8239, 0.8128, 0.8074, 0.7921,\n",
      "        0.8178, 0.7989, 0.8278, 0.7963, 0.8171, 0.8379, 0.8032, 0.8266, 0.8216,\n",
      "        0.7809, 0.7897, 0.8228, 0.8366, 0.8659, 0.8607, 0.8842, 0.8092, 0.8688,\n",
      "        0.8131, 0.8573, 0.8470, 0.8524, 0.8707, 0.8727, 0.8714, 0.8564, 0.8729,\n",
      "        0.8315, 0.8510, 0.8428, 0.8383, 0.8353, 0.8717, 0.8057, 0.8660, 0.8243,\n",
      "        0.7916, 0.8363, 0.8111, 0.8282, 0.8326, 0.7667, 0.7843, 0.7597, 0.7725,\n",
      "        0.7827, 0.7687, 0.8240, 0.8223, 0.7636, 0.8416, 0.7968, 0.8019, 0.7873,\n",
      "        0.8028, 0.8288, 0.8485, 0.7924, 0.7755, 0.8124, 0.8278, 0.8314, 0.7946,\n",
      "        0.8268, 0.8017, 0.7954, 0.8615, 0.8526, 0.8430, 0.8688, 0.8630, 0.8306,\n",
      "        0.8316, 0.8595, 0.8416, 0.8622, 0.8187, 0.7765, 0.8377, 0.7930, 0.8393,\n",
      "        0.8384, 0.8398, 0.8074, 0.8326, 0.8608, 0.8467, 0.8036, 0.8569, 0.8545,\n",
      "        0.8383], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0558, Val-Loss: 0.1057\n",
      "Epoch [20/700], Train-Loss: 0.0213, Val-Loss: 0.0306\n",
      "Epoch [30/700], Train-Loss: 0.0007, Val-Loss: 0.0030\n",
      "Epoch [40/700], Train-Loss: 0.0001, Val-Loss: 0.0008\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0951e-04, 4.7035e-05, 1.2120e-04,  ..., 5.2987e-05, 2.6932e-04,\n",
      "         1.1907e-04],\n",
      "        [1.5077e-04, 1.5861e-04, 1.8519e-04,  ..., 1.8596e-04, 2.7496e-04,\n",
      "         2.8551e-04],\n",
      "        [2.8395e-04, 2.1961e-04, 2.4371e-04,  ..., 1.7196e-04, 4.4955e-04,\n",
      "         5.9002e-04],\n",
      "        ...,\n",
      "        [1.3957e-04, 1.6926e-04, 4.3072e-04,  ..., 6.4958e-05, 1.3712e-04,\n",
      "         1.1985e-04],\n",
      "        [1.5477e-04, 1.4666e-04, 7.8785e-05,  ..., 3.5755e-05, 1.0090e-04,\n",
      "         1.6430e-05],\n",
      "        [3.2239e-04, 9.2417e-05, 2.1979e-04,  ..., 1.8362e-05, 1.2278e-04,\n",
      "         1.2681e-05]], device='cuda:0')\n",
      "pred: tensor([0.8379, 0.8229, 0.7962, 0.8169, 0.7906, 0.8249, 0.8121, 0.8073, 0.7926,\n",
      "        0.8174, 0.7984, 0.8276, 0.7992, 0.8151, 0.8383, 0.8037, 0.8259, 0.8209,\n",
      "        0.7806, 0.7894, 0.8232, 0.8387, 0.8654, 0.8619, 0.8840, 0.8093, 0.8675,\n",
      "        0.8132, 0.8583, 0.8456, 0.8535, 0.8712, 0.8735, 0.8715, 0.8581, 0.8734,\n",
      "        0.8300, 0.8511, 0.8413, 0.8434, 0.8362, 0.8721, 0.8057, 0.8661, 0.8241,\n",
      "        0.7910, 0.8365, 0.8100, 0.8289, 0.8325, 0.7661, 0.7845, 0.7605, 0.7729,\n",
      "        0.7821, 0.7691, 0.8233, 0.8223, 0.7642, 0.8410, 0.7970, 0.8010, 0.7878,\n",
      "        0.8022, 0.8306, 0.8501, 0.7921, 0.7754, 0.8119, 0.8276, 0.8317, 0.7924,\n",
      "        0.8250, 0.8031, 0.7940, 0.8619, 0.8527, 0.8429, 0.8680, 0.8636, 0.8295,\n",
      "        0.8309, 0.8589, 0.8411, 0.8619, 0.8197, 0.7759, 0.8390, 0.7914, 0.8408,\n",
      "        0.8380, 0.8395, 0.8067, 0.8322, 0.8627, 0.8480, 0.8035, 0.8558, 0.8515,\n",
      "        0.8390], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0891, Val-Loss: 0.0412\n",
      "Epoch [20/700], Train-Loss: 0.0195, Val-Loss: 0.0095\n",
      "Epoch [30/700], Train-Loss: 0.0026, Val-Loss: 0.0002\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.1169e-04, 3.6309e-04, 1.2942e-04,  ..., 4.3822e-05, 1.0601e-04,\n",
      "         4.0291e-04],\n",
      "        [1.3273e-04, 1.0412e-04, 6.6111e-05,  ..., 2.4282e-04, 5.5834e-04,\n",
      "         5.9131e-04],\n",
      "        [3.3534e-04, 1.3276e-04, 2.8937e-04,  ..., 1.6033e-04, 4.8939e-04,\n",
      "         5.5138e-04],\n",
      "        ...,\n",
      "        [1.6479e-04, 1.8752e-04, 1.7138e-04,  ..., 1.0019e-04, 9.9851e-05,\n",
      "         1.9108e-03],\n",
      "        [2.4873e-05, 7.2600e-05, 2.0301e-05,  ..., 8.5330e-05, 1.4532e-04,\n",
      "         1.7815e-04],\n",
      "        [1.9857e-04, 4.7428e-04, 8.3226e-05,  ..., 4.5107e-04, 2.7372e-05,\n",
      "         3.4742e-04]], device='cuda:0')\n",
      "pred: tensor([0.8373, 0.8238, 0.7965, 0.8193, 0.7931, 0.8256, 0.8124, 0.8066, 0.7911,\n",
      "        0.8194, 0.8004, 0.8265, 0.7967, 0.8166, 0.8375, 0.8033, 0.8248, 0.8212,\n",
      "        0.7799, 0.7900, 0.8249, 0.8376, 0.8648, 0.8607, 0.8847, 0.8091, 0.8670,\n",
      "        0.8154, 0.8573, 0.8460, 0.8538, 0.8700, 0.8728, 0.8714, 0.8576, 0.8734,\n",
      "        0.8320, 0.8506, 0.8416, 0.8408, 0.8337, 0.8714, 0.8051, 0.8653, 0.8244,\n",
      "        0.7909, 0.8355, 0.8100, 0.8275, 0.8306, 0.7667, 0.7851, 0.7608, 0.7728,\n",
      "        0.7824, 0.7689, 0.8248, 0.8241, 0.7633, 0.8417, 0.7969, 0.8025, 0.7886,\n",
      "        0.8036, 0.8313, 0.8485, 0.7891, 0.7748, 0.8141, 0.8280, 0.8311, 0.7915,\n",
      "        0.8257, 0.8025, 0.7943, 0.8617, 0.8519, 0.8427, 0.8681, 0.8642, 0.8305,\n",
      "        0.8314, 0.8590, 0.8405, 0.8608, 0.8184, 0.7763, 0.8397, 0.7908, 0.8411,\n",
      "        0.8369, 0.8395, 0.8059, 0.8306, 0.8611, 0.8468, 0.8033, 0.8578, 0.8523,\n",
      "        0.8371], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1097, Val-Loss: 0.1248\n",
      "Epoch [20/700], Train-Loss: 0.0294, Val-Loss: 0.0345\n",
      "Epoch [30/700], Train-Loss: 0.0040, Val-Loss: 0.0070\n",
      "Epoch [40/700], Train-Loss: 0.0010, Val-Loss: 0.0024\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[6.4601e-05, 1.2099e-04, 1.7328e-04,  ..., 1.1201e-04, 3.5878e-04,\n",
      "         2.5657e-03],\n",
      "        [7.5952e-05, 4.5492e-05, 5.0405e-05,  ..., 2.0222e-04, 9.4759e-05,\n",
      "         2.6821e-04],\n",
      "        [4.3618e-05, 1.0178e-04, 1.4476e-04,  ..., 1.5611e-04, 4.4037e-04,\n",
      "         7.0505e-04],\n",
      "        ...,\n",
      "        [8.7134e-05, 7.1453e-04, 1.7747e-04,  ..., 5.2106e-05, 1.8865e-04,\n",
      "         2.2929e-03],\n",
      "        [8.3484e-05, 3.9398e-04, 1.1578e-04,  ..., 4.4961e-04, 2.5455e-04,\n",
      "         2.6654e-04],\n",
      "        [5.0287e-05, 8.9454e-05, 8.6824e-05,  ..., 6.4048e-05, 4.3523e-04,\n",
      "         3.2940e-04]], device='cuda:0')\n",
      "pred: tensor([0.8365, 0.8224, 0.7961, 0.8177, 0.7890, 0.8239, 0.8132, 0.8065, 0.7908,\n",
      "        0.8167, 0.7995, 0.8270, 0.7970, 0.8164, 0.8387, 0.8033, 0.8264, 0.8196,\n",
      "        0.7803, 0.7890, 0.8232, 0.8396, 0.8657, 0.8604, 0.8856, 0.8084, 0.8682,\n",
      "        0.8148, 0.8587, 0.8465, 0.8540, 0.8701, 0.8726, 0.8723, 0.8581, 0.8739,\n",
      "        0.8323, 0.8499, 0.8428, 0.8397, 0.8371, 0.8715, 0.8056, 0.8640, 0.8245,\n",
      "        0.7919, 0.8374, 0.8115, 0.8279, 0.8336, 0.7673, 0.7834, 0.7617, 0.7728,\n",
      "        0.7817, 0.7708, 0.8241, 0.8232, 0.7636, 0.8426, 0.7978, 0.8015, 0.7881,\n",
      "        0.8019, 0.8304, 0.8476, 0.7918, 0.7745, 0.8127, 0.8286, 0.8332, 0.7921,\n",
      "        0.8269, 0.8010, 0.7936, 0.8624, 0.8522, 0.8443, 0.8675, 0.8617, 0.8309,\n",
      "        0.8312, 0.8600, 0.8418, 0.8627, 0.8186, 0.7757, 0.8397, 0.7926, 0.8392,\n",
      "        0.8376, 0.8390, 0.8068, 0.8326, 0.8620, 0.8461, 0.8030, 0.8567, 0.8532,\n",
      "        0.8383], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0781, Val-Loss: 0.0381\n",
      "Epoch [20/700], Train-Loss: 0.0177, Val-Loss: 0.0097\n",
      "Epoch [30/700], Train-Loss: 0.0035, Val-Loss: 0.0008\n",
      "Epoch [40/700], Train-Loss: 0.0008, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[6.5340e-05, 5.5103e-05, 2.6641e-04,  ..., 7.0637e-05, 1.7165e-04,\n",
      "         4.0372e-04],\n",
      "        [8.8223e-05, 1.6615e-05, 7.0489e-05,  ..., 1.9214e-04, 4.5970e-04,\n",
      "         7.2747e-04],\n",
      "        [3.2821e-04, 1.6375e-04, 1.5652e-04,  ..., 3.4877e-04, 2.0525e-04,\n",
      "         9.0405e-04],\n",
      "        ...,\n",
      "        [4.7713e-04, 9.8929e-05, 1.6916e-04,  ..., 2.8763e-05, 8.1302e-05,\n",
      "         1.9294e-04],\n",
      "        [9.9427e-05, 1.1771e-04, 5.6203e-05,  ..., 3.9079e-05, 1.6648e-04,\n",
      "         2.8063e-04],\n",
      "        [1.8093e-05, 1.2465e-05, 4.1550e-05,  ..., 1.1376e-04, 1.0590e-03,\n",
      "         3.9764e-04]], device='cuda:0')\n",
      "pred: tensor([0.8379, 0.8238, 0.7965, 0.8164, 0.7911, 0.8259, 0.8142, 0.8067, 0.7920,\n",
      "        0.8181, 0.7991, 0.8267, 0.7990, 0.8168, 0.8389, 0.8029, 0.8270, 0.8210,\n",
      "        0.7810, 0.7882, 0.8237, 0.8383, 0.8637, 0.8607, 0.8839, 0.8081, 0.8675,\n",
      "        0.8167, 0.8583, 0.8455, 0.8555, 0.8705, 0.8729, 0.8715, 0.8584, 0.8728,\n",
      "        0.8319, 0.8521, 0.8422, 0.8411, 0.8364, 0.8721, 0.8052, 0.8651, 0.8239,\n",
      "        0.7911, 0.8368, 0.8123, 0.8275, 0.8357, 0.7670, 0.7853, 0.7605, 0.7726,\n",
      "        0.7820, 0.7685, 0.8235, 0.8237, 0.7643, 0.8418, 0.7996, 0.8025, 0.7882,\n",
      "        0.8022, 0.8300, 0.8486, 0.7924, 0.7744, 0.8119, 0.8286, 0.8311, 0.7925,\n",
      "        0.8255, 0.8026, 0.7937, 0.8617, 0.8548, 0.8457, 0.8704, 0.8668, 0.8298,\n",
      "        0.8316, 0.8599, 0.8427, 0.8619, 0.8192, 0.7756, 0.8388, 0.7931, 0.8408,\n",
      "        0.8390, 0.8402, 0.8066, 0.8328, 0.8633, 0.8464, 0.8027, 0.8565, 0.8530,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1224, Val-Loss: 0.0938\n",
      "Epoch [20/700], Train-Loss: 0.0331, Val-Loss: 0.0290\n",
      "Epoch [30/700], Train-Loss: 0.0113, Val-Loss: 0.0093\n",
      "Epoch [40/700], Train-Loss: 0.0042, Val-Loss: 0.0039\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[4.7018e-05, 5.9397e-05, 1.2291e-04,  ..., 2.1173e-05, 4.8047e-05,\n",
      "         1.3157e-04],\n",
      "        [7.6230e-05, 2.8867e-04, 8.0310e-05,  ..., 1.0791e-04, 1.3738e-04,\n",
      "         3.0457e-04],\n",
      "        [1.8057e-05, 5.7901e-05, 2.3721e-03,  ..., 4.3723e-04, 1.2967e-04,\n",
      "         8.3002e-04],\n",
      "        ...,\n",
      "        [1.5453e-04, 3.8106e-04, 2.5805e-04,  ..., 2.7191e-04, 4.1154e-05,\n",
      "         2.0592e-04],\n",
      "        [5.1613e-05, 1.2491e-04, 7.8119e-05,  ..., 1.4045e-05, 4.0426e-05,\n",
      "         1.0107e-05],\n",
      "        [5.9468e-06, 1.9077e-04, 1.2966e-04,  ..., 1.3560e-04, 7.5520e-05,\n",
      "         3.2333e-04]], device='cuda:0')\n",
      "pred: tensor([0.8363, 0.8227, 0.7966, 0.8168, 0.7900, 0.8235, 0.8126, 0.8069, 0.7903,\n",
      "        0.8187, 0.7974, 0.8278, 0.7975, 0.8162, 0.8393, 0.8023, 0.8259, 0.8215,\n",
      "        0.7808, 0.7890, 0.8240, 0.8369, 0.8635, 0.8601, 0.8858, 0.8079, 0.8672,\n",
      "        0.8140, 0.8554, 0.8470, 0.8523, 0.8697, 0.8721, 0.8699, 0.8575, 0.8732,\n",
      "        0.8311, 0.8494, 0.8423, 0.8381, 0.8365, 0.8724, 0.8059, 0.8656, 0.8222,\n",
      "        0.7908, 0.8385, 0.8098, 0.8262, 0.8336, 0.7642, 0.7838, 0.7604, 0.7723,\n",
      "        0.7812, 0.7704, 0.8228, 0.8225, 0.7636, 0.8413, 0.7967, 0.8012, 0.7886,\n",
      "        0.8029, 0.8302, 0.8478, 0.7914, 0.7737, 0.8139, 0.8278, 0.8305, 0.7918,\n",
      "        0.8252, 0.8021, 0.7936, 0.8619, 0.8512, 0.8434, 0.8657, 0.8653, 0.8299,\n",
      "        0.8317, 0.8595, 0.8414, 0.8623, 0.8202, 0.7747, 0.8387, 0.7926, 0.8406,\n",
      "        0.8388, 0.8400, 0.8074, 0.8330, 0.8623, 0.8475, 0.8044, 0.8557, 0.8545,\n",
      "        0.8374], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0941, Val-Loss: 0.0521\n",
      "Epoch [20/700], Train-Loss: 0.0224, Val-Loss: 0.0153\n",
      "Epoch [30/700], Train-Loss: 0.0074, Val-Loss: 0.0039\n",
      "Epoch [40/700], Train-Loss: 0.0025, Val-Loss: 0.0014\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[4.7018e-05, 5.9397e-05, 1.2291e-04,  ..., 4.3822e-05, 1.0601e-04,\n",
      "         4.0291e-04],\n",
      "        [7.6230e-05, 2.8867e-04, 8.0310e-05,  ..., 2.4282e-04, 5.5834e-04,\n",
      "         5.9131e-04],\n",
      "        [1.8057e-05, 5.7901e-05, 2.3721e-03,  ..., 1.6033e-04, 4.8939e-04,\n",
      "         5.5138e-04],\n",
      "        ...,\n",
      "        [1.5453e-04, 3.8106e-04, 2.5805e-04,  ..., 1.0019e-04, 9.9851e-05,\n",
      "         1.9108e-03],\n",
      "        [5.1613e-05, 1.2491e-04, 7.8119e-05,  ..., 8.5330e-05, 1.4532e-04,\n",
      "         1.7815e-04],\n",
      "        [5.9468e-06, 1.9077e-04, 1.2966e-04,  ..., 4.5107e-04, 2.7372e-05,\n",
      "         3.4742e-04]], device='cuda:0')\n",
      "pred: tensor([0.8376, 0.8236, 0.7962, 0.8175, 0.7908, 0.8267, 0.8123, 0.8075, 0.7921,\n",
      "        0.8168, 0.7984, 0.8270, 0.7975, 0.8156, 0.8395, 0.8027, 0.8266, 0.8204,\n",
      "        0.7796, 0.7895, 0.8250, 0.8385, 0.8657, 0.8612, 0.8858, 0.8086, 0.8689,\n",
      "        0.8167, 0.8587, 0.8457, 0.8557, 0.8717, 0.8718, 0.8718, 0.8586, 0.8758,\n",
      "        0.8338, 0.8513, 0.8421, 0.8419, 0.8347, 0.8700, 0.8056, 0.8659, 0.8216,\n",
      "        0.7931, 0.8367, 0.8112, 0.8251, 0.8311, 0.7654, 0.7849, 0.7603, 0.7731,\n",
      "        0.7802, 0.7689, 0.8243, 0.8222, 0.7646, 0.8422, 0.7962, 0.8012, 0.7881,\n",
      "        0.8029, 0.8297, 0.8488, 0.7903, 0.7739, 0.8129, 0.8274, 0.8315, 0.7921,\n",
      "        0.8263, 0.8019, 0.7937, 0.8643, 0.8505, 0.8441, 0.8703, 0.8653, 0.8309,\n",
      "        0.8321, 0.8611, 0.8420, 0.8626, 0.8189, 0.7758, 0.8396, 0.7918, 0.8388,\n",
      "        0.8384, 0.8387, 0.8077, 0.8318, 0.8644, 0.8460, 0.8039, 0.8555, 0.8523,\n",
      "        0.8387], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1083, Val-Loss: 0.0788\n",
      "Epoch [20/700], Train-Loss: 0.0295, Val-Loss: 0.0240\n",
      "Epoch [30/700], Train-Loss: 0.0106, Val-Loss: 0.0075\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0030\n",
      "Epoch [50/700], Train-Loss: 0.0012, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[6.4601e-05, 1.2099e-04, 1.7328e-04,  ..., 3.4154e-03, 7.5337e-05,\n",
      "         1.4488e-04],\n",
      "        [7.5952e-05, 4.5492e-05, 5.0405e-05,  ..., 8.1677e-04, 1.2384e-03,\n",
      "         1.8221e-04],\n",
      "        [4.3618e-05, 1.0178e-04, 1.4476e-04,  ..., 1.5162e-04, 1.9263e-04,\n",
      "         1.0440e-04],\n",
      "        ...,\n",
      "        [8.7134e-05, 7.1453e-04, 1.7747e-04,  ..., 5.6278e-05, 1.8074e-05,\n",
      "         5.7676e-06],\n",
      "        [8.3484e-05, 3.9398e-04, 1.1578e-04,  ..., 3.9346e-04, 1.5371e-04,\n",
      "         1.2937e-04],\n",
      "        [5.0287e-05, 8.9454e-05, 8.6824e-05,  ..., 1.1926e-03, 5.5811e-04,\n",
      "         3.2006e-04]], device='cuda:0')\n",
      "pred: tensor([0.8375, 0.8235, 0.7963, 0.8170, 0.7912, 0.8265, 0.8128, 0.8078, 0.7917,\n",
      "        0.8196, 0.7990, 0.8267, 0.7973, 0.8163, 0.8378, 0.8026, 0.8250, 0.8221,\n",
      "        0.7794, 0.7884, 0.8257, 0.8381, 0.8644, 0.8598, 0.8865, 0.8092, 0.8668,\n",
      "        0.8129, 0.8547, 0.8450, 0.8526, 0.8712, 0.8732, 0.8719, 0.8569, 0.8741,\n",
      "        0.8320, 0.8481, 0.8405, 0.8406, 0.8352, 0.8713, 0.8057, 0.8662, 0.8238,\n",
      "        0.7914, 0.8365, 0.8096, 0.8279, 0.8338, 0.7666, 0.7852, 0.7608, 0.7726,\n",
      "        0.7826, 0.7686, 0.8241, 0.8229, 0.7641, 0.8419, 0.7976, 0.8010, 0.7880,\n",
      "        0.8038, 0.8289, 0.8476, 0.7916, 0.7740, 0.8125, 0.8273, 0.8326, 0.7923,\n",
      "        0.8262, 0.8024, 0.7951, 0.8620, 0.8533, 0.8419, 0.8681, 0.8645, 0.8300,\n",
      "        0.8309, 0.8608, 0.8416, 0.8632, 0.8193, 0.7757, 0.8385, 0.7922, 0.8419,\n",
      "        0.8363, 0.8389, 0.8055, 0.8317, 0.8611, 0.8473, 0.8028, 0.8553, 0.8506,\n",
      "        0.8390], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0586, Val-Loss: 0.1021\n",
      "Epoch [20/700], Train-Loss: 0.0228, Val-Loss: 0.0304\n",
      "Epoch [30/700], Train-Loss: 0.0017, Val-Loss: 0.0044\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0015\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0580e-04, 2.0370e-04, 5.0113e-05,  ..., 2.1173e-05, 4.8047e-05,\n",
      "         1.3157e-04],\n",
      "        [5.7338e-05, 8.5007e-05, 6.8355e-05,  ..., 1.0791e-04, 1.3738e-04,\n",
      "         3.0457e-04],\n",
      "        [1.3432e-04, 1.7072e-04, 1.1026e-04,  ..., 4.3723e-04, 1.2967e-04,\n",
      "         8.3002e-04],\n",
      "        ...,\n",
      "        [3.0084e-04, 3.4441e-04, 1.2375e-04,  ..., 2.7191e-04, 4.1154e-05,\n",
      "         2.0592e-04],\n",
      "        [6.6508e-05, 3.7753e-05, 1.8483e-05,  ..., 1.4045e-05, 4.0426e-05,\n",
      "         1.0107e-05],\n",
      "        [5.2934e-05, 7.5056e-05, 4.5708e-05,  ..., 1.3560e-04, 7.5520e-05,\n",
      "         3.2333e-04]], device='cuda:0')\n",
      "pred: tensor([0.8384, 0.8219, 0.7957, 0.8160, 0.7904, 0.8267, 0.8124, 0.8055, 0.7917,\n",
      "        0.8174, 0.7972, 0.8272, 0.7973, 0.8152, 0.8391, 0.8025, 0.8271, 0.8206,\n",
      "        0.7800, 0.7883, 0.8239, 0.8389, 0.8641, 0.8613, 0.8846, 0.8086, 0.8668,\n",
      "        0.8160, 0.8590, 0.8469, 0.8545, 0.8699, 0.8721, 0.8719, 0.8576, 0.8723,\n",
      "        0.8322, 0.8511, 0.8410, 0.8412, 0.8367, 0.8714, 0.8054, 0.8656, 0.8240,\n",
      "        0.7913, 0.8382, 0.8107, 0.8283, 0.8325, 0.7681, 0.7850, 0.7605, 0.7737,\n",
      "        0.7822, 0.7687, 0.8222, 0.8222, 0.7642, 0.8409, 0.7983, 0.8024, 0.7873,\n",
      "        0.8020, 0.8310, 0.8489, 0.7910, 0.7744, 0.8107, 0.8270, 0.8321, 0.7918,\n",
      "        0.8265, 0.8025, 0.7942, 0.8613, 0.8546, 0.8457, 0.8694, 0.8646, 0.8306,\n",
      "        0.8310, 0.8607, 0.8421, 0.8618, 0.8182, 0.7767, 0.8388, 0.7922, 0.8397,\n",
      "        0.8366, 0.8390, 0.8069, 0.8323, 0.8618, 0.8466, 0.8027, 0.8559, 0.8521,\n",
      "        0.8387], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1294, Val-Loss: 0.1066\n",
      "Epoch [20/700], Train-Loss: 0.0356, Val-Loss: 0.0316\n",
      "Epoch [30/700], Train-Loss: 0.0123, Val-Loss: 0.0090\n",
      "Epoch [40/700], Train-Loss: 0.0041, Val-Loss: 0.0033\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0334e-04, 2.0205e-04, 5.6140e-04,  ..., 3.3667e-04, 3.0258e-05,\n",
      "         2.5279e-04],\n",
      "        [2.0500e-04, 5.8517e-05, 2.5885e-04,  ..., 5.0969e-04, 1.5810e-04,\n",
      "         5.1069e-04],\n",
      "        [2.9521e-05, 1.8978e-04, 1.2156e-03,  ..., 3.6936e-04, 1.9496e-04,\n",
      "         7.1072e-04],\n",
      "        ...,\n",
      "        [2.0514e-05, 5.3596e-05, 3.0727e-05,  ..., 6.0039e-04, 4.9598e-05,\n",
      "         1.5404e-03],\n",
      "        [8.1360e-05, 4.9865e-05, 3.9275e-04,  ..., 4.0786e-04, 7.1276e-05,\n",
      "         5.9672e-04],\n",
      "        [1.6283e-05, 1.6343e-05, 1.2465e-04,  ..., 3.3398e-04, 3.2949e-05,\n",
      "         1.1244e-04]], device='cuda:0')\n",
      "pred: tensor([0.8367, 0.8235, 0.7960, 0.8171, 0.7902, 0.8247, 0.8115, 0.8080, 0.7913,\n",
      "        0.8175, 0.7991, 0.8267, 0.7981, 0.8156, 0.8382, 0.8029, 0.8267, 0.8218,\n",
      "        0.7795, 0.7886, 0.8237, 0.8374, 0.8641, 0.8628, 0.8849, 0.8068, 0.8690,\n",
      "        0.8132, 0.8596, 0.8468, 0.8539, 0.8705, 0.8720, 0.8719, 0.8576, 0.8719,\n",
      "        0.8316, 0.8514, 0.8396, 0.8421, 0.8359, 0.8702, 0.8053, 0.8643, 0.8255,\n",
      "        0.7901, 0.8367, 0.8133, 0.8293, 0.8326, 0.7662, 0.7851, 0.7611, 0.7723,\n",
      "        0.7815, 0.7686, 0.8240, 0.8224, 0.7635, 0.8405, 0.7982, 0.8024, 0.7890,\n",
      "        0.8028, 0.8309, 0.8482, 0.7924, 0.7743, 0.8121, 0.8286, 0.8330, 0.7914,\n",
      "        0.8252, 0.8016, 0.7929, 0.8616, 0.8517, 0.8432, 0.8689, 0.8635, 0.8298,\n",
      "        0.8308, 0.8601, 0.8413, 0.8628, 0.8178, 0.7763, 0.8373, 0.7906, 0.8408,\n",
      "        0.8386, 0.8398, 0.8068, 0.8313, 0.8617, 0.8474, 0.8041, 0.8551, 0.8505,\n",
      "        0.8383], device='cuda:0')\n",
      "mae:  tensor(0.0049) ; rank:  0.9786798679867987\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='first',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['imagenet1k'])/len(mae['imagenet1k']), '; rank: ', sum([x.correlation for x in rank['imagenet1k']])/len(rank['imagenet1k']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val Sample Selection - Cifar (open-clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip_result_performances = []\n",
    "for model_info in open_clip_model_catalog.values():\n",
    "    open_clip_result_performances.append(model_info['results']['CIFAR-100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031948958143207605"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_mae(100, open_clip_result_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6522270644105292"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_empirical_rank_score(100, timm_result_performances, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {'CIFAR-100'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  tensor(0.0376) ; rank:  0.8607151511252298\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=open_clip_model_confidence_for_val_samples_tensor,\n",
    "                                            model_catalog=open_clip_model_catalog,\n",
    "                                            evaluation_model_class=NNModel,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='random',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            train_model_index=open_clip_train_model_indices,\n",
    "                                            val_model_index=open_clip_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['CIFAR-100'])/len(mae['CIFAR-100']), '; rank: ', sum([x.correlation for x in rank['CIFAR-100']])/len(rank['CIFAR-100']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  tensor(0.0318) ; rank:  0.8318439070608654\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=open_clip_model_confidence_for_val_samples_tensor[list(open_clip_top_disagreement_indices)],\n",
    "                                            model_catalog=open_clip_model_catalog,\n",
    "                                            evaluation_model_class=NNModel,\n",
    "                                            number_bootstraping_steps=1,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='first',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            train_model_index=open_clip_train_model_indices,\n",
    "                                            val_model_index=open_clip_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['CIFAR-100'])/len(mae['CIFAR-100']), '; rank: ', sum([x.correlation for x in rank['CIFAR-100']])/len(rank['CIFAR-100']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/700], Train-Loss: 0.1203, Val-Loss: 0.1121\n",
      "Epoch [20/700], Train-Loss: 0.0305, Val-Loss: 0.0192\n",
      "Epoch [30/700], Train-Loss: 0.0100, Val-Loss: 0.0087\n",
      "Epoch [40/700], Train-Loss: 0.0032, Val-Loss: 0.0023\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0012\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0008\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "x: tensor([[2.0888e-05, 2.6076e-03, 2.4038e-07,  ..., 2.4432e-04, 6.3768e-04,\n",
      "         3.1958e-02],\n",
      "        [9.9558e-03, 2.0287e-02, 9.3228e-04,  ..., 9.0883e-03, 1.4228e-03,\n",
      "         1.5499e-02],\n",
      "        [2.3255e-04, 5.4431e-06, 1.1860e-04,  ..., 4.1066e-03, 3.8131e-03,\n",
      "         4.8331e-04],\n",
      "        ...,\n",
      "        [1.3474e-03, 9.8154e-03, 6.2061e-03,  ..., 1.3067e-02, 5.5556e-04,\n",
      "         8.7428e-03],\n",
      "        [1.4392e-05, 2.8527e-04, 9.2942e-05,  ..., 3.6416e-03, 1.2768e-07,\n",
      "         2.0201e-03],\n",
      "        [1.4361e-05, 6.0688e-04, 3.8154e-04,  ..., 1.8137e-03, 7.8826e-03,\n",
      "         1.4693e-02]], device='cuda:0')\n",
      "pred: tensor([0.7422, 0.6031, 0.8202, 0.9024, 0.7655, 0.8537, 0.6615, 0.7477, 0.6339,\n",
      "        0.7162, 0.7320, 0.8305, 0.7867, 0.8232, 0.7122], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1189, Val-Loss: 0.1095\n",
      "Epoch [20/700], Train-Loss: 0.0322, Val-Loss: 0.0215\n",
      "Epoch [30/700], Train-Loss: 0.0116, Val-Loss: 0.0103\n",
      "Epoch [40/700], Train-Loss: 0.0043, Val-Loss: 0.0038\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0009\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0008\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0007\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "x: tensor([[8.3528e-08, 3.2550e-08, 7.8179e-06,  ..., 3.1517e-04, 1.0777e-05,\n",
      "         7.1986e-05],\n",
      "        [1.6185e-03, 1.0270e-03, 1.4824e-04,  ..., 1.8306e-03, 2.4939e-05,\n",
      "         1.7992e-04],\n",
      "        [3.8523e-06, 1.7344e-09, 4.0108e-06,  ..., 4.0650e-06, 4.2641e-04,\n",
      "         7.0752e-08],\n",
      "        ...,\n",
      "        [8.2151e-04, 3.2889e-04, 2.7491e-03,  ..., 7.8001e-03, 8.9656e-05,\n",
      "         1.7775e-03],\n",
      "        [6.3036e-05, 7.8285e-06, 4.8363e-05,  ..., 1.5892e-04, 7.9128e-08,\n",
      "         4.3809e-05],\n",
      "        [7.0074e-09, 3.7773e-07, 2.3219e-05,  ..., 3.1433e-05, 5.6016e-04,\n",
      "         4.1329e-07]], device='cuda:0')\n",
      "pred: tensor([0.7314, 0.5948, 0.8076, 0.9016, 0.7474, 0.8496, 0.6563, 0.7248, 0.6303,\n",
      "        0.7199, 0.7473, 0.8282, 0.7688, 0.8109, 0.7112], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1212, Val-Loss: 0.1180\n",
      "Epoch [20/700], Train-Loss: 0.0320, Val-Loss: 0.0223\n",
      "Epoch [30/700], Train-Loss: 0.0113, Val-Loss: 0.0122\n",
      "Epoch [40/700], Train-Loss: 0.0042, Val-Loss: 0.0035\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0023\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0010\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0012\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "x: tensor([[1.7822e-06, 7.0306e-08, 2.5018e-05,  ..., 3.8198e-06, 1.5912e-04,\n",
      "         3.6382e-08],\n",
      "        [1.7688e-03, 3.9498e-03, 5.5761e-05,  ..., 2.5691e-03, 3.1544e-05,\n",
      "         3.0994e-05],\n",
      "        [2.1725e-04, 2.8350e-06, 1.5998e-04,  ..., 7.5567e-05, 1.8866e-03,\n",
      "         1.1290e-07],\n",
      "        ...,\n",
      "        [2.2667e-04, 8.2200e-04, 8.1784e-04,  ..., 3.0218e-03, 2.6070e-05,\n",
      "         5.1351e-04],\n",
      "        [2.9504e-05, 8.7595e-05, 1.7931e-05,  ..., 1.0094e-04, 2.1865e-07,\n",
      "         2.3375e-05],\n",
      "        [7.0609e-07, 2.4704e-07, 4.3523e-06,  ..., 1.5519e-05, 1.4710e-05,\n",
      "         5.9813e-07]], device='cuda:0')\n",
      "pred: tensor([0.7373, 0.5823, 0.7969, 0.9016, 0.7678, 0.8467, 0.7022, 0.7592, 0.6603,\n",
      "        0.7122, 0.7615, 0.8295, 0.7836, 0.8383, 0.7138], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1152, Val-Loss: 0.0975\n",
      "Epoch [20/700], Train-Loss: 0.0325, Val-Loss: 0.0202\n",
      "Epoch [30/700], Train-Loss: 0.0107, Val-Loss: 0.0095\n",
      "Epoch [40/700], Train-Loss: 0.0036, Val-Loss: 0.0025\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0010\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0011\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "x: tensor([[2.4555e-02, 4.5363e-06, 4.9491e-05,  ..., 1.7526e-06, 1.0134e-03,\n",
      "         3.8765e-05],\n",
      "        [1.5951e-02, 3.0608e-03, 3.7799e-05,  ..., 1.6011e-03, 7.7146e-05,\n",
      "         1.1517e-03],\n",
      "        [7.2618e-04, 2.1705e-07, 2.2780e-06,  ..., 1.5303e-07, 2.8692e-03,\n",
      "         1.7709e-09],\n",
      "        ...,\n",
      "        [1.5280e-02, 2.6635e-04, 1.6050e-03,  ..., 2.2747e-05, 6.8224e-06,\n",
      "         1.0315e-04],\n",
      "        [1.6866e-03, 8.6646e-06, 8.9515e-05,  ..., 8.6975e-07, 3.2375e-08,\n",
      "         6.0594e-06],\n",
      "        [4.6288e-03, 3.2696e-05, 3.9115e-02,  ..., 1.0479e-06, 5.4438e-03,\n",
      "         1.9981e-06]], device='cuda:0')\n",
      "pred: tensor([0.7399, 0.5879, 0.7816, 0.9026, 0.7770, 0.8444, 0.6907, 0.7599, 0.6513,\n",
      "        0.7218, 0.7449, 0.8285, 0.7639, 0.8113, 0.7068], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0555, Val-Loss: 0.0379\n",
      "Epoch [20/700], Train-Loss: 0.0124, Val-Loss: 0.0048\n",
      "Epoch [30/700], Train-Loss: 0.0020, Val-Loss: 0.0015\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0009\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0015\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0010\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0010\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "x: tensor([[1.3508e-06, 1.6852e-07, 2.4810e-06,  ..., 1.7336e-07, 4.8490e-06,\n",
      "         2.4272e-05],\n",
      "        [6.1785e-03, 2.0163e-02, 5.1732e-04,  ..., 2.3770e-04, 6.7980e-05,\n",
      "         1.0646e-03],\n",
      "        [4.6855e-06, 6.9388e-09, 1.2824e-06,  ..., 8.4529e-07, 4.7331e-03,\n",
      "         4.1655e-05],\n",
      "        ...,\n",
      "        [6.3860e-05, 1.1498e-04, 2.0919e-04,  ..., 5.1004e-04, 8.1045e-05,\n",
      "         6.0404e-03],\n",
      "        [7.2609e-06, 4.6609e-06, 2.3989e-05,  ..., 9.4934e-04, 8.5282e-07,\n",
      "         1.0862e-02],\n",
      "        [3.4689e-05, 1.1145e-05, 4.3240e-05,  ..., 5.9749e-06, 2.7936e-04,\n",
      "         1.2151e-05]], device='cuda:0')\n",
      "pred: tensor([0.7359, 0.6009, 0.8004, 0.9020, 0.7799, 0.8549, 0.6815, 0.7433, 0.6348,\n",
      "        0.7209, 0.7387, 0.8269, 0.7752, 0.8193, 0.7113], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0479, Val-Loss: 0.0286\n",
      "Epoch [20/700], Train-Loss: 0.0102, Val-Loss: 0.0042\n",
      "Epoch [30/700], Train-Loss: 0.0017, Val-Loss: 0.0008\n",
      "Epoch [40/700], Train-Loss: 0.0003, Val-Loss: 0.0005\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0010\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0007\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0006\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0005\n",
      "x: tensor([[3.4633e-08, 1.5979e-10, 4.8336e-07,  ..., 2.8452e-09, 9.2674e-06,\n",
      "         3.6229e-09],\n",
      "        [1.1720e-02, 1.7070e-02, 2.9030e-03,  ..., 5.4411e-03, 6.4602e-04,\n",
      "         2.9339e-03],\n",
      "        [2.8490e-06, 1.5497e-07, 2.6460e-05,  ..., 7.8154e-09, 4.7441e-06,\n",
      "         2.0603e-08],\n",
      "        ...,\n",
      "        [5.4044e-04, 2.0577e-03, 1.2736e-02,  ..., 3.1774e-04, 9.9576e-05,\n",
      "         2.4346e-03],\n",
      "        [7.8283e-06, 1.9466e-06, 5.9344e-05,  ..., 1.0470e-05, 1.0393e-06,\n",
      "         7.3197e-05],\n",
      "        [1.8164e-07, 1.7995e-07, 1.1719e-04,  ..., 1.0383e-09, 6.5172e-06,\n",
      "         7.8370e-08]], device='cuda:0')\n",
      "pred: tensor([0.7395, 0.5854, 0.7960, 0.9040, 0.7598, 0.8422, 0.6657, 0.7340, 0.6250,\n",
      "        0.7195, 0.7569, 0.8361, 0.7697, 0.8134, 0.7079], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1124, Val-Loss: 0.0980\n",
      "Epoch [20/700], Train-Loss: 0.0260, Val-Loss: 0.0155\n",
      "Epoch [30/700], Train-Loss: 0.0077, Val-Loss: 0.0057\n",
      "Epoch [40/700], Train-Loss: 0.0022, Val-Loss: 0.0015\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0008\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0008\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0007\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0008\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "x: tensor([[1.1442e-04, 1.6609e-03, 1.4524e-02,  ..., 3.6059e-06, 2.2209e-06,\n",
      "         6.8977e-10],\n",
      "        [1.1458e-02, 5.0172e-03, 2.0578e-04,  ..., 4.3015e-03, 7.6442e-04,\n",
      "         7.1538e-04],\n",
      "        [2.6067e-04, 5.9563e-06, 4.8404e-05,  ..., 3.3241e-05, 4.1312e-04,\n",
      "         1.6970e-09],\n",
      "        ...,\n",
      "        [7.9224e-05, 1.0038e-03, 1.4569e-03,  ..., 1.0035e-02, 9.2831e-05,\n",
      "         1.4017e-04],\n",
      "        [1.0611e-04, 2.7176e-05, 3.5516e-05,  ..., 4.7107e-04, 1.0503e-07,\n",
      "         1.8112e-05],\n",
      "        [9.0023e-06, 1.0904e-03, 4.1673e-03,  ..., 2.4837e-06, 1.4064e-05,\n",
      "         1.3165e-09]], device='cuda:0')\n",
      "pred: tensor([0.7367, 0.5958, 0.8061, 0.8993, 0.7571, 0.8432, 0.6688, 0.7570, 0.6528,\n",
      "        0.7196, 0.7341, 0.8306, 0.7713, 0.8238, 0.7129], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1164, Val-Loss: 0.1020\n",
      "Epoch [20/700], Train-Loss: 0.0311, Val-Loss: 0.0191\n",
      "Epoch [30/700], Train-Loss: 0.0106, Val-Loss: 0.0092\n",
      "Epoch [40/700], Train-Loss: 0.0034, Val-Loss: 0.0026\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0010\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0010\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0011\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0009\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "x: tensor([[2.9168e-10, 2.2553e-12, 7.5636e-08,  ..., 1.8973e-07, 1.4645e-05,\n",
      "         2.3219e-09],\n",
      "        [5.1427e-04, 4.7085e-04, 8.1854e-06,  ..., 4.2504e-04, 1.9209e-06,\n",
      "         2.3721e-05],\n",
      "        [2.2288e-07, 1.1787e-11, 1.3975e-08,  ..., 4.5935e-06, 2.1938e-04,\n",
      "         8.2017e-09],\n",
      "        ...,\n",
      "        [1.0852e-04, 1.1193e-05, 1.0843e-04,  ..., 2.9406e-04, 1.8447e-06,\n",
      "         1.5120e-05],\n",
      "        [1.8812e-05, 2.2179e-07, 1.6814e-06,  ..., 1.1891e-04, 9.3101e-10,\n",
      "         8.5341e-06],\n",
      "        [7.8148e-10, 3.9475e-13, 9.3615e-08,  ..., 1.6123e-06, 4.3522e-07,\n",
      "         1.1340e-08]], device='cuda:0')\n",
      "pred: tensor([0.7380, 0.6050, 0.7977, 0.8906, 0.7516, 0.8477, 0.7095, 0.7465, 0.5914,\n",
      "        0.7206, 0.7331, 0.8320, 0.7910, 0.8187, 0.7138], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0894, Val-Loss: 0.0741\n",
      "Epoch [20/700], Train-Loss: 0.0220, Val-Loss: 0.0110\n",
      "Epoch [30/700], Train-Loss: 0.0058, Val-Loss: 0.0047\n",
      "Epoch [40/700], Train-Loss: 0.0015, Val-Loss: 0.0007\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0013\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0007\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0009\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0007\n",
      "x: tensor([[6.9179e-05, 2.9726e-05, 6.6756e-03,  ..., 3.0138e-07, 1.0606e-03,\n",
      "         1.0427e-05],\n",
      "        [5.1343e-03, 1.4058e-02, 1.3485e-03,  ..., 3.1755e-03, 6.8867e-05,\n",
      "         4.4755e-03],\n",
      "        [3.3722e-06, 1.7941e-06, 2.4994e-06,  ..., 1.8861e-07, 1.0317e-03,\n",
      "         4.2154e-06],\n",
      "        ...,\n",
      "        [3.0095e-03, 3.1679e-03, 2.1758e-02,  ..., 6.8935e-04, 8.7242e-04,\n",
      "         4.9190e-04],\n",
      "        [2.2927e-03, 4.4249e-04, 2.8241e-01,  ..., 8.7747e-06, 1.3795e-05,\n",
      "         4.9147e-04],\n",
      "        [1.6649e-05, 4.7745e-05, 3.6715e-02,  ..., 4.1458e-07, 4.9727e-03,\n",
      "         1.3630e-04]], device='cuda:0')\n",
      "pred: tensor([0.7378, 0.5955, 0.8211, 0.9056, 0.7738, 0.8468, 0.6871, 0.7507, 0.6388,\n",
      "        0.7253, 0.7558, 0.8284, 0.7902, 0.8151, 0.7125], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1141, Val-Loss: 0.1260\n",
      "Epoch [20/700], Train-Loss: 0.0334, Val-Loss: 0.0251\n",
      "Epoch [30/700], Train-Loss: 0.0104, Val-Loss: 0.0106\n",
      "Epoch [40/700], Train-Loss: 0.0039, Val-Loss: 0.0042\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0018\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0012\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0010\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0009\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0008\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0009\n",
      "x: tensor([[5.5884e-04, 8.2667e-06, 3.4334e-05,  ..., 9.5390e-09, 1.4137e-05,\n",
      "         2.3429e-05],\n",
      "        [8.5910e-03, 5.7728e-03, 1.4640e-03,  ..., 3.2765e-03, 5.0409e-04,\n",
      "         9.5078e-04],\n",
      "        [2.1714e-03, 1.7922e-05, 1.5783e-04,  ..., 1.1690e-09, 3.3498e-04,\n",
      "         6.1431e-08],\n",
      "        ...,\n",
      "        [3.4032e-03, 3.1777e-04, 6.9667e-03,  ..., 3.2098e-05, 3.0646e-05,\n",
      "         1.8564e-04],\n",
      "        [9.6638e-03, 9.1880e-03, 2.5714e-04,  ..., 1.1441e-05, 8.2579e-07,\n",
      "         3.3064e-04],\n",
      "        [1.4274e-04, 8.8331e-06, 5.5544e-05,  ..., 3.0676e-08, 3.0977e-04,\n",
      "         1.0389e-05]], device='cuda:0')\n",
      "pred: tensor([0.7378, 0.5962, 0.8058, 0.9067, 0.7440, 0.8476, 0.6694, 0.7273, 0.6539,\n",
      "        0.7111, 0.7409, 0.8235, 0.7534, 0.8235, 0.7101], device='cuda:0')\n",
      "mae:  tensor(0.0183) ; rank:  0.9689285714285714\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=open_clip_model_confidence_for_val_samples_tensor,\n",
    "                                            model_catalog=open_clip_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='random',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=open_clip_train_model_indices,\n",
    "                                            val_model_index=open_clip_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['CIFAR-100'])/len(mae['CIFAR-100']), '; rank: ', sum([x.correlation for x in rank['CIFAR-100']])/len(rank['CIFAR-100']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/700], Train-Loss: 0.0994, Val-Loss: 0.0643\n",
      "Epoch [20/700], Train-Loss: 0.0260, Val-Loss: 0.0140\n",
      "Epoch [30/700], Train-Loss: 0.0074, Val-Loss: 0.0056\n",
      "Epoch [40/700], Train-Loss: 0.0020, Val-Loss: 0.0023\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0025\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0029\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0022\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0026\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "x: tensor([[6.2792e-05, 5.3419e-07, 9.7315e-04,  ..., 2.7731e-04, 3.5392e-05,\n",
      "         6.4168e-06],\n",
      "        [3.3585e-03, 5.9150e-03, 2.6787e-04,  ..., 6.3693e-03, 4.1996e-04,\n",
      "         4.1590e-04],\n",
      "        [9.0676e-05, 6.8793e-06, 1.1004e-04,  ..., 1.6962e-04, 2.3713e-02,\n",
      "         2.3630e-05],\n",
      "        ...,\n",
      "        [2.6817e-03, 1.0287e-03, 1.1810e-02,  ..., 3.2089e-02, 2.2758e-04,\n",
      "         1.0493e-03],\n",
      "        [3.7704e-04, 6.3233e-03, 1.7930e-03,  ..., 6.4652e-03, 5.8024e-08,\n",
      "         6.9111e-03],\n",
      "        [5.8215e-04, 6.5756e-06, 6.6864e-03,  ..., 2.6628e-03, 7.7479e-03,\n",
      "         1.1669e-04]], device='cuda:0')\n",
      "pred: tensor([0.7362, 0.6275, 0.7890, 0.8977, 0.7406, 0.8415, 0.7225, 0.7529, 0.7160,\n",
      "        0.7223, 0.7510, 0.8339, 0.7608, 0.8173, 0.7111], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0834, Val-Loss: 0.0628\n",
      "Epoch [20/700], Train-Loss: 0.0215, Val-Loss: 0.0117\n",
      "Epoch [30/700], Train-Loss: 0.0064, Val-Loss: 0.0054\n",
      "Epoch [40/700], Train-Loss: 0.0017, Val-Loss: 0.0021\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0026\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0028\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0021\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0025\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "x: tensor([[4.6154e-04, 8.4941e-04, 1.3766e-04,  ..., 6.3462e-05, 3.0720e-04,\n",
      "         2.1442e-02],\n",
      "        [5.6369e-03, 1.4078e-02, 4.2135e-04,  ..., 8.0688e-03, 7.5537e-04,\n",
      "         2.3747e-03],\n",
      "        [2.4996e-04, 3.8886e-04, 9.6083e-05,  ..., 3.4040e-03, 9.7286e-03,\n",
      "         6.6497e-04],\n",
      "        ...,\n",
      "        [2.8827e-03, 7.1259e-04, 3.8648e-03,  ..., 1.8365e-03, 5.9921e-05,\n",
      "         3.4370e-03],\n",
      "        [4.6903e-03, 3.7482e-04, 1.2650e-03,  ..., 3.7480e-05, 3.0322e-07,\n",
      "         1.5023e-03],\n",
      "        [2.0913e-04, 6.1280e-04, 6.2211e-05,  ..., 5.0610e-04, 4.8045e-02,\n",
      "         7.2134e-03]], device='cuda:0')\n",
      "pred: tensor([0.7363, 0.6304, 0.8041, 0.8929, 0.7393, 0.8491, 0.7259, 0.7600, 0.7133,\n",
      "        0.7212, 0.7511, 0.8338, 0.7639, 0.8256, 0.7116], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1023, Val-Loss: 0.0764\n",
      "Epoch [20/700], Train-Loss: 0.0271, Val-Loss: 0.0165\n",
      "Epoch [30/700], Train-Loss: 0.0092, Val-Loss: 0.0076\n",
      "Epoch [40/700], Train-Loss: 0.0028, Val-Loss: 0.0025\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0031\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0021\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0026\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "x: tensor([[5.2113e-04, 5.9542e-06, 3.5247e-05,  ..., 2.2552e-04, 1.4504e-05,\n",
      "         6.1905e-06],\n",
      "        [7.6550e-03, 2.7489e-02, 2.2636e-03,  ..., 1.7867e-02, 3.4920e-04,\n",
      "         4.8658e-03],\n",
      "        [1.4698e-03, 5.9474e-05, 1.7046e-03,  ..., 1.4835e-04, 3.3687e-03,\n",
      "         1.2175e-05],\n",
      "        ...,\n",
      "        [2.5559e-03, 5.5834e-03, 4.5863e-02,  ..., 1.4540e-03, 9.5862e-05,\n",
      "         1.5541e-03],\n",
      "        [1.2334e-04, 3.8410e-04, 1.3830e-04,  ..., 7.3081e-04, 2.0812e-08,\n",
      "         1.0506e-03],\n",
      "        [6.8054e-04, 6.3266e-05, 1.4459e-03,  ..., 8.2749e-03, 3.4620e-05,\n",
      "         4.2197e-06]], device='cuda:0')\n",
      "pred: tensor([0.7358, 0.6326, 0.7889, 0.8925, 0.7441, 0.8415, 0.7177, 0.7564, 0.7116,\n",
      "        0.7222, 0.7534, 0.8331, 0.7724, 0.8224, 0.7112], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0863, Val-Loss: 0.0616\n",
      "Epoch [20/700], Train-Loss: 0.0221, Val-Loss: 0.0123\n",
      "Epoch [30/700], Train-Loss: 0.0072, Val-Loss: 0.0064\n",
      "Epoch [40/700], Train-Loss: 0.0023, Val-Loss: 0.0024\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0029\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0026\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0022\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0026\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "x: tensor([[1.0280e-03, 6.0336e-07, 1.8709e-05,  ..., 5.7282e-04, 2.8913e-03,\n",
      "         6.7241e-04],\n",
      "        [1.0757e-02, 1.9340e-02, 4.4884e-04,  ..., 3.0165e-03, 1.5720e-04,\n",
      "         9.7624e-04],\n",
      "        [8.4133e-04, 3.6882e-08, 2.0354e-05,  ..., 1.2208e-04, 3.2083e-02,\n",
      "         1.2245e-05],\n",
      "        ...,\n",
      "        [1.6362e-03, 2.1245e-03, 9.7624e-04,  ..., 1.1729e-02, 1.0988e-03,\n",
      "         1.3136e-03],\n",
      "        [1.1142e-03, 1.0721e-03, 7.3874e-04,  ..., 4.3065e-02, 6.7186e-07,\n",
      "         3.8048e-03],\n",
      "        [1.4354e-03, 1.0125e-05, 1.4882e-03,  ..., 1.0841e-04, 3.0378e-02,\n",
      "         6.9718e-04]], device='cuda:0')\n",
      "pred: tensor([0.7369, 0.6318, 0.7957, 0.8909, 0.7358, 0.8440, 0.7218, 0.7657, 0.7128,\n",
      "        0.7232, 0.7541, 0.8285, 0.7687, 0.8172, 0.7119], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0740, Val-Loss: 0.0555\n",
      "Epoch [20/700], Train-Loss: 0.0198, Val-Loss: 0.0116\n",
      "Epoch [30/700], Train-Loss: 0.0065, Val-Loss: 0.0059\n",
      "Epoch [40/700], Train-Loss: 0.0020, Val-Loss: 0.0023\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0030\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0021\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0026\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "x: tensor([[3.5476e-04, 6.7559e-08, 3.0002e-04,  ..., 7.1298e-03, 3.8871e-02,\n",
      "         9.9602e-02],\n",
      "        [9.2611e-03, 6.2375e-03, 1.2451e-04,  ..., 6.1351e-03, 1.5920e-03,\n",
      "         2.1773e-03],\n",
      "        [5.7120e-06, 6.4368e-07, 6.4419e-06,  ..., 1.4923e-03, 1.7262e-01,\n",
      "         1.6080e-03],\n",
      "        ...,\n",
      "        [1.2426e-03, 9.0557e-04, 1.9405e-03,  ..., 8.4720e-03, 2.8499e-03,\n",
      "         1.1056e-02],\n",
      "        [5.6862e-03, 2.1500e-04, 4.1318e-04,  ..., 4.2002e-02, 1.1391e-03,\n",
      "         6.1211e-03],\n",
      "        [4.5321e-04, 2.1860e-06, 1.2573e-03,  ..., 1.1332e-02, 2.8996e-01,\n",
      "         1.3597e-02]], device='cuda:0')\n",
      "pred: tensor([0.7357, 0.6300, 0.7978, 0.8930, 0.7394, 0.8484, 0.7233, 0.7615, 0.7156,\n",
      "        0.7207, 0.7559, 0.8344, 0.7664, 0.8210, 0.7114], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1126, Val-Loss: 0.1032\n",
      "Epoch [20/700], Train-Loss: 0.0265, Val-Loss: 0.0186\n",
      "Epoch [30/700], Train-Loss: 0.0099, Val-Loss: 0.0105\n",
      "Epoch [40/700], Train-Loss: 0.0035, Val-Loss: 0.0033\n",
      "Epoch [50/700], Train-Loss: 0.0012, Val-Loss: 0.0041\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0022\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0022\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0022\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "x: tensor([[1.2449e-04, 5.6823e-07, 1.1641e-04,  ..., 1.9671e-03, 3.0991e-04,\n",
      "         2.0757e-03],\n",
      "        [8.3449e-03, 2.1871e-02, 1.3128e-03,  ..., 5.0346e-03, 1.2812e-03,\n",
      "         8.3600e-04],\n",
      "        [9.7288e-05, 5.3700e-04, 1.2207e-03,  ..., 2.9685e-03, 4.1556e-02,\n",
      "         2.2921e-04],\n",
      "        ...,\n",
      "        [7.3534e-03, 5.9990e-03, 1.2482e-02,  ..., 1.0094e-02, 4.8034e-03,\n",
      "         1.2275e-02],\n",
      "        [2.7468e-03, 4.8009e-03, 2.7303e-03,  ..., 3.1340e-03, 2.0493e-05,\n",
      "         6.0132e-03],\n",
      "        [5.8183e-04, 4.3645e-04, 7.6314e-05,  ..., 6.4676e-04, 1.0942e-02,\n",
      "         7.4487e-05]], device='cuda:0')\n",
      "pred: tensor([0.7367, 0.6282, 0.7915, 0.8949, 0.7436, 0.8493, 0.7216, 0.7613, 0.7104,\n",
      "        0.7214, 0.7467, 0.8335, 0.7649, 0.8206, 0.7119], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0710, Val-Loss: 0.0437\n",
      "Epoch [20/700], Train-Loss: 0.0173, Val-Loss: 0.0081\n",
      "Epoch [30/700], Train-Loss: 0.0037, Val-Loss: 0.0030\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0023\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0023\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0030\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0021\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0022\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "x: tensor([[7.7336e-06, 6.5809e-09, 9.5905e-06,  ..., 5.7230e-05, 2.9013e-03,\n",
      "         1.8000e-04],\n",
      "        [1.1735e-02, 1.1401e-01, 3.5082e-04,  ..., 4.3718e-04, 1.0421e-04,\n",
      "         1.4672e-04],\n",
      "        [2.9573e-05, 1.8078e-03, 6.7248e-06,  ..., 1.5463e-05, 2.2586e-02,\n",
      "         2.5277e-07],\n",
      "        ...,\n",
      "        [1.6297e-03, 1.3433e-02, 1.3780e-02,  ..., 2.8170e-03, 8.6744e-04,\n",
      "         6.8308e-04],\n",
      "        [1.3794e-03, 8.8789e-05, 1.0345e-04,  ..., 1.8843e-04, 7.4048e-05,\n",
      "         7.9679e-05],\n",
      "        [1.0324e-05, 9.4920e-04, 1.6840e-04,  ..., 3.0632e-05, 3.1766e-02,\n",
      "         7.1292e-05]], device='cuda:0')\n",
      "pred: tensor([0.7376, 0.6302, 0.7920, 0.8938, 0.7410, 0.8468, 0.7279, 0.7618, 0.7065,\n",
      "        0.7212, 0.7484, 0.8321, 0.7629, 0.8223, 0.7121], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1288, Val-Loss: 0.1309\n",
      "Epoch [20/700], Train-Loss: 0.0372, Val-Loss: 0.0274\n",
      "Epoch [30/700], Train-Loss: 0.0136, Val-Loss: 0.0136\n",
      "Epoch [40/700], Train-Loss: 0.0049, Val-Loss: 0.0042\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0043\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0022\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0022\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "x: tensor([[5.8370e-03, 2.3535e-05, 1.2474e-04,  ..., 1.6234e-05, 3.7954e-03,\n",
      "         4.9030e-03],\n",
      "        [7.7112e-03, 1.1365e-02, 1.1926e-03,  ..., 4.1118e-03, 3.1960e-03,\n",
      "         6.5467e-03],\n",
      "        [4.8736e-04, 2.1095e-08, 3.1748e-04,  ..., 3.2181e-06, 3.5102e-02,\n",
      "         1.6868e-05],\n",
      "        ...,\n",
      "        [8.8779e-03, 1.0917e-03, 2.1852e-02,  ..., 1.1122e-03, 7.2473e-05,\n",
      "         1.2221e-04],\n",
      "        [6.7119e-03, 3.8948e-04, 2.2937e-01,  ..., 1.6454e-03, 9.9056e-05,\n",
      "         1.3448e-02],\n",
      "        [1.6214e-04, 4.4563e-05, 1.5798e-04,  ..., 8.7418e-05, 1.5483e-01,\n",
      "         8.0075e-03]], device='cuda:0')\n",
      "pred: tensor([0.7374, 0.6304, 0.7851, 0.8963, 0.7332, 0.8447, 0.7234, 0.7626, 0.7051,\n",
      "        0.7205, 0.7498, 0.8330, 0.7698, 0.8191, 0.7114], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0870, Val-Loss: 0.0785\n",
      "Epoch [20/700], Train-Loss: 0.0239, Val-Loss: 0.0157\n",
      "Epoch [30/700], Train-Loss: 0.0079, Val-Loss: 0.0070\n",
      "Epoch [40/700], Train-Loss: 0.0023, Val-Loss: 0.0025\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0031\n",
      "Epoch [60/700], Train-Loss: 0.0000, Val-Loss: 0.0027\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0022\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0027\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "x: tensor([[1.3374e-05, 9.6044e-05, 1.8933e-05,  ..., 6.1642e-05, 7.8069e-04,\n",
      "         1.4822e-02],\n",
      "        [7.2364e-03, 3.6046e-02, 3.9788e-04,  ..., 6.8322e-03, 3.4760e-04,\n",
      "         5.2368e-03],\n",
      "        [4.8986e-05, 1.6895e-04, 1.3763e-05,  ..., 5.9106e-05, 7.6262e-02,\n",
      "         7.3457e-04],\n",
      "        ...,\n",
      "        [1.2656e-03, 1.0800e-02, 1.7218e-03,  ..., 1.3509e-03, 5.1197e-04,\n",
      "         1.8006e-01],\n",
      "        [1.7221e-04, 2.8289e-03, 9.0577e-05,  ..., 2.4421e-04, 7.0694e-07,\n",
      "         5.5650e-01],\n",
      "        [1.2934e-04, 8.0515e-04, 4.2677e-04,  ..., 6.6877e-04, 4.2775e-02,\n",
      "         5.0153e-02]], device='cuda:0')\n",
      "pred: tensor([0.7366, 0.6339, 0.7899, 0.8951, 0.7354, 0.8443, 0.7230, 0.7618, 0.7146,\n",
      "        0.7226, 0.7512, 0.8326, 0.7648, 0.8232, 0.7110], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0225, Val-Loss: 0.0107\n",
      "Epoch [20/700], Train-Loss: 0.0034, Val-Loss: 0.0020\n",
      "Epoch [30/700], Train-Loss: 0.0002, Val-Loss: 0.0023\n",
      "Epoch [40/700], Train-Loss: 0.0003, Val-Loss: 0.0044\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0026\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0028\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0023\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0025\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0024\n",
      "x: tensor([[3.6090e-04, 5.1406e-03, 8.5243e-04,  ..., 6.5798e-05, 1.1468e-05,\n",
      "         2.3676e-05],\n",
      "        [1.2623e-02, 2.7766e-02, 5.7018e-04,  ..., 3.0324e-03, 1.5854e-04,\n",
      "         9.3861e-04],\n",
      "        [7.9994e-04, 2.1478e-04, 6.7600e-04,  ..., 3.8343e-05, 2.3216e-03,\n",
      "         7.5922e-07],\n",
      "        ...,\n",
      "        [1.4088e-02, 1.5435e-03, 2.5148e-03,  ..., 2.2541e-02, 5.4959e-04,\n",
      "         7.6282e-03],\n",
      "        [2.4435e-03, 4.0435e-03, 1.1345e-02,  ..., 2.6180e-03, 6.0647e-07,\n",
      "         3.5825e-04],\n",
      "        [3.7329e-03, 1.1626e-01, 6.4763e-02,  ..., 1.6184e-04, 1.8758e-04,\n",
      "         7.7386e-05]], device='cuda:0')\n",
      "pred: tensor([0.7360, 0.6321, 0.7876, 0.8905, 0.7406, 0.8379, 0.7263, 0.7565, 0.7088,\n",
      "        0.7206, 0.7441, 0.8313, 0.7709, 0.8217, 0.7122], device='cuda:0')\n",
      "mae:  tensor(0.0286) ; rank:  0.9710714285714284\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=open_clip_model_confidence_for_val_samples_tensor[list(open_clip_top_disagreement_indices)],\n",
    "                                            model_catalog=open_clip_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='first',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=open_clip_train_model_indices,\n",
    "                                            val_model_index=open_clip_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['CIFAR-100'])/len(mae['CIFAR-100']), '; rank: ', sum([x.correlation for x in rank['CIFAR-100']])/len(rank['CIFAR-100']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Samples - Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_968094/1717516195.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  timm_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_main_set.tensor\"))\n"
     ]
    }
   ],
   "source": [
    "path_to_save_folder = \"/mnt/lustre/work/oh/owl813/repos/model-selection/checkpoints\"\n",
    "timm_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_main_set.tensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_scores = get_disagreement_scores(timm_model_confidence_for_main_set_tensor.transpose(0,1), n_guiding_models=100)\n",
    "top_disagreement_indices_main_set = disagreement_scores.argsort()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {'imagenet1k'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  tensor(0.0131) ; rank:  0.8744069686961529\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_main_set_tensor,\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=NNModel,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='random',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['imagenet1k'])/len(mae['imagenet1k']), '; rank: ', sum([x.correlation for x in rank['imagenet1k']])/len(rank['imagenet1k']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/700], Train-Loss: 0.0555, Val-Loss: 0.0210\n",
      "Epoch [20/700], Train-Loss: 0.0133, Val-Loss: 0.0066\n",
      "Epoch [30/700], Train-Loss: 0.0024, Val-Loss: 0.0005\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.5898e-05, 1.5055e-04, 3.3608e-05,  ..., 2.9268e-04, 4.4215e-05,\n",
      "         6.8743e-05],\n",
      "        [8.3710e-05, 8.1562e-05, 5.6517e-05,  ..., 3.0361e-04, 1.6998e-04,\n",
      "         5.1040e-05],\n",
      "        [1.1917e-04, 1.2131e-04, 1.1694e-04,  ..., 3.2447e-04, 2.3488e-04,\n",
      "         2.7677e-04],\n",
      "        ...,\n",
      "        [6.3844e-05, 1.1621e-04, 3.1610e-05,  ..., 4.1468e-04, 5.0451e-04,\n",
      "         1.6782e-04],\n",
      "        [1.1883e-04, 2.2007e-04, 5.5843e-05,  ..., 2.5630e-04, 1.1223e-04,\n",
      "         6.3600e-05],\n",
      "        [5.1760e-05, 1.1201e-04, 6.5460e-05,  ..., 2.4037e-04, 2.0353e-05,\n",
      "         2.0914e-05]], device='cuda:0')\n",
      "pred: tensor([0.8457, 0.8377, 0.7924, 0.8186, 0.7883, 0.8258, 0.8106, 0.8104, 0.7880,\n",
      "        0.8225, 0.7971, 0.8393, 0.7942, 0.8148, 0.8367, 0.8059, 0.8296, 0.8244,\n",
      "        0.7881, 0.7970, 0.8198, 0.8315, 0.8710, 0.8550, 0.8818, 0.8029, 0.8625,\n",
      "        0.8266, 0.8473, 0.8456, 0.8507, 0.8716, 0.8695, 0.8735, 0.8549, 0.8776,\n",
      "        0.8312, 0.8444, 0.8453, 0.8285, 0.8411, 0.8666, 0.7967, 0.8677, 0.8272,\n",
      "        0.8016, 0.8400, 0.8224, 0.8301, 0.8333, 0.7569, 0.7879, 0.7561, 0.7716,\n",
      "        0.7899, 0.7704, 0.8250, 0.8161, 0.7576, 0.8393, 0.8023, 0.8064, 0.7893,\n",
      "        0.8000, 0.8343, 0.8460, 0.7944, 0.7745, 0.8148, 0.8249, 0.8350, 0.7948,\n",
      "        0.8271, 0.8009, 0.7948, 0.8642, 0.8552, 0.8459, 0.8629, 0.8552, 0.8286,\n",
      "        0.8303, 0.8586, 0.8479, 0.8612, 0.8073, 0.7720, 0.8375, 0.7814, 0.8461,\n",
      "        0.8358, 0.8403, 0.8105, 0.8373, 0.8587, 0.8436, 0.8100, 0.8492, 0.8506,\n",
      "        0.8362], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1241, Val-Loss: 0.1048\n",
      "Epoch [20/700], Train-Loss: 0.0336, Val-Loss: 0.0323\n",
      "Epoch [30/700], Train-Loss: 0.0098, Val-Loss: 0.0108\n",
      "Epoch [40/700], Train-Loss: 0.0035, Val-Loss: 0.0041\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.8508e-05, 1.8606e-05, 3.4379e-05,  ..., 1.0124e-04, 1.2230e-04,\n",
      "         1.1876e-04],\n",
      "        [2.1128e-04, 1.2885e-04, 1.6916e-04,  ..., 2.2058e-04, 2.5483e-04,\n",
      "         1.5218e-04],\n",
      "        [3.0035e-04, 7.3969e-04, 4.5756e-04,  ..., 3.1715e-04, 3.0505e-04,\n",
      "         3.5292e-04],\n",
      "        ...,\n",
      "        [1.5133e-04, 4.8530e-05, 6.1952e-05,  ..., 9.3428e-05, 1.1003e-04,\n",
      "         6.5780e-05],\n",
      "        [7.1266e-05, 3.8762e-05, 8.6272e-05,  ..., 7.9561e-05, 1.1794e-04,\n",
      "         5.7637e-05],\n",
      "        [3.2899e-04, 1.4968e-04, 6.7545e-04,  ..., 1.3210e-04, 9.7054e-05,\n",
      "         4.3325e-05]], device='cuda:0')\n",
      "pred: tensor([0.8394, 0.8251, 0.7921, 0.8190, 0.7903, 0.8315, 0.8019, 0.8217, 0.7877,\n",
      "        0.8102, 0.7998, 0.8344, 0.7947, 0.8180, 0.8362, 0.8083, 0.8331, 0.8221,\n",
      "        0.7818, 0.7933, 0.8248, 0.8366, 0.8702, 0.8579, 0.8764, 0.8058, 0.8709,\n",
      "        0.8228, 0.8524, 0.8462, 0.8544, 0.8588, 0.8664, 0.8774, 0.8643, 0.8701,\n",
      "        0.8290, 0.8455, 0.8356, 0.8403, 0.8402, 0.8695, 0.8049, 0.8679, 0.8268,\n",
      "        0.8069, 0.8398, 0.8288, 0.8329, 0.8438, 0.7584, 0.7886, 0.7560, 0.7705,\n",
      "        0.7892, 0.7766, 0.8335, 0.8236, 0.7513, 0.8409, 0.8028, 0.7988, 0.7827,\n",
      "        0.8045, 0.8344, 0.8513, 0.7925, 0.7748, 0.8097, 0.8317, 0.8330, 0.8016,\n",
      "        0.8271, 0.8033, 0.7950, 0.8572, 0.8597, 0.8509, 0.8655, 0.8582, 0.8339,\n",
      "        0.8309, 0.8613, 0.8441, 0.8637, 0.8208, 0.7745, 0.8384, 0.7904, 0.8495,\n",
      "        0.8444, 0.8436, 0.8020, 0.8352, 0.8593, 0.8476, 0.8060, 0.8542, 0.8507,\n",
      "        0.8415], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0563, Val-Loss: 0.0189\n",
      "Epoch [20/700], Train-Loss: 0.0152, Val-Loss: 0.0069\n",
      "Epoch [30/700], Train-Loss: 0.0025, Val-Loss: 0.0003\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.9732e-05, 9.1415e-05, 1.1615e-04,  ..., 7.2635e-05, 1.4652e-04,\n",
      "         8.7760e-05],\n",
      "        [4.9953e-05, 5.1502e-05, 1.4512e-04,  ..., 1.4946e-04, 3.2260e-04,\n",
      "         3.4603e-04],\n",
      "        [1.4892e-04, 1.5848e-04, 4.3957e-04,  ..., 9.8661e-05, 3.3712e-04,\n",
      "         6.0156e-05],\n",
      "        ...,\n",
      "        [2.2410e-04, 4.0686e-04, 7.7247e-04,  ..., 8.4921e-05, 2.1768e-04,\n",
      "         1.0034e-04],\n",
      "        [1.1058e-04, 5.7480e-04, 1.9170e-04,  ..., 5.2961e-05, 1.5886e-04,\n",
      "         2.0224e-05],\n",
      "        [8.0113e-05, 3.3948e-04, 6.4871e-05,  ..., 1.4280e-05, 9.4743e-05,\n",
      "         1.6004e-05]], device='cuda:0')\n",
      "pred: tensor([0.8334, 0.8305, 0.8063, 0.8141, 0.7895, 0.8198, 0.8063, 0.8088, 0.7881,\n",
      "        0.8176, 0.7904, 0.8286, 0.7857, 0.8165, 0.8355, 0.8154, 0.8330, 0.8343,\n",
      "        0.7914, 0.7938, 0.8249, 0.8348, 0.8662, 0.8574, 0.8789, 0.8059, 0.8618,\n",
      "        0.8166, 0.8488, 0.8484, 0.8582, 0.8650, 0.8657, 0.8625, 0.8641, 0.8720,\n",
      "        0.8241, 0.8442, 0.8396, 0.8264, 0.8392, 0.8689, 0.7994, 0.8755, 0.8320,\n",
      "        0.8061, 0.8445, 0.8129, 0.8349, 0.8319, 0.7668, 0.7859, 0.7642, 0.7777,\n",
      "        0.7819, 0.7750, 0.8276, 0.8231, 0.7517, 0.8430, 0.7984, 0.8029, 0.7819,\n",
      "        0.8089, 0.8307, 0.8496, 0.7981, 0.7769, 0.8115, 0.8354, 0.8313, 0.8037,\n",
      "        0.8287, 0.8077, 0.8048, 0.8590, 0.8585, 0.8435, 0.8652, 0.8526, 0.8289,\n",
      "        0.8319, 0.8582, 0.8366, 0.8627, 0.8224, 0.7656, 0.8373, 0.7948, 0.8327,\n",
      "        0.8374, 0.8398, 0.8027, 0.8326, 0.8556, 0.8429, 0.7994, 0.8496, 0.8465,\n",
      "        0.8373], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0631, Val-Loss: 0.0268\n",
      "Epoch [20/700], Train-Loss: 0.0112, Val-Loss: 0.0047\n",
      "Epoch [30/700], Train-Loss: 0.0012, Val-Loss: 0.0002\n",
      "Epoch [40/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[5.5346e-05, 1.1917e-04, 9.7595e-05,  ..., 9.7024e-05, 9.8907e-05,\n",
      "         1.0853e-04],\n",
      "        [1.0837e-04, 7.0064e-05, 1.3001e-04,  ..., 8.6266e-05, 5.4264e-05,\n",
      "         1.4298e-04],\n",
      "        [2.3558e-05, 3.0526e-05, 5.4857e-05,  ..., 2.4288e-04, 7.0090e-05,\n",
      "         2.8243e-04],\n",
      "        ...,\n",
      "        [2.5087e-04, 1.5187e-04, 3.9668e-04,  ..., 1.4154e-04, 1.1648e-04,\n",
      "         1.6436e-04],\n",
      "        [6.2740e-05, 2.6744e-04, 1.3771e-04,  ..., 1.0294e-04, 9.1336e-05,\n",
      "         1.0929e-04],\n",
      "        [2.4812e-05, 1.3444e-04, 5.4965e-05,  ..., 2.2211e-05, 4.0614e-05,\n",
      "         1.6932e-04]], device='cuda:0')\n",
      "pred: tensor([0.8351, 0.8243, 0.7958, 0.8199, 0.7915, 0.8284, 0.8183, 0.8251, 0.7889,\n",
      "        0.8193, 0.7958, 0.8229, 0.7925, 0.8266, 0.8331, 0.8032, 0.8351, 0.8305,\n",
      "        0.7893, 0.7880, 0.8159, 0.8302, 0.8670, 0.8531, 0.8808, 0.8057, 0.8690,\n",
      "        0.8194, 0.8432, 0.8439, 0.8481, 0.8572, 0.8675, 0.8675, 0.8524, 0.8700,\n",
      "        0.8329, 0.8516, 0.8341, 0.8176, 0.8394, 0.8670, 0.7974, 0.8699, 0.8271,\n",
      "        0.8016, 0.8426, 0.8248, 0.8239, 0.8358, 0.7601, 0.7958, 0.7576, 0.7738,\n",
      "        0.7939, 0.7779, 0.8352, 0.8104, 0.7551, 0.8427, 0.7970, 0.8028, 0.7794,\n",
      "        0.8026, 0.8381, 0.8451, 0.8013, 0.7703, 0.8131, 0.8318, 0.8336, 0.8025,\n",
      "        0.8222, 0.7989, 0.7943, 0.8614, 0.8568, 0.8460, 0.8520, 0.8592, 0.8294,\n",
      "        0.8293, 0.8552, 0.8385, 0.8549, 0.8185, 0.7782, 0.8427, 0.7873, 0.8347,\n",
      "        0.8382, 0.8309, 0.8033, 0.8334, 0.8588, 0.8524, 0.8102, 0.8519, 0.8558,\n",
      "        0.8439], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0894, Val-Loss: 0.0492\n",
      "Epoch [20/700], Train-Loss: 0.0249, Val-Loss: 0.0172\n",
      "Epoch [30/700], Train-Loss: 0.0079, Val-Loss: 0.0042\n",
      "Epoch [40/700], Train-Loss: 0.0024, Val-Loss: 0.0012\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.4156e-04, 1.1384e-04, 1.4655e-04,  ..., 9.3449e-05, 8.3567e-05,\n",
      "         5.6269e-05],\n",
      "        [5.2416e-05, 4.8682e-05, 8.0365e-05,  ..., 7.9298e-05, 4.4614e-05,\n",
      "         3.6985e-05],\n",
      "        [6.9566e-05, 8.7581e-05, 6.5176e-05,  ..., 4.2278e-05, 7.8898e-05,\n",
      "         2.1965e-04],\n",
      "        ...,\n",
      "        [1.1073e-04, 1.6411e-04, 1.6919e-04,  ..., 1.1575e-04, 1.2765e-04,\n",
      "         1.2956e-04],\n",
      "        [1.4192e-04, 1.9950e-04, 1.6111e-04,  ..., 1.4975e-04, 5.6926e-05,\n",
      "         5.5479e-05],\n",
      "        [5.3232e-05, 7.1442e-05, 2.6019e-05,  ..., 5.1435e-05, 2.1937e-05,\n",
      "         1.8962e-04]], device='cuda:0')\n",
      "pred: tensor([0.8334, 0.8272, 0.8043, 0.8137, 0.7895, 0.8321, 0.8099, 0.8040, 0.7818,\n",
      "        0.8146, 0.7950, 0.8231, 0.7983, 0.8201, 0.8415, 0.8120, 0.8311, 0.8214,\n",
      "        0.7904, 0.7927, 0.8262, 0.8316, 0.8714, 0.8495, 0.8803, 0.8040, 0.8686,\n",
      "        0.8178, 0.8425, 0.8498, 0.8469, 0.8655, 0.8676, 0.8648, 0.8569, 0.8743,\n",
      "        0.8309, 0.8481, 0.8491, 0.8286, 0.8426, 0.8714, 0.7956, 0.8693, 0.8199,\n",
      "        0.8018, 0.8348, 0.8271, 0.8263, 0.8480, 0.7688, 0.7899, 0.7565, 0.7752,\n",
      "        0.7914, 0.7820, 0.8301, 0.8233, 0.7645, 0.8394, 0.8067, 0.8089, 0.7851,\n",
      "        0.8073, 0.8414, 0.8375, 0.8059, 0.7850, 0.8222, 0.8312, 0.8309, 0.8069,\n",
      "        0.8357, 0.8031, 0.8015, 0.8557, 0.8548, 0.8436, 0.8653, 0.8603, 0.8258,\n",
      "        0.8274, 0.8590, 0.8476, 0.8634, 0.8191, 0.7775, 0.8374, 0.7761, 0.8397,\n",
      "        0.8313, 0.8373, 0.8027, 0.8306, 0.8596, 0.8473, 0.8098, 0.8531, 0.8430,\n",
      "        0.8341], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1118, Val-Loss: 0.0729\n",
      "Epoch [20/700], Train-Loss: 0.0338, Val-Loss: 0.0273\n",
      "Epoch [30/700], Train-Loss: 0.0117, Val-Loss: 0.0084\n",
      "Epoch [40/700], Train-Loss: 0.0041, Val-Loss: 0.0031\n",
      "Epoch [50/700], Train-Loss: 0.0014, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.8938e-05, 7.9548e-05, 1.0672e-04,  ..., 2.4244e-05, 6.7772e-05,\n",
      "         7.8528e-05],\n",
      "        [9.4721e-05, 9.6864e-05, 1.0170e-04,  ..., 1.0331e-04, 8.4809e-05,\n",
      "         1.1465e-04],\n",
      "        [1.7558e-04, 1.7480e-04, 2.0071e-04,  ..., 2.3360e-04, 1.2858e-04,\n",
      "         2.2113e-04],\n",
      "        ...,\n",
      "        [9.0944e-05, 1.5403e-04, 1.4006e-04,  ..., 1.1127e-04, 1.1916e-04,\n",
      "         3.2111e-04],\n",
      "        [6.1735e-05, 1.7798e-04, 1.2563e-04,  ..., 1.3715e-04, 9.8480e-05,\n",
      "         2.9928e-04],\n",
      "        [4.9979e-05, 6.1670e-05, 2.4319e-04,  ..., 5.3058e-05, 3.6823e-05,\n",
      "         6.6107e-05]], device='cuda:0')\n",
      "pred: tensor([0.8405, 0.8252, 0.7972, 0.8160, 0.7895, 0.8294, 0.8096, 0.8159, 0.7882,\n",
      "        0.8248, 0.7911, 0.8290, 0.7976, 0.8177, 0.8387, 0.8139, 0.8274, 0.8335,\n",
      "        0.7838, 0.7891, 0.8266, 0.8397, 0.8639, 0.8570, 0.8805, 0.8106, 0.8661,\n",
      "        0.8314, 0.8547, 0.8423, 0.8507, 0.8604, 0.8659, 0.8595, 0.8567, 0.8721,\n",
      "        0.8335, 0.8569, 0.8382, 0.8299, 0.8376, 0.8683, 0.7980, 0.8690, 0.8365,\n",
      "        0.8033, 0.8502, 0.8243, 0.8392, 0.8404, 0.7579, 0.7865, 0.7573, 0.7731,\n",
      "        0.7797, 0.7784, 0.8288, 0.8294, 0.7623, 0.8334, 0.8069, 0.8011, 0.7823,\n",
      "        0.7943, 0.8352, 0.8448, 0.8007, 0.7892, 0.8141, 0.8317, 0.8309, 0.8047,\n",
      "        0.8264, 0.8108, 0.8011, 0.8524, 0.8496, 0.8509, 0.8588, 0.8625, 0.8274,\n",
      "        0.8268, 0.8525, 0.8361, 0.8538, 0.8203, 0.7780, 0.8439, 0.7877, 0.8441,\n",
      "        0.8381, 0.8452, 0.7982, 0.8366, 0.8604, 0.8486, 0.8027, 0.8566, 0.8480,\n",
      "        0.8363], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0962, Val-Loss: 0.0591\n",
      "Epoch [20/700], Train-Loss: 0.0248, Val-Loss: 0.0181\n",
      "Epoch [30/700], Train-Loss: 0.0081, Val-Loss: 0.0050\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0016\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "x: tensor([[2.7236e-04, 3.0962e-04, 1.6492e-04,  ..., 1.2564e-04, 8.2167e-05,\n",
      "         1.1666e-04],\n",
      "        [1.8964e-04, 1.3673e-04, 2.6911e-04,  ..., 3.4642e-04, 3.1369e-04,\n",
      "         2.1861e-04],\n",
      "        [1.9304e-04, 2.3845e-04, 3.7317e-04,  ..., 3.2063e-04, 2.1293e-04,\n",
      "         3.2531e-04],\n",
      "        ...,\n",
      "        [2.2754e-04, 2.1029e-04, 1.4516e-04,  ..., 1.3236e-04, 1.3629e-04,\n",
      "         1.1466e-04],\n",
      "        [1.7946e-04, 2.6148e-04, 2.6777e-04,  ..., 1.7746e-04, 1.0866e-04,\n",
      "         1.5044e-04],\n",
      "        [7.5132e-05, 8.2461e-05, 6.7877e-05,  ..., 2.7795e-04, 1.2880e-04,\n",
      "         1.8130e-04]], device='cuda:0')\n",
      "pred: tensor([0.8429, 0.8275, 0.8012, 0.8162, 0.7879, 0.8328, 0.8125, 0.8158, 0.7895,\n",
      "        0.8234, 0.7895, 0.8341, 0.7991, 0.8178, 0.8361, 0.8002, 0.8237, 0.8270,\n",
      "        0.7870, 0.7911, 0.8269, 0.8346, 0.8610, 0.8594, 0.8729, 0.8096, 0.8630,\n",
      "        0.8234, 0.8421, 0.8512, 0.8392, 0.8661, 0.8676, 0.8681, 0.8512, 0.8661,\n",
      "        0.8383, 0.8445, 0.8360, 0.8174, 0.8402, 0.8724, 0.8006, 0.8617, 0.8385,\n",
      "        0.7907, 0.8447, 0.8378, 0.8406, 0.8426, 0.7579, 0.7918, 0.7579, 0.7817,\n",
      "        0.7978, 0.7783, 0.8296, 0.8231, 0.7588, 0.8472, 0.8005, 0.8077, 0.7805,\n",
      "        0.8083, 0.8335, 0.8459, 0.7954, 0.7788, 0.8094, 0.8287, 0.8369, 0.8002,\n",
      "        0.8315, 0.7955, 0.7964, 0.8588, 0.8466, 0.8366, 0.8568, 0.8617, 0.8314,\n",
      "        0.8295, 0.8589, 0.8387, 0.8544, 0.8201, 0.7787, 0.8462, 0.7971, 0.8497,\n",
      "        0.8463, 0.8426, 0.8060, 0.8419, 0.8667, 0.8571, 0.8040, 0.8578, 0.8422,\n",
      "        0.8329], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0538, Val-Loss: 0.0145\n",
      "Epoch [20/700], Train-Loss: 0.0086, Val-Loss: 0.0018\n",
      "Epoch [30/700], Train-Loss: 0.0002, Val-Loss: 0.0009\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0014\n",
      "Epoch [50/700], Train-Loss: 0.0010, Val-Loss: 0.0012\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[9.3303e-05, 1.0395e-04, 1.5008e-04,  ..., 4.7976e-05, 4.1296e-05,\n",
      "         1.0679e-04],\n",
      "        [1.0692e-04, 9.8491e-05, 1.0249e-04,  ..., 9.5322e-05, 4.6851e-05,\n",
      "         6.4259e-05],\n",
      "        [1.1548e-04, 3.1648e-04, 9.8347e-05,  ..., 1.8019e-04, 9.2014e-05,\n",
      "         2.6413e-04],\n",
      "        ...,\n",
      "        [5.0590e-05, 8.7653e-05, 6.3482e-05,  ..., 6.0324e-05, 4.2137e-04,\n",
      "         6.3546e-05],\n",
      "        [4.0029e-04, 2.6321e-04, 3.0400e-04,  ..., 1.5158e-05, 8.4034e-06,\n",
      "         3.4402e-05],\n",
      "        [1.0724e-04, 2.0649e-04, 9.4112e-05,  ..., 9.3917e-05, 5.2927e-05,\n",
      "         1.9695e-04]], device='cuda:0')\n",
      "pred: tensor([0.8387, 0.8228, 0.8029, 0.8136, 0.7797, 0.8267, 0.8131, 0.8196, 0.7883,\n",
      "        0.8198, 0.7925, 0.8349, 0.7916, 0.8206, 0.8391, 0.8149, 0.8320, 0.8290,\n",
      "        0.7886, 0.7848, 0.8262, 0.8342, 0.8638, 0.8580, 0.8766, 0.8116, 0.8744,\n",
      "        0.8125, 0.8474, 0.8496, 0.8468, 0.8624, 0.8698, 0.8641, 0.8541, 0.8695,\n",
      "        0.8301, 0.8495, 0.8410, 0.8270, 0.8376, 0.8754, 0.7995, 0.8666, 0.8369,\n",
      "        0.7990, 0.8494, 0.8212, 0.8334, 0.8392, 0.7591, 0.7959, 0.7626, 0.7729,\n",
      "        0.7902, 0.7818, 0.8249, 0.8267, 0.7553, 0.8417, 0.8012, 0.7946, 0.7853,\n",
      "        0.7994, 0.8355, 0.8481, 0.8008, 0.7798, 0.8096, 0.8292, 0.8375, 0.7989,\n",
      "        0.8320, 0.8099, 0.8019, 0.8637, 0.8459, 0.8414, 0.8605, 0.8584, 0.8278,\n",
      "        0.8301, 0.8615, 0.8410, 0.8547, 0.8190, 0.7864, 0.8372, 0.7852, 0.8381,\n",
      "        0.8368, 0.8426, 0.8045, 0.8386, 0.8621, 0.8517, 0.8152, 0.8504, 0.8511,\n",
      "        0.8414], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1043, Val-Loss: 0.0708\n",
      "Epoch [20/700], Train-Loss: 0.0276, Val-Loss: 0.0227\n",
      "Epoch [30/700], Train-Loss: 0.0099, Val-Loss: 0.0080\n",
      "Epoch [40/700], Train-Loss: 0.0036, Val-Loss: 0.0029\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.4741e-04, 1.0693e-04, 1.0811e-04,  ..., 8.9539e-05, 5.7566e-04,\n",
      "         6.7702e-04],\n",
      "        [2.3838e-04, 1.5954e-04, 1.2411e-04,  ..., 4.8019e-05, 8.3817e-05,\n",
      "         1.5015e-04],\n",
      "        [4.8481e-04, 1.0510e-04, 2.6639e-04,  ..., 1.5284e-04, 1.0186e-03,\n",
      "         1.7430e-03],\n",
      "        ...,\n",
      "        [2.7466e-04, 1.3447e-04, 1.9762e-04,  ..., 1.0305e-04, 1.3445e-04,\n",
      "         5.8777e-04],\n",
      "        [5.7376e-05, 2.8618e-05, 1.4114e-04,  ..., 1.7004e-04, 5.9060e-04,\n",
      "         2.3181e-04],\n",
      "        [2.3603e-04, 9.1934e-05, 1.0855e-04,  ..., 1.6976e-04, 3.8421e-04,\n",
      "         4.3849e-04]], device='cuda:0')\n",
      "pred: tensor([0.8420, 0.8252, 0.7911, 0.8189, 0.7890, 0.8366, 0.8114, 0.8218, 0.7889,\n",
      "        0.8216, 0.7863, 0.8376, 0.7884, 0.8150, 0.8398, 0.8073, 0.8223, 0.8180,\n",
      "        0.7892, 0.7927, 0.8265, 0.8333, 0.8631, 0.8545, 0.8873, 0.8075, 0.8752,\n",
      "        0.8128, 0.8479, 0.8496, 0.8560, 0.8620, 0.8683, 0.8742, 0.8633, 0.8839,\n",
      "        0.8407, 0.8473, 0.8461, 0.8390, 0.8399, 0.8659, 0.7989, 0.8638, 0.8403,\n",
      "        0.8005, 0.8413, 0.8185, 0.8396, 0.8412, 0.7567, 0.7993, 0.7546, 0.7765,\n",
      "        0.7953, 0.7772, 0.8341, 0.8245, 0.7601, 0.8360, 0.8055, 0.8030, 0.7795,\n",
      "        0.8007, 0.8383, 0.8516, 0.7956, 0.7879, 0.8071, 0.8277, 0.8438, 0.8024,\n",
      "        0.8282, 0.8048, 0.8009, 0.8597, 0.8617, 0.8461, 0.8713, 0.8469, 0.8307,\n",
      "        0.8309, 0.8613, 0.8467, 0.8612, 0.8184, 0.7843, 0.8378, 0.7967, 0.8446,\n",
      "        0.8447, 0.8433, 0.8018, 0.8339, 0.8648, 0.8406, 0.8065, 0.8556, 0.8463,\n",
      "        0.8430], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0450, Val-Loss: 0.0118\n",
      "Epoch [20/700], Train-Loss: 0.0065, Val-Loss: 0.0015\n",
      "Epoch [30/700], Train-Loss: 0.0002, Val-Loss: 0.0008\n",
      "Epoch [40/700], Train-Loss: 0.0003, Val-Loss: 0.0010\n",
      "Epoch [50/700], Train-Loss: 0.0008, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[4.5584e-05, 6.5311e-05, 8.4922e-05,  ..., 2.0406e-04, 1.1998e-04,\n",
      "         1.1669e-04],\n",
      "        [9.4478e-05, 7.3388e-05, 1.9873e-04,  ..., 2.3870e-04, 9.3807e-05,\n",
      "         1.2745e-04],\n",
      "        [9.3374e-05, 1.6538e-04, 2.2306e-04,  ..., 4.7626e-04, 1.4924e-04,\n",
      "         1.7816e-04],\n",
      "        ...,\n",
      "        [2.9688e-04, 1.7112e-04, 4.0627e-04,  ..., 5.9121e-05, 9.7450e-05,\n",
      "         2.1352e-04],\n",
      "        [1.0633e-04, 9.8697e-05, 1.8499e-04,  ..., 6.0365e-05, 5.0754e-05,\n",
      "         8.2256e-05],\n",
      "        [1.8263e-04, 2.2971e-04, 7.7791e-05,  ..., 8.5438e-05, 3.4074e-05,\n",
      "         9.6631e-05]], device='cuda:0')\n",
      "pred: tensor([0.8342, 0.8263, 0.7997, 0.8124, 0.7862, 0.8329, 0.8285, 0.8206, 0.7887,\n",
      "        0.8179, 0.8016, 0.8368, 0.7941, 0.8153, 0.8395, 0.8048, 0.8257, 0.8194,\n",
      "        0.7884, 0.7968, 0.8177, 0.8312, 0.8635, 0.8546, 0.8736, 0.8030, 0.8677,\n",
      "        0.8227, 0.8425, 0.8497, 0.8463, 0.8653, 0.8666, 0.8676, 0.8531, 0.8765,\n",
      "        0.8316, 0.8454, 0.8333, 0.8164, 0.8392, 0.8773, 0.8006, 0.8666, 0.8269,\n",
      "        0.8022, 0.8349, 0.8354, 0.8249, 0.8341, 0.7703, 0.7907, 0.7597, 0.7814,\n",
      "        0.7886, 0.7731, 0.8325, 0.8279, 0.7625, 0.8445, 0.7957, 0.8015, 0.7815,\n",
      "        0.8057, 0.8360, 0.8467, 0.7883, 0.7820, 0.8061, 0.8296, 0.8257, 0.8026,\n",
      "        0.8298, 0.8051, 0.7986, 0.8611, 0.8542, 0.8414, 0.8612, 0.8530, 0.8317,\n",
      "        0.8239, 0.8591, 0.8393, 0.8640, 0.8182, 0.7735, 0.8376, 0.7969, 0.8483,\n",
      "        0.8333, 0.8428, 0.8016, 0.8304, 0.8582, 0.8515, 0.8100, 0.8649, 0.8473,\n",
      "        0.8442], device='cuda:0')\n",
      "mae:  tensor(0.0051) ; rank:  0.9737413741374137\n"
     ]
    }
   ],
   "source": [
    "mae, rank = get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_main_set_tensor,\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            sampling_strategy='random',\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False,\n",
    "                                            calculate_rank_score=True)\n",
    "\n",
    "print('mae: ', sum(mae['imagenet1k'])/len(mae['imagenet1k']), '; rank: ', sum([x.correlation for x in rank['imagenet1k']])/len(rank['imagenet1k']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oh/owl813/.conda/envs/model-selection/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f161ead74c0>\n",
      "Epoch [10/1000], Train-Loss: 0.1778, Val-Loss: 0.1739\n",
      "Epoch [20/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [30/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [40/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [50/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [60/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [70/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [80/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [90/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [100/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [110/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [120/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [130/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [140/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [150/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [160/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [170/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [180/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [190/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [200/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [210/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [220/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [230/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [240/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [250/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [260/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [270/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [280/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [290/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [300/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [310/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [320/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [330/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [340/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [350/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [360/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [370/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [380/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [390/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [400/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [410/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [420/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [430/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [440/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [450/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [460/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [470/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [480/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [490/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [500/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [510/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [520/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [530/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [540/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [550/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [560/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [570/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [580/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [590/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [600/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [610/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [620/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [630/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [640/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [650/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [660/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [670/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [680/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [690/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [700/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [710/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [720/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [730/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [740/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [750/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [760/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [770/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [780/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [790/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [800/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [810/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [820/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [830/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [840/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [850/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [860/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [870/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [880/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [890/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [900/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [910/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [920/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [930/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [940/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [950/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [960/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [970/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [980/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [990/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "Epoch [1000/1000], Train-Loss: 0.1779, Val-Loss: 0.1739\n",
      "performances_val_pred.shape: torch.Size([100, 1])\n",
      "performances_val_pred: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(17.3922, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'imagenet1k': [tensor(0.1739)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = {'imagenet1k'}\n",
    "\n",
    "get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor[list(timm_top_disagreement_indices)],\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=SimpleNN,\n",
    "                                            number_bootstraping_steps=1,\n",
    "                                            sample_size=100,\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_fitting_kwargs={\n",
    "                                                    'start_lr': 0.005,\n",
    "                                                    'epochs': 1000,\n",
    "                                                    'scheduler_class': torch.optim.lr_scheduler.ExponentialLR,\n",
    "                                                    'scheduler_args': {'gamma': float(np.power(1e-4/0.005, 1/100))},\n",
    "                                                },\n",
    "                                            pca_comp=-100,\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e427550>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e425f30>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e426830>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e426f50>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e425de0>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e427550>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e425f30>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e426830>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e426f50>\n",
      "scheduler_args:  {'gamma': 0.9616350847573034}\n",
      "scheduler: <torch.optim.lr_scheduler.ExponentialLR object at 0x7f1b3e425de0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'imagenet1k': [tensor(0.0051),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0049),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0049),\n",
       "  tensor(0.0050)]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = {'imagenet1k'}\n",
    "\n",
    "get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor[list(top_disagreement_indices)],\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=SimpleNN,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_fitting_kwargs={\n",
    "                                                    'start_lr': 0.005,\n",
    "                                                    'epochs': 1000,\n",
    "                                                    'scheduler_class': torch.optim.lr_scheduler.ExponentialLR,\n",
    "                                                    'scheduler_args': {'gamma': float(np.power(1e-4/0.005, 1/100))},\n",
    "                                                },\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/700], Train-Loss: 0.0957, Val-Loss: 0.1210\n",
      "Epoch [20/700], Train-Loss: 0.0311, Val-Loss: 0.0353\n",
      "Epoch [30/700], Train-Loss: 0.0060, Val-Loss: 0.0088\n",
      "Epoch [40/700], Train-Loss: 0.0021, Val-Loss: 0.0036\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0011\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.9985e-04, 2.1046e-04, 1.2523e-04,  ..., 1.0061e-04, 5.5266e-04,\n",
      "         5.4284e-04],\n",
      "        [5.9449e-04, 7.9159e-04, 2.4431e-04,  ..., 2.4638e-05, 5.9282e-05,\n",
      "         2.6701e-04],\n",
      "        [2.2782e-04, 1.7305e-04, 1.5070e-04,  ..., 4.9887e-04, 1.8840e-04,\n",
      "         7.0193e-04],\n",
      "        ...,\n",
      "        [2.3314e-04, 3.4799e-04, 2.3023e-04,  ..., 7.4329e-04, 7.3083e-05,\n",
      "         4.4796e-04],\n",
      "        [7.0058e-05, 2.1943e-04, 1.1099e-04,  ..., 1.3996e-04, 1.0183e-04,\n",
      "         2.1373e-04],\n",
      "        [4.9598e-04, 1.1430e-03, 1.3384e-04,  ..., 5.6112e-05, 6.3852e-05,\n",
      "         2.1804e-04]], device='cuda:0')\n",
      "pred: tensor([0.8349, 0.8208, 0.7959, 0.8175, 0.7922, 0.8247, 0.8100, 0.8066, 0.7925,\n",
      "        0.8190, 0.7979, 0.8233, 0.7966, 0.8142, 0.8383, 0.8035, 0.8260, 0.8212,\n",
      "        0.7837, 0.7889, 0.8236, 0.8356, 0.8676, 0.8625, 0.8851, 0.8102, 0.8674,\n",
      "        0.8176, 0.8595, 0.8474, 0.8516, 0.8706, 0.8713, 0.8698, 0.8580, 0.8741,\n",
      "        0.8291, 0.8491, 0.8423, 0.8337, 0.8371, 0.8722, 0.8070, 0.8663, 0.8224,\n",
      "        0.7855, 0.8335, 0.8100, 0.8284, 0.8325, 0.7684, 0.7852, 0.7626, 0.7736,\n",
      "        0.7820, 0.7698, 0.8225, 0.8231, 0.7653, 0.8421, 0.7972, 0.8067, 0.7858,\n",
      "        0.8034, 0.8301, 0.8463, 0.7910, 0.7723, 0.8110, 0.8297, 0.8303, 0.7926,\n",
      "        0.8263, 0.7945, 0.7916, 0.8607, 0.8497, 0.8456, 0.8671, 0.8610, 0.8293,\n",
      "        0.8317, 0.8617, 0.8441, 0.8630, 0.8167, 0.7771, 0.8394, 0.7945, 0.8409,\n",
      "        0.8378, 0.8397, 0.8065, 0.8320, 0.8627, 0.8453, 0.8056, 0.8539, 0.8547,\n",
      "        0.8425], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0679, Val-Loss: 0.0273\n",
      "Epoch [20/700], Train-Loss: 0.0148, Val-Loss: 0.0070\n",
      "Epoch [30/700], Train-Loss: 0.0025, Val-Loss: 0.0004\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0001\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0004\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0951e-04, 4.7035e-05, 1.2120e-04,  ..., 1.5340e-04, 1.3533e-04,\n",
      "         2.5725e-04],\n",
      "        [1.5077e-04, 1.5861e-04, 1.8519e-04,  ..., 5.3555e-05, 1.8790e-04,\n",
      "         2.6414e-04],\n",
      "        [2.8395e-04, 2.1961e-04, 2.4371e-04,  ..., 2.6952e-04, 5.8906e-04,\n",
      "         1.6918e-03],\n",
      "        ...,\n",
      "        [1.3957e-04, 1.6926e-04, 4.3072e-04,  ..., 2.6530e-04, 1.2188e-04,\n",
      "         3.4221e-04],\n",
      "        [1.5477e-04, 1.4666e-04, 7.8785e-05,  ..., 3.5434e-05, 5.2293e-05,\n",
      "         1.7772e-04],\n",
      "        [3.2239e-04, 9.2417e-05, 2.1979e-04,  ..., 4.4062e-05, 8.2017e-05,\n",
      "         1.9119e-04]], device='cuda:0')\n",
      "pred: tensor([0.8361, 0.8204, 0.7972, 0.8175, 0.7915, 0.8249, 0.8093, 0.8086, 0.7942,\n",
      "        0.8171, 0.7975, 0.8249, 0.7977, 0.8131, 0.8401, 0.8018, 0.8262, 0.8220,\n",
      "        0.7810, 0.7893, 0.8237, 0.8373, 0.8652, 0.8609, 0.8847, 0.8105, 0.8686,\n",
      "        0.8186, 0.8595, 0.8494, 0.8527, 0.8727, 0.8697, 0.8704, 0.8580, 0.8737,\n",
      "        0.8303, 0.8513, 0.8414, 0.8367, 0.8388, 0.8717, 0.8073, 0.8647, 0.8248,\n",
      "        0.7881, 0.8361, 0.8108, 0.8303, 0.8349, 0.7678, 0.7867, 0.7624, 0.7725,\n",
      "        0.7826, 0.7699, 0.8252, 0.8246, 0.7650, 0.8401, 0.7987, 0.8070, 0.7864,\n",
      "        0.8015, 0.8323, 0.8468, 0.7908, 0.7727, 0.8112, 0.8284, 0.8305, 0.7937,\n",
      "        0.8264, 0.7959, 0.7932, 0.8601, 0.8483, 0.8445, 0.8659, 0.8615, 0.8298,\n",
      "        0.8307, 0.8616, 0.8444, 0.8621, 0.8164, 0.7766, 0.8390, 0.7936, 0.8415,\n",
      "        0.8377, 0.8386, 0.8053, 0.8314, 0.8612, 0.8452, 0.8045, 0.8534, 0.8526,\n",
      "        0.8419], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1260, Val-Loss: 0.1005\n",
      "Epoch [20/700], Train-Loss: 0.0322, Val-Loss: 0.0266\n",
      "Epoch [30/700], Train-Loss: 0.0114, Val-Loss: 0.0081\n",
      "Epoch [40/700], Train-Loss: 0.0041, Val-Loss: 0.0031\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0006\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.1576e-04, 1.3208e-02, 2.3109e-04,  ..., 3.7052e-05, 9.6636e-05,\n",
      "         4.9886e-05],\n",
      "        [4.3312e-04, 2.8866e-03, 1.5048e-04,  ..., 1.7440e-04, 2.3424e-04,\n",
      "         7.6219e-04],\n",
      "        [3.2584e-04, 4.3602e-03, 1.7051e-03,  ..., 1.1612e-04, 5.9183e-04,\n",
      "         1.4361e-03],\n",
      "        ...,\n",
      "        [2.4864e-04, 1.8611e-04, 4.5682e-05,  ..., 2.2224e-04, 5.6184e-05,\n",
      "         5.6146e-04],\n",
      "        [1.9276e-04, 1.0705e-04, 1.6246e-05,  ..., 4.9884e-05, 5.5570e-05,\n",
      "         1.9534e-05],\n",
      "        [7.0577e-04, 5.0176e-04, 6.5070e-04,  ..., 5.5615e-05, 2.5234e-05,\n",
      "         1.4514e-04]], device='cuda:0')\n",
      "pred: tensor([0.8367, 0.8203, 0.7961, 0.8177, 0.7921, 0.8245, 0.8081, 0.8079, 0.7925,\n",
      "        0.8177, 0.7975, 0.8255, 0.7980, 0.8127, 0.8372, 0.8032, 0.8249, 0.8197,\n",
      "        0.7802, 0.7902, 0.8240, 0.8367, 0.8669, 0.8610, 0.8837, 0.8095, 0.8674,\n",
      "        0.8167, 0.8592, 0.8476, 0.8539, 0.8716, 0.8698, 0.8706, 0.8586, 0.8735,\n",
      "        0.8299, 0.8498, 0.8411, 0.8342, 0.8369, 0.8723, 0.8070, 0.8648, 0.8232,\n",
      "        0.7868, 0.8373, 0.8084, 0.8279, 0.8330, 0.7666, 0.7869, 0.7632, 0.7737,\n",
      "        0.7825, 0.7696, 0.8255, 0.8227, 0.7638, 0.8414, 0.7985, 0.8061, 0.7866,\n",
      "        0.8022, 0.8313, 0.8459, 0.7895, 0.7730, 0.8114, 0.8292, 0.8294, 0.7915,\n",
      "        0.8250, 0.7956, 0.7911, 0.8603, 0.8496, 0.8461, 0.8674, 0.8629, 0.8306,\n",
      "        0.8308, 0.8617, 0.8431, 0.8630, 0.8177, 0.7756, 0.8386, 0.7941, 0.8426,\n",
      "        0.8381, 0.8393, 0.8056, 0.8320, 0.8630, 0.8460, 0.8057, 0.8538, 0.8552,\n",
      "        0.8409], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0846, Val-Loss: 0.0457\n",
      "Epoch [20/700], Train-Loss: 0.0233, Val-Loss: 0.0162\n",
      "Epoch [30/700], Train-Loss: 0.0079, Val-Loss: 0.0043\n",
      "Epoch [40/700], Train-Loss: 0.0027, Val-Loss: 0.0016\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0951e-04, 4.7035e-05, 1.2120e-04,  ..., 4.3857e-05, 1.3532e-04,\n",
      "         1.8451e-04],\n",
      "        [1.5077e-04, 1.5861e-04, 1.8519e-04,  ..., 1.2728e-04, 2.5861e-04,\n",
      "         2.5697e-04],\n",
      "        [2.8395e-04, 2.1961e-04, 2.4371e-04,  ..., 1.5126e-04, 2.2207e-04,\n",
      "         3.1858e-04],\n",
      "        ...,\n",
      "        [1.3957e-04, 1.6926e-04, 4.3072e-04,  ..., 5.2830e-05, 5.1969e-04,\n",
      "         7.9775e-05],\n",
      "        [1.5477e-04, 1.4666e-04, 7.8785e-05,  ..., 6.5031e-05, 1.5108e-04,\n",
      "         4.8542e-05],\n",
      "        [3.2239e-04, 9.2417e-05, 2.1979e-04,  ..., 1.9030e-04, 1.4778e-04,\n",
      "         1.4802e-04]], device='cuda:0')\n",
      "pred: tensor([0.8365, 0.8202, 0.7975, 0.8175, 0.7933, 0.8251, 0.8088, 0.8078, 0.7938,\n",
      "        0.8183, 0.7990, 0.8249, 0.7968, 0.8156, 0.8376, 0.8021, 0.8261, 0.8214,\n",
      "        0.7809, 0.7901, 0.8233, 0.8361, 0.8657, 0.8634, 0.8841, 0.8106, 0.8674,\n",
      "        0.8163, 0.8587, 0.8487, 0.8529, 0.8727, 0.8718, 0.8706, 0.8593, 0.8721,\n",
      "        0.8295, 0.8522, 0.8407, 0.8341, 0.8375, 0.8712, 0.8074, 0.8657, 0.8244,\n",
      "        0.7854, 0.8351, 0.8120, 0.8307, 0.8329, 0.7675, 0.7863, 0.7635, 0.7742,\n",
      "        0.7816, 0.7700, 0.8245, 0.8251, 0.7655, 0.8442, 0.7974, 0.8068, 0.7868,\n",
      "        0.8038, 0.8312, 0.8455, 0.7889, 0.7719, 0.8111, 0.8294, 0.8314, 0.7924,\n",
      "        0.8266, 0.7940, 0.7927, 0.8600, 0.8499, 0.8436, 0.8662, 0.8618, 0.8299,\n",
      "        0.8317, 0.8605, 0.8433, 0.8627, 0.8161, 0.7775, 0.8390, 0.7936, 0.8403,\n",
      "        0.8379, 0.8389, 0.8062, 0.8322, 0.8619, 0.8450, 0.8054, 0.8534, 0.8548,\n",
      "        0.8412], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0327, Val-Loss: 0.0063\n",
      "Epoch [20/700], Train-Loss: 0.0041, Val-Loss: 0.0004\n",
      "Epoch [30/700], Train-Loss: 0.0002, Val-Loss: 0.0018\n",
      "Epoch [40/700], Train-Loss: 0.0009, Val-Loss: 0.0016\n",
      "Epoch [50/700], Train-Loss: 0.0009, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0900e-04, 2.5451e-04, 1.3628e-04,  ..., 1.3719e-02, 9.1578e-02,\n",
      "         5.3328e-04],\n",
      "        [8.9886e-05, 1.1179e-04, 2.2281e-04,  ..., 1.5998e-03, 7.9696e-04,\n",
      "         2.2332e-04],\n",
      "        [1.1804e-04, 9.9810e-05, 2.0715e-04,  ..., 8.3006e-04, 3.2646e-03,\n",
      "         8.8582e-04],\n",
      "        ...,\n",
      "        [4.7657e-05, 4.3143e-05, 3.2560e-05,  ..., 7.1651e-04, 5.0712e-03,\n",
      "         2.9710e-03],\n",
      "        [7.2899e-05, 4.5393e-05, 5.5345e-05,  ..., 1.2127e-04, 4.8361e-02,\n",
      "         2.9404e-04],\n",
      "        [1.4355e-04, 2.2631e-04, 5.1427e-05,  ..., 3.0082e-04, 2.9165e-02,\n",
      "         2.7750e-05]], device='cuda:0')\n",
      "pred: tensor([0.8369, 0.8208, 0.7968, 0.8171, 0.7925, 0.8232, 0.8080, 0.8051, 0.7932,\n",
      "        0.8195, 0.7979, 0.8245, 0.7964, 0.8142, 0.8384, 0.8028, 0.8259, 0.8215,\n",
      "        0.7815, 0.7891, 0.8248, 0.8361, 0.8666, 0.8613, 0.8846, 0.8102, 0.8692,\n",
      "        0.8147, 0.8594, 0.8474, 0.8547, 0.8745, 0.8709, 0.8712, 0.8589, 0.8728,\n",
      "        0.8293, 0.8530, 0.8393, 0.8373, 0.8377, 0.8726, 0.8071, 0.8658, 0.8276,\n",
      "        0.7869, 0.8366, 0.8113, 0.8315, 0.8340, 0.7688, 0.7851, 0.7633, 0.7744,\n",
      "        0.7825, 0.7695, 0.8238, 0.8238, 0.7650, 0.8435, 0.7992, 0.8065, 0.7863,\n",
      "        0.8017, 0.8321, 0.8476, 0.7899, 0.7735, 0.8120, 0.8295, 0.8300, 0.7901,\n",
      "        0.8253, 0.7952, 0.7933, 0.8606, 0.8496, 0.8457, 0.8673, 0.8625, 0.8294,\n",
      "        0.8314, 0.8616, 0.8446, 0.8627, 0.8160, 0.7765, 0.8391, 0.7922, 0.8415,\n",
      "        0.8368, 0.8393, 0.8050, 0.8314, 0.8625, 0.8459, 0.8032, 0.8530, 0.8523,\n",
      "        0.8412], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0923, Val-Loss: 0.0554\n",
      "Epoch [20/700], Train-Loss: 0.0259, Val-Loss: 0.0197\n",
      "Epoch [30/700], Train-Loss: 0.0089, Val-Loss: 0.0054\n",
      "Epoch [40/700], Train-Loss: 0.0030, Val-Loss: 0.0020\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.5236e-05, 1.1035e-04, 5.3150e-04,  ..., 6.9382e-05, 4.7482e-05,\n",
      "         8.3953e-05],\n",
      "        [3.3979e-04, 2.3477e-04, 2.4206e-04,  ..., 2.8711e-05, 5.4943e-05,\n",
      "         3.6441e-05],\n",
      "        [1.4442e-04, 1.9682e-04, 4.1818e-04,  ..., 1.2985e-04, 1.5124e-04,\n",
      "         7.7364e-05],\n",
      "        ...,\n",
      "        [8.3299e-05, 2.2976e-05, 4.0881e-04,  ..., 2.2118e-04, 4.4338e-04,\n",
      "         1.7759e-04],\n",
      "        [2.6084e-04, 3.4821e-04, 1.8876e-04,  ..., 1.2282e-04, 2.5844e-04,\n",
      "         4.4975e-05],\n",
      "        [1.0542e-05, 1.2243e-04, 4.8897e-04,  ..., 4.2916e-04, 9.8485e-05,\n",
      "         5.3630e-04]], device='cuda:0')\n",
      "pred: tensor([0.8357, 0.8213, 0.7971, 0.8168, 0.7923, 0.8232, 0.8095, 0.8057, 0.7930,\n",
      "        0.8182, 0.7980, 0.8266, 0.7968, 0.8140, 0.8385, 0.8040, 0.8257, 0.8219,\n",
      "        0.7814, 0.7901, 0.8232, 0.8360, 0.8659, 0.8623, 0.8851, 0.8095, 0.8682,\n",
      "        0.8181, 0.8578, 0.8475, 0.8526, 0.8721, 0.8691, 0.8705, 0.8582, 0.8733,\n",
      "        0.8302, 0.8510, 0.8399, 0.8365, 0.8375, 0.8719, 0.8067, 0.8647, 0.8242,\n",
      "        0.7855, 0.8361, 0.8089, 0.8300, 0.8318, 0.7676, 0.7855, 0.7634, 0.7746,\n",
      "        0.7805, 0.7700, 0.8260, 0.8251, 0.7642, 0.8411, 0.7997, 0.8075, 0.7864,\n",
      "        0.8033, 0.8321, 0.8465, 0.7896, 0.7733, 0.8129, 0.8287, 0.8288, 0.7931,\n",
      "        0.8269, 0.7954, 0.7912, 0.8593, 0.8508, 0.8451, 0.8678, 0.8617, 0.8298,\n",
      "        0.8329, 0.8610, 0.8438, 0.8616, 0.8170, 0.7777, 0.8379, 0.7925, 0.8404,\n",
      "        0.8366, 0.8388, 0.8056, 0.8315, 0.8610, 0.8462, 0.8049, 0.8535, 0.8521,\n",
      "        0.8424], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1189, Val-Loss: 0.1247\n",
      "Epoch [20/700], Train-Loss: 0.0377, Val-Loss: 0.0388\n",
      "Epoch [30/700], Train-Loss: 0.0114, Val-Loss: 0.0126\n",
      "Epoch [40/700], Train-Loss: 0.0046, Val-Loss: 0.0053\n",
      "Epoch [50/700], Train-Loss: 0.0019, Val-Loss: 0.0017\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.2912e-05, 1.5381e-04, 1.8579e-04,  ..., 6.4776e-05, 1.9848e-04,\n",
      "         6.1798e-04],\n",
      "        [5.6215e-05, 8.3628e-05, 7.5941e-05,  ..., 2.5217e-04, 1.5073e-04,\n",
      "         1.5011e-03],\n",
      "        [4.0572e-05, 7.3895e-05, 2.1536e-04,  ..., 1.7160e-04, 1.4524e-03,\n",
      "         7.4062e-04],\n",
      "        ...,\n",
      "        [2.7019e-04, 2.8594e-04, 2.7031e-04,  ..., 7.9052e-05, 4.1653e-05,\n",
      "         3.8653e-04],\n",
      "        [1.8042e-05, 8.1046e-05, 2.3616e-04,  ..., 3.0387e-04, 3.2258e-04,\n",
      "         4.3234e-04],\n",
      "        [3.7455e-06, 2.2659e-04, 5.3967e-05,  ..., 1.0475e-04, 1.0468e-04,\n",
      "         1.2378e-04]], device='cuda:0')\n",
      "pred: tensor([0.8373, 0.8210, 0.7970, 0.8181, 0.7921, 0.8241, 0.8103, 0.8069, 0.7929,\n",
      "        0.8177, 0.7990, 0.8253, 0.7971, 0.8138, 0.8372, 0.8018, 0.8259, 0.8223,\n",
      "        0.7811, 0.7906, 0.8242, 0.8375, 0.8651, 0.8617, 0.8851, 0.8095, 0.8679,\n",
      "        0.8158, 0.8590, 0.8483, 0.8534, 0.8711, 0.8709, 0.8694, 0.8581, 0.8739,\n",
      "        0.8300, 0.8512, 0.8405, 0.8342, 0.8376, 0.8725, 0.8072, 0.8653, 0.8243,\n",
      "        0.7893, 0.8354, 0.8126, 0.8298, 0.8330, 0.7672, 0.7862, 0.7632, 0.7739,\n",
      "        0.7823, 0.7694, 0.8244, 0.8237, 0.7644, 0.8414, 0.7983, 0.8058, 0.7861,\n",
      "        0.8026, 0.8314, 0.8457, 0.7893, 0.7728, 0.8129, 0.8283, 0.8298, 0.7926,\n",
      "        0.8255, 0.7954, 0.7920, 0.8610, 0.8500, 0.8439, 0.8660, 0.8605, 0.8308,\n",
      "        0.8317, 0.8613, 0.8444, 0.8629, 0.8166, 0.7767, 0.8388, 0.7936, 0.8395,\n",
      "        0.8372, 0.8388, 0.8058, 0.8317, 0.8611, 0.8450, 0.8061, 0.8541, 0.8534,\n",
      "        0.8430], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1295, Val-Loss: 0.0800\n",
      "Epoch [20/700], Train-Loss: 0.0326, Val-Loss: 0.0241\n",
      "Epoch [30/700], Train-Loss: 0.0113, Val-Loss: 0.0068\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0024\n",
      "Epoch [50/700], Train-Loss: 0.0009, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.4543e-04, 1.5095e-04, 1.4305e-04,  ..., 2.3470e-05, 1.2406e-04,\n",
      "         3.1914e-05],\n",
      "        [2.5938e-04, 3.4597e-04, 2.8609e-04,  ..., 4.1492e-04, 1.1396e-04,\n",
      "         1.9629e-04],\n",
      "        [8.5566e-05, 9.1254e-05, 5.0127e-04,  ..., 3.8940e-04, 3.3524e-04,\n",
      "         2.3578e-04],\n",
      "        ...,\n",
      "        [3.0144e-04, 1.6348e-04, 8.8155e-04,  ..., 3.1147e-04, 2.8412e-04,\n",
      "         1.9258e-05],\n",
      "        [3.4541e-04, 5.4366e-04, 3.3965e-04,  ..., 1.6953e-04, 9.3688e-05,\n",
      "         2.1705e-05],\n",
      "        [1.1827e-04, 2.5187e-03, 1.7097e-04,  ..., 9.2791e-05, 1.9835e-04,\n",
      "         1.9219e-04]], device='cuda:0')\n",
      "pred: tensor([0.8363, 0.8198, 0.7984, 0.8182, 0.7920, 0.8238, 0.8110, 0.8075, 0.7942,\n",
      "        0.8188, 0.7974, 0.8244, 0.7957, 0.8141, 0.8395, 0.8031, 0.8265, 0.8205,\n",
      "        0.7817, 0.7897, 0.8241, 0.8367, 0.8658, 0.8624, 0.8839, 0.8103, 0.8681,\n",
      "        0.8155, 0.8607, 0.8484, 0.8545, 0.8697, 0.8700, 0.8693, 0.8579, 0.8724,\n",
      "        0.8303, 0.8514, 0.8410, 0.8338, 0.8375, 0.8719, 0.8073, 0.8642, 0.8242,\n",
      "        0.7862, 0.8356, 0.8087, 0.8292, 0.8325, 0.7698, 0.7848, 0.7631, 0.7732,\n",
      "        0.7831, 0.7683, 0.8255, 0.8247, 0.7647, 0.8420, 0.7968, 0.8064, 0.7856,\n",
      "        0.8028, 0.8315, 0.8473, 0.7879, 0.7730, 0.8140, 0.8300, 0.8291, 0.7920,\n",
      "        0.8261, 0.7954, 0.7936, 0.8610, 0.8495, 0.8449, 0.8651, 0.8624, 0.8288,\n",
      "        0.8313, 0.8621, 0.8448, 0.8634, 0.8176, 0.7773, 0.8387, 0.7941, 0.8420,\n",
      "        0.8376, 0.8395, 0.8078, 0.8326, 0.8617, 0.8473, 0.8048, 0.8543, 0.8520,\n",
      "        0.8400], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1276, Val-Loss: 0.0985\n",
      "Epoch [20/700], Train-Loss: 0.0323, Val-Loss: 0.0282\n",
      "Epoch [30/700], Train-Loss: 0.0115, Val-Loss: 0.0093\n",
      "Epoch [40/700], Train-Loss: 0.0042, Val-Loss: 0.0038\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[6.9298e-04, 6.9938e-04, 2.8534e-04,  ..., 1.3719e-02, 9.1578e-02,\n",
      "         5.3328e-04],\n",
      "        [1.4987e-03, 4.2865e-04, 6.3277e-04,  ..., 1.5998e-03, 7.9696e-04,\n",
      "         2.2332e-04],\n",
      "        [4.8684e-04, 4.8106e-04, 4.6896e-04,  ..., 8.3006e-04, 3.2646e-03,\n",
      "         8.8582e-04],\n",
      "        ...,\n",
      "        [2.9585e-03, 7.1557e-03, 3.3757e-04,  ..., 7.1651e-04, 5.0712e-03,\n",
      "         2.9710e-03],\n",
      "        [2.4424e-04, 3.5792e-04, 1.9733e-04,  ..., 1.2127e-04, 4.8361e-02,\n",
      "         2.9404e-04],\n",
      "        [8.3526e-05, 2.1409e-04, 1.1590e-04,  ..., 3.0082e-04, 2.9165e-02,\n",
      "         2.7750e-05]], device='cuda:0')\n",
      "pred: tensor([0.8356, 0.8220, 0.7980, 0.8173, 0.7919, 0.8253, 0.8087, 0.8075, 0.7931,\n",
      "        0.8184, 0.7995, 0.8253, 0.7978, 0.8139, 0.8381, 0.8027, 0.8251, 0.8216,\n",
      "        0.7804, 0.7891, 0.8254, 0.8358, 0.8662, 0.8598, 0.8836, 0.8096, 0.8674,\n",
      "        0.8180, 0.8579, 0.8487, 0.8541, 0.8719, 0.8683, 0.8708, 0.8587, 0.8719,\n",
      "        0.8300, 0.8519, 0.8409, 0.8345, 0.8371, 0.8737, 0.8072, 0.8663, 0.8240,\n",
      "        0.7882, 0.8349, 0.8116, 0.8296, 0.8355, 0.7673, 0.7858, 0.7636, 0.7727,\n",
      "        0.7824, 0.7704, 0.8244, 0.8231, 0.7653, 0.8412, 0.7998, 0.8057, 0.7867,\n",
      "        0.8035, 0.8307, 0.8460, 0.7901, 0.7721, 0.8120, 0.8287, 0.8292, 0.7920,\n",
      "        0.8245, 0.7958, 0.7915, 0.8594, 0.8497, 0.8467, 0.8674, 0.8631, 0.8305,\n",
      "        0.8314, 0.8618, 0.8442, 0.8631, 0.8170, 0.7768, 0.8390, 0.7945, 0.8418,\n",
      "        0.8380, 0.8386, 0.8045, 0.8313, 0.8624, 0.8465, 0.8041, 0.8540, 0.8527,\n",
      "        0.8418], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1305, Val-Loss: 0.0786\n",
      "Epoch [20/700], Train-Loss: 0.0321, Val-Loss: 0.0244\n",
      "Epoch [30/700], Train-Loss: 0.0119, Val-Loss: 0.0081\n",
      "Epoch [40/700], Train-Loss: 0.0042, Val-Loss: 0.0033\n",
      "Epoch [50/700], Train-Loss: 0.0014, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[4.2905e-05, 1.5294e-04, 2.6219e-04,  ..., 6.4694e-05, 3.2027e-04,\n",
      "         5.9470e-03],\n",
      "        [2.1066e-04, 1.8805e-04, 9.4654e-04,  ..., 1.5612e-04, 1.6447e-03,\n",
      "         2.5011e-04],\n",
      "        [1.3149e-04, 9.4168e-05, 1.0155e-04,  ..., 3.8274e-05, 2.6164e-04,\n",
      "         8.6181e-05],\n",
      "        ...,\n",
      "        [2.4558e-04, 7.8179e-05, 2.2854e-04,  ..., 1.0366e-04, 2.0196e-03,\n",
      "         6.1040e-05],\n",
      "        [2.3510e-05, 1.7006e-05, 3.9598e-05,  ..., 1.8024e-04, 8.4055e-03,\n",
      "         2.2186e-04],\n",
      "        [1.9313e-05, 6.7720e-05, 3.4613e-04,  ..., 7.5304e-05, 1.4322e-03,\n",
      "         1.0174e-04]], device='cuda:0')\n",
      "pred: tensor([0.8371, 0.8208, 0.7961, 0.8183, 0.7941, 0.8231, 0.8081, 0.8061, 0.7924,\n",
      "        0.8178, 0.7980, 0.8267, 0.7968, 0.8160, 0.8355, 0.8029, 0.8262, 0.8214,\n",
      "        0.7830, 0.7913, 0.8232, 0.8383, 0.8664, 0.8616, 0.8835, 0.8101, 0.8665,\n",
      "        0.8166, 0.8590, 0.8482, 0.8540, 0.8707, 0.8697, 0.8696, 0.8572, 0.8720,\n",
      "        0.8319, 0.8522, 0.8420, 0.8323, 0.8360, 0.8701, 0.8067, 0.8652, 0.8243,\n",
      "        0.7865, 0.8386, 0.8082, 0.8305, 0.8337, 0.7675, 0.7862, 0.7636, 0.7735,\n",
      "        0.7824, 0.7708, 0.8241, 0.8233, 0.7639, 0.8409, 0.7980, 0.8058, 0.7869,\n",
      "        0.8012, 0.8310, 0.8467, 0.7883, 0.7730, 0.8105, 0.8277, 0.8294, 0.7927,\n",
      "        0.8248, 0.7966, 0.7914, 0.8601, 0.8501, 0.8449, 0.8666, 0.8616, 0.8304,\n",
      "        0.8304, 0.8615, 0.8430, 0.8628, 0.8194, 0.7762, 0.8404, 0.7925, 0.8407,\n",
      "        0.8379, 0.8391, 0.8070, 0.8321, 0.8630, 0.8462, 0.8048, 0.8564, 0.8544,\n",
      "        0.8411], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'imagenet1k': [tensor(0.0052),\n",
       "  tensor(0.0050),\n",
       "  tensor(0.0051),\n",
       "  tensor(0.0051),\n",
       "  tensor(0.0051),\n",
       "  tensor(0.0051),\n",
       "  tensor(0.0049),\n",
       "  tensor(0.0051),\n",
       "  tensor(0.0051),\n",
       "  tensor(0.0050)]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = {'imagenet1k'}\n",
    "\n",
    "get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor[list(top_disagreement_indices)],\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=timm_train_model_indices,\n",
    "                                            val_model_index=timm_val_model_indices,\n",
    "                                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1218, Val-Loss: 0.1300\n",
      "Epoch [20/700], Train-Loss: 0.0383, Val-Loss: 0.0377\n",
      "Epoch [30/700], Train-Loss: 0.0111, Val-Loss: 0.0118\n",
      "Epoch [40/700], Train-Loss: 0.0043, Val-Loss: 0.0049\n",
      "Epoch [50/700], Train-Loss: 0.0018, Val-Loss: 0.0016\n",
      "Epoch [60/700], Train-Loss: 0.0008, Val-Loss: 0.0007\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0864e-04, 2.6656e-05, 1.5261e-04,  ..., 7.4293e-05, 5.6690e-05,\n",
      "         9.1798e-05],\n",
      "        [2.0732e-04, 1.0673e-04, 1.1625e-04,  ..., 1.7707e-04, 1.8682e-04,\n",
      "         1.5448e-04],\n",
      "        [4.8789e-04, 2.8473e-04, 1.9328e-04,  ..., 2.0822e-04, 1.6169e-04,\n",
      "         3.1079e-04],\n",
      "        ...,\n",
      "        [6.9398e-05, 2.3543e-05, 1.2247e-04,  ..., 6.9848e-05, 1.0573e-04,\n",
      "         8.4133e-05],\n",
      "        [6.2596e-05, 3.8110e-05, 1.1764e-04,  ..., 2.2354e-04, 1.2204e-04,\n",
      "         5.0913e-05],\n",
      "        [5.8712e-05, 1.5473e-05, 1.0672e-04,  ..., 1.8017e-04, 1.4155e-04,\n",
      "         1.5928e-04]], device='cuda:0')\n",
      "pred: tensor([0.8452, 0.8185, 0.7954, 0.8110, 0.7851, 0.8243, 0.8168, 0.8120, 0.7875,\n",
      "        0.8210, 0.7961, 0.8255, 0.7959, 0.8076, 0.8311, 0.8074, 0.8266, 0.8301,\n",
      "        0.7870, 0.7942, 0.8286, 0.8372, 0.8665, 0.8572, 0.8790, 0.8084, 0.8682,\n",
      "        0.8039, 0.8460, 0.8503, 0.8513, 0.8813, 0.8643, 0.8668, 0.8661, 0.8695,\n",
      "        0.8350, 0.8423, 0.8359, 0.8357, 0.8420, 0.8759, 0.7996, 0.8738, 0.8303,\n",
      "        0.7965, 0.8350, 0.8098, 0.8330, 0.8233, 0.7653, 0.7826, 0.7449, 0.7790,\n",
      "        0.7858, 0.7699, 0.8258, 0.8379, 0.7560, 0.8462, 0.7930, 0.7998, 0.7804,\n",
      "        0.8117, 0.8264, 0.8455, 0.8046, 0.7824, 0.8059, 0.8308, 0.8275, 0.8016,\n",
      "        0.8297, 0.8081, 0.8117, 0.8667, 0.8478, 0.8572, 0.8638, 0.8586, 0.8310,\n",
      "        0.8363, 0.8590, 0.8388, 0.8623, 0.8197, 0.7696, 0.8403, 0.8050, 0.8418,\n",
      "        0.8348, 0.8382, 0.8013, 0.8339, 0.8628, 0.8445, 0.8047, 0.8689, 0.8457,\n",
      "        0.8335], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8452, 0.8185, 0.7954, 0.8110, 0.7851, 0.8243, 0.8168, 0.8120, 0.7875,\n",
      "        0.8210, 0.7961, 0.8255, 0.7959, 0.8076, 0.8311, 0.8074, 0.8266, 0.8301,\n",
      "        0.7870, 0.7942, 0.8286, 0.8372, 0.8665, 0.8572, 0.8790, 0.8084, 0.8682,\n",
      "        0.8039, 0.8460, 0.8503, 0.8513, 0.8813, 0.8643, 0.8668, 0.8661, 0.8695,\n",
      "        0.8350, 0.8423, 0.8359, 0.8357, 0.8420, 0.8759, 0.7996, 0.8738, 0.8303,\n",
      "        0.7965, 0.8350, 0.8098, 0.8330, 0.8233, 0.7653, 0.7826, 0.7449, 0.7790,\n",
      "        0.7858, 0.7699, 0.8258, 0.8379, 0.7560, 0.8462, 0.7930, 0.7998, 0.7804,\n",
      "        0.8117, 0.8264, 0.8455, 0.8046, 0.7824, 0.8059, 0.8308, 0.8275, 0.8016,\n",
      "        0.8297, 0.8081, 0.8117, 0.8667, 0.8478, 0.8572, 0.8638, 0.8586, 0.8310,\n",
      "        0.8363, 0.8590, 0.8388, 0.8623, 0.8197, 0.7696, 0.8403, 0.8050, 0.8418,\n",
      "        0.8348, 0.8382, 0.8013, 0.8339, 0.8628, 0.8445, 0.8047, 0.8689, 0.8457,\n",
      "        0.8335], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4548, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1141, Val-Loss: 0.0746\n",
      "Epoch [20/700], Train-Loss: 0.0331, Val-Loss: 0.0267\n",
      "Epoch [30/700], Train-Loss: 0.0122, Val-Loss: 0.0088\n",
      "Epoch [40/700], Train-Loss: 0.0045, Val-Loss: 0.0037\n",
      "Epoch [50/700], Train-Loss: 0.0016, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.4198e-04, 1.5728e-04, 8.4625e-05,  ..., 1.8023e-04, 2.0485e-04,\n",
      "         3.5413e-04],\n",
      "        [1.3351e-04, 1.1137e-04, 3.1136e-05,  ..., 1.3685e-04, 2.5841e-04,\n",
      "         2.9046e-04],\n",
      "        [5.5037e-05, 1.3782e-04, 1.2291e-04,  ..., 8.4749e-05, 1.7196e-04,\n",
      "         1.9411e-04],\n",
      "        ...,\n",
      "        [1.3547e-04, 1.0739e-04, 1.2545e-04,  ..., 3.2154e-05, 3.7214e-05,\n",
      "         7.5766e-05],\n",
      "        [4.2545e-05, 9.1520e-05, 6.9158e-05,  ..., 9.8110e-05, 1.3559e-04,\n",
      "         6.3487e-05],\n",
      "        [7.7010e-05, 1.0239e-04, 4.2572e-05,  ..., 7.9708e-05, 1.1319e-04,\n",
      "         5.3556e-05]], device='cuda:0')\n",
      "pred: tensor([0.8448, 0.8184, 0.7952, 0.8114, 0.7850, 0.8241, 0.8183, 0.8121, 0.7871,\n",
      "        0.8210, 0.7944, 0.8255, 0.7962, 0.8071, 0.8313, 0.8067, 0.8263, 0.8314,\n",
      "        0.7872, 0.7943, 0.8282, 0.8368, 0.8669, 0.8571, 0.8786, 0.8084, 0.8676,\n",
      "        0.8041, 0.8458, 0.8503, 0.8508, 0.8818, 0.8645, 0.8669, 0.8659, 0.8688,\n",
      "        0.8352, 0.8416, 0.8360, 0.8352, 0.8423, 0.8757, 0.7996, 0.8738, 0.8303,\n",
      "        0.7961, 0.8352, 0.8103, 0.8329, 0.8240, 0.7658, 0.7833, 0.7454, 0.7788,\n",
      "        0.7858, 0.7701, 0.8245, 0.8377, 0.7565, 0.8455, 0.7938, 0.7993, 0.7809,\n",
      "        0.8123, 0.8256, 0.8455, 0.8046, 0.7820, 0.8063, 0.8309, 0.8275, 0.8007,\n",
      "        0.8294, 0.8075, 0.8111, 0.8669, 0.8475, 0.8570, 0.8642, 0.8582, 0.8308,\n",
      "        0.8366, 0.8591, 0.8390, 0.8623, 0.8199, 0.7698, 0.8403, 0.8049, 0.8409,\n",
      "        0.8348, 0.8375, 0.8017, 0.8342, 0.8623, 0.8442, 0.8037, 0.8689, 0.8461,\n",
      "        0.8328], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8448, 0.8184, 0.7952, 0.8114, 0.7850, 0.8241, 0.8183, 0.8121, 0.7871,\n",
      "        0.8210, 0.7944, 0.8255, 0.7962, 0.8071, 0.8313, 0.8067, 0.8263, 0.8314,\n",
      "        0.7872, 0.7943, 0.8282, 0.8368, 0.8669, 0.8571, 0.8786, 0.8084, 0.8676,\n",
      "        0.8041, 0.8458, 0.8503, 0.8508, 0.8818, 0.8645, 0.8669, 0.8659, 0.8688,\n",
      "        0.8352, 0.8416, 0.8360, 0.8352, 0.8423, 0.8757, 0.7996, 0.8738, 0.8303,\n",
      "        0.7961, 0.8352, 0.8103, 0.8329, 0.8240, 0.7658, 0.7833, 0.7454, 0.7788,\n",
      "        0.7858, 0.7701, 0.8245, 0.8377, 0.7565, 0.8455, 0.7938, 0.7993, 0.7809,\n",
      "        0.8123, 0.8256, 0.8455, 0.8046, 0.7820, 0.8063, 0.8309, 0.8275, 0.8007,\n",
      "        0.8294, 0.8075, 0.8111, 0.8669, 0.8475, 0.8570, 0.8642, 0.8582, 0.8308,\n",
      "        0.8366, 0.8591, 0.8390, 0.8623, 0.8199, 0.7698, 0.8403, 0.8049, 0.8409,\n",
      "        0.8348, 0.8375, 0.8017, 0.8342, 0.8623, 0.8442, 0.8037, 0.8689, 0.8461,\n",
      "        0.8328], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4554, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0559, Val-Loss: 0.0195\n",
      "Epoch [20/700], Train-Loss: 0.0097, Val-Loss: 0.0033\n",
      "Epoch [30/700], Train-Loss: 0.0005, Val-Loss: 0.0006\n",
      "Epoch [40/700], Train-Loss: 0.0003, Val-Loss: 0.0008\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0004, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.2671e-05, 1.2242e-04, 7.3331e-05,  ..., 7.4293e-05, 5.6690e-05,\n",
      "         9.1798e-05],\n",
      "        [8.7883e-05, 5.9838e-05, 1.1558e-04,  ..., 1.7707e-04, 1.8682e-04,\n",
      "         1.5448e-04],\n",
      "        [3.9131e-04, 5.5965e-04, 1.6758e-04,  ..., 2.0822e-04, 1.6169e-04,\n",
      "         3.1079e-04],\n",
      "        ...,\n",
      "        [1.0008e-04, 7.2365e-05, 1.4835e-04,  ..., 6.9848e-05, 1.0573e-04,\n",
      "         8.4133e-05],\n",
      "        [9.9553e-05, 9.3472e-05, 2.7783e-05,  ..., 2.2354e-04, 1.2204e-04,\n",
      "         5.0913e-05],\n",
      "        [1.5575e-04, 2.7984e-04, 8.1942e-05,  ..., 1.8017e-04, 1.4155e-04,\n",
      "         1.5928e-04]], device='cuda:0')\n",
      "pred: tensor([0.8446, 0.8167, 0.7952, 0.8114, 0.7849, 0.8230, 0.8172, 0.8126, 0.7882,\n",
      "        0.8211, 0.7950, 0.8252, 0.7956, 0.8068, 0.8309, 0.8079, 0.8264, 0.8311,\n",
      "        0.7874, 0.7951, 0.8285, 0.8373, 0.8663, 0.8572, 0.8785, 0.8084, 0.8678,\n",
      "        0.8032, 0.8463, 0.8504, 0.8502, 0.8805, 0.8651, 0.8671, 0.8660, 0.8691,\n",
      "        0.8346, 0.8413, 0.8356, 0.8350, 0.8416, 0.8755, 0.7999, 0.8733, 0.8298,\n",
      "        0.7957, 0.8339, 0.8100, 0.8323, 0.8242, 0.7653, 0.7839, 0.7454, 0.7781,\n",
      "        0.7862, 0.7700, 0.8254, 0.8380, 0.7564, 0.8458, 0.7931, 0.8006, 0.7793,\n",
      "        0.8122, 0.8254, 0.8454, 0.8049, 0.7827, 0.8054, 0.8306, 0.8270, 0.8013,\n",
      "        0.8297, 0.8079, 0.8111, 0.8663, 0.8479, 0.8570, 0.8645, 0.8582, 0.8305,\n",
      "        0.8367, 0.8596, 0.8386, 0.8624, 0.8208, 0.7704, 0.8404, 0.8055, 0.8419,\n",
      "        0.8344, 0.8379, 0.8019, 0.8341, 0.8631, 0.8453, 0.8035, 0.8684, 0.8455,\n",
      "        0.8338], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8446, 0.8167, 0.7952, 0.8114, 0.7849, 0.8230, 0.8172, 0.8126, 0.7882,\n",
      "        0.8211, 0.7950, 0.8252, 0.7956, 0.8068, 0.8309, 0.8079, 0.8264, 0.8311,\n",
      "        0.7874, 0.7951, 0.8285, 0.8373, 0.8663, 0.8572, 0.8785, 0.8084, 0.8678,\n",
      "        0.8032, 0.8463, 0.8504, 0.8502, 0.8805, 0.8651, 0.8671, 0.8660, 0.8691,\n",
      "        0.8346, 0.8413, 0.8356, 0.8350, 0.8416, 0.8755, 0.7999, 0.8733, 0.8298,\n",
      "        0.7957, 0.8339, 0.8100, 0.8323, 0.8242, 0.7653, 0.7839, 0.7454, 0.7781,\n",
      "        0.7862, 0.7700, 0.8254, 0.8380, 0.7564, 0.8458, 0.7931, 0.8006, 0.7793,\n",
      "        0.8122, 0.8254, 0.8454, 0.8049, 0.7827, 0.8054, 0.8306, 0.8270, 0.8013,\n",
      "        0.8297, 0.8079, 0.8111, 0.8663, 0.8479, 0.8570, 0.8645, 0.8582, 0.8305,\n",
      "        0.8367, 0.8596, 0.8386, 0.8624, 0.8208, 0.7704, 0.8404, 0.8055, 0.8419,\n",
      "        0.8344, 0.8379, 0.8019, 0.8341, 0.8631, 0.8453, 0.8035, 0.8684, 0.8455,\n",
      "        0.8338], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4542, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1064, Val-Loss: 0.0649\n",
      "Epoch [20/700], Train-Loss: 0.0294, Val-Loss: 0.0218\n",
      "Epoch [30/700], Train-Loss: 0.0099, Val-Loss: 0.0055\n",
      "Epoch [40/700], Train-Loss: 0.0032, Val-Loss: 0.0019\n",
      "Epoch [50/700], Train-Loss: 0.0007, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0399e-04, 6.9778e-05, 7.7126e-05,  ..., 1.4529e-04, 1.9167e-04,\n",
      "         1.2392e-04],\n",
      "        [1.5072e-04, 1.2528e-04, 1.0852e-04,  ..., 9.8431e-05, 1.4666e-04,\n",
      "         1.4412e-04],\n",
      "        [2.0161e-04, 1.8552e-04, 2.5217e-04,  ..., 1.1041e-04, 9.2022e-04,\n",
      "         2.2020e-04],\n",
      "        ...,\n",
      "        [1.3083e-04, 8.2358e-05, 1.1578e-04,  ..., 1.0397e-04, 1.3477e-04,\n",
      "         1.8008e-04],\n",
      "        [1.6022e-04, 8.1273e-05, 1.1009e-04,  ..., 7.7348e-05, 1.4007e-04,\n",
      "         1.2425e-04],\n",
      "        [2.8519e-04, 1.7667e-04, 2.3549e-04,  ..., 2.7001e-04, 1.6652e-04,\n",
      "         1.2839e-04]], device='cuda:0')\n",
      "pred: tensor([0.8453, 0.8182, 0.7952, 0.8113, 0.7848, 0.8243, 0.8175, 0.8124, 0.7875,\n",
      "        0.8216, 0.7950, 0.8254, 0.7960, 0.8075, 0.8313, 0.8067, 0.8266, 0.8305,\n",
      "        0.7882, 0.7955, 0.8287, 0.8361, 0.8665, 0.8573, 0.8786, 0.8075, 0.8678,\n",
      "        0.8034, 0.8458, 0.8505, 0.8506, 0.8814, 0.8647, 0.8665, 0.8659, 0.8687,\n",
      "        0.8351, 0.8419, 0.8356, 0.8359, 0.8417, 0.8759, 0.7997, 0.8741, 0.8311,\n",
      "        0.7968, 0.8344, 0.8089, 0.8335, 0.8243, 0.7657, 0.7831, 0.7454, 0.7786,\n",
      "        0.7866, 0.7695, 0.8253, 0.8381, 0.7576, 0.8457, 0.7933, 0.8003, 0.7800,\n",
      "        0.8123, 0.8255, 0.8457, 0.8045, 0.7826, 0.8059, 0.8304, 0.8283, 0.8013,\n",
      "        0.8297, 0.8081, 0.8104, 0.8667, 0.8483, 0.8579, 0.8647, 0.8589, 0.8304,\n",
      "        0.8367, 0.8597, 0.8385, 0.8623, 0.8206, 0.7707, 0.8403, 0.8055, 0.8423,\n",
      "        0.8355, 0.8382, 0.8004, 0.8342, 0.8624, 0.8450, 0.8038, 0.8688, 0.8457,\n",
      "        0.8338], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8453, 0.8182, 0.7952, 0.8113, 0.7848, 0.8243, 0.8175, 0.8124, 0.7875,\n",
      "        0.8216, 0.7950, 0.8254, 0.7960, 0.8075, 0.8313, 0.8067, 0.8266, 0.8305,\n",
      "        0.7882, 0.7955, 0.8287, 0.8361, 0.8665, 0.8573, 0.8786, 0.8075, 0.8678,\n",
      "        0.8034, 0.8458, 0.8505, 0.8506, 0.8814, 0.8647, 0.8665, 0.8659, 0.8687,\n",
      "        0.8351, 0.8419, 0.8356, 0.8359, 0.8417, 0.8759, 0.7997, 0.8741, 0.8311,\n",
      "        0.7968, 0.8344, 0.8089, 0.8335, 0.8243, 0.7657, 0.7831, 0.7454, 0.7786,\n",
      "        0.7866, 0.7695, 0.8253, 0.8381, 0.7576, 0.8457, 0.7933, 0.8003, 0.7800,\n",
      "        0.8123, 0.8255, 0.8457, 0.8045, 0.7826, 0.8059, 0.8304, 0.8283, 0.8013,\n",
      "        0.8297, 0.8081, 0.8104, 0.8667, 0.8483, 0.8579, 0.8647, 0.8589, 0.8304,\n",
      "        0.8367, 0.8597, 0.8385, 0.8623, 0.8206, 0.7707, 0.8403, 0.8055, 0.8423,\n",
      "        0.8355, 0.8382, 0.8004, 0.8342, 0.8624, 0.8450, 0.8038, 0.8688, 0.8457,\n",
      "        0.8338], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4580, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1308, Val-Loss: 0.0946\n",
      "Epoch [20/700], Train-Loss: 0.0377, Val-Loss: 0.0333\n",
      "Epoch [30/700], Train-Loss: 0.0133, Val-Loss: 0.0113\n",
      "Epoch [40/700], Train-Loss: 0.0050, Val-Loss: 0.0047\n",
      "Epoch [50/700], Train-Loss: 0.0019, Val-Loss: 0.0013\n",
      "Epoch [60/700], Train-Loss: 0.0007, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[6.5752e-05, 6.2433e-05, 4.2760e-05,  ..., 9.6683e-05, 6.1611e-04,\n",
      "         6.8910e-04],\n",
      "        [1.6062e-04, 1.8433e-04, 1.5495e-04,  ..., 8.7616e-05, 1.4646e-04,\n",
      "         7.8308e-04],\n",
      "        [1.3224e-04, 2.4203e-04, 5.7142e-05,  ..., 1.0875e-04, 2.5014e-04,\n",
      "         1.0675e-03],\n",
      "        ...,\n",
      "        [1.2814e-04, 9.2756e-05, 9.3897e-05,  ..., 1.4346e-04, 1.1603e-04,\n",
      "         1.3772e-04],\n",
      "        [7.4217e-05, 5.3437e-05, 3.1779e-05,  ..., 7.6424e-05, 8.5165e-05,\n",
      "         1.1697e-04],\n",
      "        [3.1863e-05, 4.7194e-05, 9.8388e-05,  ..., 1.8884e-05, 5.5960e-05,\n",
      "         2.3180e-04]], device='cuda:0')\n",
      "pred: tensor([0.8451, 0.8182, 0.7949, 0.8114, 0.7844, 0.8240, 0.8171, 0.8125, 0.7879,\n",
      "        0.8211, 0.7950, 0.8244, 0.7963, 0.8073, 0.8309, 0.8076, 0.8270, 0.8309,\n",
      "        0.7873, 0.7957, 0.8281, 0.8367, 0.8659, 0.8570, 0.8786, 0.8080, 0.8684,\n",
      "        0.8030, 0.8456, 0.8504, 0.8513, 0.8813, 0.8641, 0.8665, 0.8659, 0.8688,\n",
      "        0.8353, 0.8425, 0.8356, 0.8348, 0.8418, 0.8761, 0.7998, 0.8739, 0.8303,\n",
      "        0.7971, 0.8346, 0.8102, 0.8332, 0.8239, 0.7657, 0.7835, 0.7458, 0.7789,\n",
      "        0.7861, 0.7705, 0.8255, 0.8377, 0.7561, 0.8452, 0.7945, 0.8004, 0.7805,\n",
      "        0.8119, 0.8260, 0.8454, 0.8049, 0.7826, 0.8063, 0.8314, 0.8275, 0.8015,\n",
      "        0.8295, 0.8081, 0.8115, 0.8672, 0.8480, 0.8577, 0.8647, 0.8581, 0.8305,\n",
      "        0.8367, 0.8592, 0.8386, 0.8621, 0.8196, 0.7703, 0.8398, 0.8049, 0.8417,\n",
      "        0.8352, 0.8380, 0.8007, 0.8336, 0.8618, 0.8445, 0.8045, 0.8689, 0.8459,\n",
      "        0.8332], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8451, 0.8182, 0.7949, 0.8114, 0.7844, 0.8240, 0.8171, 0.8125, 0.7879,\n",
      "        0.8211, 0.7950, 0.8244, 0.7963, 0.8073, 0.8309, 0.8076, 0.8270, 0.8309,\n",
      "        0.7873, 0.7957, 0.8281, 0.8367, 0.8659, 0.8570, 0.8786, 0.8080, 0.8684,\n",
      "        0.8030, 0.8456, 0.8504, 0.8513, 0.8813, 0.8641, 0.8665, 0.8659, 0.8688,\n",
      "        0.8353, 0.8425, 0.8356, 0.8348, 0.8418, 0.8761, 0.7998, 0.8739, 0.8303,\n",
      "        0.7971, 0.8346, 0.8102, 0.8332, 0.8239, 0.7657, 0.7835, 0.7458, 0.7789,\n",
      "        0.7861, 0.7705, 0.8255, 0.8377, 0.7561, 0.8452, 0.7945, 0.8004, 0.7805,\n",
      "        0.8119, 0.8260, 0.8454, 0.8049, 0.7826, 0.8063, 0.8314, 0.8275, 0.8015,\n",
      "        0.8295, 0.8081, 0.8115, 0.8672, 0.8480, 0.8577, 0.8647, 0.8581, 0.8305,\n",
      "        0.8367, 0.8592, 0.8386, 0.8621, 0.8196, 0.7703, 0.8398, 0.8049, 0.8417,\n",
      "        0.8352, 0.8380, 0.8007, 0.8336, 0.8618, 0.8445, 0.8045, 0.8689, 0.8459,\n",
      "        0.8332], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4498, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0643, Val-Loss: 0.0300\n",
      "Epoch [20/700], Train-Loss: 0.0103, Val-Loss: 0.0045\n",
      "Epoch [30/700], Train-Loss: 0.0015, Val-Loss: 0.0003\n",
      "Epoch [40/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0002, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.6756e-04, 1.7607e-04, 1.8609e-04,  ..., 9.6443e-05, 9.8552e-05,\n",
      "         9.9408e-05],\n",
      "        [1.4189e-04, 1.8278e-04, 1.4433e-04,  ..., 1.2162e-04, 7.8145e-05,\n",
      "         2.3431e-04],\n",
      "        [2.9181e-04, 2.4005e-04, 1.4129e-04,  ..., 2.0816e-05, 1.6480e-05,\n",
      "         2.0755e-04],\n",
      "        ...,\n",
      "        [2.0657e-04, 2.9203e-04, 3.2444e-04,  ..., 9.1303e-05, 1.5656e-04,\n",
      "         1.3841e-04],\n",
      "        [1.7788e-04, 1.4662e-04, 2.0026e-04,  ..., 1.1029e-04, 1.1988e-04,\n",
      "         3.6244e-04],\n",
      "        [4.2711e-04, 2.5965e-04, 1.8545e-04,  ..., 2.2916e-04, 9.5048e-05,\n",
      "         2.5550e-04]], device='cuda:0')\n",
      "pred: tensor([0.8450, 0.8180, 0.7955, 0.8115, 0.7852, 0.8245, 0.8183, 0.8128, 0.7872,\n",
      "        0.8205, 0.7949, 0.8259, 0.7966, 0.8076, 0.8311, 0.8073, 0.8263, 0.8310,\n",
      "        0.7877, 0.7949, 0.8286, 0.8366, 0.8666, 0.8576, 0.8787, 0.8082, 0.8682,\n",
      "        0.8034, 0.8464, 0.8505, 0.8512, 0.8815, 0.8645, 0.8671, 0.8663, 0.8693,\n",
      "        0.8350, 0.8413, 0.8358, 0.8359, 0.8418, 0.8761, 0.7983, 0.8739, 0.8306,\n",
      "        0.7961, 0.8343, 0.8098, 0.8327, 0.8243, 0.7652, 0.7833, 0.7459, 0.7791,\n",
      "        0.7856, 0.7705, 0.8254, 0.8375, 0.7566, 0.8461, 0.7934, 0.7994, 0.7806,\n",
      "        0.8122, 0.8260, 0.8459, 0.8048, 0.7829, 0.8058, 0.8312, 0.8283, 0.8010,\n",
      "        0.8299, 0.8084, 0.8108, 0.8666, 0.8478, 0.8571, 0.8641, 0.8587, 0.8310,\n",
      "        0.8365, 0.8589, 0.8386, 0.8619, 0.8192, 0.7694, 0.8406, 0.8050, 0.8418,\n",
      "        0.8347, 0.8380, 0.8017, 0.8334, 0.8626, 0.8445, 0.8040, 0.8690, 0.8452,\n",
      "        0.8331], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8450, 0.8180, 0.7955, 0.8115, 0.7852, 0.8245, 0.8183, 0.8128, 0.7872,\n",
      "        0.8205, 0.7949, 0.8259, 0.7966, 0.8076, 0.8311, 0.8073, 0.8263, 0.8310,\n",
      "        0.7877, 0.7949, 0.8286, 0.8366, 0.8666, 0.8576, 0.8787, 0.8082, 0.8682,\n",
      "        0.8034, 0.8464, 0.8505, 0.8512, 0.8815, 0.8645, 0.8671, 0.8663, 0.8693,\n",
      "        0.8350, 0.8413, 0.8358, 0.8359, 0.8418, 0.8761, 0.7983, 0.8739, 0.8306,\n",
      "        0.7961, 0.8343, 0.8098, 0.8327, 0.8243, 0.7652, 0.7833, 0.7459, 0.7791,\n",
      "        0.7856, 0.7705, 0.8254, 0.8375, 0.7566, 0.8461, 0.7934, 0.7994, 0.7806,\n",
      "        0.8122, 0.8260, 0.8459, 0.8048, 0.7829, 0.8058, 0.8312, 0.8283, 0.8010,\n",
      "        0.8299, 0.8084, 0.8108, 0.8666, 0.8478, 0.8571, 0.8641, 0.8587, 0.8310,\n",
      "        0.8365, 0.8589, 0.8386, 0.8619, 0.8192, 0.7694, 0.8406, 0.8050, 0.8418,\n",
      "        0.8347, 0.8380, 0.8017, 0.8334, 0.8626, 0.8445, 0.8040, 0.8690, 0.8452,\n",
      "        0.8331], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4554, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0519, Val-Loss: 0.0195\n",
      "Epoch [20/700], Train-Loss: 0.0133, Val-Loss: 0.0068\n",
      "Epoch [30/700], Train-Loss: 0.0029, Val-Loss: 0.0007\n",
      "Epoch [40/700], Train-Loss: 0.0007, Val-Loss: 0.0002\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.1544e-05, 1.0362e-04, 8.0644e-05,  ..., 8.5663e-05, 9.8137e-05,\n",
      "         6.2374e-04],\n",
      "        [3.0205e-04, 2.9678e-04, 4.3386e-04,  ..., 9.7652e-05, 6.6601e-05,\n",
      "         2.3659e-03],\n",
      "        [8.9177e-05, 1.3298e-04, 9.8451e-05,  ..., 1.2841e-04, 7.8605e-05,\n",
      "         8.0947e-04],\n",
      "        ...,\n",
      "        [9.6870e-05, 8.2513e-05, 1.5951e-04,  ..., 4.0087e-04, 3.3120e-04,\n",
      "         1.2191e-03],\n",
      "        [5.4832e-05, 2.0702e-04, 1.3314e-04,  ..., 8.4719e-05, 3.7672e-05,\n",
      "         1.4274e-03],\n",
      "        [1.2050e-04, 1.0172e-04, 2.3194e-04,  ..., 8.3179e-05, 6.0059e-05,\n",
      "         1.1373e-03]], device='cuda:0')\n",
      "pred: tensor([0.8450, 0.8175, 0.7959, 0.8120, 0.7847, 0.8233, 0.8179, 0.8123, 0.7876,\n",
      "        0.8207, 0.7958, 0.8255, 0.7973, 0.8074, 0.8313, 0.8078, 0.8265, 0.8306,\n",
      "        0.7876, 0.7943, 0.8282, 0.8368, 0.8662, 0.8576, 0.8788, 0.8081, 0.8682,\n",
      "        0.8025, 0.8463, 0.8505, 0.8506, 0.8819, 0.8646, 0.8670, 0.8663, 0.8690,\n",
      "        0.8347, 0.8416, 0.8353, 0.8355, 0.8424, 0.8757, 0.8001, 0.8738, 0.8298,\n",
      "        0.7961, 0.8343, 0.8107, 0.8327, 0.8251, 0.7646, 0.7829, 0.7448, 0.7791,\n",
      "        0.7867, 0.7698, 0.8248, 0.8374, 0.7567, 0.8456, 0.7938, 0.7996, 0.7803,\n",
      "        0.8128, 0.8259, 0.8458, 0.8046, 0.7823, 0.8063, 0.8306, 0.8270, 0.8010,\n",
      "        0.8300, 0.8081, 0.8109, 0.8672, 0.8479, 0.8571, 0.8640, 0.8580, 0.8308,\n",
      "        0.8365, 0.8592, 0.8384, 0.8627, 0.8198, 0.7703, 0.8401, 0.8053, 0.8415,\n",
      "        0.8346, 0.8388, 0.8022, 0.8340, 0.8627, 0.8453, 0.8042, 0.8686, 0.8455,\n",
      "        0.8336], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8450, 0.8175, 0.7959, 0.8120, 0.7847, 0.8233, 0.8179, 0.8123, 0.7876,\n",
      "        0.8207, 0.7958, 0.8255, 0.7973, 0.8074, 0.8313, 0.8078, 0.8265, 0.8306,\n",
      "        0.7876, 0.7943, 0.8282, 0.8368, 0.8662, 0.8576, 0.8788, 0.8081, 0.8682,\n",
      "        0.8025, 0.8463, 0.8505, 0.8506, 0.8819, 0.8646, 0.8670, 0.8663, 0.8690,\n",
      "        0.8347, 0.8416, 0.8353, 0.8355, 0.8424, 0.8757, 0.8001, 0.8738, 0.8298,\n",
      "        0.7961, 0.8343, 0.8107, 0.8327, 0.8251, 0.7646, 0.7829, 0.7448, 0.7791,\n",
      "        0.7867, 0.7698, 0.8248, 0.8374, 0.7567, 0.8456, 0.7938, 0.7996, 0.7803,\n",
      "        0.8128, 0.8259, 0.8458, 0.8046, 0.7823, 0.8063, 0.8306, 0.8270, 0.8010,\n",
      "        0.8300, 0.8081, 0.8109, 0.8672, 0.8479, 0.8571, 0.8640, 0.8580, 0.8308,\n",
      "        0.8365, 0.8592, 0.8384, 0.8627, 0.8198, 0.7703, 0.8401, 0.8053, 0.8415,\n",
      "        0.8346, 0.8388, 0.8022, 0.8340, 0.8627, 0.8453, 0.8042, 0.8686, 0.8455,\n",
      "        0.8336], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4588, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0963, Val-Loss: 0.0601\n",
      "Epoch [20/700], Train-Loss: 0.0269, Val-Loss: 0.0206\n",
      "Epoch [30/700], Train-Loss: 0.0094, Val-Loss: 0.0057\n",
      "Epoch [40/700], Train-Loss: 0.0032, Val-Loss: 0.0021\n",
      "Epoch [50/700], Train-Loss: 0.0008, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.4890e-04, 1.0325e-04, 1.3349e-04,  ..., 6.9762e-05, 8.2765e-04,\n",
      "         1.0557e-04],\n",
      "        [8.2466e-05, 9.3001e-05, 7.7789e-05,  ..., 8.7798e-05, 8.1897e-04,\n",
      "         1.0279e-04],\n",
      "        [7.3133e-05, 5.9779e-05, 7.6025e-05,  ..., 3.8429e-04, 5.0232e-02,\n",
      "         1.5246e-04],\n",
      "        ...,\n",
      "        [9.6277e-05, 1.2663e-04, 1.3268e-04,  ..., 1.1145e-04, 4.8023e-03,\n",
      "         9.3659e-05],\n",
      "        [1.3721e-04, 6.4822e-05, 3.2170e-04,  ..., 6.5355e-05, 1.7636e-03,\n",
      "         7.6809e-05],\n",
      "        [8.5063e-05, 1.6679e-04, 1.1467e-04,  ..., 4.9640e-05, 2.3074e-03,\n",
      "         6.9348e-05]], device='cuda:0')\n",
      "pred: tensor([0.8455, 0.8177, 0.7959, 0.8118, 0.7852, 0.8239, 0.8174, 0.8123, 0.7874,\n",
      "        0.8212, 0.7958, 0.8262, 0.7962, 0.8070, 0.8318, 0.8071, 0.8266, 0.8313,\n",
      "        0.7873, 0.7952, 0.8280, 0.8358, 0.8661, 0.8578, 0.8786, 0.8079, 0.8680,\n",
      "        0.8025, 0.8463, 0.8506, 0.8505, 0.8811, 0.8649, 0.8664, 0.8659, 0.8689,\n",
      "        0.8348, 0.8415, 0.8356, 0.8346, 0.8417, 0.8755, 0.8004, 0.8735, 0.8305,\n",
      "        0.7965, 0.8349, 0.8099, 0.8331, 0.8248, 0.7649, 0.7826, 0.7444, 0.7785,\n",
      "        0.7871, 0.7696, 0.8257, 0.8383, 0.7570, 0.8465, 0.7942, 0.8005, 0.7811,\n",
      "        0.8121, 0.8261, 0.8461, 0.8048, 0.7821, 0.8049, 0.8305, 0.8282, 0.8009,\n",
      "        0.8306, 0.8083, 0.8103, 0.8664, 0.8485, 0.8575, 0.8648, 0.8591, 0.8304,\n",
      "        0.8363, 0.8588, 0.8388, 0.8616, 0.8201, 0.7706, 0.8403, 0.8052, 0.8422,\n",
      "        0.8350, 0.8388, 0.8012, 0.8339, 0.8624, 0.8458, 0.8040, 0.8684, 0.8461,\n",
      "        0.8336], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8455, 0.8177, 0.7959, 0.8118, 0.7852, 0.8239, 0.8174, 0.8123, 0.7874,\n",
      "        0.8212, 0.7958, 0.8262, 0.7962, 0.8070, 0.8318, 0.8071, 0.8266, 0.8313,\n",
      "        0.7873, 0.7952, 0.8280, 0.8358, 0.8661, 0.8578, 0.8786, 0.8079, 0.8680,\n",
      "        0.8025, 0.8463, 0.8506, 0.8505, 0.8811, 0.8649, 0.8664, 0.8659, 0.8689,\n",
      "        0.8348, 0.8415, 0.8356, 0.8346, 0.8417, 0.8755, 0.8004, 0.8735, 0.8305,\n",
      "        0.7965, 0.8349, 0.8099, 0.8331, 0.8248, 0.7649, 0.7826, 0.7444, 0.7785,\n",
      "        0.7871, 0.7696, 0.8257, 0.8383, 0.7570, 0.8465, 0.7942, 0.8005, 0.7811,\n",
      "        0.8121, 0.8261, 0.8461, 0.8048, 0.7821, 0.8049, 0.8305, 0.8282, 0.8009,\n",
      "        0.8306, 0.8083, 0.8103, 0.8664, 0.8485, 0.8575, 0.8648, 0.8591, 0.8304,\n",
      "        0.8363, 0.8588, 0.8388, 0.8616, 0.8201, 0.7706, 0.8403, 0.8052, 0.8422,\n",
      "        0.8350, 0.8388, 0.8012, 0.8339, 0.8624, 0.8458, 0.8040, 0.8684, 0.8461,\n",
      "        0.8336], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4537, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1177, Val-Loss: 0.0695\n",
      "Epoch [20/700], Train-Loss: 0.0323, Val-Loss: 0.0239\n",
      "Epoch [30/700], Train-Loss: 0.0104, Val-Loss: 0.0058\n",
      "Epoch [40/700], Train-Loss: 0.0034, Val-Loss: 0.0021\n",
      "Epoch [50/700], Train-Loss: 0.0008, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.0481e-05, 7.2118e-05, 7.6087e-05,  ..., 1.5216e-04, 1.0322e-04,\n",
      "         1.1010e-04],\n",
      "        [2.9990e-05, 2.9020e-05, 4.5722e-05,  ..., 1.3990e-04, 8.2025e-05,\n",
      "         8.0819e-05],\n",
      "        [2.5894e-04, 2.2964e-04, 4.7710e-04,  ..., 1.9980e-04, 7.1789e-05,\n",
      "         1.0735e-04],\n",
      "        ...,\n",
      "        [4.8968e-05, 7.1922e-05, 9.9063e-05,  ..., 9.0680e-05, 8.1210e-05,\n",
      "         1.7210e-04],\n",
      "        [6.1924e-05, 6.1390e-05, 1.1941e-04,  ..., 7.8139e-05, 7.1186e-05,\n",
      "         6.0636e-05],\n",
      "        [5.5050e-05, 6.3739e-05, 6.6008e-05,  ..., 8.3903e-05, 7.1012e-05,\n",
      "         7.0553e-05]], device='cuda:0')\n",
      "pred: tensor([0.8453, 0.8178, 0.7954, 0.8119, 0.7842, 0.8238, 0.8173, 0.8119, 0.7879,\n",
      "        0.8214, 0.7947, 0.8252, 0.7961, 0.8068, 0.8311, 0.8074, 0.8275, 0.8315,\n",
      "        0.7881, 0.7952, 0.8287, 0.8358, 0.8661, 0.8571, 0.8785, 0.8081, 0.8682,\n",
      "        0.8034, 0.8457, 0.8503, 0.8506, 0.8814, 0.8648, 0.8665, 0.8657, 0.8686,\n",
      "        0.8349, 0.8412, 0.8360, 0.8358, 0.8417, 0.8759, 0.8004, 0.8737, 0.8305,\n",
      "        0.7972, 0.8346, 0.8095, 0.8334, 0.8246, 0.7656, 0.7833, 0.7455, 0.7799,\n",
      "        0.7870, 0.7702, 0.8254, 0.8376, 0.7560, 0.8456, 0.7940, 0.8002, 0.7804,\n",
      "        0.8126, 0.8257, 0.8457, 0.8046, 0.7821, 0.8060, 0.8309, 0.8280, 0.8011,\n",
      "        0.8296, 0.8078, 0.8110, 0.8666, 0.8478, 0.8575, 0.8643, 0.8584, 0.8307,\n",
      "        0.8366, 0.8591, 0.8389, 0.8620, 0.8199, 0.7699, 0.8401, 0.8054, 0.8420,\n",
      "        0.8350, 0.8382, 0.8008, 0.8341, 0.8626, 0.8455, 0.8037, 0.8686, 0.8454,\n",
      "        0.8334], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8453, 0.8178, 0.7954, 0.8119, 0.7842, 0.8238, 0.8173, 0.8119, 0.7879,\n",
      "        0.8214, 0.7947, 0.8252, 0.7961, 0.8068, 0.8311, 0.8074, 0.8275, 0.8315,\n",
      "        0.7881, 0.7952, 0.8287, 0.8358, 0.8661, 0.8571, 0.8785, 0.8081, 0.8682,\n",
      "        0.8034, 0.8457, 0.8503, 0.8506, 0.8814, 0.8648, 0.8665, 0.8657, 0.8686,\n",
      "        0.8349, 0.8412, 0.8360, 0.8358, 0.8417, 0.8759, 0.8004, 0.8737, 0.8305,\n",
      "        0.7972, 0.8346, 0.8095, 0.8334, 0.8246, 0.7656, 0.7833, 0.7455, 0.7799,\n",
      "        0.7870, 0.7702, 0.8254, 0.8376, 0.7560, 0.8456, 0.7940, 0.8002, 0.7804,\n",
      "        0.8126, 0.8257, 0.8457, 0.8046, 0.7821, 0.8060, 0.8309, 0.8280, 0.8011,\n",
      "        0.8296, 0.8078, 0.8110, 0.8666, 0.8478, 0.8575, 0.8643, 0.8584, 0.8307,\n",
      "        0.8366, 0.8591, 0.8389, 0.8620, 0.8199, 0.7699, 0.8401, 0.8054, 0.8420,\n",
      "        0.8350, 0.8382, 0.8008, 0.8341, 0.8626, 0.8455, 0.8037, 0.8686, 0.8454,\n",
      "        0.8334], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4568, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1015, Val-Loss: 0.0978\n",
      "Epoch [20/700], Train-Loss: 0.0278, Val-Loss: 0.0284\n",
      "Epoch [30/700], Train-Loss: 0.0079, Val-Loss: 0.0087\n",
      "Epoch [40/700], Train-Loss: 0.0029, Val-Loss: 0.0037\n",
      "Epoch [50/700], Train-Loss: 0.0011, Val-Loss: 0.0012\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [100/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [110/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0001, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.6286e-04, 2.0920e-04, 4.2846e-04,  ..., 1.3546e-04, 8.5323e-05,\n",
      "         1.1664e-04],\n",
      "        [7.9731e-05, 1.3545e-04, 6.9440e-05,  ..., 1.3214e-04, 1.0955e-04,\n",
      "         8.0665e-05],\n",
      "        [1.2634e-04, 8.5742e-05, 1.3077e-04,  ..., 1.1911e-04, 2.1095e-04,\n",
      "         1.3637e-04],\n",
      "        ...,\n",
      "        [1.2788e-04, 1.5253e-04, 1.1691e-04,  ..., 1.7564e-04, 1.1430e-04,\n",
      "         1.8680e-04],\n",
      "        [7.0528e-05, 1.3049e-04, 7.4324e-05,  ..., 2.1367e-05, 5.7063e-05,\n",
      "         1.9363e-05],\n",
      "        [6.5743e-05, 7.8219e-05, 1.2194e-04,  ..., 8.0033e-05, 3.0805e-05,\n",
      "         6.3922e-05]], device='cuda:0')\n",
      "pred: tensor([0.8447, 0.8177, 0.7953, 0.8111, 0.7843, 0.8246, 0.8173, 0.8123, 0.7876,\n",
      "        0.8214, 0.7952, 0.8257, 0.7958, 0.8075, 0.8311, 0.8075, 0.8263, 0.8316,\n",
      "        0.7869, 0.7946, 0.8284, 0.8366, 0.8661, 0.8574, 0.8786, 0.8081, 0.8679,\n",
      "        0.8032, 0.8465, 0.8502, 0.8509, 0.8810, 0.8639, 0.8665, 0.8661, 0.8690,\n",
      "        0.8353, 0.8419, 0.8353, 0.8356, 0.8418, 0.8759, 0.7990, 0.8735, 0.8299,\n",
      "        0.7965, 0.8347, 0.8101, 0.8324, 0.8244, 0.7655, 0.7829, 0.7450, 0.7788,\n",
      "        0.7860, 0.7701, 0.8249, 0.8378, 0.7557, 0.8456, 0.7940, 0.8004, 0.7802,\n",
      "        0.8124, 0.8262, 0.8457, 0.8042, 0.7830, 0.8057, 0.8309, 0.8278, 0.8013,\n",
      "        0.8292, 0.8081, 0.8112, 0.8666, 0.8481, 0.8573, 0.8645, 0.8587, 0.8305,\n",
      "        0.8368, 0.8593, 0.8387, 0.8620, 0.8201, 0.7705, 0.8403, 0.8048, 0.8418,\n",
      "        0.8350, 0.8382, 0.8010, 0.8339, 0.8619, 0.8453, 0.8039, 0.8690, 0.8459,\n",
      "        0.8333], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8447, 0.8177, 0.7953, 0.8111, 0.7843, 0.8246, 0.8173, 0.8123, 0.7876,\n",
      "        0.8214, 0.7952, 0.8257, 0.7958, 0.8075, 0.8311, 0.8075, 0.8263, 0.8316,\n",
      "        0.7869, 0.7946, 0.8284, 0.8366, 0.8661, 0.8574, 0.8786, 0.8081, 0.8679,\n",
      "        0.8032, 0.8465, 0.8502, 0.8509, 0.8810, 0.8639, 0.8665, 0.8661, 0.8690,\n",
      "        0.8353, 0.8419, 0.8353, 0.8356, 0.8418, 0.8759, 0.7990, 0.8735, 0.8299,\n",
      "        0.7965, 0.8347, 0.8101, 0.8324, 0.8244, 0.7655, 0.7829, 0.7450, 0.7788,\n",
      "        0.7860, 0.7701, 0.8249, 0.8378, 0.7557, 0.8456, 0.7940, 0.8004, 0.7802,\n",
      "        0.8124, 0.8262, 0.8457, 0.8042, 0.7830, 0.8057, 0.8309, 0.8278, 0.8013,\n",
      "        0.8292, 0.8081, 0.8112, 0.8666, 0.8481, 0.8573, 0.8645, 0.8587, 0.8305,\n",
      "        0.8368, 0.8593, 0.8387, 0.8620, 0.8201, 0.7705, 0.8403, 0.8048, 0.8418,\n",
      "        0.8350, 0.8382, 0.8010, 0.8339, 0.8619, 0.8453, 0.8039, 0.8690, 0.8459,\n",
      "        0.8333], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.4525, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'imagenet1k': [tensor(0.0045),\n",
       "  tensor(0.0046),\n",
       "  tensor(0.0045),\n",
       "  tensor(0.0046),\n",
       "  tensor(0.0045),\n",
       "  tensor(0.0046),\n",
       "  tensor(0.0046),\n",
       "  tensor(0.0045),\n",
       "  tensor(0.0046),\n",
       "  tensor(0.0045)]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = {'imagenet1k'}\n",
    "\n",
    "get_average_absolute_error_with_model_class(model_confidences_tensor=timm_model_confidence_for_val_samples_tensor[list(top_disagreement_indices)],\n",
    "                                            model_catalog=timm_model_catalog,\n",
    "                                            evaluation_model_class=MLPRegressor,\n",
    "                                            number_bootstraping_steps=10,\n",
    "                                            sample_size=100,\n",
    "                                            for_datasets=all_datasets,\n",
    "                                            k_fold_splits=2,\n",
    "                                            model_init_kwargs={\n",
    "                                                'hidden_channels': [128, 128, 1],\n",
    "                                            },\n",
    "                                            model_fitting_kwargs={\n",
    "                                                'n_epochs': 700,\n",
    "                                                'lr': 0.001,\n",
    "                                            },\n",
    "                                            train_model_index=train_model_indices,\n",
    "                                            val_model_index=val_model_indices,\n",
    "                                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4024989/1717516195.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  timm_model_confidence_for_main_set_tensor = torch.load(f=os.path.join(path_to_save_folder, \"timm_model_confidences_for_main_set.tensor\"))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_disagreement_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m disagreement_scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_disagreement_scores\u001b[49m(timm_model_confidence_for_main_set_tensor\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m), n_guiding_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m top_disagreement_indices_main_set \u001b[38;5;241m=\u001b[39m disagreement_scores\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m100\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_disagreement_scores' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0654, Val-Loss: 0.0185\n",
      "Epoch [20/700], Train-Loss: 0.0102, Val-Loss: 0.0020\n",
      "Epoch [30/700], Train-Loss: 0.0001, Val-Loss: 0.0010\n",
      "Epoch [40/700], Train-Loss: 0.0004, Val-Loss: 0.0014\n",
      "Epoch [50/700], Train-Loss: 0.0011, Val-Loss: 0.0015\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0004\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[4.0003e-05, 3.4070e-05, 3.8410e-04,  ..., 1.2945e-04, 1.0834e-04,\n",
      "         5.6772e-03],\n",
      "        [1.3613e-04, 2.5505e-04, 4.8400e-04,  ..., 3.2290e-05, 1.4255e-04,\n",
      "         1.4443e-04],\n",
      "        [6.0777e-05, 1.4441e-04, 4.7096e-04,  ..., 3.4035e-04, 3.4236e-03,\n",
      "         1.0091e-03],\n",
      "        ...,\n",
      "        [2.5596e-05, 2.1479e-05, 6.5103e-04,  ..., 4.3629e-04, 1.0797e-04,\n",
      "         1.8498e-04],\n",
      "        [7.3979e-05, 1.1292e-04, 6.0736e-04,  ..., 6.0448e-05, 3.0035e-05,\n",
      "         1.0755e-04],\n",
      "        [5.5532e-05, 1.1397e-04, 4.9701e-05,  ..., 1.4497e-04, 8.2222e-05,\n",
      "         7.5901e-03]], device='cuda:0')\n",
      "pred: tensor([0.8453, 0.8184, 0.7934, 0.8098, 0.7916, 0.8208, 0.8004, 0.8068, 0.7922,\n",
      "        0.8106, 0.7967, 0.8274, 0.7999, 0.8121, 0.8379, 0.8063, 0.8250, 0.8208,\n",
      "        0.7855, 0.7845, 0.8236, 0.8302, 0.8639, 0.8539, 0.8850, 0.8112, 0.8644,\n",
      "        0.8150, 0.8474, 0.8465, 0.8487, 0.8681, 0.8623, 0.8756, 0.8574, 0.8750,\n",
      "        0.8347, 0.8433, 0.8400, 0.8344, 0.8385, 0.8704, 0.8035, 0.8605, 0.8225,\n",
      "        0.7922, 0.8389, 0.8232, 0.8264, 0.8351, 0.7690, 0.7862, 0.7647, 0.7751,\n",
      "        0.7728, 0.7799, 0.8299, 0.8280, 0.7628, 0.8392, 0.8020, 0.8028, 0.7932,\n",
      "        0.8060, 0.8359, 0.8445, 0.7975, 0.7796, 0.8192, 0.8291, 0.8312, 0.8053,\n",
      "        0.8304, 0.8047, 0.7989, 0.8563, 0.8573, 0.8495, 0.8623, 0.8598, 0.8250,\n",
      "        0.8304, 0.8558, 0.8412, 0.8569, 0.8061, 0.7880, 0.8407, 0.7836, 0.8379,\n",
      "        0.8334, 0.8407, 0.8054, 0.8300, 0.8694, 0.8500, 0.8086, 0.8527, 0.8597,\n",
      "        0.8313], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8453, 0.8184, 0.7934, 0.8098, 0.7916, 0.8208, 0.8004, 0.8068, 0.7922,\n",
      "        0.8106, 0.7967, 0.8274, 0.7999, 0.8121, 0.8379, 0.8063, 0.8250, 0.8208,\n",
      "        0.7855, 0.7845, 0.8236, 0.8302, 0.8639, 0.8539, 0.8850, 0.8112, 0.8644,\n",
      "        0.8150, 0.8474, 0.8465, 0.8487, 0.8681, 0.8623, 0.8756, 0.8574, 0.8750,\n",
      "        0.8347, 0.8433, 0.8400, 0.8344, 0.8385, 0.8704, 0.8035, 0.8605, 0.8225,\n",
      "        0.7922, 0.8389, 0.8232, 0.8264, 0.8351, 0.7690, 0.7862, 0.7647, 0.7751,\n",
      "        0.7728, 0.7799, 0.8299, 0.8280, 0.7628, 0.8392, 0.8020, 0.8028, 0.7932,\n",
      "        0.8060, 0.8359, 0.8445, 0.7975, 0.7796, 0.8192, 0.8291, 0.8312, 0.8053,\n",
      "        0.8304, 0.8047, 0.7989, 0.8563, 0.8573, 0.8495, 0.8623, 0.8598, 0.8250,\n",
      "        0.8304, 0.8558, 0.8412, 0.8569, 0.8061, 0.7880, 0.8407, 0.7836, 0.8379,\n",
      "        0.8334, 0.8407, 0.8054, 0.8300, 0.8694, 0.8500, 0.8086, 0.8527, 0.8597,\n",
      "        0.8313], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5524, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0815, Val-Loss: 0.0376\n",
      "Epoch [20/700], Train-Loss: 0.0243, Val-Loss: 0.0153\n",
      "Epoch [30/700], Train-Loss: 0.0079, Val-Loss: 0.0038\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0014\n",
      "Epoch [50/700], Train-Loss: 0.0005, Val-Loss: 0.0001\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[8.2853e-05, 1.7964e-04, 3.6742e-04,  ..., 2.2532e-04, 2.1226e-04,\n",
      "         3.1608e-04],\n",
      "        [4.8426e-04, 2.1095e-04, 1.6788e-04,  ..., 5.7579e-05, 7.0928e-05,\n",
      "         8.3491e-05],\n",
      "        [2.4057e-04, 4.3214e-04, 6.3710e-04,  ..., 6.9584e-04, 2.9212e-04,\n",
      "         1.0211e-03],\n",
      "        ...,\n",
      "        [2.8214e-04, 5.3969e-05, 1.7190e-04,  ..., 6.7136e-04, 1.7950e-04,\n",
      "         1.8901e-04],\n",
      "        [1.8888e-05, 1.8356e-04, 1.2028e-05,  ..., 1.2339e-03, 2.1041e-04,\n",
      "         3.2388e-04],\n",
      "        [1.1653e-04, 1.8865e-04, 4.6469e-05,  ..., 1.6140e-04, 1.4213e-04,\n",
      "         9.1289e-04]], device='cuda:0')\n",
      "pred: tensor([0.8462, 0.8189, 0.7941, 0.8101, 0.7907, 0.8205, 0.8011, 0.8043, 0.7916,\n",
      "        0.8091, 0.7950, 0.8293, 0.7989, 0.8118, 0.8353, 0.8062, 0.8246, 0.8222,\n",
      "        0.7881, 0.7836, 0.8248, 0.8297, 0.8624, 0.8538, 0.8835, 0.8096, 0.8602,\n",
      "        0.8144, 0.8457, 0.8460, 0.8494, 0.8696, 0.8620, 0.8735, 0.8597, 0.8750,\n",
      "        0.8328, 0.8451, 0.8407, 0.8346, 0.8387, 0.8692, 0.8028, 0.8607, 0.8221,\n",
      "        0.7918, 0.8388, 0.8249, 0.8268, 0.8368, 0.7658, 0.7863, 0.7662, 0.7741,\n",
      "        0.7747, 0.7797, 0.8314, 0.8281, 0.7643, 0.8390, 0.7992, 0.8029, 0.7932,\n",
      "        0.8062, 0.8356, 0.8448, 0.7980, 0.7791, 0.8184, 0.8293, 0.8310, 0.8040,\n",
      "        0.8294, 0.8061, 0.7997, 0.8578, 0.8572, 0.8504, 0.8629, 0.8598, 0.8257,\n",
      "        0.8308, 0.8571, 0.8424, 0.8579, 0.8069, 0.7879, 0.8414, 0.7852, 0.8368,\n",
      "        0.8317, 0.8431, 0.8039, 0.8313, 0.8697, 0.8484, 0.8079, 0.8512, 0.8629,\n",
      "        0.8356], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8462, 0.8189, 0.7941, 0.8101, 0.7907, 0.8205, 0.8011, 0.8043, 0.7916,\n",
      "        0.8091, 0.7950, 0.8293, 0.7989, 0.8118, 0.8353, 0.8062, 0.8246, 0.8222,\n",
      "        0.7881, 0.7836, 0.8248, 0.8297, 0.8624, 0.8538, 0.8835, 0.8096, 0.8602,\n",
      "        0.8144, 0.8457, 0.8460, 0.8494, 0.8696, 0.8620, 0.8735, 0.8597, 0.8750,\n",
      "        0.8328, 0.8451, 0.8407, 0.8346, 0.8387, 0.8692, 0.8028, 0.8607, 0.8221,\n",
      "        0.7918, 0.8388, 0.8249, 0.8268, 0.8368, 0.7658, 0.7863, 0.7662, 0.7741,\n",
      "        0.7747, 0.7797, 0.8314, 0.8281, 0.7643, 0.8390, 0.7992, 0.8029, 0.7932,\n",
      "        0.8062, 0.8356, 0.8448, 0.7980, 0.7791, 0.8184, 0.8293, 0.8310, 0.8040,\n",
      "        0.8294, 0.8061, 0.7997, 0.8578, 0.8572, 0.8504, 0.8629, 0.8598, 0.8257,\n",
      "        0.8308, 0.8571, 0.8424, 0.8579, 0.8069, 0.7879, 0.8414, 0.7852, 0.8368,\n",
      "        0.8317, 0.8431, 0.8039, 0.8313, 0.8697, 0.8484, 0.8079, 0.8512, 0.8629,\n",
      "        0.8356], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5419, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1067, Val-Loss: 0.1307\n",
      "Epoch [20/700], Train-Loss: 0.0341, Val-Loss: 0.0386\n",
      "Epoch [30/700], Train-Loss: 0.0056, Val-Loss: 0.0090\n",
      "Epoch [40/700], Train-Loss: 0.0017, Val-Loss: 0.0032\n",
      "Epoch [50/700], Train-Loss: 0.0004, Val-Loss: 0.0009\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0005\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0002\n",
      "Epoch [90/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.0333e-04, 2.4949e-04, 1.1486e-04,  ..., 3.6737e-04, 2.3843e-04,\n",
      "         6.5284e-05],\n",
      "        [3.6620e-04, 4.5402e-04, 4.4260e-04,  ..., 5.6338e-05, 7.5149e-05,\n",
      "         2.0336e-04],\n",
      "        [2.6639e-04, 2.2046e-04, 1.2523e-04,  ..., 2.4156e-04, 1.0758e-04,\n",
      "         1.0482e-04],\n",
      "        ...,\n",
      "        [4.1306e-04, 1.4397e-04, 5.3280e-05,  ..., 2.2863e-04, 3.8104e-04,\n",
      "         2.6377e-05],\n",
      "        [2.3529e-04, 1.2062e-04, 1.3536e-04,  ..., 9.8643e-05, 1.1231e-04,\n",
      "         5.9705e-05],\n",
      "        [1.0472e-04, 2.8588e-04, 7.8103e-05,  ..., 3.1115e-05, 2.8627e-05,\n",
      "         1.8472e-04]], device='cuda:0')\n",
      "pred: tensor([0.8474, 0.8187, 0.7948, 0.8080, 0.7923, 0.8214, 0.8029, 0.8029, 0.7931,\n",
      "        0.8084, 0.7958, 0.8302, 0.8003, 0.8119, 0.8385, 0.8048, 0.8248, 0.8215,\n",
      "        0.7857, 0.7846, 0.8247, 0.8295, 0.8650, 0.8532, 0.8858, 0.8112, 0.8632,\n",
      "        0.8151, 0.8458, 0.8471, 0.8478, 0.8668, 0.8620, 0.8740, 0.8581, 0.8745,\n",
      "        0.8358, 0.8442, 0.8417, 0.8337, 0.8387, 0.8688, 0.8039, 0.8605, 0.8218,\n",
      "        0.7922, 0.8401, 0.8226, 0.8261, 0.8381, 0.7674, 0.7859, 0.7652, 0.7750,\n",
      "        0.7729, 0.7794, 0.8326, 0.8299, 0.7632, 0.8395, 0.8008, 0.8040, 0.7928,\n",
      "        0.8082, 0.8358, 0.8456, 0.7975, 0.7794, 0.8182, 0.8298, 0.8304, 0.8044,\n",
      "        0.8296, 0.8062, 0.7995, 0.8577, 0.8601, 0.8505, 0.8628, 0.8611, 0.8248,\n",
      "        0.8300, 0.8552, 0.8423, 0.8568, 0.8062, 0.7864, 0.8412, 0.7866, 0.8381,\n",
      "        0.8329, 0.8426, 0.8066, 0.8328, 0.8704, 0.8499, 0.8075, 0.8524, 0.8611,\n",
      "        0.8332], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8474, 0.8187, 0.7948, 0.8080, 0.7923, 0.8214, 0.8029, 0.8029, 0.7931,\n",
      "        0.8084, 0.7958, 0.8302, 0.8003, 0.8119, 0.8385, 0.8048, 0.8248, 0.8215,\n",
      "        0.7857, 0.7846, 0.8247, 0.8295, 0.8650, 0.8532, 0.8858, 0.8112, 0.8632,\n",
      "        0.8151, 0.8458, 0.8471, 0.8478, 0.8668, 0.8620, 0.8740, 0.8581, 0.8745,\n",
      "        0.8358, 0.8442, 0.8417, 0.8337, 0.8387, 0.8688, 0.8039, 0.8605, 0.8218,\n",
      "        0.7922, 0.8401, 0.8226, 0.8261, 0.8381, 0.7674, 0.7859, 0.7652, 0.7750,\n",
      "        0.7729, 0.7794, 0.8326, 0.8299, 0.7632, 0.8395, 0.8008, 0.8040, 0.7928,\n",
      "        0.8082, 0.8358, 0.8456, 0.7975, 0.7794, 0.8182, 0.8298, 0.8304, 0.8044,\n",
      "        0.8296, 0.8062, 0.7995, 0.8577, 0.8601, 0.8505, 0.8628, 0.8611, 0.8248,\n",
      "        0.8300, 0.8552, 0.8423, 0.8568, 0.8062, 0.7864, 0.8412, 0.7866, 0.8381,\n",
      "        0.8329, 0.8426, 0.8066, 0.8328, 0.8704, 0.8499, 0.8075, 0.8524, 0.8611,\n",
      "        0.8332], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5644, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0647, Val-Loss: 0.1059\n",
      "Epoch [20/700], Train-Loss: 0.0241, Val-Loss: 0.0303\n",
      "Epoch [30/700], Train-Loss: 0.0021, Val-Loss: 0.0047\n",
      "Epoch [40/700], Train-Loss: 0.0005, Val-Loss: 0.0016\n",
      "Epoch [50/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.2724e-05, 2.3033e-05, 6.0433e-05,  ..., 3.5280e-05, 7.1828e-05,\n",
      "         4.5098e-01],\n",
      "        [6.4252e-05, 9.2029e-05, 2.4904e-04,  ..., 2.0631e-04, 2.9225e-04,\n",
      "         6.9225e-03],\n",
      "        [3.4468e-05, 9.5296e-05, 1.5881e-04,  ..., 1.0128e-04, 1.9012e-04,\n",
      "         3.5496e-02],\n",
      "        ...,\n",
      "        [6.7547e-05, 8.1614e-05, 5.5862e-04,  ..., 5.8921e-05, 4.8907e-04,\n",
      "         1.4117e-01],\n",
      "        [1.0265e-04, 7.8667e-05, 3.8452e-05,  ..., 1.3171e-04, 3.5665e-05,\n",
      "         4.6062e-01],\n",
      "        [3.4749e-05, 8.9194e-05, 8.3200e-05,  ..., 8.0648e-05, 1.8373e-04,\n",
      "         6.9654e-02]], device='cuda:0')\n",
      "pred: tensor([0.8459, 0.8166, 0.7938, 0.8071, 0.7904, 0.8200, 0.8012, 0.8057, 0.7917,\n",
      "        0.8096, 0.7953, 0.8265, 0.8003, 0.8102, 0.8369, 0.8049, 0.8259, 0.8214,\n",
      "        0.7857, 0.7829, 0.8232, 0.8313, 0.8635, 0.8522, 0.8822, 0.8091, 0.8639,\n",
      "        0.8145, 0.8448, 0.8456, 0.8483, 0.8684, 0.8613, 0.8746, 0.8579, 0.8732,\n",
      "        0.8330, 0.8456, 0.8408, 0.8335, 0.8378, 0.8676, 0.8033, 0.8598, 0.8206,\n",
      "        0.7920, 0.8410, 0.8243, 0.8262, 0.8359, 0.7672, 0.7865, 0.7643, 0.7742,\n",
      "        0.7736, 0.7815, 0.8332, 0.8298, 0.7636, 0.8401, 0.8011, 0.8033, 0.7913,\n",
      "        0.8063, 0.8368, 0.8446, 0.7970, 0.7789, 0.8178, 0.8294, 0.8301, 0.8039,\n",
      "        0.8299, 0.8065, 0.8003, 0.8563, 0.8574, 0.8497, 0.8645, 0.8579, 0.8251,\n",
      "        0.8309, 0.8555, 0.8411, 0.8559, 0.8072, 0.7865, 0.8417, 0.7861, 0.8363,\n",
      "        0.8311, 0.8393, 0.8057, 0.8307, 0.8706, 0.8507, 0.8074, 0.8535, 0.8625,\n",
      "        0.8317], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8459, 0.8166, 0.7938, 0.8071, 0.7904, 0.8200, 0.8012, 0.8057, 0.7917,\n",
      "        0.8096, 0.7953, 0.8265, 0.8003, 0.8102, 0.8369, 0.8049, 0.8259, 0.8214,\n",
      "        0.7857, 0.7829, 0.8232, 0.8313, 0.8635, 0.8522, 0.8822, 0.8091, 0.8639,\n",
      "        0.8145, 0.8448, 0.8456, 0.8483, 0.8684, 0.8613, 0.8746, 0.8579, 0.8732,\n",
      "        0.8330, 0.8456, 0.8408, 0.8335, 0.8378, 0.8676, 0.8033, 0.8598, 0.8206,\n",
      "        0.7920, 0.8410, 0.8243, 0.8262, 0.8359, 0.7672, 0.7865, 0.7643, 0.7742,\n",
      "        0.7736, 0.7815, 0.8332, 0.8298, 0.7636, 0.8401, 0.8011, 0.8033, 0.7913,\n",
      "        0.8063, 0.8368, 0.8446, 0.7970, 0.7789, 0.8178, 0.8294, 0.8301, 0.8039,\n",
      "        0.8299, 0.8065, 0.8003, 0.8563, 0.8574, 0.8497, 0.8645, 0.8579, 0.8251,\n",
      "        0.8309, 0.8555, 0.8411, 0.8559, 0.8072, 0.7865, 0.8417, 0.7861, 0.8363,\n",
      "        0.8311, 0.8393, 0.8057, 0.8307, 0.8706, 0.8507, 0.8074, 0.8535, 0.8625,\n",
      "        0.8317], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5640, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1080, Val-Loss: 0.0883\n",
      "Epoch [20/700], Train-Loss: 0.0304, Val-Loss: 0.0268\n",
      "Epoch [30/700], Train-Loss: 0.0107, Val-Loss: 0.0091\n",
      "Epoch [40/700], Train-Loss: 0.0040, Val-Loss: 0.0036\n",
      "Epoch [50/700], Train-Loss: 0.0015, Val-Loss: 0.0010\n",
      "Epoch [60/700], Train-Loss: 0.0005, Val-Loss: 0.0003\n",
      "Epoch [70/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[7.9381e-05, 3.9592e-04, 1.8544e-04,  ..., 1.1630e-04, 1.4332e-04,\n",
      "         7.3323e-04],\n",
      "        [5.5164e-05, 5.2636e-05, 1.1772e-04,  ..., 4.9157e-05, 6.2931e-05,\n",
      "         2.0934e-05],\n",
      "        [9.0557e-05, 1.0841e-04, 1.2263e-04,  ..., 3.1909e-04, 1.6432e-04,\n",
      "         1.1032e-03],\n",
      "        ...,\n",
      "        [6.2979e-05, 1.3960e-05, 1.6092e-04,  ..., 2.0023e-04, 5.2692e-05,\n",
      "         2.2396e-05],\n",
      "        [4.3068e-06, 1.3777e-05, 4.2615e-05,  ..., 2.5601e-04, 3.6719e-05,\n",
      "         5.0465e-04],\n",
      "        [1.6846e-06, 3.3026e-06, 2.6755e-05,  ..., 5.9509e-05, 5.4385e-05,\n",
      "         1.0338e-03]], device='cuda:0')\n",
      "pred: tensor([0.8445, 0.8182, 0.7939, 0.8078, 0.7918, 0.8224, 0.8003, 0.8057, 0.7921,\n",
      "        0.8102, 0.7949, 0.8278, 0.7995, 0.8120, 0.8386, 0.8039, 0.8256, 0.8219,\n",
      "        0.7865, 0.7834, 0.8245, 0.8301, 0.8636, 0.8522, 0.8858, 0.8101, 0.8631,\n",
      "        0.8154, 0.8460, 0.8479, 0.8485, 0.8688, 0.8636, 0.8759, 0.8596, 0.8760,\n",
      "        0.8349, 0.8435, 0.8404, 0.8358, 0.8390, 0.8703, 0.8038, 0.8598, 0.8213,\n",
      "        0.7924, 0.8410, 0.8242, 0.8263, 0.8378, 0.7671, 0.7865, 0.7645, 0.7756,\n",
      "        0.7739, 0.7801, 0.8338, 0.8294, 0.7630, 0.8393, 0.8019, 0.8021, 0.7927,\n",
      "        0.8062, 0.8355, 0.8443, 0.7981, 0.7788, 0.8186, 0.8283, 0.8296, 0.8054,\n",
      "        0.8312, 0.8062, 0.8016, 0.8561, 0.8572, 0.8499, 0.8640, 0.8612, 0.8248,\n",
      "        0.8304, 0.8552, 0.8413, 0.8571, 0.8073, 0.7869, 0.8406, 0.7858, 0.8385,\n",
      "        0.8331, 0.8412, 0.8051, 0.8332, 0.8728, 0.8514, 0.8092, 0.8528, 0.8620,\n",
      "        0.8312], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8445, 0.8182, 0.7939, 0.8078, 0.7918, 0.8224, 0.8003, 0.8057, 0.7921,\n",
      "        0.8102, 0.7949, 0.8278, 0.7995, 0.8120, 0.8386, 0.8039, 0.8256, 0.8219,\n",
      "        0.7865, 0.7834, 0.8245, 0.8301, 0.8636, 0.8522, 0.8858, 0.8101, 0.8631,\n",
      "        0.8154, 0.8460, 0.8479, 0.8485, 0.8688, 0.8636, 0.8759, 0.8596, 0.8760,\n",
      "        0.8349, 0.8435, 0.8404, 0.8358, 0.8390, 0.8703, 0.8038, 0.8598, 0.8213,\n",
      "        0.7924, 0.8410, 0.8242, 0.8263, 0.8378, 0.7671, 0.7865, 0.7645, 0.7756,\n",
      "        0.7739, 0.7801, 0.8338, 0.8294, 0.7630, 0.8393, 0.8019, 0.8021, 0.7927,\n",
      "        0.8062, 0.8355, 0.8443, 0.7981, 0.7788, 0.8186, 0.8283, 0.8296, 0.8054,\n",
      "        0.8312, 0.8062, 0.8016, 0.8561, 0.8572, 0.8499, 0.8640, 0.8612, 0.8248,\n",
      "        0.8304, 0.8552, 0.8413, 0.8571, 0.8073, 0.7869, 0.8406, 0.7858, 0.8385,\n",
      "        0.8331, 0.8412, 0.8051, 0.8332, 0.8728, 0.8514, 0.8092, 0.8528, 0.8620,\n",
      "        0.8312], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5584, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1005, Val-Loss: 0.0690\n",
      "Epoch [20/700], Train-Loss: 0.0278, Val-Loss: 0.0230\n",
      "Epoch [30/700], Train-Loss: 0.0100, Val-Loss: 0.0076\n",
      "Epoch [40/700], Train-Loss: 0.0037, Val-Loss: 0.0029\n",
      "Epoch [50/700], Train-Loss: 0.0012, Val-Loss: 0.0007\n",
      "Epoch [60/700], Train-Loss: 0.0003, Val-Loss: 0.0002\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[2.1126e-04, 2.3543e-04, 3.3219e-04,  ..., 1.8982e-04, 7.3841e-05,\n",
      "         1.3083e-04],\n",
      "        [2.2847e-04, 1.2592e-04, 1.0226e-04,  ..., 4.5387e-04, 6.7586e-04,\n",
      "         7.2939e-04],\n",
      "        [2.4583e-04, 1.5934e-04, 5.0526e-04,  ..., 3.1502e-04, 1.0631e-03,\n",
      "         3.6644e-04],\n",
      "        ...,\n",
      "        [7.3208e-05, 9.9591e-05, 2.7783e-04,  ..., 1.9772e-04, 2.4072e-04,\n",
      "         3.9183e-04],\n",
      "        [1.6717e-04, 1.1686e-04, 3.7559e-04,  ..., 3.4150e-05, 7.2246e-04,\n",
      "         2.4337e-03],\n",
      "        [8.0087e-05, 5.5845e-05, 2.2777e-04,  ..., 1.4019e-04, 1.4137e-04,\n",
      "         1.9116e-04]], device='cuda:0')\n",
      "pred: tensor([0.8462, 0.8193, 0.7948, 0.8081, 0.7902, 0.8230, 0.8005, 0.8069, 0.7911,\n",
      "        0.8099, 0.7952, 0.8277, 0.8007, 0.8118, 0.8371, 0.8047, 0.8249, 0.8219,\n",
      "        0.7848, 0.7827, 0.8248, 0.8310, 0.8626, 0.8528, 0.8839, 0.8109, 0.8637,\n",
      "        0.8167, 0.8453, 0.8454, 0.8493, 0.8708, 0.8634, 0.8758, 0.8581, 0.8731,\n",
      "        0.8337, 0.8454, 0.8405, 0.8356, 0.8390, 0.8685, 0.8034, 0.8594, 0.8215,\n",
      "        0.7903, 0.8425, 0.8242, 0.8260, 0.8385, 0.7671, 0.7875, 0.7663, 0.7751,\n",
      "        0.7729, 0.7799, 0.8312, 0.8291, 0.7637, 0.8402, 0.8034, 0.8032, 0.7923,\n",
      "        0.8070, 0.8359, 0.8446, 0.7962, 0.7798, 0.8180, 0.8293, 0.8321, 0.8057,\n",
      "        0.8311, 0.8063, 0.7996, 0.8578, 0.8572, 0.8492, 0.8649, 0.8597, 0.8254,\n",
      "        0.8300, 0.8541, 0.8412, 0.8565, 0.8065, 0.7869, 0.8411, 0.7841, 0.8371,\n",
      "        0.8319, 0.8405, 0.8059, 0.8327, 0.8706, 0.8510, 0.8099, 0.8520, 0.8620,\n",
      "        0.8325], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8462, 0.8193, 0.7948, 0.8081, 0.7902, 0.8230, 0.8005, 0.8069, 0.7911,\n",
      "        0.8099, 0.7952, 0.8277, 0.8007, 0.8118, 0.8371, 0.8047, 0.8249, 0.8219,\n",
      "        0.7848, 0.7827, 0.8248, 0.8310, 0.8626, 0.8528, 0.8839, 0.8109, 0.8637,\n",
      "        0.8167, 0.8453, 0.8454, 0.8493, 0.8708, 0.8634, 0.8758, 0.8581, 0.8731,\n",
      "        0.8337, 0.8454, 0.8405, 0.8356, 0.8390, 0.8685, 0.8034, 0.8594, 0.8215,\n",
      "        0.7903, 0.8425, 0.8242, 0.8260, 0.8385, 0.7671, 0.7875, 0.7663, 0.7751,\n",
      "        0.7729, 0.7799, 0.8312, 0.8291, 0.7637, 0.8402, 0.8034, 0.8032, 0.7923,\n",
      "        0.8070, 0.8359, 0.8446, 0.7962, 0.7798, 0.8180, 0.8293, 0.8321, 0.8057,\n",
      "        0.8311, 0.8063, 0.7996, 0.8578, 0.8572, 0.8492, 0.8649, 0.8597, 0.8254,\n",
      "        0.8300, 0.8541, 0.8412, 0.8565, 0.8065, 0.7869, 0.8411, 0.7841, 0.8371,\n",
      "        0.8319, 0.8405, 0.8059, 0.8327, 0.8706, 0.8510, 0.8099, 0.8520, 0.8620,\n",
      "        0.8325], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5704, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.0950, Val-Loss: 0.0514\n",
      "Epoch [20/700], Train-Loss: 0.0236, Val-Loss: 0.0165\n",
      "Epoch [30/700], Train-Loss: 0.0080, Val-Loss: 0.0044\n",
      "Epoch [40/700], Train-Loss: 0.0026, Val-Loss: 0.0015\n",
      "Epoch [50/700], Train-Loss: 0.0006, Val-Loss: 0.0002\n",
      "Epoch [60/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.7623e-04, 6.0355e-05, 3.4524e-05,  ..., 5.9760e-05, 3.7086e-04,\n",
      "         2.9056e-04],\n",
      "        [2.3378e-04, 1.0104e-04, 5.7230e-05,  ..., 6.2056e-05, 9.0798e-04,\n",
      "         5.6694e-04],\n",
      "        [6.3996e-05, 1.7485e-04, 1.0499e-04,  ..., 2.6780e-04, 6.0548e-04,\n",
      "         7.6777e-04],\n",
      "        ...,\n",
      "        [1.7077e-04, 1.3929e-04, 1.9961e-04,  ..., 9.0695e-05, 4.4025e-04,\n",
      "         7.6188e-04],\n",
      "        [9.4850e-05, 1.6962e-05, 2.1100e-04,  ..., 1.2951e-04, 5.7147e-05,\n",
      "         1.2665e-04],\n",
      "        [8.9473e-06, 2.0655e-05, 1.0020e-05,  ..., 2.3022e-04, 2.7550e-04,\n",
      "         1.6144e-04]], device='cuda:0')\n",
      "pred: tensor([0.8441, 0.8160, 0.7941, 0.8084, 0.7914, 0.8220, 0.8004, 0.8066, 0.7921,\n",
      "        0.8105, 0.7962, 0.8272, 0.7995, 0.8125, 0.8394, 0.8043, 0.8250, 0.8212,\n",
      "        0.7856, 0.7829, 0.8250, 0.8306, 0.8637, 0.8531, 0.8865, 0.8101, 0.8624,\n",
      "        0.8170, 0.8453, 0.8463, 0.8483, 0.8688, 0.8627, 0.8736, 0.8579, 0.8753,\n",
      "        0.8355, 0.8428, 0.8402, 0.8334, 0.8390, 0.8673, 0.8037, 0.8607, 0.8227,\n",
      "        0.7921, 0.8413, 0.8250, 0.8258, 0.8356, 0.7672, 0.7877, 0.7651, 0.7747,\n",
      "        0.7726, 0.7804, 0.8320, 0.8283, 0.7634, 0.8402, 0.8016, 0.8027, 0.7929,\n",
      "        0.8075, 0.8373, 0.8447, 0.7979, 0.7790, 0.8176, 0.8314, 0.8308, 0.8037,\n",
      "        0.8303, 0.8053, 0.8002, 0.8569, 0.8561, 0.8495, 0.8623, 0.8626, 0.8255,\n",
      "        0.8307, 0.8554, 0.8419, 0.8568, 0.8082, 0.7883, 0.8412, 0.7837, 0.8383,\n",
      "        0.8331, 0.8421, 0.8068, 0.8321, 0.8703, 0.8505, 0.8095, 0.8514, 0.8611,\n",
      "        0.8312], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8441, 0.8160, 0.7941, 0.8084, 0.7914, 0.8220, 0.8004, 0.8066, 0.7921,\n",
      "        0.8105, 0.7962, 0.8272, 0.7995, 0.8125, 0.8394, 0.8043, 0.8250, 0.8212,\n",
      "        0.7856, 0.7829, 0.8250, 0.8306, 0.8637, 0.8531, 0.8865, 0.8101, 0.8624,\n",
      "        0.8170, 0.8453, 0.8463, 0.8483, 0.8688, 0.8627, 0.8736, 0.8579, 0.8753,\n",
      "        0.8355, 0.8428, 0.8402, 0.8334, 0.8390, 0.8673, 0.8037, 0.8607, 0.8227,\n",
      "        0.7921, 0.8413, 0.8250, 0.8258, 0.8356, 0.7672, 0.7877, 0.7651, 0.7747,\n",
      "        0.7726, 0.7804, 0.8320, 0.8283, 0.7634, 0.8402, 0.8016, 0.8027, 0.7929,\n",
      "        0.8075, 0.8373, 0.8447, 0.7979, 0.7790, 0.8176, 0.8314, 0.8308, 0.8037,\n",
      "        0.8303, 0.8053, 0.8002, 0.8569, 0.8561, 0.8495, 0.8623, 0.8626, 0.8255,\n",
      "        0.8307, 0.8554, 0.8419, 0.8568, 0.8082, 0.7883, 0.8412, 0.7837, 0.8383,\n",
      "        0.8331, 0.8421, 0.8068, 0.8321, 0.8703, 0.8505, 0.8095, 0.8514, 0.8611,\n",
      "        0.8312], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5627, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1154, Val-Loss: 0.0714\n",
      "Epoch [20/700], Train-Loss: 0.0313, Val-Loss: 0.0241\n",
      "Epoch [30/700], Train-Loss: 0.0108, Val-Loss: 0.0070\n",
      "Epoch [40/700], Train-Loss: 0.0038, Val-Loss: 0.0026\n",
      "Epoch [50/700], Train-Loss: 0.0011, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.0425e-04, 5.7449e-05, 1.6070e-04,  ..., 1.1870e-04, 1.6792e-04,\n",
      "         3.0551e-04],\n",
      "        [3.5096e-05, 9.3552e-05, 1.4070e-04,  ..., 1.6505e-04, 2.3641e-04,\n",
      "         4.3064e-04],\n",
      "        [1.5840e-04, 1.3406e-03, 3.4310e-04,  ..., 9.4179e-05, 1.8273e-04,\n",
      "         1.8060e-04],\n",
      "        ...,\n",
      "        [1.9542e-04, 3.7277e-05, 1.1119e-04,  ..., 5.2043e-05, 1.9323e-04,\n",
      "         1.8366e-04],\n",
      "        [5.8807e-05, 1.8564e-04, 1.8325e-04,  ..., 4.9969e-05, 2.1021e-04,\n",
      "         1.7964e-04],\n",
      "        [2.2039e-04, 9.0683e-04, 4.3592e-05,  ..., 3.4308e-05, 5.5663e-05,\n",
      "         4.7972e-05]], device='cuda:0')\n",
      "pred: tensor([0.8457, 0.8195, 0.7935, 0.8082, 0.7901, 0.8226, 0.8018, 0.8064, 0.7917,\n",
      "        0.8105, 0.7945, 0.8270, 0.8006, 0.8124, 0.8360, 0.8058, 0.8253, 0.8224,\n",
      "        0.7853, 0.7831, 0.8249, 0.8300, 0.8647, 0.8511, 0.8845, 0.8102, 0.8644,\n",
      "        0.8141, 0.8454, 0.8461, 0.8480, 0.8675, 0.8630, 0.8751, 0.8567, 0.8736,\n",
      "        0.8357, 0.8438, 0.8418, 0.8375, 0.8369, 0.8668, 0.8043, 0.8606, 0.8200,\n",
      "        0.7913, 0.8405, 0.8227, 0.8259, 0.8353, 0.7681, 0.7871, 0.7641, 0.7758,\n",
      "        0.7731, 0.7803, 0.8323, 0.8287, 0.7631, 0.8390, 0.8011, 0.8033, 0.7910,\n",
      "        0.8065, 0.8354, 0.8443, 0.7974, 0.7791, 0.8201, 0.8295, 0.8299, 0.8038,\n",
      "        0.8307, 0.8051, 0.7997, 0.8549, 0.8569, 0.8495, 0.8643, 0.8621, 0.8253,\n",
      "        0.8315, 0.8554, 0.8410, 0.8570, 0.8074, 0.7876, 0.8417, 0.7867, 0.8378,\n",
      "        0.8336, 0.8399, 0.8058, 0.8312, 0.8712, 0.8495, 0.8095, 0.8514, 0.8630,\n",
      "        0.8326], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8457, 0.8195, 0.7935, 0.8082, 0.7901, 0.8226, 0.8018, 0.8064, 0.7917,\n",
      "        0.8105, 0.7945, 0.8270, 0.8006, 0.8124, 0.8360, 0.8058, 0.8253, 0.8224,\n",
      "        0.7853, 0.7831, 0.8249, 0.8300, 0.8647, 0.8511, 0.8845, 0.8102, 0.8644,\n",
      "        0.8141, 0.8454, 0.8461, 0.8480, 0.8675, 0.8630, 0.8751, 0.8567, 0.8736,\n",
      "        0.8357, 0.8438, 0.8418, 0.8375, 0.8369, 0.8668, 0.8043, 0.8606, 0.8200,\n",
      "        0.7913, 0.8405, 0.8227, 0.8259, 0.8353, 0.7681, 0.7871, 0.7641, 0.7758,\n",
      "        0.7731, 0.7803, 0.8323, 0.8287, 0.7631, 0.8390, 0.8011, 0.8033, 0.7910,\n",
      "        0.8065, 0.8354, 0.8443, 0.7974, 0.7791, 0.8201, 0.8295, 0.8299, 0.8038,\n",
      "        0.8307, 0.8051, 0.7997, 0.8549, 0.8569, 0.8495, 0.8643, 0.8621, 0.8253,\n",
      "        0.8315, 0.8554, 0.8410, 0.8570, 0.8074, 0.7876, 0.8417, 0.7867, 0.8378,\n",
      "        0.8336, 0.8399, 0.8058, 0.8312, 0.8712, 0.8495, 0.8095, 0.8514, 0.8630,\n",
      "        0.8326], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5648, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1100, Val-Loss: 0.0653\n",
      "Epoch [20/700], Train-Loss: 0.0299, Val-Loss: 0.0226\n",
      "Epoch [30/700], Train-Loss: 0.0100, Val-Loss: 0.0066\n",
      "Epoch [40/700], Train-Loss: 0.0036, Val-Loss: 0.0025\n",
      "Epoch [50/700], Train-Loss: 0.0011, Val-Loss: 0.0005\n",
      "Epoch [60/700], Train-Loss: 0.0002, Val-Loss: 0.0001\n",
      "Epoch [70/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [80/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[1.1038e-03, 2.3793e-04, 9.4434e-03,  ..., 7.1377e-05, 2.5004e-04,\n",
      "         3.0586e-04],\n",
      "        [1.4492e-03, 8.4505e-04, 2.9392e-03,  ..., 8.5542e-05, 9.7957e-05,\n",
      "         1.6502e-04],\n",
      "        [6.2718e-04, 6.2656e-04, 9.7615e-03,  ..., 2.2340e-04, 3.4721e-04,\n",
      "         8.5560e-05],\n",
      "        ...,\n",
      "        [6.1268e-04, 2.7982e-04, 6.9316e-04,  ..., 1.9882e-04, 7.5385e-04,\n",
      "         2.2112e-04],\n",
      "        [2.2697e-04, 1.9699e-04, 1.6021e-03,  ..., 2.9628e-04, 2.0258e-04,\n",
      "         1.6864e-04],\n",
      "        [6.5144e-04, 3.8737e-04, 3.9046e-04,  ..., 9.7604e-05, 6.2761e-05,\n",
      "         3.5385e-04]], device='cuda:0')\n",
      "pred: tensor([0.8439, 0.8179, 0.7954, 0.8091, 0.7924, 0.8233, 0.8020, 0.8068, 0.7925,\n",
      "        0.8086, 0.7954, 0.8265, 0.8008, 0.8119, 0.8362, 0.8049, 0.8241, 0.8209,\n",
      "        0.7871, 0.7817, 0.8242, 0.8305, 0.8668, 0.8523, 0.8855, 0.8099, 0.8655,\n",
      "        0.8159, 0.8453, 0.8479, 0.8494, 0.8702, 0.8648, 0.8762, 0.8589, 0.8763,\n",
      "        0.8341, 0.8447, 0.8426, 0.8324, 0.8383, 0.8666, 0.8040, 0.8596, 0.8210,\n",
      "        0.7930, 0.8406, 0.8244, 0.8260, 0.8363, 0.7660, 0.7867, 0.7653, 0.7757,\n",
      "        0.7748, 0.7803, 0.8329, 0.8290, 0.7624, 0.8398, 0.8016, 0.8027, 0.7924,\n",
      "        0.8073, 0.8340, 0.8437, 0.7972, 0.7791, 0.8189, 0.8289, 0.8296, 0.8048,\n",
      "        0.8299, 0.8063, 0.8001, 0.8582, 0.8588, 0.8488, 0.8644, 0.8600, 0.8267,\n",
      "        0.8299, 0.8552, 0.8416, 0.8575, 0.8080, 0.7868, 0.8396, 0.7820, 0.8374,\n",
      "        0.8323, 0.8406, 0.8055, 0.8311, 0.8716, 0.8503, 0.8082, 0.8512, 0.8626,\n",
      "        0.8328], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8439, 0.8179, 0.7954, 0.8091, 0.7924, 0.8233, 0.8020, 0.8068, 0.7925,\n",
      "        0.8086, 0.7954, 0.8265, 0.8008, 0.8119, 0.8362, 0.8049, 0.8241, 0.8209,\n",
      "        0.7871, 0.7817, 0.8242, 0.8305, 0.8668, 0.8523, 0.8855, 0.8099, 0.8655,\n",
      "        0.8159, 0.8453, 0.8479, 0.8494, 0.8702, 0.8648, 0.8762, 0.8589, 0.8763,\n",
      "        0.8341, 0.8447, 0.8426, 0.8324, 0.8383, 0.8666, 0.8040, 0.8596, 0.8210,\n",
      "        0.7930, 0.8406, 0.8244, 0.8260, 0.8363, 0.7660, 0.7867, 0.7653, 0.7757,\n",
      "        0.7748, 0.7803, 0.8329, 0.8290, 0.7624, 0.8398, 0.8016, 0.8027, 0.7924,\n",
      "        0.8073, 0.8340, 0.8437, 0.7972, 0.7791, 0.8189, 0.8289, 0.8296, 0.8048,\n",
      "        0.8299, 0.8063, 0.8001, 0.8582, 0.8588, 0.8488, 0.8644, 0.8600, 0.8267,\n",
      "        0.8299, 0.8552, 0.8416, 0.8575, 0.8080, 0.7868, 0.8396, 0.7820, 0.8374,\n",
      "        0.8323, 0.8406, 0.8055, 0.8311, 0.8716, 0.8503, 0.8082, 0.8512, 0.8626,\n",
      "        0.8328], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5632, device='cuda:0')\n",
      "perfomances_train tensor([0.8786, 0.8790, 0.8816, 0.8580, 0.8541, 0.8787, 0.8664, 0.8195, 0.8596,\n",
      "        0.8549, 0.8621, 0.8703, 0.8629, 0.8555, 0.8653, 0.8800, 0.8660, 0.8824,\n",
      "        0.8558, 0.8493, 0.8510, 0.8624, 0.8838, 0.8008, 0.7709, 0.7929, 0.7730,\n",
      "        0.7549, 0.8648, 0.8155, 0.8589, 0.7868, 0.7932, 0.7688, 0.7809, 0.8165,\n",
      "        0.7565, 0.8383, 0.8536, 0.8325, 0.8575, 0.8131, 0.8241, 0.8493, 0.7667,\n",
      "        0.7511, 0.8024, 0.8777, 0.8746, 0.7732, 0.7505, 0.8610, 0.7983, 0.7926,\n",
      "        0.7696, 0.7727, 0.8685, 0.7878, 0.7661, 0.8087, 0.7791, 0.7551, 0.7579,\n",
      "        0.8113, 0.8453, 0.8564, 0.8179, 0.8074, 0.8511, 0.8421, 0.8024, 0.8222,\n",
      "        0.7545, 0.7884, 0.8259, 0.8141, 0.8860, 0.8217, 0.8335, 0.8682, 0.8841,\n",
      "        0.7596, 0.8148, 0.8407, 0.8524, 0.8231, 0.8276, 0.8248, 0.8649, 0.7975,\n",
      "        0.8336, 0.8658, 0.8239, 0.8352, 0.8239, 0.8170, 0.8460, 0.8740, 0.8464,\n",
      "        0.8200, 0.8448, 0.8654, 0.8055, 0.8206, 0.8298, 0.8303, 0.8335, 0.8664,\n",
      "        0.8270, 0.7849, 0.7652, 0.8649, 0.8364, 0.8151, 0.8339, 0.8789, 0.8560,\n",
      "        0.8461, 0.8323, 0.8097, 0.8644, 0.8425, 0.8333, 0.8633, 0.8384, 0.8217,\n",
      "        0.8714, 0.8747, 0.8213, 0.8448, 0.8383, 0.8031, 0.7785, 0.8080, 0.7849,\n",
      "        0.8093, 0.8317, 0.7559, 0.8392, 0.8070, 0.8333, 0.7890, 0.7973, 0.8265,\n",
      "        0.8038, 0.8259, 0.7988, 0.8463, 0.8381, 0.8026, 0.7866, 0.7922, 0.7526,\n",
      "        0.8258, 0.7729, 0.8025, 0.7828, 0.7760, 0.8052, 0.8443, 0.8388, 0.8457,\n",
      "        0.8699, 0.8379, 0.8077, 0.8439, 0.7986, 0.7964, 0.8137, 0.7666, 0.8573,\n",
      "        0.8484, 0.8359, 0.8478, 0.8308, 0.8309, 0.8312, 0.8339, 0.7805, 0.8868,\n",
      "        0.8639, 0.8963, 0.8827, 0.8715, 0.8955, 0.9005, 0.7764, 0.7931, 0.8094,\n",
      "        0.7758, 0.8105, 0.8510, 0.7802, 0.8048, 0.7991, 0.7763, 0.8089, 0.8298,\n",
      "        0.8034, 0.8032, 0.7954, 0.8416, 0.8428, 0.7925, 0.8335, 0.8011, 0.8047,\n",
      "        0.8444, 0.8526, 0.8510, 0.8189, 0.8439, 0.8606, 0.8289, 0.8488, 0.8520,\n",
      "        0.8385, 0.8378, 0.8651, 0.8258, 0.8121, 0.8507, 0.8335, 0.8095, 0.8679,\n",
      "        0.8258, 0.7974, 0.8474, 0.8603, 0.8543, 0.8508, 0.7782, 0.8348, 0.8504,\n",
      "        0.8377, 0.8406, 0.8429, 0.8198, 0.8587, 0.7858, 0.7913, 0.8003, 0.8206,\n",
      "        0.8082, 0.8228, 0.7593, 0.7655, 0.7735, 0.7991, 0.8198, 0.7728, 0.8312,\n",
      "        0.7821, 0.8250, 0.8102, 0.7824, 0.7849, 0.7802, 0.8410, 0.8230, 0.7754,\n",
      "        0.8355, 0.7694, 0.7895, 0.8312, 0.7945, 0.7510, 0.8238, 0.7696, 0.8292,\n",
      "        0.7892, 0.7848, 0.8229, 0.8027, 0.8130, 0.8420, 0.8223, 0.8169, 0.7823,\n",
      "        0.8231, 0.7919, 0.7699, 0.7888, 0.8141, 0.8125, 0.8104, 0.8341, 0.8220,\n",
      "        0.7560, 0.8087, 0.8147, 0.8371, 0.7968, 0.8232, 0.7799, 0.7747, 0.8293,\n",
      "        0.8209, 0.7880, 0.8312], device='cuda:0')\n",
      "perfomances_val tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "Epoch [10/700], Train-Loss: 0.1008, Val-Loss: 0.1057\n",
      "Epoch [20/700], Train-Loss: 0.0304, Val-Loss: 0.0316\n",
      "Epoch [30/700], Train-Loss: 0.0084, Val-Loss: 0.0100\n",
      "Epoch [40/700], Train-Loss: 0.0032, Val-Loss: 0.0040\n",
      "Epoch [50/700], Train-Loss: 0.0013, Val-Loss: 0.0014\n",
      "Epoch [60/700], Train-Loss: 0.0006, Val-Loss: 0.0006\n",
      "Epoch [70/700], Train-Loss: 0.0002, Val-Loss: 0.0002\n",
      "Epoch [80/700], Train-Loss: 0.0001, Val-Loss: 0.0001\n",
      "Epoch [90/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [100/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [110/700], Train-Loss: 0.0000, Val-Loss: 0.0001\n",
      "Epoch [120/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [130/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [140/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [150/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [160/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [170/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [180/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [190/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [200/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [210/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [220/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [230/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [240/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [250/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [260/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [270/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [280/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [290/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [300/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [310/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [320/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [330/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [340/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [350/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [360/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [370/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [380/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [390/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [400/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [410/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [420/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [430/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [440/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [450/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [460/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [470/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [480/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [490/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [500/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [510/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [520/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [530/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [540/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [550/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [560/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [570/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [580/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [590/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [600/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [610/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [620/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [630/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [640/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [650/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [660/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [670/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [680/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [690/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "Epoch [700/700], Train-Loss: 0.0000, Val-Loss: 0.0000\n",
      "x: tensor([[3.9969e-05, 1.7899e-04, 5.3469e-04,  ..., 2.0658e-02, 3.0495e-04,\n",
      "         1.2725e-04],\n",
      "        [1.7437e-04, 3.9155e-04, 3.6527e-04,  ..., 4.4410e-04, 6.2795e-05,\n",
      "         1.9495e-04],\n",
      "        [6.2475e-05, 4.6785e-04, 5.9729e-04,  ..., 5.5392e-03, 1.5692e-04,\n",
      "         2.2377e-04],\n",
      "        ...,\n",
      "        [3.2494e-05, 2.3972e-04, 9.4807e-04,  ..., 1.5260e-02, 7.0103e-05,\n",
      "         8.5667e-05],\n",
      "        [2.5078e-05, 1.0855e-04, 2.7537e-04,  ..., 6.9613e-02, 1.6322e-04,\n",
      "         6.0601e-05],\n",
      "        [4.2359e-06, 1.0026e-04, 1.1907e-04,  ..., 3.1224e-02, 1.2706e-04,\n",
      "         1.7032e-04]], device='cuda:0')\n",
      "pred: tensor([0.8452, 0.8171, 0.7937, 0.8080, 0.7898, 0.8201, 0.8003, 0.8056, 0.7915,\n",
      "        0.8087, 0.7946, 0.8286, 0.7992, 0.8116, 0.8390, 0.8043, 0.8250, 0.8210,\n",
      "        0.7855, 0.7837, 0.8244, 0.8310, 0.8625, 0.8529, 0.8841, 0.8097, 0.8642,\n",
      "        0.8153, 0.8456, 0.8477, 0.8486, 0.8709, 0.8618, 0.8766, 0.8585, 0.8743,\n",
      "        0.8353, 0.8449, 0.8417, 0.8321, 0.8388, 0.8681, 0.8038, 0.8602, 0.8216,\n",
      "        0.7905, 0.8395, 0.8236, 0.8271, 0.8351, 0.7672, 0.7869, 0.7655, 0.7757,\n",
      "        0.7740, 0.7798, 0.8326, 0.8301, 0.7636, 0.8412, 0.8006, 0.8038, 0.7928,\n",
      "        0.8069, 0.8361, 0.8448, 0.7972, 0.7797, 0.8186, 0.8286, 0.8318, 0.8057,\n",
      "        0.8310, 0.8069, 0.8004, 0.8561, 0.8595, 0.8515, 0.8653, 0.8612, 0.8255,\n",
      "        0.8309, 0.8552, 0.8412, 0.8567, 0.8075, 0.7866, 0.8407, 0.7872, 0.8370,\n",
      "        0.8326, 0.8413, 0.8050, 0.8316, 0.8702, 0.8502, 0.8087, 0.8533, 0.8637,\n",
      "        0.8322], device='cuda:0')\n",
      "performances_val_pred.shape: torch.Size([100])\n",
      "performances_val_pred: tensor([0.8452, 0.8171, 0.7937, 0.8080, 0.7898, 0.8201, 0.8003, 0.8056, 0.7915,\n",
      "        0.8087, 0.7946, 0.8286, 0.7992, 0.8116, 0.8390, 0.8043, 0.8250, 0.8210,\n",
      "        0.7855, 0.7837, 0.8244, 0.8310, 0.8625, 0.8529, 0.8841, 0.8097, 0.8642,\n",
      "        0.8153, 0.8456, 0.8477, 0.8486, 0.8709, 0.8618, 0.8766, 0.8585, 0.8743,\n",
      "        0.8353, 0.8449, 0.8417, 0.8321, 0.8388, 0.8681, 0.8038, 0.8602, 0.8216,\n",
      "        0.7905, 0.8395, 0.8236, 0.8271, 0.8351, 0.7672, 0.7869, 0.7655, 0.7757,\n",
      "        0.7740, 0.7798, 0.8326, 0.8301, 0.7636, 0.8412, 0.8006, 0.8038, 0.7928,\n",
      "        0.8069, 0.8361, 0.8448, 0.7972, 0.7797, 0.8186, 0.8286, 0.8318, 0.8057,\n",
      "        0.8310, 0.8069, 0.8004, 0.8561, 0.8595, 0.8515, 0.8653, 0.8612, 0.8255,\n",
      "        0.8309, 0.8552, 0.8412, 0.8567, 0.8075, 0.7866, 0.8407, 0.7872, 0.8370,\n",
      "        0.8326, 0.8413, 0.8050, 0.8316, 0.8702, 0.8502, 0.8087, 0.8533, 0.8637,\n",
      "        0.8322], device='cuda:0')\n",
      "perfomances_val.shape: torch.Size([100])\n",
      "perfomances_val: tensor([0.8377, 0.8283, 0.7927, 0.8113, 0.7821, 0.8208, 0.8097, 0.8105, 0.7895,\n",
      "        0.8119, 0.7926, 0.8284, 0.7952, 0.8184, 0.8341, 0.8088, 0.8370, 0.8317,\n",
      "        0.7856, 0.7961, 0.8261, 0.8367, 0.8661, 0.8616, 0.8807, 0.8075, 0.8687,\n",
      "        0.8143, 0.8524, 0.8538, 0.8550, 0.8674, 0.8686, 0.8698, 0.8640, 0.8744,\n",
      "        0.8297, 0.8449, 0.8376, 0.8210, 0.8407, 0.8741, 0.8073, 0.8731, 0.8271,\n",
      "        0.7974, 0.8416, 0.8216, 0.8308, 0.8344, 0.7518, 0.7879, 0.7507, 0.7676,\n",
      "        0.7784, 0.7747, 0.8336, 0.8271, 0.7577, 0.8461, 0.7945, 0.7992, 0.7800,\n",
      "        0.8084, 0.8358, 0.8520, 0.8047, 0.7854, 0.8086, 0.8327, 0.8374, 0.8083,\n",
      "        0.8352, 0.8130, 0.8045, 0.8645, 0.8561, 0.8445, 0.8623, 0.8595, 0.8350,\n",
      "        0.8404, 0.8636, 0.8486, 0.8654, 0.8197, 0.7710, 0.8391, 0.7858, 0.8389,\n",
      "        0.8391, 0.8364, 0.8008, 0.8346, 0.8621, 0.8432, 0.8027, 0.8604, 0.8494,\n",
      "        0.8380], device='cuda:0')\n",
      "perfomances_val: tensor(0.5595, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'imagenet1k': [tensor(0.0055),\n",
       "  tensor(0.0054),\n",
       "  tensor(0.0056),\n",
       "  tensor(0.0056),\n",
       "  tensor(0.0056),\n",
       "  tensor(0.0057),\n",
       "  tensor(0.0056),\n",
       "  tensor(0.0056),\n",
       "  tensor(0.0056),\n",
       "  tensor(0.0056)]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
